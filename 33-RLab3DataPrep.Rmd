# R Lab 3 - Preparing the data

## Preparing the data for a regression analysis with `lm()`

In chapter 2 we estimated a simple linear regression by writing a set of simple R commands to perform the necessary calculations. But we can use the `lm()` command, instead. We will use three datasets in this lab. Let's start with the first one, `vehicles`.

```{r, warning=FALSE, message=FALSE}
library(fueleconomy)  #install.packages("fueleconomy")

data(vehicles)

df <- as.data.frame(vehicles)

head(df)
str(df)
```

### Factor variables

If we want to estimate a model, we need to check the structure of the model variables in the data. For example, you can see that some of the variables are "characters", such as *make*. It is always a good practice to transform them into *factor* variables also known as *indicator*, *categorical*, or *dummy* variables. [You can still use "character" variables in `lm()`]{.ul} or use them in descriptive analyses, but we will lose many features. There are many different ways to convert one type of vector to another type, here are two simple ways:

```{r, warning=FALSE, message=FALSE}
#First way:
df <- as.data.frame(vehicles)
for (i in 1:ncol(df)) {
  if(is.character(df[,i])) df[,i] <- as.factor(df[,i])
}

#2nd way:
df <- as.data.frame(vehicles)
colms <- sapply(df, is.character)
df[colms] <- lapply(df[colms], as.factor)
str(df)

#We won't learn the advance use of 'apply' family in this text.
#But this is a helpful line.  apply() works only with matrices.
#sapply() and lapply() are for lists.  But a data frame is also a list
#Therefore df[2] instead of df[, 2] can work very well.
#How about df[1:10, 2] vs df[[2]][1:10].  Same!

```

You can also numeric or integer types for indicator variables. Again the good practice is to convert them to factor variables.

### Dummy Coding

Let's look at drive in our data. In dummy coding, you will always have a contrast matrix with one less column than levels of the original variable. In our example, our categorical variable has 7 levels so we will have contrast matrices with 6 columns and 7 rows.

```{r, warning=FALSE, message=FALSE}
tapply(df$hwy, df$drive, mean) # Mean hwy MPG for each drive type.

#This is also a nice function. Try this:
#This is similar to "egen" in Stata.

tapply(df$hwy, df$drive, function(x) c(mean(x), sd(x)))

```

Dummy coding is a very commonly used coding scheme. It compares each level of the categorical variable to a fixed reference level. For example, we can choose `drive = 1` as the reference group and compare the mean of each level of drive to the reference level of 1. This is the default for disordered factors in R.

```{r, warning=FALSE, message=FALSE}
#assigning the treatment contrasts to drive
contrasts(df$drive)
contrasts(df$drive) <- contr.treatment(7, base=4) #Changing the base
contrasts(df$drive)
```

We can make good tables as well:

```{r, warning=FALSE, message=FALSE}
summary(df$drive)
table(df$fuel, df$drive)
```

### Column (Variable) names

Before any type of data analysis, we need to take care of several things. One of these is that we usually do not use the whole data, but a subset of the data. For example, you may want to remove some observations or keep only some types. And most importantly we need to take care of missing values. We will look at these now.

First, look at the column (variable) names. Do they have generic names (that is, $x_1$, \$x_2\$, etc)? Or do the names have typo problems, or are too long/short? In our `vehicles` data the names seem fine. Let's use another data, `Autompg`.

```{r, warning=FALSE, message=FALSE}
library(tidyverse)

autompg <- read_csv("auto-mpg.csv", show_col_types = FALSE)

#autompg = read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data",
#quote = "\"", comment.char = "", stringsAsFactors = FALSE)

colnames(autompg)
str(autompg)

#I don't like them! How about this:
colnames(autompg) = c("mpg", "cyl", "disp", "hp", "wt", "acc", "year", "origin", "name")
colnames(autompg)
str(autompg)
```

### Data subsetting and missing values

OK, they are fine now. Let's see if they have any missing value. Missing values are defined by `NA` in R. We'll see later `NaN` as well.

```{r, warning=FALSE, message=FALSE}
any(is.na(autompg))
#But we have to be careful.  HP is a character vector.  Why?  Perhaps it contains a character?
which(autompg$hp == "?")

#Pay attention to subset().  This will be a time-saver
subset_hp <- subset(autompg, autompg$hp != "?")
dim(subset_hp)
dim(autompg)
#Those 6 observations are dropped and the new data frame is "subset_hp"

#You can drop columns (variables) as well
autompg_less = subset(autompg, select = c("mpg", "cyl", "disp", "hp", "wt", "acc", "year"))
dim(autompg_less)

#Traditional way to do those subselections
subset_hp <- autompg[autompg$hp != "?",]
dim(subset_hp)
dim(autompg)
#And
autompg_less = autompg[, c(1:7)]
dim(autompg_less)
```

Look at the `help(subset)`: "This is a convenience function intended for use interactively. *For programming it is better to use the standard subsetting functions like []*, and in particular the non-standard evaluation of argument subset can have unanticipated consequences".

## "DUMMY" variable models

We can use our dataset, `df`, which is a cleaned version of vehicles, to try out a "dummy" variable model.

```{r, warning=FALSE, message=FALSE}
#Remember we had this:
tapply(df$hwy, df$drive, mean)
contrasts(df$drive) <- contr.treatment(7, base=1) #Setting the base back to 1
```

Lets try to make a regression on highway fuel economy based upon our drive dummy variable.

```{r, warning=FALSE, message=FALSE}
model_nocons <- lm(hwy ~ drive + 0, data = df) # "0" means no constant in lm()
summary(model_nocons)
```

We can add in an intercept now:

```{r, warning=FALSE, message=FALSE}
model_withcons <- lm(hwy ~ drive, data = df)
summary(model_withcons)
```

You can see that the intercept is the first category, `2WDrive`. The rest of the coefficient are the difference of each drive type from `2wdrive`. Let's change the base to 4:

```{r, warning=FALSE, message=FALSE}
contrasts(df$drive) <- contr.treatment(7, base=4) #Changing the base
model_withcons <- lm(hwy ~ drive, data = df)
summary(model_withcons)
```

### `mtcars` example

Now it's time to estimate a model that defines `mpg` (fuel efficiency, in miles per gallon), as a function of `hp` (horsepower - in foot-pounds per second), and `am` (transmission, Automatic or Manual).

```{r, warning=FALSE, message=FALSE}
#First, let's plot it (play with parameter in plot() to see the difference)
plot(mpg ~ hp, data = mtcars, cex = 2, col="darkgrey")
```

let's start with a simple linear regression where $Y$ is mpg and $x_{1}$ is hp. This is our first and simplest `lm()` application. For the sake of simplicity, let's drop the index $i$ for observations.

$$
Y=\beta_{0}+\beta_{1} x_{1}+\epsilon
$$

```{r, warning=FALSE, message=FALSE}
model1 <- lm(mpg ~ hp, data = mtcars)
model1
plot(mpg ~ hp, data = mtcars, col = am + 1, cex = 2)
abline(model1, lwd = 3, col = "green")
```

As you see the red, manual, observations are mostly above the line, while the black, automatic, observations are mostly below the line. This means not only our model underestimates (overestimates) the fuel efficiency of manual (automatic) transmissions, but also the effect of `hp` on `mpg` will be biased. This is because OLS tries to minimize the MSE for all observation not for manual and automatic transmissions separately. To correct for this, we add a predictor to our model, namely, `am` as $x_{2}$, as follows:

$$
Y=\beta_{0}+\beta_{1} x_{1}+\beta_{2} x_{2}+\epsilon
$$ 
where $x_{1}$ and $Y$ remain the same, but now:

$$
x_{2}=\left\{\begin{array}{ll}{1} & {\text { manual transmission }} \\ {0} & {\text { automatic transmission }}\end{array}\right.
$$

We call $x_{2}$ as a "dummy" variable, which is a numerical variable that is used in a regression analysis to "code" for a binary categorical variable. Note that *am* is already a dummy variable, since it uses the values 0 and 1 to represent automatic and manual transmissions. As we have seen earlier, often, a variable like am would be stored as a character vector. Converting them to *factor* variables will take care of creating dummy variables.

```{r, warning=FALSE, message=FALSE}
model2 <- lm(mpg ~ hp + am, data = mtcars)
model2
```

Note the difference in $\hat{\beta}_{1}$. Since $x_{2}$ can only take values 0 and 1, we can write two different models, one for manual and one for automatic transmissions. For automatic transmissions, that is $x_{2}$ = 0, we have,

$$
Y=\beta_{0}+\beta_{1} x_{1}+\epsilon
$$ 
  
Then for manual transmissions, that is $x_2 = 1$, we have,

$$
Y=\left(\beta_{0}+\beta_{2}\right)+\beta_{1} x_{1}+\epsilon
$$
Here is our interpretations:
  
- $\hat{\beta}_{0} = 26.5849137$ is the estimated average mpg for a car with an automatic transmission and 0 hp.
- $\hat{\beta}_{0} + \hat{\beta}_{2} = 31.8619991$ is the estimated average mpg for a car with a manual transmission and 0 hp.
- $\hat{\beta}_{2} = 5.2770853$ is the estimated difference in average mpg for cars with manual transmissions as compared to those with automatic transmission, *for any hp*.
- $\hat{\beta}_{1} = âˆ’0.0588878$ is the estimated change in average mpg for an increase in one hp, for either transmission types.

To show them on a plot, we can combine the coefficients from `Model2` to calculate the estimated slope and intercepts, as we already described above

```{r, warning=FALSE, message=FALSE}
int_auto = coef(model2)[1]
int_manu = coef(model2)[1] + coef(model2)[3]
slope = coef(model2)[2]

#And re-plot them
plot(mpg ~ hp, data = mtcars, col = am + 1, cex = 2)
abline(int_auto, slope, col = 1, lty = 1, lwd = 2) # add line for auto
abline(int_manu, slope, col = 2, lty = 2, lwd = 2) # add line for manual
legend("topright", c("Automatic", "Manual"), col = c(1, 2), pch = c(1, 1))
```

The above picture makes it clear that $\beta_{2}$ is significant, which you can verify mathematically with a hypothesis test.

In the model,

$$
Y=\beta_{0}+\beta_{1} x_{1}+\beta_{2} x_{2}+\epsilon
$$

We see that the effect of hp ($x_{1}$) is the same irrespective of whether the car is manual or automatic. This is captured by $\beta_{1}$ which is the average change in $Y$ for an increase in $x_{1}$, no matter the value of $x_{2}$. Although $\beta_{2}$ captures the difference in the average of Y for manual cars (remember $x_{2} = 1$ for manuals), we do not know if the effect of hp would be different for manual cars. This is a restriction that we may not want to have and might venture a more flexible model.

To remove the "same slope" restriction, we will now discuss interaction. Essentially, we would like a model that allows for two different slopes one for each transmission type. Consider the following model,

$$
Y=\beta_{0}+\beta_{1} x_{1}+\beta_{2} x_{2}+\beta_{3} x_{1} x_{2}+\epsilon
$$

where $x_{1}$, $x_{2}$, and $Y$ are the same as before, but we have added a new interaction term $x_{1}x_{2}$ which is the product of $x_{1}$ and $x_{2}$. So its effect on mpg is captured by the additional parameter $\beta_{3}$. This model estimates differences in two slopes and two intercepts. Let's see this mathematically:

For manual cars, that is $x_{2}$ = 0, we have:

$$
Y=\beta_{0}+\beta_{1} x_{1}+\epsilon
$$

For automatic cars, that is $x_{2}$ = 1, we have

$$
Y=\left(\beta_{0}+\beta_{2}\right)+\left(\beta_{1}+\beta_{3}\right) x_{1}+\epsilon
$$

These two models have both different slopes and intercepts.
  
- $\beta_{0}$ is the average mpg for a manual car with 0 hp.
- $\beta_{1}$ is the change in average mpg for an increase of one hp, for manual cars.
- $\beta_{0} + \beta_{2}$ is the average mpg for a automatic car with 0 hp.
- $\beta_{1} + \beta_{3}$ is the change in average mpg for an increase of one hp, for manual cars.

How do we fit this model in R? There are a number of ways.

### `model.matrix()`

```{r, warning=FALSE, message=FALSE}
# These 2 are the same models
model1 = lm(mpg ~ hp + am + hp:am, data = mtcars)
model2 = lm(mpg ~ hp + am + hp*am, data = mtcars)

#Important note: even our am is a "numerical" variable, 0 and 1 are
#indicators, not numbers.  So converting them to a factor variable is the
#proper way to handle categorical variables.  The reason is simple.  When you have a
#large dataset with many X variables, some of the indicator variables
#are going to be "numeric" not "character". For example, you amy have a
#variable with 10 categories identified with numbers from 1 to 10.
#R will take it as a continuous variable. If you convert it to factor variable
#everything will be easy.  LET"S SEE:

mtcars$am <- as.factor(mtcars$am)
str(mtcars)

#Now we can use a better way to build a model, specially for larger datasets:
X <- model.matrix(~ hp + am + hp:am, data = mtcars)
head(X)

#Or, even better
X <- model.matrix(~ hp*am, data = mtcars)
head(X)

#Here the "base" for am is 0.  We can make it without the intercept
#REMEMBER the "DUMMY TRAP"

X <- model.matrix(~ hp*am + 0, data = mtcars)
head(X)

#How about changing the base for am to 1 (0, manual is the base in ma, remember)?
#The level which is chosen for the reference level is the level which is contrasted against.
#By default, this is simply the first level alphabetically.
#We can specify that we want to be the reference level by using the relevel function:
table(mtcars$am)
levels(mtcars$am)
str(mtcars$am)
levels(mtcars$am) <- c("Manual", "Auto")
table(mtcars$am)
levels(mtcars$am)
mtcars$am <- relevel(mtcars$am, "Auto") 
str(mtcars$am)

X <- model.matrix(~ hp*am, data = mtcars)
head(X)
#More on this https://hopstat.wordpress.com/2014/06/26/be-careful-with-using-model-design-in-r/
#and https://genomicsclass.github.io/book/pages/expressing_design_formula.html

#Final Note: if use model.matrix() for lm() you have to be careful about the X1, which is 1
Y <- mtcars$mpg
model3 <- lm(Y ~ X)
summary(model3)  #Pay attention to F, R-squared etc)

#Becasue X has also have 1, lm() drops one of them
model4 <- lm(Y ~ X - 1)
summary(model4)  #Pay attention to F, R-squared etc)
# So model4 is the correct one. 1 should be removed from lm()

# If you remove 1 from the "design" matrix
X <- model.matrix(~ hp*am - 1, data = mtcars) #Remove 1 from the "design" matrix
model5 <- lm(Y ~ X)
summary(model5)
#It doesn't work!
```

In concluding this section, we can see the effect of `hp` on `mpg` is almost identical for both manual and auto.

```{r, warning=FALSE, message=FALSE}
#First let's use a new model with the original levels in am
str(mtcars$am)
mtcars$am <- relevel(mtcars$am, "Manual") 
str(mtcars$am)
Y <- mtcars$mpg
X <- model.matrix(~ hp*am, data = mtcars)
head(X)
model <- lm(Y ~ X - 1)
summary(model)

```

### Example with a bigger data set: `Autompg`

Our results could be different in a larger and more realistic dataset such as `Autompg`, that we downloaded and cleaned earlier. Lets give it a go:

```{r, warning=FALSE, message=FALSE}
# remove the plymouth reliant, as it causes some issues
autompg = subset(autompg, autompg$name != "plymouth reliant")

#Change horsepower from character to numeric
#We should have converted it to a factor variable
#But for the sake of this example, we keep it as numeric.
#And we manually create a dummary variable for
#foreign vs domestic cars: domestic = 1.
autompg$hp = as.numeric(autompg$hp)
autompg$domestic = as.numeric(autompg$origin == 1)

#Remove 3 and 5 cylinder cars (which are very rare.)
autompg = autompg[autompg$cyl != 5,]
autompg = autompg[autompg$cyl != 3,]

#Change cyl to a factor variable
autompg$cyl = as.factor(autompg$cyl)
str(autompg)
```

We'll now be concerned with three variables: `mpg`, `disp`, and `domestic`. We will use `mpg` as the response. We can fit a model,

$$
Y=\beta_{0}+\beta_{1} disp+\beta_{2} domestic+\epsilon
$$

where,$Y$ is `mpg`, the fuel efficiency in miles per gallon, `disp` is the displacement in cubic inches,and domestic as described below, which is a dummy variable.

$$
domestic=\left\{\begin{array}{ll}{1} & {\text { Domestic }} \\ {0} & {\text { Foreign }}\end{array}\right.
$$

We will fit this model, extract the slope and intercept for the "two lines," plot the data and add the lines.

```{r, warning=FALSE, message=FALSE}
#lm()
model1 = lm(mpg ~ disp + domestic, data = autompg)

#Extracting slope and intercept coefficents
int_for = coef(model1)[1]
int_dom = coef(model1)[1] + coef(model1)[3]
slope_for = coef(model1)[2]
slope_dom = coef(model1)[2]

#Plot
plot(mpg ~ disp, data = autompg, col = domestic + 1, pch = domestic + 1)
abline(int_for, slope_for, col = 1, lty = 1, lwd = 2) # add line for foreign cars
abline(int_dom, slope_dom, col = 2, lty = 2, lwd = 2) # add line for domestic cars
legend("topright", c("Foreign", "Domestic"), pch = c(1, 2), col = c(1, 2))
```

This is a model that allows for two parallel lines, meaning the `mpg` can be different on average between foreign and domestic cars of the same engine displacement, but the change in average `mpg` for an increase in displacement is the same for both. We can see this model isn't doing very well here. The red line fits the red points fairly well, but the black line isn't doing very well for the black points, it should clearly have a more negative slope. Essentially, we would like a model that allows for two different slopes. Consider the following model,

$$
Y=\beta_{0}+\beta_{1} disp+\beta_{2} domestic+\beta_{3} disp*domestic+\epsilon
$$

Now we have added a new interaction term $disp*domestic$, as we described earlier.

```{r, warning=FALSE, message=FALSE}
model2 = lm(mpg ~ disp * domestic, data = autompg)
summary(model2)

#Extracting slope and intercept coefficents
int_for = coef(model2)[1]
int_dom = coef(model2)[1] + coef(model2)[3]
slope_for = coef(model2)[2]
slope_dom = coef(model2)[2] + coef(model2)[4]

#Plot
plot(mpg ~ disp, data = autompg, col = domestic + 1, pch = domestic + 1)
abline(int_for, slope_for, col = 1, lty = 1, lwd = 2) 
abline(int_dom, slope_dom, col = 2, lty = 2, lwd = 2)
legend("topright", c("Foreign", "Domestic"), pch = c(1, 2), col = c(1, 2))
```

We see that these lines fit the data much better

### Some more data management tools for subsetting: `complete.cases()`, `is.na()`, and `within()`

Let's try out some new tools using our first dataset: vehicles.

```{r, warning=FALSE, message=FALSE}
str(vehicles)

#First, let's check if there is any NA in the data
head(is.na(vehicles)) # you can see from here what is.na() does.  So:
index <- which(rowSums(is.na(vehicles))>0)
#Dropping observations with NA and assigning it toa new dataset, "data"
data <- vehicles[-index, ]


#We can also use complete.cases to identify row index with NA
index <- which(!complete.cases(vehicles))
index

#Much easier option. Let's used here "df", since we cleaned it earlier
dim(df)
data <- df[complete.cases(df), ]
dim(data)
```

Our "cleaned" data is now ready. We would like make `hwy` an indicator variable. Let's name the new variable mpg and if `hyw` \> 23, `mpg` = 1 and 0 otherwise. Let's see how we can do it?

```{r, warning=FALSE, message=FALSE}
#1st way:
mpg <- c(rep(0, nrow(data))) #Create vector mpg
data2 <- cbind(data, mpg) # add it to data
data2 <- within(data2, mpg[hwy > 23] <- 1) #You can add more conditions here with &

#2nd way
rm(data2)
mpg <- c(rep(0, nrow(data))) #Create vector mpg
data2 <- cbind(data, mpg) # add it to data
data2$mpg[data2$hwy > 23] <- 1

#3nd way
data$mpg[data$hwy > 23] <- 1
data$mpg[is.na(data$mpg)] <- 0
str(data)

#Now with():  This is more like subset()
#Let's see the mean of hwy for diesel and cyl == 4
mean(with(data, hwy[cyl == 4  &  fuel =="Diesel"]))
#Ofcourse you can do it with 
index <- which(data$cyl == 4  &  data$fuel =="Diesel")
mean(data$hwy[index])
```

In our next lab, we will work with simulated data and see how they can give us useful insights when modelling.
