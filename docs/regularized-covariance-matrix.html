<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 33 Regularized Covariance Matrix | Toolbox for Social Scientists and Policy Analysts</title>
  <meta name="description" content="Chapter 33 Regularized Covariance Matrix | Toolbox for Social Scientists and Policy Analysts" />
  <meta name="generator" content="bookdown 0.32.1 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 33 Regularized Covariance Matrix | Toolbox for Social Scientists and Policy Analysts" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://yaydede.github.io/toolbox//png/cover2.png" />
  
  <meta name="github-repo" content="yaydede/ToolShed_draft" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 33 Regularized Covariance Matrix | Toolbox for Social Scientists and Policy Analysts" />
  
  
  <meta name="twitter:image" content="https://yaydede.github.io/toolbox//png/cover2.png" />

<meta name="author" content="Yigit Aydede" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="fundementals.html"/>
<link rel="next" href="r-lab-1---basics-i.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Toolbox</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who"><i class="fa fa-check"></i>Who</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> How we define Machine Learning</a></li>
<li class="chapter" data-level="2" data-path="preliminaries.html"><a href="preliminaries.html"><i class="fa fa-check"></i><b>2</b> Preliminaries</a>
<ul>
<li class="chapter" data-level="2.1" data-path="preliminaries.html"><a href="preliminaries.html#data-and-dataset-types"><i class="fa fa-check"></i><b>2.1</b> Data and dataset types</a></li>
<li class="chapter" data-level="2.2" data-path="preliminaries.html"><a href="preliminaries.html#plots"><i class="fa fa-check"></i><b>2.2</b> Plots</a></li>
<li class="chapter" data-level="2.3" data-path="preliminaries.html"><a href="preliminaries.html#probability-distributions-with-r"><i class="fa fa-check"></i><b>2.3</b> Probability Distributions with R</a></li>
<li class="chapter" data-level="2.4" data-path="preliminaries.html"><a href="preliminaries.html#regressions"><i class="fa fa-check"></i><b>2.4</b> Regressions</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="preliminaries.html"><a href="preliminaries.html#ordinary-least-squares-ols"><i class="fa fa-check"></i><b>2.4.1</b> Ordinary Least Squares (OLS)</a></li>
<li class="chapter" data-level="2.4.2" data-path="preliminaries.html"><a href="preliminaries.html#maximum-likelihood-estimators"><i class="fa fa-check"></i><b>2.4.2</b> Maximum Likelihood Estimators</a></li>
<li class="chapter" data-level="2.4.3" data-path="preliminaries.html"><a href="preliminaries.html#estimating-mle-with-r"><i class="fa fa-check"></i><b>2.4.3</b> Estimating MLE with R</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="preliminaries.html"><a href="preliminaries.html#blue"><i class="fa fa-check"></i><b>2.5</b> BLUE</a></li>
<li class="chapter" data-level="2.6" data-path="preliminaries.html"><a href="preliminaries.html#modeling-the-data"><i class="fa fa-check"></i><b>2.6</b> Modeling the data</a></li>
<li class="chapter" data-level="2.7" data-path="preliminaries.html"><a href="preliminaries.html#causal-vs.-predictive-models"><i class="fa fa-check"></i><b>2.7</b> Causal vs.Â Predictive Models</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="preliminaries.html"><a href="preliminaries.html#causal-models"><i class="fa fa-check"></i><b>2.7.1</b> Causal Models</a></li>
<li class="chapter" data-level="2.7.2" data-path="preliminaries.html"><a href="preliminaries.html#predictive-models"><i class="fa fa-check"></i><b>2.7.2</b> Predictive Models</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="preliminaries.html"><a href="preliminaries.html#simulation"><i class="fa fa-check"></i><b>2.8</b> Simulation</a></li>
</ul></li>
<li class="part"><span><b>I Formal Look at Prediction</b></span></li>
<li class="chapter" data-level="" data-path="learning-systems.html"><a href="learning-systems.html"><i class="fa fa-check"></i>Learning Systems</a></li>
<li class="chapter" data-level="3" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html"><i class="fa fa-check"></i><b>3</b> Bias-Variance Tradeoff</a>
<ul>
<li class="chapter" data-level="3.1" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#estimator-and-mse"><i class="fa fa-check"></i><b>3.1</b> Estimator and MSE</a></li>
<li class="chapter" data-level="3.2" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#prediction---mspe"><i class="fa fa-check"></i><b>3.2</b> Prediction - MSPE</a></li>
<li class="chapter" data-level="3.3" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#biased-estimator-as-a-predictor"><i class="fa fa-check"></i><b>3.3</b> Biased estimator as a predictor</a></li>
<li class="chapter" data-level="3.4" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#dropping-a-variable-in-a-regression"><i class="fa fa-check"></i><b>3.4</b> Dropping a variable in a regression</a></li>
<li class="chapter" data-level="3.5" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#uncertainty-in-estimations-and-predictions"><i class="fa fa-check"></i><b>3.5</b> Uncertainty in estimations and predictions</a></li>
<li class="chapter" data-level="3.6" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#prediction-interval-for-unbiased-ols-predictor"><i class="fa fa-check"></i><b>3.6</b> Prediction interval for unbiased OLS predictor</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="overfitting.html"><a href="overfitting.html"><i class="fa fa-check"></i><b>4</b> Overfitting</a></li>
<li class="part"><span><b>II Nonparametric Estimations</b></span></li>
<li class="chapter" data-level="" data-path="parametric-vs.-nonparametric-methods.html"><a href="parametric-vs.-nonparametric-methods.html"><i class="fa fa-check"></i>Parametric vs.Â Nonparametric methods</a></li>
<li class="chapter" data-level="5" data-path="parametric-estimations.html"><a href="parametric-estimations.html"><i class="fa fa-check"></i><b>5</b> Parametric Estimations</a>
<ul>
<li class="chapter" data-level="5.1" data-path="parametric-estimations.html"><a href="parametric-estimations.html#linear-probability-models-lpm"><i class="fa fa-check"></i><b>5.1</b> Linear Probability Models (LPM)</a></li>
<li class="chapter" data-level="5.2" data-path="parametric-estimations.html"><a href="parametric-estimations.html#logistic-regression"><i class="fa fa-check"></i><b>5.2</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="parametric-estimations.html"><a href="parametric-estimations.html#estimating-logistic-regression"><i class="fa fa-check"></i><b>5.2.1</b> Estimating Logistic Regression</a></li>
<li class="chapter" data-level="5.2.2" data-path="parametric-estimations.html"><a href="parametric-estimations.html#cost-functions"><i class="fa fa-check"></i><b>5.2.2</b> Cost functions</a></li>
<li class="chapter" data-level="5.2.3" data-path="parametric-estimations.html"><a href="parametric-estimations.html#deviance"><i class="fa fa-check"></i><b>5.2.3</b> Deviance</a></li>
<li class="chapter" data-level="5.2.4" data-path="parametric-estimations.html"><a href="parametric-estimations.html#predictive-accuracy"><i class="fa fa-check"></i><b>5.2.4</b> Predictive accuracy</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html"><i class="fa fa-check"></i><b>6</b> Nonparametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="6.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#density-estimations"><i class="fa fa-check"></i><b>6.1</b> Density Estimations</a></li>
<li class="chapter" data-level="6.2" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#kernel-regressions"><i class="fa fa-check"></i><b>6.2</b> Kernel regressions</a></li>
<li class="chapter" data-level="6.3" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#regression-splines"><i class="fa fa-check"></i><b>6.3</b> Regression Splines</a></li>
<li class="chapter" data-level="6.4" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#mars---multivariate-adaptive-regression-splines"><i class="fa fa-check"></i><b>6.4</b> MARS - Multivariate Adaptive Regression Splines</a></li>
<li class="chapter" data-level="6.5" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#gam---generalized-additive-model"><i class="fa fa-check"></i><b>6.5</b> GAM - Generalized Additive Model</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>7</b> Smoothing</a>
<ul>
<li class="chapter" data-level="7.1" data-path="smoothing.html"><a href="smoothing.html#using-bins"><i class="fa fa-check"></i><b>7.1</b> Using bins</a></li>
<li class="chapter" data-level="7.2" data-path="smoothing.html"><a href="smoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>7.2</b> Kernel smoothing</a></li>
<li class="chapter" data-level="7.3" data-path="smoothing.html"><a href="smoothing.html#locally-weighted-regression-loess"><i class="fa fa-check"></i><b>7.3</b> Locally weighted regression <code>loess()</code></a></li>
<li class="chapter" data-level="7.4" data-path="smoothing.html"><a href="smoothing.html#smooth-spline-regression"><i class="fa fa-check"></i><b>7.4</b> Smooth Spline Regression</a></li>
<li class="chapter" data-level="7.5" data-path="smoothing.html"><a href="smoothing.html#multivariate-loess"><i class="fa fa-check"></i><b>7.5</b> Multivariate Loess</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html"><i class="fa fa-check"></i><b>8</b> Nonparametric Classifier - kNN</a>
<ul>
<li class="chapter" data-level="8.1" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#mnist-dataset"><i class="fa fa-check"></i><b>8.1</b> <code>mnist</code> Dataset</a></li>
<li class="chapter" data-level="8.2" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#linear-classifiers-again"><i class="fa fa-check"></i><b>8.2</b> Linear classifiers (again)</a></li>
<li class="chapter" data-level="8.3" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>8.3</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="8.4" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#knn-with-caret"><i class="fa fa-check"></i><b>8.4</b> kNN with caret</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#mnist_27"><i class="fa fa-check"></i><b>8.4.1</b> <code>mnist_27</code></a></li>
<li class="chapter" data-level="8.4.2" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#adult-dataset"><i class="fa fa-check"></i><b>8.4.2</b> Adult dataset</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Self-Learning</b></span></li>
<li class="chapter" data-level="9" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html"><i class="fa fa-check"></i><b>9</b> Hyperparameter Tuning</a>
<ul>
<li class="chapter" data-level="9.1" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#training-validation-and-test-datasets"><i class="fa fa-check"></i><b>9.1</b> Training, validation, and test datasets</a></li>
<li class="chapter" data-level="9.2" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#splitting-the-data-randomly"><i class="fa fa-check"></i><b>9.2</b> Splitting the data randomly</a></li>
<li class="chapter" data-level="9.3" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>9.3</b> k-fold cross validation</a></li>
<li class="chapter" data-level="9.4" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#cross-validated-grid-search"><i class="fa fa-check"></i><b>9.4</b> Cross-validated grid search</a></li>
<li class="chapter" data-level="9.5" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#bootstrapped-grid-search"><i class="fa fa-check"></i><b>9.5</b> Bootstrapped grid search</a></li>
<li class="chapter" data-level="9.6" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#when-the-data-is-time-series"><i class="fa fa-check"></i><b>9.6</b> When the data is time-series</a></li>
<li class="chapter" data-level="9.7" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#speed"><i class="fa fa-check"></i><b>9.7</b> Speed</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html"><i class="fa fa-check"></i><b>10</b> Tuning in Classification</a>
<ul>
<li class="chapter" data-level="10.1" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html#confusion-matrix"><i class="fa fa-check"></i><b>10.1</b> Confusion matrix</a></li>
<li class="chapter" data-level="10.2" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html#performance-measures"><i class="fa fa-check"></i><b>10.2</b> Performance measures</a></li>
<li class="chapter" data-level="10.3" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html#roc-curve"><i class="fa fa-check"></i><b>10.3</b> ROC Curve</a></li>
<li class="chapter" data-level="10.4" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html#auc---area-under-the-curve"><i class="fa fa-check"></i><b>10.4</b> AUC - Area Under the Curve</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="classification-example.html"><a href="classification-example.html"><i class="fa fa-check"></i><b>11</b> Classification Example</a>
<ul>
<li class="chapter" data-level="11.1" data-path="classification-example.html"><a href="classification-example.html#lpm"><i class="fa fa-check"></i><b>11.1</b> LPM</a></li>
<li class="chapter" data-level="11.2" data-path="classification-example.html"><a href="classification-example.html#logistic-regression-1"><i class="fa fa-check"></i><b>11.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="11.3" data-path="classification-example.html"><a href="classification-example.html#knn"><i class="fa fa-check"></i><b>11.3</b> kNN</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="classification-example.html"><a href="classification-example.html#knn-10-fold-cv"><i class="fa fa-check"></i><b>11.3.1</b> kNN 10-fold CV</a></li>
<li class="chapter" data-level="11.3.2" data-path="classification-example.html"><a href="classification-example.html#knn-with-caret-1"><i class="fa fa-check"></i><b>11.3.2</b> kNN with <code>caret</code></a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Tree-based Models</b></span></li>
<li class="chapter" data-level="12" data-path="cart.html"><a href="cart.html"><i class="fa fa-check"></i><b>12</b> CART</a>
<ul>
<li class="chapter" data-level="12.1" data-path="cart.html"><a href="cart.html#cart---classification-tree"><i class="fa fa-check"></i><b>12.1</b> CART - Classification Tree</a></li>
<li class="chapter" data-level="12.2" data-path="cart.html"><a href="cart.html#rpart---recursive-partitioning"><i class="fa fa-check"></i><b>12.2</b> <code>rpart</code> - Recursive Partitioning</a></li>
<li class="chapter" data-level="12.3" data-path="cart.html"><a href="cart.html#pruning"><i class="fa fa-check"></i><b>12.3</b> Pruning</a></li>
<li class="chapter" data-level="12.4" data-path="cart.html"><a href="cart.html#classification-with-titanic"><i class="fa fa-check"></i><b>12.4</b> Classification with Titanic</a></li>
<li class="chapter" data-level="12.5" data-path="cart.html"><a href="cart.html#regression-tree"><i class="fa fa-check"></i><b>12.5</b> Regression Tree</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ensemble-learning.html"><a href="ensemble-learning.html"><i class="fa fa-check"></i><b>13</b> Ensemble Learning</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ensemble-learning.html"><a href="ensemble-learning.html#bagging"><i class="fa fa-check"></i><b>13.1</b> Bagging</a></li>
<li class="chapter" data-level="13.2" data-path="ensemble-learning.html"><a href="ensemble-learning.html#random-forest"><i class="fa fa-check"></i><b>13.2</b> Random Forest</a></li>
<li class="chapter" data-level="13.3" data-path="ensemble-learning.html"><a href="ensemble-learning.html#boosting"><i class="fa fa-check"></i><b>13.3</b> Boosting</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="ensemble-learning.html"><a href="ensemble-learning.html#sequential-ensemble-with-gbm"><i class="fa fa-check"></i><b>13.3.1</b> Sequential ensemble with <code>gbm</code></a></li>
<li class="chapter" data-level="13.3.2" data-path="ensemble-learning.html"><a href="ensemble-learning.html#adaboost"><i class="fa fa-check"></i><b>13.3.2</b> AdaBoost</a></li>
<li class="chapter" data-level="13.3.3" data-path="ensemble-learning.html"><a href="ensemble-learning.html#xgboost"><i class="fa fa-check"></i><b>13.3.3</b> XGBoost</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ensemble-applications.html"><a href="ensemble-applications.html"><i class="fa fa-check"></i><b>14</b> Ensemble Applications</a>
<ul>
<li class="chapter" data-level="14.1" data-path="ensemble-applications.html"><a href="ensemble-applications.html#classification"><i class="fa fa-check"></i><b>14.1</b> Classification</a></li>
<li class="chapter" data-level="14.2" data-path="ensemble-applications.html"><a href="ensemble-applications.html#regression"><i class="fa fa-check"></i><b>14.2</b> Regression</a></li>
<li class="chapter" data-level="14.3" data-path="ensemble-applications.html"><a href="ensemble-applications.html#exploration"><i class="fa fa-check"></i><b>14.3</b> Exploration</a></li>
<li class="chapter" data-level="14.4" data-path="ensemble-applications.html"><a href="ensemble-applications.html#boosting-applications"><i class="fa fa-check"></i><b>14.4</b> Boosting Applications</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="ensemble-applications.html"><a href="ensemble-applications.html#regression-1"><i class="fa fa-check"></i><b>14.4.1</b> Regression</a></li>
<li class="chapter" data-level="14.4.2" data-path="ensemble-applications.html"><a href="ensemble-applications.html#random-search-with-parallel-processing"><i class="fa fa-check"></i><b>14.4.2</b> Random search with parallel processing</a></li>
<li class="chapter" data-level="14.4.3" data-path="ensemble-applications.html"><a href="ensemble-applications.html#boosting-vs.-others"><i class="fa fa-check"></i><b>14.4.3</b> Boosting vs.Â Others</a></li>
<li class="chapter" data-level="14.4.4" data-path="ensemble-applications.html"><a href="ensemble-applications.html#classification-1"><i class="fa fa-check"></i><b>14.4.4</b> Classification</a></li>
<li class="chapter" data-level="14.4.5" data-path="ensemble-applications.html"><a href="ensemble-applications.html#adaboost.m1"><i class="fa fa-check"></i><b>14.4.5</b> AdaBoost.M1</a></li>
<li class="chapter" data-level="14.4.6" data-path="ensemble-applications.html"><a href="ensemble-applications.html#classification-with-xgboost"><i class="fa fa-check"></i><b>14.4.6</b> Classification with XGBoost</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>V SVM &amp; Neural Networks</b></span></li>
<li class="chapter" data-level="15" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>15</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="15.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#optimal-separating-classifier"><i class="fa fa-check"></i><b>15.1</b> Optimal Separating Classifier</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#the-margin"><i class="fa fa-check"></i><b>15.1.1</b> The Margin</a></li>
<li class="chapter" data-level="15.1.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#the-non-separable-case"><i class="fa fa-check"></i><b>15.1.2</b> The Non-Separable Case</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#nonlinear-boundary-with-kernels"><i class="fa fa-check"></i><b>15.2</b> Nonlinear Boundary with Kernels</a></li>
<li class="chapter" data-level="15.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#application-with-svm"><i class="fa fa-check"></i><b>15.3</b> Application with SVM</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html"><i class="fa fa-check"></i><b>16</b> Artificial Neural Networks</a>
<ul>
<li class="chapter" data-level="16.1" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#neural-network---the-idea"><i class="fa fa-check"></i><b>16.1</b> Neural Network - the idea</a></li>
<li class="chapter" data-level="16.2" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#backpropagation"><i class="fa fa-check"></i><b>16.2</b> Backpropagation</a></li>
<li class="chapter" data-level="16.3" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#neural-network---more-inputs"><i class="fa fa-check"></i><b>16.3</b> Neural Network - More inputs</a></li>
<li class="chapter" data-level="16.4" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#deep-learning"><i class="fa fa-check"></i><b>16.4</b> Deep Learning</a></li>
</ul></li>
<li class="part"><span><b>VI Penalized Regressions</b></span></li>
<li class="chapter" data-level="" data-path="parametric-models-in-prediction.html"><a href="parametric-models-in-prediction.html"><i class="fa fa-check"></i>Parametric models in prediction</a></li>
<li class="chapter" data-level="17" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>17</b> Ridge</a></li>
<li class="chapter" data-level="18" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>18</b> Lasso</a></li>
<li class="chapter" data-level="19" data-path="adaptive-lasso.html"><a href="adaptive-lasso.html"><i class="fa fa-check"></i><b>19</b> Adaptive Lasso</a></li>
<li class="chapter" data-level="20" data-path="sparsity.html"><a href="sparsity.html"><i class="fa fa-check"></i><b>20</b> Sparsity</a></li>
<li class="part"><span><b>VII Time Series</b></span></li>
<li class="chapter" data-level="" data-path="forecasting.html"><a href="forecasting.html"><i class="fa fa-check"></i>Forecasting</a></li>
<li class="chapter" data-level="21" data-path="arima-models.html"><a href="arima-models.html"><i class="fa fa-check"></i><b>21</b> ARIMA models</a>
<ul>
<li class="chapter" data-level="21.1" data-path="arima-models.html"><a href="arima-models.html#hyndman-khandakar-algorithm"><i class="fa fa-check"></i><b>21.1</b> Hyndman-Khandakar algorithm</a></li>
<li class="chapter" data-level="21.2" data-path="arima-models.html"><a href="arima-models.html#ts-plots"><i class="fa fa-check"></i><b>21.2</b> TS Plots</a></li>
<li class="chapter" data-level="21.3" data-path="arima-models.html"><a href="arima-models.html#box-cox-transformation"><i class="fa fa-check"></i><b>21.3</b> Box-Cox transformation</a></li>
<li class="chapter" data-level="21.4" data-path="arima-models.html"><a href="arima-models.html#stationarity"><i class="fa fa-check"></i><b>21.4</b> Stationarity</a></li>
<li class="chapter" data-level="21.5" data-path="arima-models.html"><a href="arima-models.html#modeling-arima"><i class="fa fa-check"></i><b>21.5</b> Modeling ARIMA</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="grid-search-for-arima.html"><a href="grid-search-for-arima.html"><i class="fa fa-check"></i><b>22</b> Grid search for ARIMA</a></li>
<li class="chapter" data-level="23" data-path="time-series-embedding.html"><a href="time-series-embedding.html"><i class="fa fa-check"></i><b>23</b> Time Series Embedding</a>
<ul>
<li class="chapter" data-level="23.1" data-path="time-series-embedding.html"><a href="time-series-embedding.html#var-for-recursive-forecasting"><i class="fa fa-check"></i><b>23.1</b> VAR for Recursive Forecasting</a></li>
<li class="chapter" data-level="23.2" data-path="time-series-embedding.html"><a href="time-series-embedding.html#embedding-for-direct-forecast"><i class="fa fa-check"></i><b>23.2</b> Embedding for Direct Forecast</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="random-forest-1.html"><a href="random-forest-1.html"><i class="fa fa-check"></i><b>24</b> Random Forest</a>
<ul>
<li class="chapter" data-level="24.1" data-path="random-forest-1.html"><a href="random-forest-1.html#univariate"><i class="fa fa-check"></i><b>24.1</b> Univariate</a></li>
<li class="chapter" data-level="24.2" data-path="random-forest-1.html"><a href="random-forest-1.html#multivariate"><i class="fa fa-check"></i><b>24.2</b> Multivariate</a></li>
<li class="chapter" data-level="24.3" data-path="random-forest-1.html"><a href="random-forest-1.html#rolling-and-expanding-windows"><i class="fa fa-check"></i><b>24.3</b> Rolling and expanding windows</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html"><i class="fa fa-check"></i><b>25</b> Recurrent Neural Networks</a>
<ul>
<li class="chapter" data-level="25.1" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#keras"><i class="fa fa-check"></i><b>25.1</b> Keras</a></li>
<li class="chapter" data-level="25.2" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#input-tensors"><i class="fa fa-check"></i><b>25.2</b> Input Tensors</a></li>
<li class="chapter" data-level="25.3" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#plain-rnn"><i class="fa fa-check"></i><b>25.3</b> Plain RNN</a></li>
<li class="chapter" data-level="25.4" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#lstm"><i class="fa fa-check"></i><b>25.4</b> LSTM</a></li>
</ul></li>
<li class="part"><span><b>VIII Dimension Reduction Methods</b></span></li>
<li class="chapter" data-level="" data-path="matrix-decompositions.html"><a href="matrix-decompositions.html"><i class="fa fa-check"></i>Matrix Decompositions</a></li>
<li class="chapter" data-level="26" data-path="eigenvectors-and-eigenvalues.html"><a href="eigenvectors-and-eigenvalues.html"><i class="fa fa-check"></i><b>26</b> Eigenvectors and eigenvalues</a></li>
<li class="chapter" data-level="27" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html"><i class="fa fa-check"></i><b>27</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="28" data-path="rankr-approximations.html"><a href="rankr-approximations.html"><i class="fa fa-check"></i><b>28</b> Rank(r) Approximations</a></li>
<li class="chapter" data-level="29" data-path="moore-penrose-inverse.html"><a href="moore-penrose-inverse.html"><i class="fa fa-check"></i><b>29</b> Moore-Penrose inverse</a></li>
<li class="chapter" data-level="30" data-path="principle-component-analysis.html"><a href="principle-component-analysis.html"><i class="fa fa-check"></i><b>30</b> Principle Component Analysis</a></li>
<li class="chapter" data-level="31" data-path="factor-analysis.html"><a href="factor-analysis.html"><i class="fa fa-check"></i><b>31</b> Factor Analysis</a></li>
<li class="part"><span><b>IX Network Analysis</b></span></li>
<li class="chapter" data-level="" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html"><i class="fa fa-check"></i>Graphical Network Analysis</a></li>
<li class="chapter" data-level="32" data-path="fundementals.html"><a href="fundementals.html"><i class="fa fa-check"></i><b>32</b> Fundementals</a>
<ul>
<li class="chapter" data-level="32.1" data-path="fundementals.html"><a href="fundementals.html#covariance"><i class="fa fa-check"></i><b>32.1</b> Covariance</a></li>
<li class="chapter" data-level="32.2" data-path="fundementals.html"><a href="fundementals.html#correlation"><i class="fa fa-check"></i><b>32.2</b> Correlation</a></li>
<li class="chapter" data-level="32.3" data-path="fundementals.html"><a href="fundementals.html#precision-matrix"><i class="fa fa-check"></i><b>32.3</b> Precision Matrix</a></li>
<li class="chapter" data-level="32.4" data-path="fundementals.html"><a href="fundementals.html#semi-partial-correlation"><i class="fa fa-check"></i><b>32.4</b> Semi-partial Correlation</a></li>
</ul></li>
<li class="chapter" data-level="33" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html"><i class="fa fa-check"></i><b>33</b> Regularized Covariance Matrix</a>
<ul>
<li class="chapter" data-level="33.1" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html#multivariate-gaussian-distribution"><i class="fa fa-check"></i><b>33.1</b> Multivariate Gaussian Distribution</a></li>
<li class="chapter" data-level="33.2" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html#high-dimensional-data"><i class="fa fa-check"></i><b>33.2</b> High-dimensional data</a></li>
<li class="chapter" data-level="33.3" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html#ridge-ell_2-and-glasso-ell_1"><i class="fa fa-check"></i><b>33.3</b> Ridge (<span class="math inline">\(\ell_{2}\)</span>) and glasso (<span class="math inline">\(\ell_{1}\)</span>)</a></li>
</ul></li>
<li class="part"><span><b>X Labs</b></span></li>
<li class="chapter" data-level="34" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html"><i class="fa fa-check"></i><b>34</b> R Lab 1 - Basics I</a>
<ul>
<li class="chapter" data-level="34.1" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#r-rstudio-and-r-packages"><i class="fa fa-check"></i><b>34.1</b> R, RStudio, and R Packages</a></li>
<li class="chapter" data-level="34.2" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#rstudio"><i class="fa fa-check"></i><b>34.2</b> RStudio</a></li>
<li class="chapter" data-level="34.3" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#working-directory"><i class="fa fa-check"></i><b>34.3</b> Working directory</a></li>
<li class="chapter" data-level="34.4" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#data-types-and-stuctures"><i class="fa fa-check"></i><b>34.4</b> Data Types and Stuctures</a></li>
<li class="chapter" data-level="34.5" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#vectors"><i class="fa fa-check"></i><b>34.5</b> Vectors</a></li>
<li class="chapter" data-level="34.6" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#subsetting-vectors"><i class="fa fa-check"></i><b>34.6</b> Subsetting Vectors</a></li>
<li class="chapter" data-level="34.7" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#vectorization-or-vector-operations"><i class="fa fa-check"></i><b>34.7</b> Vectorization or vector operations</a></li>
<li class="chapter" data-level="34.8" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#matrices"><i class="fa fa-check"></i><b>34.8</b> Matrices</a></li>
<li class="chapter" data-level="34.9" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#matrix-operations"><i class="fa fa-check"></i><b>34.9</b> Matrix Operations</a></li>
<li class="chapter" data-level="34.10" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#subsetting-matrix"><i class="fa fa-check"></i><b>34.10</b> Subsetting Matrix</a></li>
<li class="chapter" data-level="34.11" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#r-style-guide"><i class="fa fa-check"></i><b>34.11</b> R-Style Guide</a></li>
</ul></li>
<li class="chapter" data-level="35" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html"><i class="fa fa-check"></i><b>35</b> R Lab 2 - Basics II</a>
<ul>
<li class="chapter" data-level="35.1" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#data-frames-and-lists"><i class="fa fa-check"></i><b>35.1</b> Data frames and lists</a>
<ul>
<li class="chapter" data-level="35.1.1" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#lists"><i class="fa fa-check"></i><b>35.1.1</b> Lists</a></li>
<li class="chapter" data-level="35.1.2" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#data-frames"><i class="fa fa-check"></i><b>35.1.2</b> Data Frames</a></li>
<li class="chapter" data-level="35.1.3" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#reading-importing-and-writting-exporting-data-files"><i class="fa fa-check"></i><b>35.1.3</b> Reading (importing) and writting (exporting) data files</a></li>
<li class="chapter" data-level="35.1.4" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#subsetting-data-frames"><i class="fa fa-check"></i><b>35.1.4</b> Subsetting Data Frames</a></li>
<li class="chapter" data-level="35.1.5" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#plotting-from-data-frame"><i class="fa fa-check"></i><b>35.1.5</b> Plotting from data frame</a></li>
<li class="chapter" data-level="35.1.6" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#some-useful-functions"><i class="fa fa-check"></i><b>35.1.6</b> Some useful functions</a></li>
<li class="chapter" data-level="35.1.7" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#categorical-variables-in-data-frames"><i class="fa fa-check"></i><b>35.1.7</b> Categorical Variables in Data Frames</a></li>
</ul></li>
<li class="chapter" data-level="35.2" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#programming-basics"><i class="fa fa-check"></i><b>35.2</b> Programming Basics</a>
<ul>
<li class="chapter" data-level="35.2.1" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#ifelse"><i class="fa fa-check"></i><b>35.2.1</b> if/Else</a></li>
<li class="chapter" data-level="35.2.2" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#loops"><i class="fa fa-check"></i><b>35.2.2</b> Loops</a></li>
<li class="chapter" data-level="35.2.3" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#the-apply-family"><i class="fa fa-check"></i><b>35.2.3</b> The <code>apply()</code> family</a></li>
<li class="chapter" data-level="35.2.4" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#functions"><i class="fa fa-check"></i><b>35.2.4</b> Functions</a></li>
<li class="chapter" data-level="35.2.5" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#dplyr"><i class="fa fa-check"></i><b>35.2.5</b> <code>dplyr()</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="36" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html"><i class="fa fa-check"></i><b>36</b> R Lab 3 - Preparing the data</a>
<ul>
<li class="chapter" data-level="36.1" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#preparing-the-data-for-a-regression-analysis-with-lm"><i class="fa fa-check"></i><b>36.1</b> Preparing the data for a regression analysis with <code>lm()</code></a>
<ul>
<li class="chapter" data-level="36.1.1" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#factor-variables"><i class="fa fa-check"></i><b>36.1.1</b> Factor variables</a></li>
<li class="chapter" data-level="36.1.2" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#dummy-coding"><i class="fa fa-check"></i><b>36.1.2</b> Dummy Coding</a></li>
<li class="chapter" data-level="36.1.3" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#column-variable-names"><i class="fa fa-check"></i><b>36.1.3</b> Column (Variable) names</a></li>
<li class="chapter" data-level="36.1.4" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#data-subsetting-and-missing-values"><i class="fa fa-check"></i><b>36.1.4</b> Data subsetting and missing values</a></li>
</ul></li>
<li class="chapter" data-level="36.2" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#dummy-variable-models"><i class="fa fa-check"></i><b>36.2</b> âDUMMYâ variable models</a>
<ul>
<li class="chapter" data-level="36.2.1" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#mtcars-example"><i class="fa fa-check"></i><b>36.2.1</b> <code>mtcars</code> example</a></li>
<li class="chapter" data-level="36.2.2" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#model.matrix"><i class="fa fa-check"></i><b>36.2.2</b> <code>model.matrix()</code></a></li>
<li class="chapter" data-level="36.2.3" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#example-with-a-bigger-data-set-autompg"><i class="fa fa-check"></i><b>36.2.3</b> Example with a bigger data set: <code>Autompg</code></a></li>
<li class="chapter" data-level="36.2.4" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#some-more-data-management-tools-for-subsetting-complete.cases-is.na-and-within"><i class="fa fa-check"></i><b>36.2.4</b> Some more data management tools for subsetting: <code>complete.cases()</code>, <code>is.na()</code>, and <code>within()</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="37" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html"><i class="fa fa-check"></i><b>37</b> R Lab 4 - Simulation in R</a>
<ul>
<li class="chapter" data-level="37.1" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#sampling-in-r-sample"><i class="fa fa-check"></i><b>37.1</b> Sampling in R: <code>sample()</code></a></li>
<li class="chapter" data-level="37.2" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#random-number-generating-with-probablity-distributions"><i class="fa fa-check"></i><b>37.2</b> Random number generating with probablity distributions</a></li>
<li class="chapter" data-level="37.3" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#simulation-for-statistical-inference"><i class="fa fa-check"></i><b>37.3</b> Simulation for statistical inference</a></li>
<li class="chapter" data-level="37.4" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#creataing-data-with-a-data-generating-model-dgm"><i class="fa fa-check"></i><b>37.4</b> Creataing data with a Data Generating Model (DGM)</a></li>
<li class="chapter" data-level="37.5" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#bootstrapping"><i class="fa fa-check"></i><b>37.5</b> Bootstrapping</a></li>
<li class="chapter" data-level="37.6" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#monty-hall---fun-example"><i class="fa fa-check"></i><b>37.6</b> Monty Hall - Fun example</a></li>
</ul></li>
<li class="part"><span><b>XI Appendix</b></span></li>
<li class="chapter" data-level="38" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html"><i class="fa fa-check"></i><b>38</b> Algorithmic Optimization</a>
<ul>
<li class="chapter" data-level="38.1" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#brute-force-optimization"><i class="fa fa-check"></i><b>38.1</b> Brute-force optimization</a></li>
<li class="chapter" data-level="38.2" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#derivative-based-methods"><i class="fa fa-check"></i><b>38.2</b> Derivative-based methods</a></li>
<li class="chapter" data-level="38.3" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#ml-estimation-with-logistic-regression"><i class="fa fa-check"></i><b>38.3</b> ML Estimation with logistic regression</a></li>
<li class="chapter" data-level="38.4" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#gradient-descent-algorithm"><i class="fa fa-check"></i><b>38.4</b> Gradient Descent Algorithm</a>
<ul>
<li class="chapter" data-level="38.4.1" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#one-variable"><i class="fa fa-check"></i><b>38.4.1</b> One-variable</a></li>
<li class="chapter" data-level="38.4.2" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#adjustable-lr-and-sgd"><i class="fa fa-check"></i><b>38.4.2</b> Adjustable <code>lr</code> and SGD</a></li>
<li class="chapter" data-level="38.4.3" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#multivariable"><i class="fa fa-check"></i><b>38.4.3</b> Multivariable</a></li>
</ul></li>
<li class="chapter" data-level="38.5" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#optimization-with-r"><i class="fa fa-check"></i><b>38.5</b> Optimization with R</a></li>
</ul></li>
<li class="chapter" data-level="39" data-path="imbalanced-data.html"><a href="imbalanced-data.html"><i class="fa fa-check"></i><b>39</b> Imbalanced Data</a>
<ul>
<li class="chapter" data-level="39.1" data-path="imbalanced-data.html"><a href="imbalanced-data.html#smote"><i class="fa fa-check"></i><b>39.1</b> <code>SMOTE</code></a></li>
<li class="chapter" data-level="39.2" data-path="imbalanced-data.html"><a href="imbalanced-data.html#fraud-detection"><i class="fa fa-check"></i><b>39.2</b> Fraud detection</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/yaydede/toolbox" target="blank"> 2023 Yigit Aydede - Bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Toolbox for Social Scientists and Policy Analysts</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regularized-covariance-matrix" class="section level1 hasAnchor" number="33">
<h1><span class="header-section-number">Chapter 33</span> Regularized Covariance Matrix<a href="regularized-covariance-matrix.html#regularized-covariance-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Due an increasing availability of high-dimensional data sets, graphical models have become powerful tools to discover conditional dependencies over a graph structure.</p>
<p>However, there are two main challenges in identifying the relations in a network: first, the edges (relationships) may not be identified by Pearson or Spearman correlations as they often lead to spurious associations due to missing confounding factors. Second, although, applications with partial correlations might address this issue, traditional precision estimators are not well-defined in case of high-dimensional data.</p>
<p>Why is a covariance matrix <span class="math inline">\(S\)</span> singular when <span class="math inline">\(n&lt;p\)</span> in <span class="math inline">\(\mathbf{X}\)</span>? Consider the <span class="math inline">\(n \times p\)</span> matrix of sample data, <span class="math inline">\(\mathbf{X}\)</span>. Since we know that the rank of <span class="math inline">\(\mathbf{X}\)</span> is at most <span class="math inline">\(\min (n, p)\)</span>. Hence, in</p>
<p><span class="math display">\[
\mathbf{S}=\frac{1}{n} \mathbf{X}_{c}^{\prime} \mathbf{X}_{c},
\]</span></p>
<p><span class="math inline">\(\operatorname{rank}(\mathbf{X}_c)\)</span> will be <span class="math inline">\(n\)</span>. It is clear that the rank of <span class="math inline">\(\mathbf{S}\)</span> wonât be larger than the rank of <span class="math inline">\(\mathbf{X}_c\)</span>. Since <span class="math inline">\(\mathbf{S}\)</span> is <span class="math inline">\(p \times p\)</span> and its rank is <span class="math inline">\(n\)</span>, <span class="math inline">\(\mathbf{S}\)</span> will be singular. Thatâs, if <span class="math inline">\(n&lt;p\)</span> then <span class="math inline">\(\operatorname{rank}(\mathbf{X})&lt;p\)</span> in which case <span class="math inline">\(\operatorname{rank}(\mathbf{S})&lt;p\)</span>.</p>
<p>This brought several novel precision estimators in applications. Generally, these novel estimators overcome the undersampling by maximization of the log-likelihood augmented with a so-called penalty. A penalty discourages large values among the elements of the precision matrix estimate. This reduces the risk of overfitting but also yields a well-defined penalized precision matrix estimator.</p>
<p>To solve the problem, as we have seen before in Section 6, penalized estimators adds a penalty to the likelihood functions ( <span class="math inline">\(\ell_2\)</span> in Ridge and <span class="math inline">\(\ell_1\)</span> in lasso) that makes the eigenvalues of <span class="math inline">\(\mathbf{S}\)</span> shrink in a particular manner to combat <span class="math inline">\(p \geq n\)</span>. The graphical lasso (gLasso) is the <span class="math inline">\(\ell_1\)</span>-equivalent to graphical ridge. A nice feature of the <span class="math inline">\(\ell_1\)</span> penalty automatically induces sparsity and thus also select the edges in the underlying graph. The <span class="math inline">\(\ell_2\)</span> penalty in Ridge relies on an extra step that selects the edges after the regularized precision matrix with shrunken correlations is estimated.</p>
<p>In this chapter we will see graphical ridge and lasso applications based on Gaussian graphical models that will provide sparse precision matrices in case of <span class="math inline">\(n&lt;p\)</span>.</p>
<div id="multivariate-gaussian-distribution" class="section level2 hasAnchor" number="33.1">
<h2><span class="header-section-number">33.1</span> Multivariate Gaussian Distribution<a href="regularized-covariance-matrix.html#multivariate-gaussian-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Before understanding <span class="math inline">\(\ell_1\)</span> or <span class="math inline">\(\ell_2\)</span> regularization, we need to see the multivariate Gaussian distribution, its parameterization and maximum likelihood estimation (MLE) solutions.</p>
<p>The multivariate Gaussian distribution of a random vector <span class="math inline">\(\mathbf{X} \in \mathbf{R}^{p}\)</span> is commonly expressed in terms of the parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma\)</span>, where <span class="math inline">\(\mu\)</span> is an <span class="math inline">\(p \times 1\)</span> vector and <span class="math inline">\(\Sigma\)</span> is an <span class="math inline">\(p \times p\)</span>, a nonsingular symmetric covariance matrix. Hence, we have the following form for the density function:</p>
<p><span class="math display">\[
f(x \mid \mu, \Sigma)=\frac{1}{(2 \pi)^{p / 2}|\Sigma|^{1 / 2}} \exp \left\{-\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)\right\},
\]</span>
where <span class="math inline">\(|\Sigma|\)</span> is the determinant of the covariance matrix. The likelihood function is:</p>
<p><span class="math display">\[
\mathcal{L}(\mu, \Sigma)=(2 \pi)^{-\frac{n p}{2}} \prod_{i=1}^{n} \operatorname{det}(\Sigma)^{-\frac{1}{2}} \exp \left(-\frac{1}{2}\left(x_{i}-\mu\right)^{\mathrm{T}} \Sigma^{-1}\left(x_{i}-\mu\right)\right)
\]</span>
Since the estimate <span class="math inline">\(\bar{x}\)</span> does not depend on <span class="math inline">\(\Sigma\)</span>, we can just substitute it for <span class="math inline">\(\mu\)</span> in the likelihood function,</p>
<p><span class="math display">\[
\mathcal{L}(\bar{x}, \Sigma) \propto \operatorname{det}(\Sigma)^{-\frac{n}{2}} \exp \left(-\frac{1}{2} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{\mathrm{T}} \Sigma^{-1}\left(x_{i}-\bar{x}\right)\right)
\]</span></p>
<p>We seek the value of <span class="math inline">\(\Sigma\)</span> that maximizes the likelihood of the data (in practice it is easier to work with <span class="math inline">\(\log \mathcal{L}\)</span> ). With the cyclical nature of trace,</p>
<p><span class="math display">\[
\begin{aligned}
\mathcal{L}(\bar{x}, \Sigma) &amp; \propto \operatorname{det}(\Sigma)^{-\frac{n}{2}} \exp \left(-\frac{1}{2} \sum_{i=1}^{n}\left(\left(x_{i}-\bar{x}\right)^{\mathrm{T}} \Sigma^{-1}\left(x_{i}-\bar{x}\right)\right)\right) \\
&amp;=\operatorname{det}(\Sigma)^{-\frac{n}{2}} \exp \left(-\frac{1}{2} \sum_{i=1}^{n} \operatorname{tr}\left(\left(x_{i}-\bar{x}\right)\left(x_{i}-\bar{x}\right)^{\mathrm{T}} \Sigma^{-1}\right)\right) \\
&amp;=\operatorname{det}(\Sigma)^{-\frac{n}{2}} \exp \left(-\frac{1}{2} \operatorname{tr}\left(\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(x_{i}-\bar{x}\right)^{\mathrm{T}} \Sigma^{-1}\right)\right) \\
&amp;=\operatorname{det}(\Sigma)^{-\frac{n}{2}} \exp \left(-\frac{1}{2} \operatorname{tr}\left(S \Sigma^{-1}\right)\right)
\end{aligned}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
S=\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(x_{i}-\bar{x}\right)^{\mathrm{T}} \in \mathbf{R}^{p \times p}
\]</span></p>
<p>And finally, we re-write the likelihood in the log form using the trace trick:</p>
<p><span class="math display">\[
\ln \mathcal{L}(\mu, \Sigma)=\text { const }-\frac{n}{2} \ln \operatorname{det}(\Sigma)-\frac{1}{2} \operatorname{tr}\left[\Sigma^{-1} \sum_{i=1}^{n}\left(x_{i}-\mu\right)\left(x_{i}-\mu\right)^{\mathrm{T}}\right]
\]</span></p>
<p>or, for a multivariate normal model with mean 0 and covariance <span class="math inline">\(\Sigma\)</span>, the likelihood function in this case is given by</p>
<p><span class="math display">\[
\ell(\Omega ; S)=\ln |\Omega|-\operatorname{tr}(S \Omega)
\]</span></p>
<p>where <span class="math inline">\(\Omega=\Sigma^{-1}\)</span> is the so-called precision matrix (also sometimes called the concentration matrix), which we want to estimate, which we will denote <span class="math inline">\(P\)</span>. Indeed, one can naturally try to use the inverse of <span class="math inline">\(S\)</span> for this.</p>
<p>For an intuitive way to see the whole algebra, letâs start with the general normal density</p>
<p><span class="math display">\[
\frac{1}{\sqrt{2 \pi}} \frac{1}{\sigma} \exp \left(-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}\right)
\]</span>
The log-likelihood is
<span class="math display">\[
\mathcal{L}(\mu, \sigma)=\text { A constant }-\frac{n}{2} \log \left(\sigma^{2}\right)-\frac{1}{2} \sum_{i=1}^{n}\left(\frac{x_{i}-\mu}{\sigma}\right)^{2},
\]</span>
maximization of which is equivalent to minimizing</p>
<p><span class="math display">\[
\mathcal{L}(\mu, \sigma)=n \log \left(\sigma^{2}\right)+\sum_{i=1}^{n}\left(\frac{x_{i}-\mu}{\sigma}\right)^{2}
\]</span></p>
<p>We can look at the general multivariate normal (MVN) density</p>
<p><span class="math display">\[
(\sqrt{2 \pi})^{-d}|\boldsymbol{\Sigma}|^{-1 / 2} \exp \left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{t} \mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right)
\]</span></p>
<p>Note that <span class="math inline">\(|\boldsymbol{\Sigma}|^{-1 / 2}\)</span>, which is the reciprocal of the square root of the determinant of the covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span>, does what <span class="math inline">\(1 / \sigma\)</span> does in the univariate case. Moreover, <span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span> does what <span class="math inline">\(1 / \sigma^{2}\)</span> does in the univariate case.</p>
<p>The maximization of likelihood would lead to minimizing (analogous to the univariate case)</p>
<p><span class="math display">\[
n \log |\boldsymbol{\Sigma}|+\sum_{i=1}^{n}(\mathbf{x}-\boldsymbol{\mu})^{t} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})
\]</span></p>
<p>Again, <span class="math inline">\(n \log |\mathbf{\Sigma}|\)</span> takes the spot of <span class="math inline">\(n \log \left(\sigma^{2}\right)\)</span> which was there in the univariate case.</p>
<p>If the data is not high-dimensional, the estimations are simple. Letâs start with a data matrix of 10x6, where no need for regularization.</p>
<div class="sourceCode" id="cb1164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1164-1"><a href="regularized-covariance-matrix.html#cb1164-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">10</span></span>
<span id="cb1164-2"><a href="regularized-covariance-matrix.html#cb1164-2" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="dv">6</span></span>
<span id="cb1164-3"><a href="regularized-covariance-matrix.html#cb1164-3" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span> (<span class="fu">rnorm</span>(n<span class="sc">*</span>p), n, p)</span>
<span id="cb1164-4"><a href="regularized-covariance-matrix.html#cb1164-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1164-5"><a href="regularized-covariance-matrix.html#cb1164-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Cov. &amp; Precision Matrices</span></span>
<span id="cb1164-6"><a href="regularized-covariance-matrix.html#cb1164-6" aria-hidden="true" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="fu">cov</span>(X)</span>
<span id="cb1164-7"><a href="regularized-covariance-matrix.html#cb1164-7" aria-hidden="true" tabindex="-1"></a>pm <span class="ot">&lt;-</span> <span class="fu">solve</span>(S) <span class="co"># precision</span></span>
<span id="cb1164-8"><a href="regularized-covariance-matrix.html#cb1164-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1164-9"><a href="regularized-covariance-matrix.html#cb1164-9" aria-hidden="true" tabindex="-1"></a><span class="sc">-</span>pm[<span class="dv">1</span>,<span class="dv">2</span>]<span class="sc">/</span>(<span class="fu">sqrt</span>(pm[<span class="dv">1</span>,<span class="dv">1</span>])<span class="sc">*</span><span class="fu">sqrt</span>(pm[<span class="dv">2</span>,<span class="dv">2</span>])) </span></code></pre></div>
<pre><code>## [1] -0.2624583</code></pre>
<div class="sourceCode" id="cb1166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1166-1"><a href="regularized-covariance-matrix.html#cb1166-1" aria-hidden="true" tabindex="-1"></a><span class="sc">-</span><span class="fu">cov2cor</span>(pm)</span></code></pre></div>
<pre><code>##             [,1]       [,2]        [,3]       [,4]       [,5]        [,6]
## [1,] -1.00000000 -0.2624583 -0.07539084 -0.0243017  0.2806425  0.07722534
## [2,] -0.26245832 -1.0000000 -0.20914601  0.6093819  0.8705547  0.92841840
## [3,] -0.07539084 -0.2091460 -1.00000000  0.5038206  0.3854259  0.17786621
## [4,] -0.02430170  0.6093819  0.50382057 -1.0000000 -0.7077681 -0.59671523
## [5,]  0.28064253  0.8705547  0.38542591 -0.7077681 -1.0000000 -0.81834859
## [6,]  0.07722534  0.9284184  0.17786621 -0.5967152 -0.8183486 -1.00000000</code></pre>
<div class="sourceCode" id="cb1168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1168-1"><a href="regularized-covariance-matrix.html#cb1168-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ppcor</span></span>
<span id="cb1168-2"><a href="regularized-covariance-matrix.html#cb1168-2" aria-hidden="true" tabindex="-1"></a>pc <span class="ot">&lt;-</span> ppcor<span class="sc">::</span><span class="fu">pcor</span>(X)</span>
<span id="cb1168-3"><a href="regularized-covariance-matrix.html#cb1168-3" aria-hidden="true" tabindex="-1"></a>pc<span class="sc">$</span>estimate</span></code></pre></div>
<pre><code>##             [,1]       [,2]        [,3]       [,4]       [,5]        [,6]
## [1,]  1.00000000 -0.2624583 -0.07539084 -0.0243017  0.2806425  0.07722534
## [2,] -0.26245832  1.0000000 -0.20914601  0.6093819  0.8705547  0.92841840
## [3,] -0.07539084 -0.2091460  1.00000000  0.5038206  0.3854259  0.17786621
## [4,] -0.02430170  0.6093819  0.50382057  1.0000000 -0.7077681 -0.59671523
## [5,]  0.28064253  0.8705547  0.38542591 -0.7077681  1.0000000 -0.81834859
## [6,]  0.07722534  0.9284184  0.17786621 -0.5967152 -0.8183486  1.00000000</code></pre>
<div class="sourceCode" id="cb1170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1170-1"><a href="regularized-covariance-matrix.html#cb1170-1" aria-hidden="true" tabindex="-1"></a><span class="co"># glasso</span></span>
<span id="cb1170-2"><a href="regularized-covariance-matrix.html#cb1170-2" aria-hidden="true" tabindex="-1"></a>glassoFast<span class="sc">::</span><span class="fu">glassoFast</span>(S,<span class="at">rho=</span><span class="dv">0</span>)</span></code></pre></div>
<pre><code>## $w
##             [,1]       [,2]        [,3]       [,4]       [,5]        [,6]
## [1,]  0.96488347 -0.3669924 -0.05084732 -0.3942968  0.3425267 -0.60817759
## [2,] -0.36699244  0.8628771  0.10206296  0.1145241  0.4109566  0.97482805
## [3,] -0.05084732  0.1020630  0.84126394  0.4221780  0.2838247 -0.09339049
## [4,] -0.39429677  0.1145241  0.42217800  1.5438364 -0.5940417  0.07396640
## [5,]  0.34252671  0.4109566  0.28382471 -0.5940417  1.7049602 -0.27314141
## [6,] -0.60817759  0.9748281 -0.09339049  0.0739664 -0.2731414  1.76560301
## 
## $wi
##             [,1]      [,2]       [,3]        [,4]       [,5]       [,6]
## [1,]  1.63072965  1.251464  0.1281174  0.04057159 -0.6730262 -0.2282176
## [2,]  1.25146372 13.949565  1.0396007 -2.96449098 -6.1069996 -8.0353859
## [3,]  0.12811735  1.039601  1.7731286 -0.87381872 -0.9637300 -0.5482904
## [4,]  0.04057159 -2.964491 -0.8738187  1.69695224  1.7315820  1.8009499
## [5,] -0.67302623 -6.107000 -0.9637300  1.73158197  3.5280253  3.5617146
## [6,] -0.22821759 -8.035386 -0.5482904  1.80094988  3.5617146  5.3702153
## 
## $errflag
## [1] 0
## 
## $niter
## [1] 1</code></pre>
<div class="sourceCode" id="cb1172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1172-1"><a href="regularized-covariance-matrix.html#cb1172-1" aria-hidden="true" tabindex="-1"></a>Rl <span class="ot">&lt;-</span> glassoFast<span class="sc">::</span><span class="fu">glassoFast</span>(S,<span class="at">rho=</span><span class="dv">0</span>)<span class="sc">$</span>wi <span class="co">#</span></span>
<span id="cb1172-2"><a href="regularized-covariance-matrix.html#cb1172-2" aria-hidden="true" tabindex="-1"></a><span class="sc">-</span>Rl[<span class="dv">1</span>,<span class="dv">2</span>]<span class="sc">/</span>(<span class="fu">sqrt</span>(Rl[<span class="dv">1</span>,<span class="dv">1</span>])<span class="sc">*</span><span class="fu">sqrt</span>(Rl[<span class="dv">2</span>,<span class="dv">2</span>])) </span></code></pre></div>
<pre><code>## [1] -0.2623898</code></pre>
<div class="sourceCode" id="cb1174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1174-1"><a href="regularized-covariance-matrix.html#cb1174-1" aria-hidden="true" tabindex="-1"></a><span class="sc">-</span><span class="fu">cov2cor</span>(Rl)</span></code></pre></div>
<pre><code>##             [,1]       [,2]        [,3]        [,4]       [,5]        [,6]
## [1,] -1.00000000 -0.2623898 -0.07534368 -0.02438913  0.2805919  0.07711916
## [2,] -0.26238980 -1.0000000 -0.20903361  0.60930535  0.8705259  0.92839024
## [3,] -0.07534368 -0.2090336 -1.00000000  0.50375158  0.3853181  0.17768250
## [4,] -0.02438913  0.6093054  0.50375158 -1.00000000 -0.7076889 -0.59658313
## [5,]  0.28059192  0.8705259  0.38531810 -0.70768892 -1.0000000 -0.81827150
## [6,]  0.07711916  0.9283902  0.17768250 -0.59658313 -0.8182715 -1.00000000</code></pre>
</div>
<div id="high-dimensional-data" class="section level2 hasAnchor" number="33.2">
<h2><span class="header-section-number">33.2</span> High-dimensional data<a href="regularized-covariance-matrix.html#high-dimensional-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now with a data matrix of 6x10:</p>
<div class="sourceCode" id="cb1176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1176-1"><a href="regularized-covariance-matrix.html#cb1176-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">6</span></span>
<span id="cb1176-2"><a href="regularized-covariance-matrix.html#cb1176-2" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="dv">10</span></span>
<span id="cb1176-3"><a href="regularized-covariance-matrix.html#cb1176-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb1176-4"><a href="regularized-covariance-matrix.html#cb1176-4" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span> (<span class="fu">rnorm</span>(n<span class="sc">*</span>p), n, p)</span>
<span id="cb1176-5"><a href="regularized-covariance-matrix.html#cb1176-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1176-6"><a href="regularized-covariance-matrix.html#cb1176-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Cov. &amp; Precision Matrices</span></span>
<span id="cb1176-7"><a href="regularized-covariance-matrix.html#cb1176-7" aria-hidden="true" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="fu">cov</span>(X)</span>
<span id="cb1176-8"><a href="regularized-covariance-matrix.html#cb1176-8" aria-hidden="true" tabindex="-1"></a>S</span></code></pre></div>
<pre><code>##               [,1]        [,2]        [,3]        [,4]         [,5]        [,6]
##  [1,]  0.889211221 -0.17223814 -0.36660043  0.35320957 -0.629545741 -0.27978848
##  [2,] -0.172238139  0.34416306 -0.09280183 -0.04282613  0.139236591 -0.26060435
##  [3,] -0.366600426 -0.09280183  1.46701338 -0.50796342 -0.024550727 -0.11504405
##  [4,]  0.353209573 -0.04282613 -0.50796342  1.24117592 -0.292005017  0.42646139
##  [5,] -0.629545741  0.13923659 -0.02455073 -0.29200502  0.553562287  0.26275658
##  [6,] -0.279788479 -0.26060435 -0.11504405  0.42646139  0.262756584  0.81429052
##  [7,]  0.143364328 -0.14895377  0.29598156  0.30839120 -0.275296303  0.04418159
##  [8,] -0.273835576  0.17201439 -0.31052657 -0.39667581  0.376175973 -0.02536104
##  [9,] -0.008919669  0.24390178 -0.50198614  0.52741301  0.008044799 -0.01297542
## [10,] -0.304722895  0.33936685 -1.08854590  0.20441696  0.499437080  0.20218868
##              [,7]        [,8]         [,9]      [,10]
##  [1,]  0.14336433 -0.27383558 -0.008919669 -0.3047229
##  [2,] -0.14895377  0.17201439  0.243901782  0.3393668
##  [3,]  0.29598156 -0.31052657 -0.501986137 -1.0885459
##  [4,]  0.30839120 -0.39667581  0.527413006  0.2044170
##  [5,] -0.27529630  0.37617597  0.008044799  0.4994371
##  [6,]  0.04418159 -0.02536104 -0.012975416  0.2021887
##  [7,]  0.37576405 -0.40476558  0.046294293 -0.4691147
##  [8,] -0.40476558  0.46612332 -0.026813818  0.5588965
##  [9,]  0.04629429 -0.02681382  0.540956259  0.5036908
## [10,] -0.46911465  0.55889647  0.503690786  1.3107637</code></pre>
<div class="sourceCode" id="cb1178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1178-1"><a href="regularized-covariance-matrix.html#cb1178-1" aria-hidden="true" tabindex="-1"></a><span class="fu">try</span>(<span class="fu">solve</span>(S), <span class="at">silent =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## Error in solve.default(S) : 
##   system is computationally singular: reciprocal condition number = 3.99819e-19</code></pre>
<p>The standard definition for the inverse of a matrix fails if the matrix is not square or singular. However, one can generalize the inverse using singular value decomposition. Any rectangular real matrix <span class="math inline">\(\mathbf{M}\)</span> can be decomposed as <span class="math inline">\(\mathbf{M=U \Sigma V^{&#39;}}\)</span>, where <span class="math inline">\(\mathbf{U}\)</span> and <span class="math inline">\(\mathbf{V}\)</span> are orthogonal and <span class="math inline">\(\mathbf{D}\)</span> is a diagonal matrix containing only the positive singular values. The pseudoinverse, also known as <strong>Moore-Penrose</strong> or generalized inverse is then obtained as</p>
<p><span class="math display">\[
\mathbf{M^+} = \mathbf{V \Sigma^{-1} U&#39;}
\]</span></p>
<p>Donât be confused due to notation: <span class="math inline">\(\Sigma\)</span> is not the covariance matrix here</p>
<p>With using the method of generalized inverse by <code>ppcor</code> and <code>corpcor</code>:</p>
<div class="sourceCode" id="cb1180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1180-1"><a href="regularized-covariance-matrix.html#cb1180-1" aria-hidden="true" tabindex="-1"></a>Si <span class="ot">&lt;-</span> corpcor<span class="sc">::</span><span class="fu">pseudoinverse</span>(S)</span>
<span id="cb1180-2"><a href="regularized-covariance-matrix.html#cb1180-2" aria-hidden="true" tabindex="-1"></a><span class="sc">-</span>Si[<span class="dv">1</span>,<span class="dv">2</span>]<span class="sc">/</span>(<span class="fu">sqrt</span>(Si[<span class="dv">1</span>,<span class="dv">1</span>])<span class="sc">*</span><span class="fu">sqrt</span>(Si[<span class="dv">2</span>,<span class="dv">2</span>])) </span></code></pre></div>
<pre><code>## [1] -0.4823509</code></pre>
<div class="sourceCode" id="cb1182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1182-1"><a href="regularized-covariance-matrix.html#cb1182-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ppcor</span></span>
<span id="cb1182-2"><a href="regularized-covariance-matrix.html#cb1182-2" aria-hidden="true" tabindex="-1"></a>pc <span class="ot">&lt;-</span> ppcor<span class="sc">::</span><span class="fu">pcor</span>(X)</span></code></pre></div>
<pre><code>## Warning in ppcor::pcor(X): The inverse of variance-covariance matrix is
## calculated using Moore-Penrose generalized matrix invers due to its determinant
## of zero.</code></pre>
<pre><code>## Warning in sqrt((n - 2 - gp)/(1 - pcor^2)): NaNs produced</code></pre>
<div class="sourceCode" id="cb1185"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1185-1"><a href="regularized-covariance-matrix.html#cb1185-1" aria-hidden="true" tabindex="-1"></a>pc<span class="sc">$</span>estimate</span></code></pre></div>
<pre><code>##              [,1]        [,2]        [,3]       [,4]        [,5]          [,6]
##  [1,]  1.00000000 -0.48235089 -0.43471080 -0.6132218  0.59239395 -0.1515785108
##  [2,] -0.48235089  1.00000000 -0.85835176 -0.7984656  0.08341783  0.1922476120
##  [3,] -0.43471080 -0.85835176  1.00000000 -0.8107355 -0.06073205 -0.1395456329
##  [4,] -0.61322177 -0.79846556 -0.81073546  1.0000000  0.11814582 -0.3271223659
##  [5,]  0.59239395  0.08341783 -0.06073205  0.1181458  1.00000000 -0.4056046405
##  [6,] -0.15157851  0.19224761 -0.13954563 -0.3271224 -0.40560464  1.0000000000
##  [7,]  0.81227748  0.76456650  0.76563183  0.7861380 -0.07927500  0.2753626258
##  [8,] -0.74807903 -0.67387820 -0.64812735 -0.6321303 -0.04063566 -0.2660628754
##  [9,]  0.79435763  0.32542381  0.52481792  0.5106454 -0.08284875  0.5458020595
## [10,]  0.01484899 -0.34289348  0.01425498 -0.2181704 -0.41275254  0.0006582396
##             [,7]        [,8]        [,9]         [,10]
##  [1,]  0.8122775 -0.74807903  0.79435763  0.0148489929
##  [2,]  0.7645665 -0.67387820  0.32542381 -0.3428934821
##  [3,]  0.7656318 -0.64812735  0.52481792  0.0142549759
##  [4,]  0.7861380 -0.63213032  0.51064540 -0.2181703890
##  [5,] -0.0792750 -0.04063566 -0.08284875 -0.4127525424
##  [6,]  0.2753626 -0.26606288  0.54580206  0.0006582396
##  [7,]  1.0000000  0.96888026 -0.84167300  0.2703213517
##  [8,]  0.9688803  1.00000000  0.84455999 -0.3746342510
##  [9,] -0.8416730  0.84455999  1.00000000 -0.0701428715
## [10,]  0.2703214 -0.37463425 -0.07014287  1.0000000000</code></pre>
<div class="sourceCode" id="cb1187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1187-1"><a href="regularized-covariance-matrix.html#cb1187-1" aria-hidden="true" tabindex="-1"></a><span class="co"># corpcor with pseudo inverse</span></span>
<span id="cb1187-2"><a href="regularized-covariance-matrix.html#cb1187-2" aria-hidden="true" tabindex="-1"></a>corpcor<span class="sc">::</span><span class="fu">cor2pcor</span>(S)</span></code></pre></div>
<pre><code>##              [,1]        [,2]        [,3]       [,4]        [,5]          [,6]
##  [1,]  1.00000000 -0.48235089 -0.43471080 -0.6132218  0.59239395 -0.1515785108
##  [2,] -0.48235089  1.00000000 -0.85835176 -0.7984656  0.08341783  0.1922476120
##  [3,] -0.43471080 -0.85835176  1.00000000 -0.8107355 -0.06073205 -0.1395456329
##  [4,] -0.61322177 -0.79846556 -0.81073546  1.0000000  0.11814582 -0.3271223659
##  [5,]  0.59239395  0.08341783 -0.06073205  0.1181458  1.00000000 -0.4056046405
##  [6,] -0.15157851  0.19224761 -0.13954563 -0.3271224 -0.40560464  1.0000000000
##  [7,]  0.81227748  0.76456650  0.76563183  0.7861380 -0.07927500  0.2753626258
##  [8,] -0.74807903 -0.67387820 -0.64812735 -0.6321303 -0.04063566 -0.2660628754
##  [9,]  0.79435763  0.32542381  0.52481792  0.5106454 -0.08284875  0.5458020595
## [10,]  0.01484899 -0.34289348  0.01425498 -0.2181704 -0.41275254  0.0006582396
##             [,7]        [,8]        [,9]         [,10]
##  [1,]  0.8122775 -0.74807903  0.79435763  0.0148489929
##  [2,]  0.7645665 -0.67387820  0.32542381 -0.3428934821
##  [3,]  0.7656318 -0.64812735  0.52481792  0.0142549759
##  [4,]  0.7861380 -0.63213032  0.51064540 -0.2181703890
##  [5,] -0.0792750 -0.04063566 -0.08284875 -0.4127525424
##  [6,]  0.2753626 -0.26606288  0.54580206  0.0006582396
##  [7,]  1.0000000  0.96888026 -0.84167300  0.2703213517
##  [8,]  0.9688803  1.00000000  0.84455999 -0.3746342510
##  [9,] -0.8416730  0.84455999  1.00000000 -0.0701428715
## [10,]  0.2703214 -0.37463425 -0.07014287  1.0000000000</code></pre>
<p>However, we know from Chapter 29 that these solutions are not stable. Further, we also want to identify the sparsity in the precision matrix that differentiates the significant edges from insignificant ones for a network analysis .</p>
</div>
<div id="ridge-ell_2-and-glasso-ell_1" class="section level2 hasAnchor" number="33.3">
<h2><span class="header-section-number">33.3</span> Ridge (<span class="math inline">\(\ell_{2}\)</span>) and glasso (<span class="math inline">\(\ell_{1}\)</span>)<a href="regularized-covariance-matrix.html#ridge-ell_2-and-glasso-ell_1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A contemporary use for precision matrices is found in network reconstruction through graphical modeling (Network Analysis).</p>
<p>In a multivariate normal model, <span class="math inline">\(p_{i j}=p_{j i}=0\)</span> (the entries in the precision matrix) if and only if <span class="math inline">\(X_{i}\)</span> and <span class="math inline">\(X_{j}\)</span> are independent when condition ong all other variables. In real world applications, <span class="math inline">\(P\)</span> (the precision matrix) is often relatively sparse with lots of zeros. With the close relationship between <span class="math inline">\(P\)</span> and the partial correlations, <strong>the non-zero entries of the precision matrix can be interpreted the edges of a graph where nodes correspond to the variables.</strong></p>
<p>Regularization helps us find the sparsified partial correlation matrix. We first start with Ridge and <code>rags2ridges</code> (see, <a href="https://cran.r-project.org/web/packages/rags2ridges/vignettes/rags2ridges.html">Introduction to rags2ridges</a>), which is an R-package for fast and proper <span class="math inline">\(\ell_{2}\)</span>-penalized estimation of precision (and covariance) matrices also called ridge estimation.</p>
<p>Their algorithm solves the following:</p>
<p><span class="math display">\[
\ell(\Omega ; S)=\ln |\Omega|-\operatorname{tr}(S \Omega)-\frac{\lambda}{2}\|\Omega-T\|_{2}^{2}
\]</span></p>
<p>where <span class="math inline">\(\lambda&gt;0\)</span> is the ridge penalty parameter, <span class="math inline">\(T\)</span> is a <span class="math inline">\(p \times p\)</span> known target matrix and <span class="math inline">\(\|\cdot\|_{2}\)</span> is the <span class="math inline">\(\ell_{2}\)</span>-norm. Assume for now the target matrix is an all zero matrix and thus out of the equation. The core function of <code>rags2ridges</code> is <code>ridgeP</code> which computes this estimate in a fast manner.</p>
<p>Letâs try some simulations:</p>
<div class="sourceCode" id="cb1189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1189-1"><a href="regularized-covariance-matrix.html#cb1189-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rags2ridges)</span>
<span id="cb1189-2"><a href="regularized-covariance-matrix.html#cb1189-2" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">6</span></span>
<span id="cb1189-3"><a href="regularized-covariance-matrix.html#cb1189-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb1189-4"><a href="regularized-covariance-matrix.html#cb1189-4" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">createS</span>(<span class="at">n =</span> n, <span class="at">p =</span> p, <span class="at">dataset =</span> <span class="cn">TRUE</span>)</span>
<span id="cb1189-5"><a href="regularized-covariance-matrix.html#cb1189-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1189-6"><a href="regularized-covariance-matrix.html#cb1189-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Cov. &amp; Precision Matrices</span></span>
<span id="cb1189-7"><a href="regularized-covariance-matrix.html#cb1189-7" aria-hidden="true" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="fu">cov</span>(X)</span>
<span id="cb1189-8"><a href="regularized-covariance-matrix.html#cb1189-8" aria-hidden="true" tabindex="-1"></a>S</span></code></pre></div>
<pre><code>##             A           B           C           D           E          F
## A  0.45682789 -0.11564467  0.13200583 -0.01595920  0.09809975 0.01702341
## B -0.11564467  0.55871526 -0.06301115  0.12714447  0.16007573 0.01767518
## C  0.13200583 -0.06301115  0.85789870 -0.03128875 -0.05379863 0.13134788
## D -0.01595920  0.12714447 -0.03128875  0.99469250  0.03927349 0.10959642
## E  0.09809975  0.16007573 -0.05379863  0.03927349  0.91136419 0.02529372
## F  0.01702341  0.01767518  0.13134788  0.10959642  0.02529372 1.27483389</code></pre>
<div class="sourceCode" id="cb1191"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1191-1"><a href="regularized-covariance-matrix.html#cb1191-1" aria-hidden="true" tabindex="-1"></a><span class="fu">try</span>(<span class="fu">solve</span>(S), <span class="at">silent =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>##              A           B           C           D           E            F
## A  2.534157787  0.60434887 -0.37289623 -0.03330977 -0.39969959  0.006995153
## B  0.604348874  2.09324877  0.02734562 -0.23919383 -0.42049199 -0.011003651
## C -0.372896230  0.02734562  1.25338509  0.03989939  0.11121753 -0.130174434
## D -0.033309770 -0.23919383  0.03989939  1.04638061  0.00537128 -0.090412788
## E -0.399699586 -0.42049199  0.11121753  0.00537128  1.22116416 -0.024982169
## F  0.006995153 -0.01100365 -0.13017443 -0.09041279 -0.02498217  0.806155504</code></pre>
<div class="sourceCode" id="cb1193"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1193-1"><a href="regularized-covariance-matrix.html#cb1193-1" aria-hidden="true" tabindex="-1"></a>P <span class="ot">&lt;-</span> rags2ridges<span class="sc">::</span><span class="fu">ridgeP</span>(S, <span class="at">lambda =</span> <span class="fl">0.0001</span>)</span>
<span id="cb1193-2"><a href="regularized-covariance-matrix.html#cb1193-2" aria-hidden="true" tabindex="-1"></a>P</span></code></pre></div>
<pre><code>## A 6 x 6 ridge precision matrix estimate with lambda = 0.000100
##              A           B           C            D            E            F
## A  2.533115451  0.60366542 -0.37265274 -0.033220588 -0.399324682  0.006973044
## B  0.603665423  2.09268463  0.02745898 -0.239097683 -0.420206304 -0.011013671
## C -0.372652744  0.02745898  1.25336484  0.039885330  0.111143219 -0.130167968
## D -0.033220588 -0.23909768  0.03988533  1.046411061  0.005328985 -0.090412858
## E -0.399324682 -0.42020630  0.11114322  0.005328985  1.221068947 -0.024975879
## F  0.006973044 -0.01101367 -0.13016797 -0.090412858 -0.024975879  0.806196516</code></pre>
<div class="sourceCode" id="cb1195"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1195-1"><a href="regularized-covariance-matrix.html#cb1195-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rags2ridges)</span>
<span id="cb1195-2"><a href="regularized-covariance-matrix.html#cb1195-2" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">25</span></span>
<span id="cb1195-3"><a href="regularized-covariance-matrix.html#cb1195-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb1195-4"><a href="regularized-covariance-matrix.html#cb1195-4" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">createS</span>(<span class="at">n =</span> n, <span class="at">p =</span> p, <span class="at">dataset =</span> <span class="cn">TRUE</span>)</span>
<span id="cb1195-5"><a href="regularized-covariance-matrix.html#cb1195-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1195-6"><a href="regularized-covariance-matrix.html#cb1195-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Cov. &amp; Precision Matrices</span></span>
<span id="cb1195-7"><a href="regularized-covariance-matrix.html#cb1195-7" aria-hidden="true" tabindex="-1"></a>S <span class="ot">&lt;-</span> <span class="fu">cov</span>(X)</span>
<span id="cb1195-8"><a href="regularized-covariance-matrix.html#cb1195-8" aria-hidden="true" tabindex="-1"></a><span class="fu">try</span>(<span class="fu">solve</span>(S), <span class="at">silent =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## Error in solve.default(S) : 
##   system is computationally singular: reciprocal condition number = 2.65379e-19</code></pre>
<div class="sourceCode" id="cb1197"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1197-1"><a href="regularized-covariance-matrix.html#cb1197-1" aria-hidden="true" tabindex="-1"></a>P <span class="ot">&lt;-</span> rags2ridges<span class="sc">::</span><span class="fu">ridgeP</span>(S, <span class="at">lambda =</span> <span class="fl">1.17</span>)</span>
<span id="cb1197-2"><a href="regularized-covariance-matrix.html#cb1197-2" aria-hidden="true" tabindex="-1"></a>P[<span class="dv">1</span><span class="sc">:</span><span class="dv">7</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">7</span>]</span></code></pre></div>
<pre><code>##              A           B           C            D          E           F
## A  2.743879476 -0.03541676 -0.01830371  0.008774811 -0.1056438  0.01539484
## B -0.035416755  2.63060175  0.23945569  0.088696164 -0.2786984 -0.29657059
## C -0.018303709  0.23945569  2.55818158 -0.092298329  0.1512445  0.08314785
## D  0.008774811  0.08869616 -0.09229833  2.373307290  0.3717918  0.01829917
## E -0.105643841 -0.27869839  0.15124449  0.371791841  2.3048669 -0.32627382
## F  0.015394836 -0.29657059  0.08314785  0.018299166 -0.3262738  2.79070578
## G -0.059760460 -0.18022734 -0.08924614 -0.149071791 -0.1574611  0.06178467
##             G
## A -0.05976046
## B -0.18022734
## C -0.08924614
## D -0.14907179
## E -0.15746109
## F  0.06178467
## G  2.61837378</code></pre>
<p>What Lambda should we choose? One strategy for choosing <span class="math inline">\(\lambda\)</span> is selecting it to be stable yet precise (a bias-variance trade-off). Automatic k-fold cross-validation can be done with <code>optPenalty.kCVauto()</code> is well suited for this.</p>
<div class="sourceCode" id="cb1199"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1199-1"><a href="regularized-covariance-matrix.html#cb1199-1" aria-hidden="true" tabindex="-1"></a>opt <span class="ot">&lt;-</span> <span class="fu">optPenalty.kCVauto</span>(X, <span class="at">lambdaMin =</span> <span class="fl">0.001</span>, <span class="at">lambdaMax =</span> <span class="dv">100</span>)</span>
<span id="cb1199-2"><a href="regularized-covariance-matrix.html#cb1199-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(opt)</span></code></pre></div>
<pre><code>## List of 2
##  $ optLambda: num 0.721
##  $ optPrec  : &#39;ridgeP&#39; num [1:25, 1:25] 2.7894 -0.0521 -0.0324 0.0211 -0.1459 ...
##   ..- attr(*, &quot;lambda&quot;)= num 0.721
##   ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..$ : chr [1:25] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; ...
##   .. ..$ : chr [1:25] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; ...</code></pre>
<div class="sourceCode" id="cb1201"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1201-1"><a href="regularized-covariance-matrix.html#cb1201-1" aria-hidden="true" tabindex="-1"></a>op <span class="ot">&lt;-</span> opt<span class="sc">$</span>optLambda</span></code></pre></div>
<p>We know that Ridge will not provide a sparse solution. Yet, we need a sparse precision matrix for network analysis. The <span class="math inline">\(\ell_{2}\)</span> penalty of <code>rags2ridges</code> relies on an extra step that selects the edges after the precision matrix is estimated. The extra step is explained in their <a href="https://www.sciencedirect.com/science/article/pii/S0167947316301141">paper</a> (<span class="citation">(<a href="#ref-VANWIERINGEN2016284" role="doc-biblioref">van Wieringen and Peeters 2016</a>)</span>):</p>
<blockquote>
<p>While some may argue this as a drawback (typically due to a lack of perceived simplicity), it is often beneficial to separate the âvariable selectionâ and estimation.</p>
</blockquote>
<blockquote>
<p>First, a separate post-hoc selection step allows for greater flexibility. Secondly, when co-linearity is present the L1 penalty is âunstableâ in the selection between the items, i.e, if 2 covariances are co-linear only one of them will typically be selected in a unpredictable way whereas the L2 will put equal weight on both and âaverageâ their effect. Ultimately, this means that the L2 estimate is typically more stable than the L1.</p>
</blockquote>
<blockquote>
<p>At last point to mention here is also that the true underlying graph might not always be very sparse (or sparse at all).</p>
</blockquote>
<p>The function <code>spasify()</code> handles the the spasification by applying the FDR (False Discovery Rate) method:</p>
<div class="sourceCode" id="cb1202"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1202-1"><a href="regularized-covariance-matrix.html#cb1202-1" aria-hidden="true" tabindex="-1"></a>P <span class="ot">&lt;-</span> <span class="fu">ridgeP</span>(S, <span class="at">lambda =</span> op)</span>
<span id="cb1202-2"><a href="regularized-covariance-matrix.html#cb1202-2" aria-hidden="true" tabindex="-1"></a>spar <span class="ot">&lt;-</span> <span class="fu">sparsify</span>(P, <span class="at">threshold =</span> <span class="st">&quot;localFDR&quot;</span>)</span></code></pre></div>
<pre><code>## Step 1... determine cutoff point
## Step 2... estimate parameters of null distribution and eta0
## Step 3... compute p-values and estimate empirical PDF/CDF
## Step 4... compute q-values and local fdr
## Step 5... prepare for plotting</code></pre>
<p><img src="30-RegularizedCovMatrix_files/figure-html/rn7-1.png" width="672" /></p>
<pre><code>## 
## - Retained elements:  0 
## - Corresponding to 0 % of possible edges 
## </code></pre>
<div class="sourceCode" id="cb1205"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1205-1"><a href="regularized-covariance-matrix.html#cb1205-1" aria-hidden="true" tabindex="-1"></a>spar</span></code></pre></div>
<pre><code>## $sparseParCor
## A 25 x 25 ridge precision matrix estimate with lambda = 0.721308
##   A B C D E F â¦
## A 1 0 0 0 0 0 â¦
## B 0 1 0 0 0 0 â¦
## C 0 0 1 0 0 0 â¦
## D 0 0 0 1 0 0 â¦
## E 0 0 0 0 1 0 â¦
## F 0 0 0 0 0 1 â¦
## â¦ 19 more rows and 19 more columns
## 
## $sparsePrecision
## A 25 x 25 ridge precision matrix estimate with lambda = 0.721308
##          A        B        C        D        E        F â¦
## A 2.626295 0.000000 0.000000 0.000000 0.000000 0.000000 â¦
## B 0.000000 2.528829 0.000000 0.000000 0.000000 0.000000 â¦
## C 0.000000 0.000000 2.409569 0.000000 0.000000 0.000000 â¦
## D 0.000000 0.000000 0.000000 2.179168 0.000000 0.000000 â¦
## E 0.000000 0.000000 0.000000 0.000000 2.145443 0.000000 â¦
## F 0.000000 0.000000 0.000000 0.000000 0.000000 2.724853 â¦
## â¦ 19 more rows and 19 more columns</code></pre>
<p>The steps are explained in their paper. After edge selections, <code>GGMnetworkStats()</code> can be utilized to get summary statistics of the resulting graph topology:</p>
<div class="sourceCode" id="cb1207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1207-1"><a href="regularized-covariance-matrix.html#cb1207-1" aria-hidden="true" tabindex="-1"></a>fc <span class="ot">&lt;-</span> <span class="fu">GGMnetworkStats</span>(P)</span>
<span id="cb1207-2"><a href="regularized-covariance-matrix.html#cb1207-2" aria-hidden="true" tabindex="-1"></a>fc</span></code></pre></div>
<pre><code>## $degree
##  A  B  C  D  E  F  G  H  I  J  K  L  M  N  O  P  Q  R  S  T  U  V  W  X  Y 
## 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 
## 
## $betweenness
## A B C D E F G H I J K L M N O P Q R S T U V W X Y 
## 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
## 
## $closeness
##          A          B          C          D          E          F          G 
## 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 
##          H          I          J          K          L          M          N 
## 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 
##          O          P          Q          R          S          T          U 
## 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 0.04166667 
##          V          W          X          Y 
## 0.04166667 0.04166667 0.04166667 0.04166667 
## 
## $eigenCentrality
##   A   B   C   D   E   F   G   H   I   J   K   L   M   N   O   P   Q   R   S   T 
## 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 
##   U   V   W   X   Y 
## 0.2 0.2 0.2 0.2 0.2 
## 
## $nNeg
##  A  B  C  D  E  F  G  H  I  J  K  L  M  N  O  P  Q  R  S  T  U  V  W  X  Y 
## 13 14 10 13 13 12 17 13 14 15  9  9 16  9 13 11 10 16  6 12  9  9 13 14 16 
## 
## $nPos
##  A  B  C  D  E  F  G  H  I  J  K  L  M  N  O  P  Q  R  S  T  U  V  W  X  Y 
## 11 10 14 11 11 12  7 11 10  9 15 15  8 15 11 13 14  8 18 12 15 15 11 10  8 
## 
## $chordal
## [1] TRUE
## 
## $mutualInfo
##         A         B         C         D         E         F         G         H 
## 0.1807197 0.4154956 0.3812980 0.4185954 0.5532431 0.3059883 0.4113305 0.3120867 
##         I         J         K         L         M         N         O         P 
## 0.2574861 0.5072752 0.7451307 0.3550588 0.3779470 0.4719414 0.3452659 0.3017873 
##         Q         R         S         T         U         V         W         X 
## 0.2619416 0.4744925 0.1594554 0.1324863 0.2547769 0.2546357 0.2225756 0.2803463 
##         Y 
## 0.1775594 
## 
## $variance
##         A         B         C         D         E         F         G         H 
## 0.4561861 0.5991397 0.6076539 0.6974337 0.8105009 0.4983632 0.6001093 0.4855818 
##         I         J         K         L         M         N         O         P 
## 0.4370395 0.6095917 0.9474860 0.6172182 0.7700104 0.7586031 0.6285586 0.4753553 
##         Q         R         S         T         U         V         W         X 
## 0.5676784 0.7166316 0.4281188 0.4160550 0.5350846 0.5846058 0.5149757 0.4866827 
##         Y 
## 0.4732289 
## 
## $partialVar
##         A         B         C         D         E         F         G         H 
## 0.3807646 0.3954400 0.4150120 0.4588907 0.4661042 0.3669923 0.3977332 0.3554061 
##         I         J         K         L         M         N         O         P 
## 0.3378282 0.3670559 0.4497453 0.4327515 0.5276626 0.4732091 0.4450397 0.3515231 
##         Q         R         S         T         U         V         W         X 
## 0.4368603 0.4458887 0.3650175 0.3644288 0.4147384 0.4531858 0.4122146 0.3676995 
##         Y 
## 0.3962399</code></pre>
<p>While the <span class="math inline">\(\ell_{2}\)</span> penalty of graphical ridge relies on an extra step to select the edges after <span class="math inline">\(P\)</span> is estimated, the graphical lasso (<code>gLasso</code>) is the <span class="math inline">\(\ell_{1}\)</span>-equivalent to graphical ridge, where the <span class="math inline">\(\ell_{1}\)</span> penalty automatically induces sparsity and select the edges in the underlying graph.</p>
<p>The graphical lasso aims to solve the following regularized maximum likelihood problem:</p>
<p><span class="math display">\[
\mathcal{L}(\Omega)=\operatorname{tr}(\Omega S)-\log |\Omega|+\lambda\|\Omega\|_1
\]</span></p>
<div class="sourceCode" id="cb1209"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1209-1"><a href="regularized-covariance-matrix.html#cb1209-1" aria-hidden="true" tabindex="-1"></a>gl <span class="ot">&lt;-</span> glasso<span class="sc">::</span><span class="fu">glasso</span>(S, <span class="at">rho =</span> <span class="fl">0.2641</span>, <span class="at">approx =</span> <span class="cn">FALSE</span>)[<span class="fu">c</span>(<span class="st">&#39;w&#39;</span>, <span class="st">&#39;wi&#39;</span>)]</span>
<span id="cb1209-2"><a href="regularized-covariance-matrix.html#cb1209-2" aria-hidden="true" tabindex="-1"></a><span class="sc">-</span> <span class="fu">cov2cor</span>(gl<span class="sc">$</span>wi)[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]</span></code></pre></div>
<pre><code>##       [,1]        [,2]        [,3]       [,4]        [,5]       [,6]
##  [1,]   -1  0.00000000  0.00000000  0.0000000  0.00000000  0.0000000
##  [2,]    0 -1.00000000 -0.07439378  0.0000000  0.09011239  0.1280014
##  [3,]    0 -0.07439164 -1.00000000  0.0000000  0.00000000  0.0000000
##  [4,]    0  0.00000000  0.00000000 -1.0000000 -0.15354175  0.0000000
##  [5,]    0  0.09011181  0.00000000 -0.1535415 -1.00000000  0.1502234
##  [6,]    0  0.12800152  0.00000000  0.0000000  0.15022332 -1.0000000
##  [7,]    0  0.00000000  0.00000000  0.0000000  0.00000000  0.0000000
##  [8,]    0  0.00000000  0.00000000  0.0000000  0.10986140  0.0000000
##  [9,]    0  0.00000000 -0.09170730  0.0000000  0.26299733  0.0000000
## [10,]    0  0.00000000  0.20156741  0.0000000  0.00000000  0.0000000
##               [,7]         [,8]        [,9]       [,10]
##  [1,]  0.000000000  0.000000000  0.00000000  0.00000000
##  [2,]  0.000000000  0.000000000  0.00000000  0.00000000
##  [3,]  0.000000000  0.000000000 -0.09170577  0.20156678
##  [4,]  0.000000000  0.000000000  0.00000000  0.00000000
##  [5,]  0.000000000  0.109861399  0.26299730  0.00000000
##  [6,]  0.000000000  0.000000000  0.00000000  0.00000000
##  [7,] -1.000000000 -0.007621682  0.00000000  0.05018338
##  [8,] -0.007621648 -1.000000000  0.00000000  0.00000000
##  [9,]  0.000000000  0.000000000 -1.00000000  0.00000000
## [10,]  0.050183134  0.000000000  0.00000000 -1.00000000</code></pre>
<p>The <code>glasso</code> package does not provide an option for tuning parameter selection. In practice, users apply can be done by cross-validation and eBIC. There are also multiple packages and fucntion to plot the networks for a visual inspection.</p>

</div>
</div>



<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-VANWIERINGEN2016284" class="csl-entry">
van Wieringen, Wessel N., and Carel F. W. Peeters. 2016. <span>âRidge Estimation of Inverse Covariance Matrices from High-Dimensional Data.â</span> <em>Computational Statistics &amp; Data Analysis</em> 103: 284â303. https://doi.org/<a href="https://doi.org/10.1016/j.csda.2016.05.012">https://doi.org/10.1016/j.csda.2016.05.012</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="fundementals.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="r-lab-1---basics-i.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/yaydede/toolbox/edit/master/30-RegularizedCovMatrix.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["YA_TextBook.pdf", "YA_TextBook.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
