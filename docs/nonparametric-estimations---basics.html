<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Nonparametric Estimations - Basics | Toolbox for Social Scientists and Policy Analysts</title>
  <meta name="description" content="Chapter 6 Nonparametric Estimations - Basics | Toolbox for Social Scientists and Policy Analysts" />
  <meta name="generator" content="bookdown 0.32.1 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Nonparametric Estimations - Basics | Toolbox for Social Scientists and Policy Analysts" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://yaydede.github.io/toolbox//png/cover2.png" />
  
  <meta name="github-repo" content="yaydede/ToolShed_draft" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Nonparametric Estimations - Basics | Toolbox for Social Scientists and Policy Analysts" />
  
  
  <meta name="twitter:image" content="https://yaydede.github.io/toolbox//png/cover2.png" />

<meta name="author" content="Yigit Aydede" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="parametric-estimations.html"/>
<link rel="next" href="smoothing.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Toolbox</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who"><i class="fa fa-check"></i>Who</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> How we define Machine Learning</a></li>
<li class="chapter" data-level="2" data-path="preliminaries.html"><a href="preliminaries.html"><i class="fa fa-check"></i><b>2</b> Preliminaries</a>
<ul>
<li class="chapter" data-level="2.1" data-path="preliminaries.html"><a href="preliminaries.html#data-and-dataset-types"><i class="fa fa-check"></i><b>2.1</b> Data and dataset types</a></li>
<li class="chapter" data-level="2.2" data-path="preliminaries.html"><a href="preliminaries.html#plots"><i class="fa fa-check"></i><b>2.2</b> Plots</a></li>
<li class="chapter" data-level="2.3" data-path="preliminaries.html"><a href="preliminaries.html#probability-distributions-with-r"><i class="fa fa-check"></i><b>2.3</b> Probability Distributions with R</a></li>
<li class="chapter" data-level="2.4" data-path="preliminaries.html"><a href="preliminaries.html#regressions"><i class="fa fa-check"></i><b>2.4</b> Regressions</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="preliminaries.html"><a href="preliminaries.html#ordinary-least-squares-ols"><i class="fa fa-check"></i><b>2.4.1</b> Ordinary Least Squares (OLS)</a></li>
<li class="chapter" data-level="2.4.2" data-path="preliminaries.html"><a href="preliminaries.html#maximum-likelihood-estimators"><i class="fa fa-check"></i><b>2.4.2</b> Maximum Likelihood Estimators</a></li>
<li class="chapter" data-level="2.4.3" data-path="preliminaries.html"><a href="preliminaries.html#estimating-mle-with-r"><i class="fa fa-check"></i><b>2.4.3</b> Estimating MLE with R</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="preliminaries.html"><a href="preliminaries.html#blue"><i class="fa fa-check"></i><b>2.5</b> BLUE</a></li>
<li class="chapter" data-level="2.6" data-path="preliminaries.html"><a href="preliminaries.html#modeling-the-data"><i class="fa fa-check"></i><b>2.6</b> Modeling the data</a></li>
<li class="chapter" data-level="2.7" data-path="preliminaries.html"><a href="preliminaries.html#causal-vs.-predictive-models"><i class="fa fa-check"></i><b>2.7</b> Causal vs. Predictive Models</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="preliminaries.html"><a href="preliminaries.html#causal-models"><i class="fa fa-check"></i><b>2.7.1</b> Causal Models</a></li>
<li class="chapter" data-level="2.7.2" data-path="preliminaries.html"><a href="preliminaries.html#predictive-models"><i class="fa fa-check"></i><b>2.7.2</b> Predictive Models</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="preliminaries.html"><a href="preliminaries.html#simulation"><i class="fa fa-check"></i><b>2.8</b> Simulation</a></li>
</ul></li>
<li class="part"><span><b>I Formal Look at Prediction</b></span></li>
<li class="chapter" data-level="" data-path="learning-systems.html"><a href="learning-systems.html"><i class="fa fa-check"></i>Learning Systems</a></li>
<li class="chapter" data-level="3" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html"><i class="fa fa-check"></i><b>3</b> Bias-Variance Tradeoff</a>
<ul>
<li class="chapter" data-level="3.1" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#estimator-and-mse"><i class="fa fa-check"></i><b>3.1</b> Estimator and MSE</a></li>
<li class="chapter" data-level="3.2" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#prediction---mspe"><i class="fa fa-check"></i><b>3.2</b> Prediction - MSPE</a></li>
<li class="chapter" data-level="3.3" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#biased-estimator-as-a-predictor"><i class="fa fa-check"></i><b>3.3</b> Biased estimator as a predictor</a></li>
<li class="chapter" data-level="3.4" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#dropping-a-variable-in-a-regression"><i class="fa fa-check"></i><b>3.4</b> Dropping a variable in a regression</a></li>
<li class="chapter" data-level="3.5" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#uncertainty-in-estimations-and-predictions"><i class="fa fa-check"></i><b>3.5</b> Uncertainty in estimations and predictions</a></li>
<li class="chapter" data-level="3.6" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#prediction-interval-for-unbiased-ols-predictor"><i class="fa fa-check"></i><b>3.6</b> Prediction interval for unbiased OLS predictor</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="overfitting.html"><a href="overfitting.html"><i class="fa fa-check"></i><b>4</b> Overfitting</a></li>
<li class="part"><span><b>II Nonparametric Estimations</b></span></li>
<li class="chapter" data-level="" data-path="parametric-vs.-nonparametric-methods.html"><a href="parametric-vs.-nonparametric-methods.html"><i class="fa fa-check"></i>Parametric vs. Nonparametric methods</a></li>
<li class="chapter" data-level="5" data-path="parametric-estimations.html"><a href="parametric-estimations.html"><i class="fa fa-check"></i><b>5</b> Parametric Estimations</a>
<ul>
<li class="chapter" data-level="5.1" data-path="parametric-estimations.html"><a href="parametric-estimations.html#linear-probability-models-lpm"><i class="fa fa-check"></i><b>5.1</b> Linear Probability Models (LPM)</a></li>
<li class="chapter" data-level="5.2" data-path="parametric-estimations.html"><a href="parametric-estimations.html#logistic-regression"><i class="fa fa-check"></i><b>5.2</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="parametric-estimations.html"><a href="parametric-estimations.html#estimating-logistic-regression"><i class="fa fa-check"></i><b>5.2.1</b> Estimating Logistic Regression</a></li>
<li class="chapter" data-level="5.2.2" data-path="parametric-estimations.html"><a href="parametric-estimations.html#cost-functions"><i class="fa fa-check"></i><b>5.2.2</b> Cost functions</a></li>
<li class="chapter" data-level="5.2.3" data-path="parametric-estimations.html"><a href="parametric-estimations.html#deviance"><i class="fa fa-check"></i><b>5.2.3</b> Deviance</a></li>
<li class="chapter" data-level="5.2.4" data-path="parametric-estimations.html"><a href="parametric-estimations.html#predictive-accuracy"><i class="fa fa-check"></i><b>5.2.4</b> Predictive accuracy</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html"><i class="fa fa-check"></i><b>6</b> Nonparametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="6.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#density-estimations"><i class="fa fa-check"></i><b>6.1</b> Density Estimations</a></li>
<li class="chapter" data-level="6.2" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#kernel-regressions"><i class="fa fa-check"></i><b>6.2</b> Kernel regressions</a></li>
<li class="chapter" data-level="6.3" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#regression-splines"><i class="fa fa-check"></i><b>6.3</b> Regression Splines</a></li>
<li class="chapter" data-level="6.4" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#mars---multivariate-adaptive-regression-splines"><i class="fa fa-check"></i><b>6.4</b> MARS - Multivariate Adaptive Regression Splines</a></li>
<li class="chapter" data-level="6.5" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#gam---generalized-additive-model"><i class="fa fa-check"></i><b>6.5</b> GAM - Generalized Additive Model</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>7</b> Smoothing</a>
<ul>
<li class="chapter" data-level="7.1" data-path="smoothing.html"><a href="smoothing.html#using-bins"><i class="fa fa-check"></i><b>7.1</b> Using bins</a></li>
<li class="chapter" data-level="7.2" data-path="smoothing.html"><a href="smoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>7.2</b> Kernel smoothing</a></li>
<li class="chapter" data-level="7.3" data-path="smoothing.html"><a href="smoothing.html#locally-weighted-regression-loess"><i class="fa fa-check"></i><b>7.3</b> Locally weighted regression <code>loess()</code></a></li>
<li class="chapter" data-level="7.4" data-path="smoothing.html"><a href="smoothing.html#smooth-spline-regression"><i class="fa fa-check"></i><b>7.4</b> Smooth Spline Regression</a></li>
<li class="chapter" data-level="7.5" data-path="smoothing.html"><a href="smoothing.html#multivariate-loess"><i class="fa fa-check"></i><b>7.5</b> Multivariate Loess</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html"><i class="fa fa-check"></i><b>8</b> Nonparametric Classifier - kNN</a>
<ul>
<li class="chapter" data-level="8.1" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#mnist-dataset"><i class="fa fa-check"></i><b>8.1</b> <code>mnist</code> Dataset</a></li>
<li class="chapter" data-level="8.2" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#linear-classifiers-again"><i class="fa fa-check"></i><b>8.2</b> Linear classifiers (again)</a></li>
<li class="chapter" data-level="8.3" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>8.3</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="8.4" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#knn-with-caret"><i class="fa fa-check"></i><b>8.4</b> kNN with caret</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#mnist_27"><i class="fa fa-check"></i><b>8.4.1</b> <code>mnist_27</code></a></li>
<li class="chapter" data-level="8.4.2" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#adult-dataset"><i class="fa fa-check"></i><b>8.4.2</b> Adult dataset</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Self-Learning</b></span></li>
<li class="chapter" data-level="9" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html"><i class="fa fa-check"></i><b>9</b> Hyperparameter Tuning</a>
<ul>
<li class="chapter" data-level="9.1" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#training-validation-and-test-datasets"><i class="fa fa-check"></i><b>9.1</b> Training, validation, and test datasets</a></li>
<li class="chapter" data-level="9.2" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#splitting-the-data-randomly"><i class="fa fa-check"></i><b>9.2</b> Splitting the data randomly</a></li>
<li class="chapter" data-level="9.3" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>9.3</b> k-fold cross validation</a></li>
<li class="chapter" data-level="9.4" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#cross-validated-grid-search"><i class="fa fa-check"></i><b>9.4</b> Cross-validated grid search</a></li>
<li class="chapter" data-level="9.5" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#bootstrapped-grid-search"><i class="fa fa-check"></i><b>9.5</b> Bootstrapped grid search</a></li>
<li class="chapter" data-level="9.6" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#when-the-data-is-time-series"><i class="fa fa-check"></i><b>9.6</b> When the data is time-series</a></li>
<li class="chapter" data-level="9.7" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#speed"><i class="fa fa-check"></i><b>9.7</b> Speed</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html"><i class="fa fa-check"></i><b>10</b> Tuning in Classification</a>
<ul>
<li class="chapter" data-level="10.1" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html#confusion-matrix"><i class="fa fa-check"></i><b>10.1</b> Confusion matrix</a></li>
<li class="chapter" data-level="10.2" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html#performance-measures"><i class="fa fa-check"></i><b>10.2</b> Performance measures</a></li>
<li class="chapter" data-level="10.3" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html#roc-curve"><i class="fa fa-check"></i><b>10.3</b> ROC Curve</a></li>
<li class="chapter" data-level="10.4" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html#auc---area-under-the-curve"><i class="fa fa-check"></i><b>10.4</b> AUC - Area Under the Curve</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="classification-example.html"><a href="classification-example.html"><i class="fa fa-check"></i><b>11</b> Classification Example</a>
<ul>
<li class="chapter" data-level="11.1" data-path="classification-example.html"><a href="classification-example.html#lpm"><i class="fa fa-check"></i><b>11.1</b> LPM</a></li>
<li class="chapter" data-level="11.2" data-path="classification-example.html"><a href="classification-example.html#logistic-regression-1"><i class="fa fa-check"></i><b>11.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="11.3" data-path="classification-example.html"><a href="classification-example.html#knn"><i class="fa fa-check"></i><b>11.3</b> kNN</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="classification-example.html"><a href="classification-example.html#knn-10-fold-cv"><i class="fa fa-check"></i><b>11.3.1</b> kNN 10-fold CV</a></li>
<li class="chapter" data-level="11.3.2" data-path="classification-example.html"><a href="classification-example.html#knn-with-caret-1"><i class="fa fa-check"></i><b>11.3.2</b> kNN with <code>caret</code></a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Tree-based Models</b></span></li>
<li class="chapter" data-level="12" data-path="cart.html"><a href="cart.html"><i class="fa fa-check"></i><b>12</b> CART</a>
<ul>
<li class="chapter" data-level="12.1" data-path="cart.html"><a href="cart.html#cart---classification-tree"><i class="fa fa-check"></i><b>12.1</b> CART - Classification Tree</a></li>
<li class="chapter" data-level="12.2" data-path="cart.html"><a href="cart.html#rpart---recursive-partitioning"><i class="fa fa-check"></i><b>12.2</b> <code>rpart</code> - Recursive Partitioning</a></li>
<li class="chapter" data-level="12.3" data-path="cart.html"><a href="cart.html#pruning"><i class="fa fa-check"></i><b>12.3</b> Pruning</a></li>
<li class="chapter" data-level="12.4" data-path="cart.html"><a href="cart.html#classification-with-titanic"><i class="fa fa-check"></i><b>12.4</b> Classification with Titanic</a></li>
<li class="chapter" data-level="12.5" data-path="cart.html"><a href="cart.html#regression-tree"><i class="fa fa-check"></i><b>12.5</b> Regression Tree</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ensemble-learning.html"><a href="ensemble-learning.html"><i class="fa fa-check"></i><b>13</b> Ensemble Learning</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ensemble-learning.html"><a href="ensemble-learning.html#bagging"><i class="fa fa-check"></i><b>13.1</b> Bagging</a></li>
<li class="chapter" data-level="13.2" data-path="ensemble-learning.html"><a href="ensemble-learning.html#random-forest"><i class="fa fa-check"></i><b>13.2</b> Random Forest</a></li>
<li class="chapter" data-level="13.3" data-path="ensemble-learning.html"><a href="ensemble-learning.html#boosting"><i class="fa fa-check"></i><b>13.3</b> Boosting</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="ensemble-learning.html"><a href="ensemble-learning.html#sequential-ensemble-with-gbm"><i class="fa fa-check"></i><b>13.3.1</b> Sequential ensemble with <code>gbm</code></a></li>
<li class="chapter" data-level="13.3.2" data-path="ensemble-learning.html"><a href="ensemble-learning.html#adaboost"><i class="fa fa-check"></i><b>13.3.2</b> AdaBoost</a></li>
<li class="chapter" data-level="13.3.3" data-path="ensemble-learning.html"><a href="ensemble-learning.html#xgboost"><i class="fa fa-check"></i><b>13.3.3</b> XGBoost</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ensemble-applications.html"><a href="ensemble-applications.html"><i class="fa fa-check"></i><b>14</b> Ensemble Applications</a>
<ul>
<li class="chapter" data-level="14.1" data-path="ensemble-applications.html"><a href="ensemble-applications.html#classification"><i class="fa fa-check"></i><b>14.1</b> Classification</a></li>
<li class="chapter" data-level="14.2" data-path="ensemble-applications.html"><a href="ensemble-applications.html#regression"><i class="fa fa-check"></i><b>14.2</b> Regression</a></li>
<li class="chapter" data-level="14.3" data-path="ensemble-applications.html"><a href="ensemble-applications.html#exploration"><i class="fa fa-check"></i><b>14.3</b> Exploration</a></li>
<li class="chapter" data-level="14.4" data-path="ensemble-applications.html"><a href="ensemble-applications.html#boosting-applications"><i class="fa fa-check"></i><b>14.4</b> Boosting Applications</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="ensemble-applications.html"><a href="ensemble-applications.html#regression-1"><i class="fa fa-check"></i><b>14.4.1</b> Regression</a></li>
<li class="chapter" data-level="14.4.2" data-path="ensemble-applications.html"><a href="ensemble-applications.html#random-search-with-parallel-processing"><i class="fa fa-check"></i><b>14.4.2</b> Random search with parallel processing</a></li>
<li class="chapter" data-level="14.4.3" data-path="ensemble-applications.html"><a href="ensemble-applications.html#boosting-vs.-others"><i class="fa fa-check"></i><b>14.4.3</b> Boosting vs. Others</a></li>
<li class="chapter" data-level="14.4.4" data-path="ensemble-applications.html"><a href="ensemble-applications.html#classification-1"><i class="fa fa-check"></i><b>14.4.4</b> Classification</a></li>
<li class="chapter" data-level="14.4.5" data-path="ensemble-applications.html"><a href="ensemble-applications.html#adaboost.m1"><i class="fa fa-check"></i><b>14.4.5</b> AdaBoost.M1</a></li>
<li class="chapter" data-level="14.4.6" data-path="ensemble-applications.html"><a href="ensemble-applications.html#classification-with-xgboost"><i class="fa fa-check"></i><b>14.4.6</b> Classification with XGBoost</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>V SVM &amp; Neural Networks</b></span></li>
<li class="chapter" data-level="15" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>15</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="15.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#optimal-separating-classifier"><i class="fa fa-check"></i><b>15.1</b> Optimal Separating Classifier</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#the-margin"><i class="fa fa-check"></i><b>15.1.1</b> The Margin</a></li>
<li class="chapter" data-level="15.1.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#the-non-separable-case"><i class="fa fa-check"></i><b>15.1.2</b> The Non-Separable Case</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#nonlinear-boundary-with-kernels"><i class="fa fa-check"></i><b>15.2</b> Nonlinear Boundary with Kernels</a></li>
<li class="chapter" data-level="15.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#application-with-svm"><i class="fa fa-check"></i><b>15.3</b> Application with SVM</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html"><i class="fa fa-check"></i><b>16</b> Artificial Neural Networks</a>
<ul>
<li class="chapter" data-level="16.1" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#neural-network---the-idea"><i class="fa fa-check"></i><b>16.1</b> Neural Network - the idea</a></li>
<li class="chapter" data-level="16.2" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#backpropagation"><i class="fa fa-check"></i><b>16.2</b> Backpropagation</a></li>
<li class="chapter" data-level="16.3" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#neural-network---more-inputs"><i class="fa fa-check"></i><b>16.3</b> Neural Network - More inputs</a></li>
<li class="chapter" data-level="16.4" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#deep-learning"><i class="fa fa-check"></i><b>16.4</b> Deep Learning</a></li>
</ul></li>
<li class="part"><span><b>VI Penalized Regressions</b></span></li>
<li class="chapter" data-level="" data-path="parametric-models-in-prediction.html"><a href="parametric-models-in-prediction.html"><i class="fa fa-check"></i>Parametric models in prediction</a></li>
<li class="chapter" data-level="17" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>17</b> Ridge</a></li>
<li class="chapter" data-level="18" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>18</b> Lasso</a></li>
<li class="chapter" data-level="19" data-path="adaptive-lasso.html"><a href="adaptive-lasso.html"><i class="fa fa-check"></i><b>19</b> Adaptive Lasso</a></li>
<li class="chapter" data-level="20" data-path="sparsity.html"><a href="sparsity.html"><i class="fa fa-check"></i><b>20</b> Sparsity</a></li>
<li class="part"><span><b>VII Time Series</b></span></li>
<li class="chapter" data-level="" data-path="forecasting.html"><a href="forecasting.html"><i class="fa fa-check"></i>Forecasting</a></li>
<li class="chapter" data-level="21" data-path="arima-models.html"><a href="arima-models.html"><i class="fa fa-check"></i><b>21</b> ARIMA models</a>
<ul>
<li class="chapter" data-level="21.1" data-path="arima-models.html"><a href="arima-models.html#hyndman-khandakar-algorithm"><i class="fa fa-check"></i><b>21.1</b> Hyndman-Khandakar algorithm</a></li>
<li class="chapter" data-level="21.2" data-path="arima-models.html"><a href="arima-models.html#ts-plots"><i class="fa fa-check"></i><b>21.2</b> TS Plots</a></li>
<li class="chapter" data-level="21.3" data-path="arima-models.html"><a href="arima-models.html#box-cox-transformation"><i class="fa fa-check"></i><b>21.3</b> Box-Cox transformation</a></li>
<li class="chapter" data-level="21.4" data-path="arima-models.html"><a href="arima-models.html#stationarity"><i class="fa fa-check"></i><b>21.4</b> Stationarity</a></li>
<li class="chapter" data-level="21.5" data-path="arima-models.html"><a href="arima-models.html#modeling-arima"><i class="fa fa-check"></i><b>21.5</b> Modeling ARIMA</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="grid-search-for-arima.html"><a href="grid-search-for-arima.html"><i class="fa fa-check"></i><b>22</b> Grid search for ARIMA</a></li>
<li class="chapter" data-level="23" data-path="time-series-embedding.html"><a href="time-series-embedding.html"><i class="fa fa-check"></i><b>23</b> Time Series Embedding</a>
<ul>
<li class="chapter" data-level="23.1" data-path="time-series-embedding.html"><a href="time-series-embedding.html#var-for-recursive-forecasting"><i class="fa fa-check"></i><b>23.1</b> VAR for Recursive Forecasting</a></li>
<li class="chapter" data-level="23.2" data-path="time-series-embedding.html"><a href="time-series-embedding.html#embedding-for-direct-forecast"><i class="fa fa-check"></i><b>23.2</b> Embedding for Direct Forecast</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="random-forest-1.html"><a href="random-forest-1.html"><i class="fa fa-check"></i><b>24</b> Random Forest</a>
<ul>
<li class="chapter" data-level="24.1" data-path="random-forest-1.html"><a href="random-forest-1.html#univariate"><i class="fa fa-check"></i><b>24.1</b> Univariate</a></li>
<li class="chapter" data-level="24.2" data-path="random-forest-1.html"><a href="random-forest-1.html#multivariate"><i class="fa fa-check"></i><b>24.2</b> Multivariate</a></li>
<li class="chapter" data-level="24.3" data-path="random-forest-1.html"><a href="random-forest-1.html#rolling-and-expanding-windows"><i class="fa fa-check"></i><b>24.3</b> Rolling and expanding windows</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html"><i class="fa fa-check"></i><b>25</b> Recurrent Neural Networks</a>
<ul>
<li class="chapter" data-level="25.1" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#keras"><i class="fa fa-check"></i><b>25.1</b> Keras</a></li>
<li class="chapter" data-level="25.2" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#input-tensors"><i class="fa fa-check"></i><b>25.2</b> Input Tensors</a></li>
<li class="chapter" data-level="25.3" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#plain-rnn"><i class="fa fa-check"></i><b>25.3</b> Plain RNN</a></li>
<li class="chapter" data-level="25.4" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#lstm"><i class="fa fa-check"></i><b>25.4</b> LSTM</a></li>
</ul></li>
<li class="part"><span><b>VIII Dimension Reduction Methods</b></span></li>
<li class="chapter" data-level="" data-path="matrix-decompositions.html"><a href="matrix-decompositions.html"><i class="fa fa-check"></i>Matrix Decompositions</a></li>
<li class="chapter" data-level="26" data-path="eigenvectors-and-eigenvalues.html"><a href="eigenvectors-and-eigenvalues.html"><i class="fa fa-check"></i><b>26</b> Eigenvectors and eigenvalues</a></li>
<li class="chapter" data-level="27" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html"><i class="fa fa-check"></i><b>27</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="28" data-path="rankr-approximations.html"><a href="rankr-approximations.html"><i class="fa fa-check"></i><b>28</b> Rank(r) Approximations</a></li>
<li class="chapter" data-level="29" data-path="moore-penrose-inverse.html"><a href="moore-penrose-inverse.html"><i class="fa fa-check"></i><b>29</b> Moore-Penrose inverse</a></li>
<li class="chapter" data-level="30" data-path="principle-component-analysis.html"><a href="principle-component-analysis.html"><i class="fa fa-check"></i><b>30</b> Principle Component Analysis</a></li>
<li class="chapter" data-level="31" data-path="factor-analysis.html"><a href="factor-analysis.html"><i class="fa fa-check"></i><b>31</b> Factor Analysis</a></li>
<li class="part"><span><b>IX Network Analysis</b></span></li>
<li class="chapter" data-level="" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html"><i class="fa fa-check"></i>Graphical Network Analysis</a></li>
<li class="chapter" data-level="32" data-path="fundementals.html"><a href="fundementals.html"><i class="fa fa-check"></i><b>32</b> Fundementals</a>
<ul>
<li class="chapter" data-level="32.1" data-path="fundementals.html"><a href="fundementals.html#covariance"><i class="fa fa-check"></i><b>32.1</b> Covariance</a></li>
<li class="chapter" data-level="32.2" data-path="fundementals.html"><a href="fundementals.html#correlation"><i class="fa fa-check"></i><b>32.2</b> Correlation</a></li>
<li class="chapter" data-level="32.3" data-path="fundementals.html"><a href="fundementals.html#precision-matrix"><i class="fa fa-check"></i><b>32.3</b> Precision Matrix</a></li>
<li class="chapter" data-level="32.4" data-path="fundementals.html"><a href="fundementals.html#semi-partial-correlation"><i class="fa fa-check"></i><b>32.4</b> Semi-partial Correlation</a></li>
</ul></li>
<li class="chapter" data-level="33" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html"><i class="fa fa-check"></i><b>33</b> Regularized Covariance Matrix</a>
<ul>
<li class="chapter" data-level="33.1" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html#multivariate-gaussian-distribution"><i class="fa fa-check"></i><b>33.1</b> Multivariate Gaussian Distribution</a></li>
<li class="chapter" data-level="33.2" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html#high-dimensional-data"><i class="fa fa-check"></i><b>33.2</b> High-dimensional data</a></li>
<li class="chapter" data-level="33.3" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html#ridge-ell_2-and-glasso-ell_1"><i class="fa fa-check"></i><b>33.3</b> Ridge (<span class="math inline">\(\ell_{2}\)</span>) and glasso (<span class="math inline">\(\ell_{1}\)</span>)</a></li>
</ul></li>
<li class="part"><span><b>X Labs</b></span></li>
<li class="chapter" data-level="34" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html"><i class="fa fa-check"></i><b>34</b> R Lab 1 - Basics I</a>
<ul>
<li class="chapter" data-level="34.1" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#r-rstudio-and-r-packages"><i class="fa fa-check"></i><b>34.1</b> R, RStudio, and R Packages</a></li>
<li class="chapter" data-level="34.2" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#rstudio"><i class="fa fa-check"></i><b>34.2</b> RStudio</a></li>
<li class="chapter" data-level="34.3" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#working-directory"><i class="fa fa-check"></i><b>34.3</b> Working directory</a></li>
<li class="chapter" data-level="34.4" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#data-types-and-stuctures"><i class="fa fa-check"></i><b>34.4</b> Data Types and Stuctures</a></li>
<li class="chapter" data-level="34.5" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#vectors"><i class="fa fa-check"></i><b>34.5</b> Vectors</a></li>
<li class="chapter" data-level="34.6" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#subsetting-vectors"><i class="fa fa-check"></i><b>34.6</b> Subsetting Vectors</a></li>
<li class="chapter" data-level="34.7" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#vectorization-or-vector-operations"><i class="fa fa-check"></i><b>34.7</b> Vectorization or vector operations</a></li>
<li class="chapter" data-level="34.8" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#matrices"><i class="fa fa-check"></i><b>34.8</b> Matrices</a></li>
<li class="chapter" data-level="34.9" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#matrix-operations"><i class="fa fa-check"></i><b>34.9</b> Matrix Operations</a></li>
<li class="chapter" data-level="34.10" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#subsetting-matrix"><i class="fa fa-check"></i><b>34.10</b> Subsetting Matrix</a></li>
<li class="chapter" data-level="34.11" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#r-style-guide"><i class="fa fa-check"></i><b>34.11</b> R-Style Guide</a></li>
</ul></li>
<li class="chapter" data-level="35" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html"><i class="fa fa-check"></i><b>35</b> R Lab 2 - Basics II</a>
<ul>
<li class="chapter" data-level="35.1" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#data-frames-and-lists"><i class="fa fa-check"></i><b>35.1</b> Data frames and lists</a>
<ul>
<li class="chapter" data-level="35.1.1" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#lists"><i class="fa fa-check"></i><b>35.1.1</b> Lists</a></li>
<li class="chapter" data-level="35.1.2" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#data-frames"><i class="fa fa-check"></i><b>35.1.2</b> Data Frames</a></li>
<li class="chapter" data-level="35.1.3" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#reading-importing-and-writting-exporting-data-files"><i class="fa fa-check"></i><b>35.1.3</b> Reading (importing) and writting (exporting) data files</a></li>
<li class="chapter" data-level="35.1.4" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#subsetting-data-frames"><i class="fa fa-check"></i><b>35.1.4</b> Subsetting Data Frames</a></li>
<li class="chapter" data-level="35.1.5" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#plotting-from-data-frame"><i class="fa fa-check"></i><b>35.1.5</b> Plotting from data frame</a></li>
<li class="chapter" data-level="35.1.6" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#some-useful-functions"><i class="fa fa-check"></i><b>35.1.6</b> Some useful functions</a></li>
<li class="chapter" data-level="35.1.7" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#categorical-variables-in-data-frames"><i class="fa fa-check"></i><b>35.1.7</b> Categorical Variables in Data Frames</a></li>
</ul></li>
<li class="chapter" data-level="35.2" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#programming-basics"><i class="fa fa-check"></i><b>35.2</b> Programming Basics</a>
<ul>
<li class="chapter" data-level="35.2.1" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#ifelse"><i class="fa fa-check"></i><b>35.2.1</b> if/Else</a></li>
<li class="chapter" data-level="35.2.2" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#loops"><i class="fa fa-check"></i><b>35.2.2</b> Loops</a></li>
<li class="chapter" data-level="35.2.3" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#the-apply-family"><i class="fa fa-check"></i><b>35.2.3</b> The <code>apply()</code> family</a></li>
<li class="chapter" data-level="35.2.4" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#functions"><i class="fa fa-check"></i><b>35.2.4</b> Functions</a></li>
<li class="chapter" data-level="35.2.5" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#dplyr"><i class="fa fa-check"></i><b>35.2.5</b> <code>dplyr()</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="36" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html"><i class="fa fa-check"></i><b>36</b> R Lab 3 - Preparing the data</a>
<ul>
<li class="chapter" data-level="36.1" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#preparing-the-data-for-a-regression-analysis-with-lm"><i class="fa fa-check"></i><b>36.1</b> Preparing the data for a regression analysis with <code>lm()</code></a>
<ul>
<li class="chapter" data-level="36.1.1" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#factor-variables"><i class="fa fa-check"></i><b>36.1.1</b> Factor variables</a></li>
<li class="chapter" data-level="36.1.2" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#dummy-coding"><i class="fa fa-check"></i><b>36.1.2</b> Dummy Coding</a></li>
<li class="chapter" data-level="36.1.3" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#column-variable-names"><i class="fa fa-check"></i><b>36.1.3</b> Column (Variable) names</a></li>
<li class="chapter" data-level="36.1.4" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#data-subsetting-and-missing-values"><i class="fa fa-check"></i><b>36.1.4</b> Data subsetting and missing values</a></li>
</ul></li>
<li class="chapter" data-level="36.2" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#dummy-variable-models"><i class="fa fa-check"></i><b>36.2</b> “DUMMY” variable models</a>
<ul>
<li class="chapter" data-level="36.2.1" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#mtcars-example"><i class="fa fa-check"></i><b>36.2.1</b> <code>mtcars</code> example</a></li>
<li class="chapter" data-level="36.2.2" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#model.matrix"><i class="fa fa-check"></i><b>36.2.2</b> <code>model.matrix()</code></a></li>
<li class="chapter" data-level="36.2.3" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#example-with-a-bigger-data-set-autompg"><i class="fa fa-check"></i><b>36.2.3</b> Example with a bigger data set: <code>Autompg</code></a></li>
<li class="chapter" data-level="36.2.4" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#some-more-data-management-tools-for-subsetting-complete.cases-is.na-and-within"><i class="fa fa-check"></i><b>36.2.4</b> Some more data management tools for subsetting: <code>complete.cases()</code>, <code>is.na()</code>, and <code>within()</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="37" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html"><i class="fa fa-check"></i><b>37</b> R Lab 4 - Simulation in R</a>
<ul>
<li class="chapter" data-level="37.1" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#sampling-in-r-sample"><i class="fa fa-check"></i><b>37.1</b> Sampling in R: <code>sample()</code></a></li>
<li class="chapter" data-level="37.2" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#random-number-generating-with-probablity-distributions"><i class="fa fa-check"></i><b>37.2</b> Random number generating with probablity distributions</a></li>
<li class="chapter" data-level="37.3" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#simulation-for-statistical-inference"><i class="fa fa-check"></i><b>37.3</b> Simulation for statistical inference</a></li>
<li class="chapter" data-level="37.4" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#creataing-data-with-a-data-generating-model-dgm"><i class="fa fa-check"></i><b>37.4</b> Creataing data with a Data Generating Model (DGM)</a></li>
<li class="chapter" data-level="37.5" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#bootstrapping"><i class="fa fa-check"></i><b>37.5</b> Bootstrapping</a></li>
<li class="chapter" data-level="37.6" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#monty-hall---fun-example"><i class="fa fa-check"></i><b>37.6</b> Monty Hall - Fun example</a></li>
</ul></li>
<li class="part"><span><b>XI Appendix</b></span></li>
<li class="chapter" data-level="38" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html"><i class="fa fa-check"></i><b>38</b> Algorithmic Optimization</a>
<ul>
<li class="chapter" data-level="38.1" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#brute-force-optimization"><i class="fa fa-check"></i><b>38.1</b> Brute-force optimization</a></li>
<li class="chapter" data-level="38.2" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#derivative-based-methods"><i class="fa fa-check"></i><b>38.2</b> Derivative-based methods</a></li>
<li class="chapter" data-level="38.3" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#ml-estimation-with-logistic-regression"><i class="fa fa-check"></i><b>38.3</b> ML Estimation with logistic regression</a></li>
<li class="chapter" data-level="38.4" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#gradient-descent-algorithm"><i class="fa fa-check"></i><b>38.4</b> Gradient Descent Algorithm</a>
<ul>
<li class="chapter" data-level="38.4.1" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#one-variable"><i class="fa fa-check"></i><b>38.4.1</b> One-variable</a></li>
<li class="chapter" data-level="38.4.2" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#multivariable"><i class="fa fa-check"></i><b>38.4.2</b> Multivariable</a></li>
</ul></li>
<li class="chapter" data-level="38.5" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#optimization-with-r"><i class="fa fa-check"></i><b>38.5</b> Optimization with R</a></li>
</ul></li>
<li class="chapter" data-level="39" data-path="imbalanced-data.html"><a href="imbalanced-data.html"><i class="fa fa-check"></i><b>39</b> Imbalanced Data</a>
<ul>
<li class="chapter" data-level="39.1" data-path="imbalanced-data.html"><a href="imbalanced-data.html#smote"><i class="fa fa-check"></i><b>39.1</b> <code>SMOTE</code></a></li>
<li class="chapter" data-level="39.2" data-path="imbalanced-data.html"><a href="imbalanced-data.html#fraud-detection"><i class="fa fa-check"></i><b>39.2</b> Fraud detection</a></li>
</ul></li>
<li class="chapter" data-level="40" data-path="footnotes-and-citations.html"><a href="footnotes-and-citations.html"><i class="fa fa-check"></i><b>40</b> Footnotes and citations</a>
<ul>
<li class="chapter" data-level="40.1" data-path="footnotes-and-citations.html"><a href="footnotes-and-citations.html#footnotes"><i class="fa fa-check"></i><b>40.1</b> Footnotes</a></li>
<li class="chapter" data-level="40.2" data-path="footnotes-and-citations.html"><a href="footnotes-and-citations.html#citations"><i class="fa fa-check"></i><b>40.2</b> Citations</a></li>
</ul></li>
<li class="chapter" data-level="41" data-path="blocks.html"><a href="blocks.html"><i class="fa fa-check"></i><b>41</b> Blocks</a>
<ul>
<li class="chapter" data-level="41.1" data-path="blocks.html"><a href="blocks.html#equations"><i class="fa fa-check"></i><b>41.1</b> Equations</a></li>
<li class="chapter" data-level="41.2" data-path="blocks.html"><a href="blocks.html#theorems-and-proofs"><i class="fa fa-check"></i><b>41.2</b> Theorems and proofs</a></li>
<li class="chapter" data-level="41.3" data-path="blocks.html"><a href="blocks.html#callout-blocks"><i class="fa fa-check"></i><b>41.3</b> Callout blocks</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/yaydede/toolbox" target="blank"> 2023 Yigit Aydede - Bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Toolbox for Social Scientists and Policy Analysts</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="nonparametric-estimations---basics" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Nonparametric Estimations - Basics<a href="nonparametric-estimations---basics.html#nonparametric-estimations---basics" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>The models we see in the previous chapters are parametric, which means that they have to assume a certain structure on the regression function <span class="math inline">\(m\)</span> controlled by parameters before the estimations. Therefore, the results from parametric models are the best if the specification of <span class="math inline">\(m\)</span> is correct. Avoiding this assumption is the strongest point of nonparametric methods, which do not require any hard-to-satisfy pre-determined regression functions.</p>
<p>Before talking about a nonparametric estimator for the regression function <span class="math inline">\(m\)</span>, we should first look at a simple nonparametric density estimation of <span class="math inline">\(X\)</span>. We aim to estimate <span class="math inline">\(f(x)\)</span> from a sample and without assuming any specific form for <span class="math inline">\(f\)</span>.</p>
<p><img src="png/progressive_plot4.gif" style="display: block; margin: auto;" /></p>
<p><img src="png/progressive_plot2.gif" style="display: block; margin: auto;" /></p>
<p>These live plots demonstrate the importance of nonparametric methods for visualizing and understanding data. The purpose of these graph is to showcase the role of smoothing techniques in identifying the underlying pattern in noisy data. The graphs is generated using a set of 300 data points that are created by adding noise to a sine function.</p>
<p>Plots display a step-by-step process of applying a simple nonparametric smoothing method called moving average, with a window size of 4 and 20. In this method, for each point in the dataset, an average is calculated using the data points within the defined window. The red line in the plot represents the smoothed curve, which is generated by connecting these averaged points. During the animation, the window moves through the dataset, and two vertical green lines represent the boundaries of the window. The blue horizontal line indicates the average of the data points within the window. As the plot progresses, the red line gradually takes the shape of the smoothed curve, highlighting the underlying sine function.</p>
<p>Nonparametric smoothing methods, like the moving average used in this example, are crucial in situations where the data is noisy or when the underlying function is unknown. By applying such methods, it is possible to obtain a clearer understanding of trends and patterns in the data, which can then be used for further analysis and decision-making processes. These techniques are versatile and can be applied to a wide range of datasets and fields, such as finance, economics, and engineering, among others.</p>
<p>Here is the code that can be used for different window sizes:</p>
<div class="sourceCode" id="cb245"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb245-1"><a href="nonparametric-estimations---basics.html#cb245-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">300</span></span>
<span id="cb245-2"><a href="nonparametric-estimations---basics.html#cb245-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb245-3"><a href="nonparametric-estimations---basics.html#cb245-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">runif</span>(n) <span class="sc">*</span> <span class="dv">2</span> <span class="sc">*</span> pi)</span>
<span id="cb245-4"><a href="nonparametric-estimations---basics.html#cb245-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(n) <span class="sc">/</span> <span class="dv">4</span></span>
<span id="cb245-5"><a href="nonparametric-estimations---basics.html#cb245-5" aria-hidden="true" tabindex="-1"></a>dfm <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">time =</span> <span class="dv">1</span><span class="sc">:</span>n, <span class="at">y =</span> y)</span>
<span id="cb245-6"><a href="nonparametric-estimations---basics.html#cb245-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb245-7"><a href="nonparametric-estimations---basics.html#cb245-7" aria-hidden="true" tabindex="-1"></a>new_line <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="fu">matrix</span>(<span class="at">nrow =</span> <span class="dv">0</span>, <span class="at">ncol =</span> <span class="dv">2</span>))</span>
<span id="cb245-8"><a href="nonparametric-estimations---basics.html#cb245-8" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(new_line) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;time&quot;</span>, <span class="st">&quot;y&quot;</span>)</span>
<span id="cb245-9"><a href="nonparametric-estimations---basics.html#cb245-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb245-10"><a href="nonparametric-estimations---basics.html#cb245-10" aria-hidden="true" tabindex="-1"></a><span class="co"># window = 20</span></span>
<span id="cb245-11"><a href="nonparametric-estimations---basics.html#cb245-11" aria-hidden="true" tabindex="-1"></a><span class="co"># for (i in 1:(n - window)) {</span></span>
<span id="cb245-12"><a href="nonparametric-estimations---basics.html#cb245-12" aria-hidden="true" tabindex="-1"></a><span class="co">#   flush.console()</span></span>
<span id="cb245-13"><a href="nonparametric-estimations---basics.html#cb245-13" aria-hidden="true" tabindex="-1"></a><span class="co">#   new &lt;- data.frame(time = dfm$time[i:(i + window)],</span></span>
<span id="cb245-14"><a href="nonparametric-estimations---basics.html#cb245-14" aria-hidden="true" tabindex="-1"></a><span class="co">#                     y = dfm$y[i:(i + window)])</span></span>
<span id="cb245-15"><a href="nonparametric-estimations---basics.html#cb245-15" aria-hidden="true" tabindex="-1"></a><span class="co">#   new_line[i, ] &lt;- c((i+(window/2-1)), mean(new$y)) </span></span>
<span id="cb245-16"><a href="nonparametric-estimations---basics.html#cb245-16" aria-hidden="true" tabindex="-1"></a><span class="co">#   plot(dfm$time, dfm$y)</span></span>
<span id="cb245-17"><a href="nonparametric-estimations---basics.html#cb245-17" aria-hidden="true" tabindex="-1"></a><span class="co">#   abline(v=i, col = &quot;green&quot;)</span></span>
<span id="cb245-18"><a href="nonparametric-estimations---basics.html#cb245-18" aria-hidden="true" tabindex="-1"></a><span class="co">#   abline(v=(i+window), col = &quot;green&quot;)</span></span>
<span id="cb245-19"><a href="nonparametric-estimations---basics.html#cb245-19" aria-hidden="true" tabindex="-1"></a><span class="co">#   lines(new$time, rep(mean(new$y), (window+1)),</span></span>
<span id="cb245-20"><a href="nonparametric-estimations---basics.html#cb245-20" aria-hidden="true" tabindex="-1"></a><span class="co">#         lwd = 4,</span></span>
<span id="cb245-21"><a href="nonparametric-estimations---basics.html#cb245-21" aria-hidden="true" tabindex="-1"></a><span class="co">#         col = &quot;blue&quot;)</span></span>
<span id="cb245-22"><a href="nonparametric-estimations---basics.html#cb245-22" aria-hidden="true" tabindex="-1"></a><span class="co">#   lines(new_line$time, new_line$y,</span></span>
<span id="cb245-23"><a href="nonparametric-estimations---basics.html#cb245-23" aria-hidden="true" tabindex="-1"></a><span class="co">#         type = &quot;l&quot;,</span></span>
<span id="cb245-24"><a href="nonparametric-estimations---basics.html#cb245-24" aria-hidden="true" tabindex="-1"></a><span class="co">#         col = &quot;red&quot;,</span></span>
<span id="cb245-25"><a href="nonparametric-estimations---basics.html#cb245-25" aria-hidden="true" tabindex="-1"></a><span class="co">#         lwd = 3)</span></span>
<span id="cb245-26"><a href="nonparametric-estimations---basics.html#cb245-26" aria-hidden="true" tabindex="-1"></a><span class="co">#   Sys.sleep(.09)</span></span>
<span id="cb245-27"><a href="nonparametric-estimations---basics.html#cb245-27" aria-hidden="true" tabindex="-1"></a><span class="co"># }</span></span></code></pre></div>
<div id="density-estimations" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Density Estimations<a href="nonparametric-estimations---basics.html#density-estimations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will only look at one-variable Kernel density estimations. Let’s assume that a sample of <span class="math inline">\(n\)</span> observations, <span class="math inline">\(y_1,...,y_n\)</span>, is drawn from a parametric distribution <span class="math inline">\(f(y,\theta)\)</span>. If the data are i.i.d., the joint density function is:</p>
<p><span class="math display" id="eq:6-1">\[\begin{equation}
f(y;\theta)=\prod_{i=1}^{n} f\left(y_{i} ; \theta\right)
  \tag{6.1}
\end{equation}\]</span></p>
<p>To estimate, we find the parameters that maximize this density function (“likelihood”), or its logarithmic transformation:</p>
<p><span class="math display" id="eq:6-2">\[\begin{equation}
\ell(y ; \theta)=\log f(y ; \theta)=\sum_{i=1}^{n} \log f\left(y_{i} ; \theta\right)
  \tag{6.2}
\end{equation}\]</span></p>
<p>We apply the maximum likelihood estimation (MLE) method to recover <span class="math inline">\(\theta\)</span>. This is called <strong>parametric estimation</strong> and if our pre-determined density model is not right, that is, if <span class="math inline">\(f\)</span> is misspecified, we will have a biased estimator for <span class="math inline">\(\theta\)</span>. To avoid this problem, we can use <strong>nonparametric estimation</strong>, which does not require an assumption about the distribution of the data.</p>
<p>The starting point for a density estimation is a histogram. We define the intervals by choosing a number of bins and a starting value for the first interval. Here, we use 0 as a starting value and 10 bins:</p>
<div class="sourceCode" id="cb246"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb246-1"><a href="nonparametric-estimations---basics.html#cb246-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Random integers from 1 to 100</span></span>
<span id="cb246-2"><a href="nonparametric-estimations---basics.html#cb246-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb246-3"><a href="nonparametric-estimations---basics.html#cb246-3" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>, <span class="dv">100</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb246-4"><a href="nonparametric-estimations---basics.html#cb246-4" aria-hidden="true" tabindex="-1"></a><span class="fu">stem</span>(data)</span></code></pre></div>
<pre><code>## 
##   The decimal point is 1 digit(s) to the right of the |
## 
##   0 | 46777999
##   1 | 23344456
##   2 | 12335555677
##   3 | 00111224456889
##   4 | 01122337
##   5 | 0012337
##   6 | 003477999
##   7 | 1222466899
##   8 | 112366799
##   9 | 0011123334566799</code></pre>
<div class="sourceCode" id="cb248"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb248-1"><a href="nonparametric-estimations---basics.html#cb248-1" aria-hidden="true" tabindex="-1"></a>foo <span class="ot">&lt;-</span> <span class="fu">hist</span>(data, <span class="at">nclass =</span> <span class="dv">10</span>,</span>
<span id="cb248-2"><a href="nonparametric-estimations---basics.html#cb248-2" aria-hidden="true" tabindex="-1"></a>            <span class="at">col =</span> <span class="st">&quot;lightblue&quot;</span>,</span>
<span id="cb248-3"><a href="nonparametric-estimations---basics.html#cb248-3" aria-hidden="true" tabindex="-1"></a>            <span class="at">cex.main =</span> <span class="fl">0.80</span>, <span class="at">cex.axis =</span> <span class="fl">0.75</span>)</span></code></pre></div>
<p><img src="06-Basics_files/figure-html/nb1-1.png" width="672" /></p>
<div class="sourceCode" id="cb249"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb249-1"><a href="nonparametric-estimations---basics.html#cb249-1" aria-hidden="true" tabindex="-1"></a>foo<span class="sc">$</span>counts</span></code></pre></div>
<pre><code>##  [1]  8  8 13 13  9  7  7 10 11 14</code></pre>
<div class="sourceCode" id="cb251"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb251-1"><a href="nonparametric-estimations---basics.html#cb251-1" aria-hidden="true" tabindex="-1"></a>foo<span class="sc">$</span>density</span></code></pre></div>
<pre><code>##  [1] 0.008 0.008 0.013 0.013 0.009 0.007 0.007 0.010 0.011 0.014</code></pre>
<div class="sourceCode" id="cb253"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb253-1"><a href="nonparametric-estimations---basics.html#cb253-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(foo<span class="sc">$</span>density)</span></code></pre></div>
<pre><code>## [1] 0.1</code></pre>
<p>Not that the sum of these densities is not one. The vertical scale of a ‘frequency histogram’ shows the number of observations in each bin. From above, we know that the tallest bar has 14 observations, so this bar accounts for relative frequency 14/100=0.14 of the observations. As the relative frequency indicate probability their total would be 1. We are looking for a density function which gives the “height” of each observation. Since the width of this bar is 10, the density of each observation in the bin is 0.014.</p>
<p>We can have a formula to calculate the density for each data point:</p>
<p><span class="math display" id="eq:6-3">\[\begin{equation}
\hat{f}(y)=\frac{1}{n} \times \frac{\text{ number of observations in the interval of } y}{\text { width of the interval }}
  \tag{6.3}
\end{equation}\]</span></p>
<p>Here is the pdf on the same data with binwidth = 4 for our example:</p>
<div class="sourceCode" id="cb255"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb255-1"><a href="nonparametric-estimations---basics.html#cb255-1" aria-hidden="true" tabindex="-1"></a><span class="co"># to put pdf and X&#39;s on the same graph, we scale the data</span></span>
<span id="cb255-2"><a href="nonparametric-estimations---basics.html#cb255-2" aria-hidden="true" tabindex="-1"></a>foo <span class="ot">&lt;-</span> <span class="fu">hist</span>(data<span class="sc">/</span>(<span class="dv">10</span><span class="sc">*</span><span class="fu">mean</span>(data)),</span>
<span id="cb255-3"><a href="nonparametric-estimations---basics.html#cb255-3" aria-hidden="true" tabindex="-1"></a>            <span class="at">nclass =</span> <span class="dv">25</span>,</span>
<span id="cb255-4"><a href="nonparametric-estimations---basics.html#cb255-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">cex.main =</span> <span class="fl">0.80</span>,</span>
<span id="cb255-5"><a href="nonparametric-estimations---basics.html#cb255-5" aria-hidden="true" tabindex="-1"></a>            <span class="at">cex.axis =</span> <span class="fl">0.75</span>,</span>
<span id="cb255-6"><a href="nonparametric-estimations---basics.html#cb255-6" aria-hidden="true" tabindex="-1"></a>            <span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.2</span>),</span>
<span id="cb255-7"><a href="nonparametric-estimations---basics.html#cb255-7" aria-hidden="true" tabindex="-1"></a>            <span class="at">main =</span> <span class="cn">NULL</span>)</span>
<span id="cb255-8"><a href="nonparametric-estimations---basics.html#cb255-8" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(foo<span class="sc">$</span>mids, foo<span class="sc">$</span>density, <span class="at">col=</span><span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>) <span class="co">#Naive</span></span></code></pre></div>
<p><img src="06-Basics_files/figure-html/bn2-1.png" width="672" /></p>
<p>The number of bins defines the degree of smoothness of the histogram. We can have the following general expression for a nonparametric density estimation:</p>
<p><span class="math display" id="eq:6-4">\[\begin{equation}
f(x) \cong \frac{k}{n h} \text { where }\left\{\begin{array}{ll}{h} &amp; {\text { binwidth }} \\ {n} &amp; {\text { total } \text { number of observation points }} \\ {k} &amp; {\text { number of observations inside } h}\end{array}\right.
  \tag{6.4}
\end{equation}\]</span></p>
<p>Note that, in practical density estimation problems, two basic approaches can be adopted: (1) we can fix <span class="math inline">\(h\)</span> (width of the interval) and determine <span class="math inline">\(k\)</span> in each bin from the data, which is the subject of this chapter and called <strong>kernel density estimation (KDE)</strong>; or (2) we can fix <span class="math inline">\(k\)</span> in each bin and determine <span class="math inline">\(h\)</span> from the data. This gives rise to the <strong>k-nearest-neighbors (kNN)</strong> approach, which we cover in the next chapters.</p>
<p>The global density cab be obtained with a <em>moving window</em> (intervals with intersections), which is also called as <strong>Naive estimator</strong> (a.k.a. Parzen windows). The naive estimator is not sensitive to the position of bins, but it is not smooth either:</p>
<p><span class="math display" id="eq:6-5">\[\begin{equation}
\hat{f}(y)=\frac{1}{n h} \sum_{i=1}^{n} I\left(y-\frac{h}{2}&lt;y_{i}&lt;y+\frac{h}{2}\right),
  \tag{6.5}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(I(.)\)</span> is an indicator function, which results in value of 1 if the expression inside of the function is satisfied (0 otherwise). Thus, it counts the number of observations in a given window. The binwidth (<span class="math inline">\(h\)</span>) defines the bin range by adding and subtracting <span class="math inline">\(h/2\)</span> from <span class="math inline">\(y\)</span>. We can rearrange 6.5 differently:</p>
<p><span class="math display">\[
\hat{f}(y)=\frac{1}{2n h} \sum_{i=1}^{n} I\left(y-h&lt;y_{i}&lt;y+h\right).
\]</span></p>
<p>If we rewrite the inequality by subtracting <span class="math inline">\(y\)</span> and divide it by <span class="math inline">\(h\)</span>:</p>
<p><span class="math display">\[
\hat{f}(y)=\frac{1}{2n h} \sum_{i=1}^{n} I\left(-1&lt;\frac{y-y_{i}}{h}&lt;1\right),
\]</span>
which can be written more compactly:</p>
<p><span class="math display" id="eq:6-6">\[\begin{equation}
\hat{f}(y)=\frac{1}{2nh} \sum_{i=1}^{n} w\left(\frac{y-y_{i}}{h}\right) \quad \text { where } \quad w(x)=\left\{\begin{array}{ll}{1} &amp; {\text { if }|x|&lt;1} \\ {0} &amp; {\text { otherwise }}\end{array}\right.
  \tag{6.6}
\end{equation}\]</span></p>
<p>Consider a sample <span class="math inline">\(\left\{X_{i}\right\}_{i=1}^{10}\)</span>, which is 4, 5, 5, 6, 12, 14, 15, 15, 16, 17. And the bin width is <span class="math inline">\(h=4\)</span>. What’s the density of 3, <span class="math inline">\(\hat{f}(3)\)</span>? Note that we do not have 3 in the data.</p>
<p><span class="math display">\[
\begin{aligned} \hat{f}(3) &amp;=\frac{1}{2\times10 \times4}\left\{w\left(\frac{3-4}{4}\right)+w\left(\frac{3-5}{4}\right)+\ldots+w\left(\frac{3-17}{4}\right)\right\} \\ &amp;=\frac{1}{80}\left\{1+1+1+1+0+\ldots+0\right\} \\ &amp;=\frac{1}{20} \end{aligned}
\]</span></p>
<p>This “naive” estimator yields density estimates that have discontinuities and weights equal at all points <span class="math inline">\(x_i\)</span> regardless of their distance to the estimation point <span class="math inline">\(x\)</span>. In other words, in any given bin, <span class="math inline">\(x\)</span>’s have a uniform distribution. That’s why, <span class="math inline">\(w(x)\)</span> is commonly replaced with a smooth kernel function <span class="math inline">\(K(x)\)</span>. Kernel replaces it with usually, but not always, with a radially symmetric and unimodal pdf, such as the Gaussian. You can choose <strong>“gaussian”, “epanechnikov”, “rectangular”, “triangular”, “biweight”, “cosine”, “optcosine”</strong> distributions in the R’s <code>density()</code> function.</p>
<p>With the Kernel density estimator replacing <span class="math inline">\(w\)</span> in 6.6 by a kernel function <span class="math inline">\(K\)</span>:</p>
<p><span class="math display" id="eq:6-7">\[\begin{equation}
\hat{f}(y)=\frac{1}{2n h} \sum_{i=1}^{n} K\left(\frac{y-y_{i}}{h}\right),
  \tag{6.7}
\end{equation}\]</span></p>
<p>Here are the samples of kernels, <span class="math inline">\(K(x)\)</span>:</p>
<p><span class="math display">\[
\text { Rectangular (Uniform): } ~~ K(x)=\left\{\begin{array}{ll}{\frac{1}{2}} &amp; {|x|&lt;1} \\ {0} &amp; {\text { otherwise }}\end{array}\right.
\]</span>
<span class="math display">\[
\text { Epanechnikov: } ~~ K(x)=\left\{\begin{array}{cc}{\frac{3}{4}\left(1-\frac{1}{5} x^{2}\right) / \sqrt{5}} &amp; {|x|&lt;\sqrt{5}} \\ {0} &amp; {\text { otherwise }}\end{array}\right.
\]</span>
<span class="math display">\[
\text { Gaussian: } ~~ K(x)=\frac{1}{\sqrt{2 \pi}} e^{(-1 / 2) x^{2}}
\]</span></p>
<p>Although the kernel density estimator depends on the choices of the kernel function <span class="math inline">\(K\)</span>, it is very sensitive to <span class="math inline">\(h\)</span>, not to <span class="math inline">\(K\)</span>.</p>
<p>In R, the standard kernel density estimation is obtained by <code>density()</code>, which uses <strong>Silverman rule-of-thumb</strong> to select the optimal bandwidth, <span class="math inline">\(h\)</span>, and the <strong>Gaussian kernel</strong>. Here is an example with our artificial data:</p>
<div class="sourceCode" id="cb256"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb256-1"><a href="nonparametric-estimations---basics.html#cb256-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">readRDS</span>(<span class="st">&quot;fes73.rds&quot;</span>)</span>
<span id="cb256-2"><a href="nonparametric-estimations---basics.html#cb256-2" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> X<span class="sc">/</span><span class="fu">mean</span>(X)</span>
<span id="cb256-3"><a href="nonparametric-estimations---basics.html#cb256-3" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(X[X <span class="sc">&lt;</span> <span class="fl">3.5</span>], <span class="at">nclass =</span> <span class="dv">130</span>, <span class="at">probability =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> <span class="st">&quot;white&quot;</span>,</span>
<span id="cb256-4"><a href="nonparametric-estimations---basics.html#cb256-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">cex.axis =</span> <span class="fl">0.75</span>, <span class="at">cex.main =</span> <span class="fl">0.8</span>, <span class="at">main =</span> <span class="cn">NULL</span>)</span>
<span id="cb256-5"><a href="nonparametric-estimations---basics.html#cb256-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">density</span>(X, <span class="at">adjust =</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">4</span>), <span class="at">col =</span> <span class="st">&quot;red&quot;</span>) <span class="co"># bandwith/4</span></span>
<span id="cb256-6"><a href="nonparametric-estimations---basics.html#cb256-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">density</span>(X, <span class="at">adjust =</span> <span class="dv">1</span>), <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>) </span>
<span id="cb256-7"><a href="nonparametric-estimations---basics.html#cb256-7" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">density</span>(X, <span class="at">adjust =</span> <span class="dv">4</span>), <span class="at">col =</span> <span class="st">&quot;green&quot;</span>) <span class="co"># bandwith x 4</span></span>
<span id="cb256-8"><a href="nonparametric-estimations---basics.html#cb256-8" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">density</span>(X, <span class="at">kernel =</span> <span class="st">&quot;rectangular&quot;</span>, <span class="at">adjust =</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">4</span>),</span>
<span id="cb256-9"><a href="nonparametric-estimations---basics.html#cb256-9" aria-hidden="true" tabindex="-1"></a>      <span class="at">col =</span> <span class="st">&quot;black&quot;</span>) <span class="co"># bandwith x 4</span></span></code></pre></div>
<p><img src="06-Basics_files/figure-html/bn3-1.png" width="672" /></p>
<div class="sourceCode" id="cb257"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb257-1"><a href="nonparametric-estimations---basics.html#cb257-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Here is the details of the last one</span></span>
<span id="cb257-2"><a href="nonparametric-estimations---basics.html#cb257-2" aria-hidden="true" tabindex="-1"></a><span class="fu">density</span>(X, <span class="at">adjust =</span> <span class="dv">4</span>)</span></code></pre></div>
<pre><code>## 
## Call:
##  density.default(x = X, adjust = 4)
## 
## Data: X (6968 obs.); Bandwidth &#39;bw&#39; = 0.3423
## 
##        x                y            
##  Min.   :-0.954   Min.   :0.0000000  
##  1st Qu.: 2.352   1st Qu.:0.0000576  
##  Median : 5.657   Median :0.0005510  
##  Mean   : 5.657   Mean   :0.0755509  
##  3rd Qu.: 8.963   3rd Qu.:0.0269050  
##  Max.   :12.269   Max.   :0.6282958</code></pre>
<p>Bigger the bandwidth <span class="math inline">\(h\)</span> smoother the pdf. Which one is better? There are several bandwidth selection methods to identify the best fitting <span class="math inline">\(h\)</span>, which are beyond the scope of this chapter.</p>
<p>Why do we estimate pdf with KDE? Note that, when you explore our density object by <code>str()</code>, you’ll see that <code>y</code> will get you the pdf values of density for each value of <span class="math inline">\(X\)</span> you have in our data. Of course pdf is a function: the values of pdf are <span class="math inline">\(Y\)</span> and the input values are <span class="math inline">\(X\)</span>. Hence, given a new data point on <span class="math inline">\(X\)</span>, we may want to find the outcome of <span class="math inline">\(Y\)</span> (the value of pdf for that data point) based on the function, the kernel density estimator that we have from the <code>density()</code> function result. When can do it with <code>approxfun()</code>:</p>
<div class="sourceCode" id="cb259"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb259-1"><a href="nonparametric-estimations---basics.html#cb259-1" aria-hidden="true" tabindex="-1"></a>poo <span class="ot">&lt;-</span> <span class="fu">density</span>(X, <span class="at">adjust =</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">4</span>)</span>
<span id="cb259-2"><a href="nonparametric-estimations---basics.html#cb259-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb259-3"><a href="nonparametric-estimations---basics.html#cb259-3" aria-hidden="true" tabindex="-1"></a>dens <span class="ot">&lt;-</span> <span class="fu">approxfun</span>(poo)</span>
<span id="cb259-4"><a href="nonparametric-estimations---basics.html#cb259-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb259-5"><a href="nonparametric-estimations---basics.html#cb259-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(poo, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span>
<span id="cb259-6"><a href="nonparametric-estimations---basics.html#cb259-6" aria-hidden="true" tabindex="-1"></a>x_new <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.5</span>,<span class="fl">1.5</span>,<span class="fl">2.2</span>)</span>
<span id="cb259-7"><a href="nonparametric-estimations---basics.html#cb259-7" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x_new, <span class="fu">dens</span>(x_new), <span class="at">col=</span><span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="06-Basics_files/figure-html/bn4-1.png" width="672" /></p>
<div class="sourceCode" id="cb260"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb260-1"><a href="nonparametric-estimations---basics.html#cb260-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dens</span>(<span class="fl">1.832</span>)</span></code></pre></div>
<pre><code>## [1] 0.1511783</code></pre>
<p>This is a predicted value of pdf when <span class="math inline">\(x=1.832\)</span> estimated by KDE without specifying the model apriori, which is like a magic! Based on the sample we have, we just predicted <span class="math inline">\(Y\)</span> without explicitly modeling it.</p>
<p>Keep in mind that our objective here is not to estimate probabilities. We can do it if we want. But then, of course we have to remember that values of a density curve are not the same as probabilities. Taking the integral of the desired section in the estimated pdf would give us the corresponding probability.</p>
<div class="sourceCode" id="cb262"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb262-1"><a href="nonparametric-estimations---basics.html#cb262-1" aria-hidden="true" tabindex="-1"></a><span class="fu">integrate</span>(dens, <span class="at">lower =</span> <span class="dv">1</span>, <span class="at">upper =</span> <span class="fl">1.832</span>)</span></code></pre></div>
<pre><code>## 0.3574063 with absolute error &lt; 5.8e-06</code></pre>
</div>
<div id="kernel-regressions" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Kernel regressions<a href="nonparametric-estimations---basics.html#kernel-regressions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Theoretically, nonparametric density estimation can be easily extended to several dimensions (multivariate distributions). For instance, suppose we are interested in predicting <span class="math inline">\(Y\)</span> by using several predictors. We have almost no idea what functional form the predictive model could take. If we have a sufficiently large sample, we may obtain a reasonably accurate estimate of the joint probability density (by kernel density estimation or similar) of <span class="math inline">\(Y\)</span> and the <span class="math inline">\(X\)</span>’s.</p>
<p>In practice, however, we rarely have enough sample to execute robust density estimations. As the dimension increases, KDE rapidly needs many more samples. Even in low dimensions, a KDE-based model has mostly no ability to generalize. In other words, if our test set has parts outside our training distribution, we cannot use our KDE-based model for forecasting.</p>
<p>In regression functions, the outcome is the conditional mean of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>’s. Since nonparametric regressions are agnostic about the functional form between the outcome and the covariates, they are immune to <strong>misspecification error</strong>.</p>
<p>The traditional regression model fits the model:</p>
<p><span class="math display" id="eq:6-8">\[\begin{equation}
y=m(\mathbf{x}, \boldsymbol{\beta})+\varepsilon
  \tag{6.8}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\beta\)</span> is a vector of parameters to be estimated, and <strong>x</strong> is a vector of predictors. The errors, <span class="math inline">\(\varepsilon\)</span> are assumed to be i.i.d, <span class="math inline">\(\varepsilon \sim NID(0, \sigma^2)\)</span>. The function <span class="math inline">\(m(\mathbf{x},\beta)\)</span>, which links the conditional averages of <span class="math inline">\(y\)</span> to the predictors, is specified in advance. The generic nonparametric regression model is written similarly, but the function <span class="math inline">\(m\)</span> remains unspecified:</p>
<p><span class="math display">\[
\begin{aligned} y &amp;=m(\mathbf{x})+\varepsilon \\ &amp;=m\left(x_{1}, x_{2}, \ldots, x_{p}\right)+\varepsilon, \end{aligned}
\]</span>
where <span class="math inline">\(\varepsilon \sim NID(0, \sigma^2)\)</span> again.</p>
<p>An important special case of the general model is nonparametric simple regression, where there is only one predictor:</p>
<p><span class="math display">\[
y=m(x)+\varepsilon
\]</span></p>
<p>With its definition, we can rewrite <span class="math inline">\(m\)</span> as</p>
<p><span class="math display" id="eq:6-10">\[\begin{equation}
\begin{split}
\begin{aligned} m(x) &amp;=\mathbb{E}[Y | X=x] \\ &amp;=\int y f_{Y | X=x}(y) \mathrm{d} y \\ &amp;=\frac{\int y f(x, y) \mathrm{d} y}{f_{X}(x)} \end{aligned}
  \end{split}
  \tag{6.9}
\end{equation}\]</span></p>
<p>This shows that the regression function can be computed from the joint density <span class="math inline">\(f(x,y)\)</span> and the marginal <span class="math inline">\(f(x)\)</span>. Therefore, given a sample <span class="math inline">\(\left\{\left(X_{i}, Y_{i}\right)\right\}_{i=1}^{n}\)</span>, a nonparametric estimate of <span class="math inline">\(m\)</span> may follow by replacing the these densities with their kernel density estimators, as we have see earlier in this section.</p>
<p>A limitation of the bin smoothing approach in kernel density estimations is that we need small windows for the “approximately constant” assumptions to hold. As a result, we end up with a small number of data points to average and obtain imprecise estimates of <span class="math inline">\(f(x)\)</span>. Locally estimated scatter-plot smoothing (LOESS, <code>loess</code>) permits us to consider larger window sizes, which is a nonparametric approach that fits multiple regressions in local neighborhood.</p>
<p>It is called local regression because, instead of assuming the function is approximately constant in a window, it fits a local regression at the “neighborhood” of <span class="math inline">\(x_0\)</span>. The distance from <span class="math inline">\(x_0\)</span> is controlled by the <code>span</code> setting, which determines the width of the moving (sliding) window when smoothing the data. The parameter <code>span</code>represents the proportion of the data (size of the sliding window) that is considered to be neighboring <span class="math inline">\(x_0\)</span>. For example, if N is the number of data points and <code>span</code> = 0.5, then for a given <span class="math inline">\(x_0\)</span>, <code>loess</code> will use the <span class="math inline">\(0.5\times N\)</span> closest points to <span class="math inline">\(x_0\)</span> for the fit. Usually <code>span</code> should be between 0 and 1. When its larger than 1, the regression will be over-smoothed. Moreover, the weighting in the regression is proportional to <span class="math inline">\(1-(\text{distance}/\text{maximum distance})^3)^3\)</span>, which is called the Tukey tri-weight. Different than the Gaussian kernel, the Tukey tri-weight covers more points closer to the center point.</p>
<p>We will not see the theoretical derivations of kernel regressions but an illustration of local polynomial of order 0, 1 and 2, below. (Examples from Ahamada and Flachaire) <span class="citation">(<a href="#ref-Ibrahim_2011" role="doc-biblioref"><strong>Ibrahim_2011?</strong></a>)</span>. The Nadaraya–Watson estimator is a local polynomial of order 0, which estimates a local mean of <span class="math inline">\(Y_1...Y_n\)</span> around <span class="math inline">\(X=x_0\)</span>.</p>
<div class="sourceCode" id="cb264"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb264-1"><a href="nonparametric-estimations---basics.html#cb264-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Simulating our data</span></span>
<span id="cb264-2"><a href="nonparametric-estimations---basics.html#cb264-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">300</span></span>
<span id="cb264-3"><a href="nonparametric-estimations---basics.html#cb264-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb264-4"><a href="nonparametric-estimations---basics.html#cb264-4" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">sort</span>(<span class="fu">runif</span>(n)<span class="sc">*</span><span class="dv">2</span><span class="sc">*</span>pi)</span>
<span id="cb264-5"><a href="nonparametric-estimations---basics.html#cb264-5" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(n)<span class="sc">/</span><span class="dv">4</span></span>
<span id="cb264-6"><a href="nonparametric-estimations---basics.html#cb264-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y)</span></code></pre></div>
<p><img src="06-Basics_files/figure-html/bn5-1.png" width="672" /></p>
<div class="sourceCode" id="cb265"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb265-1"><a href="nonparametric-estimations---basics.html#cb265-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Estimation</span></span>
<span id="cb265-2"><a href="nonparametric-estimations---basics.html#cb265-2" aria-hidden="true" tabindex="-1"></a>loe0 <span class="ot">&lt;-</span> <span class="fu">loess</span>(y<span class="sc">~</span>x, <span class="at">degree =</span> <span class="dv">0</span>, <span class="at">span =</span> <span class="fl">0.5</span>) <span class="co">#Nadaraya-Watson</span></span>
<span id="cb265-3"><a href="nonparametric-estimations---basics.html#cb265-3" aria-hidden="true" tabindex="-1"></a>loe1 <span class="ot">&lt;-</span> <span class="fu">loess</span>(y<span class="sc">~</span>x, <span class="at">degree =</span> <span class="dv">1</span>, <span class="at">span =</span> <span class="fl">0.5</span>) <span class="co">#Local linear</span></span>
<span id="cb265-4"><a href="nonparametric-estimations---basics.html#cb265-4" aria-hidden="true" tabindex="-1"></a>loe2 <span class="ot">&lt;-</span> <span class="fu">loess</span>(y<span class="sc">~</span>x, <span class="at">degree =</span> <span class="dv">2</span>, <span class="at">span =</span> <span class="fl">0.5</span>) <span class="co">#Locally quadratic</span></span>
<span id="cb265-5"><a href="nonparametric-estimations---basics.html#cb265-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb265-6"><a href="nonparametric-estimations---basics.html#cb265-6" aria-hidden="true" tabindex="-1"></a><span class="co">#To have a plot, we first calculate the fitted values on a grid,</span></span>
<span id="cb265-7"><a href="nonparametric-estimations---basics.html#cb265-7" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(x), <span class="fu">max</span>(x), <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb265-8"><a href="nonparametric-estimations---basics.html#cb265-8" aria-hidden="true" tabindex="-1"></a>fit0 <span class="ot">&lt;-</span> <span class="fu">predict</span>(loe0, t)</span>
<span id="cb265-9"><a href="nonparametric-estimations---basics.html#cb265-9" aria-hidden="true" tabindex="-1"></a>fit1 <span class="ot">&lt;-</span> <span class="fu">predict</span>(loe1, t)</span>
<span id="cb265-10"><a href="nonparametric-estimations---basics.html#cb265-10" aria-hidden="true" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">predict</span>(loe2, t)</span>
<span id="cb265-11"><a href="nonparametric-estimations---basics.html#cb265-11" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">cex.main =</span> <span class="fl">0.80</span>, <span class="at">cex.axis =</span> <span class="fl">0.75</span>)</span>
<span id="cb265-12"><a href="nonparametric-estimations---basics.html#cb265-12" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(t, fit0, <span class="at">col =</span> <span class="st">&quot;green&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span>
<span id="cb265-13"><a href="nonparametric-estimations---basics.html#cb265-13" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(t, fit1, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb265-14"><a href="nonparametric-estimations---basics.html#cb265-14" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(t, fit2, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span></code></pre></div>
<p><img src="06-Basics_files/figure-html/bn5-2.png" width="672" /></p>
<p>And, its sensitivity to the bandwidth:</p>
<div class="sourceCode" id="cb266"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb266-1"><a href="nonparametric-estimations---basics.html#cb266-1" aria-hidden="true" tabindex="-1"></a>fit0 <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="fu">loess</span>(y<span class="sc">~</span>x, <span class="at">degree=</span><span class="dv">2</span>, <span class="at">span =</span> <span class="fl">0.05</span>)) <span class="co">#minimum, 5%*300 = 14 obs. </span></span>
<span id="cb266-2"><a href="nonparametric-estimations---basics.html#cb266-2" aria-hidden="true" tabindex="-1"></a>fit1 <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="fu">loess</span>(y<span class="sc">~</span>x, <span class="at">degree=</span><span class="dv">2</span>, <span class="at">span =</span> <span class="fl">0.75</span>)) <span class="co">#default</span></span>
<span id="cb266-3"><a href="nonparametric-estimations---basics.html#cb266-3" aria-hidden="true" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="fu">loess</span>(y<span class="sc">~</span>x, <span class="at">degree=</span><span class="dv">2</span>, <span class="at">span =</span> <span class="dv">2</span>)) </span>
<span id="cb266-4"><a href="nonparametric-estimations---basics.html#cb266-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb266-5"><a href="nonparametric-estimations---basics.html#cb266-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">cex.main =</span> <span class="fl">0.80</span>, <span class="at">cex.axis =</span> <span class="fl">0.75</span>)</span>
<span id="cb266-6"><a href="nonparametric-estimations---basics.html#cb266-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, fit0, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;green&quot;</span>)</span>
<span id="cb266-7"><a href="nonparametric-estimations---basics.html#cb266-7" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, fit1, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb266-8"><a href="nonparametric-estimations---basics.html#cb266-8" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, fit2, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span></code></pre></div>
<p><img src="06-Basics_files/figure-html/bn6-1.png" width="672" /></p>
<p>The bandwidth we choose will be determined by the prediction accuracy. This subject is related to <strong>cross-validation</strong>, which we will see later as a whole chapter.</p>
</div>
<div id="regression-splines" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Regression Splines<a href="nonparametric-estimations---basics.html#regression-splines" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In a model, non-linearity can be captured by estimating a linear regression through several intervals, which is called as <strong>piecewise linear model</strong>.</p>
<p><span class="math display" id="eq:6-11">\[\begin{equation}
\begin{split}
\begin{array}{ll}{y=\alpha_{1}+\beta_{1} x+\varepsilon_{1}} &amp; {\text { if } \quad x \in\left[z_{0} ; z_{1}\right]} \\ {y=\alpha_{2}+\beta_{2} x+\varepsilon_{2}} &amp; {\text { if } \quad x \in\left[z_{1} ; z_{2}\right]} \\ {\cdots} \\ {y=\alpha_{k}+\beta_{k} x+\varepsilon_{k}} &amp; {\text { if } \quad x \in\left[z_{k-1} ; z_{k}\right]}\end{array}
  \end{split}
  \tag{6.10}
\end{equation}\]</span></p>
<p>This function will not have a smooth transitions at the knots, <span class="math inline">\(z.\)</span>, which brings us to a regression <strong>spline, which is a piecewise regression model</strong> with a smooth transition at the knots. First, let’s see how a piecewise regression works with an example. To show evidence of nonlinearity between short and long-term interest rates, <a href="https://www.sciencedirect.com/science/article/abs/pii/0304407695017542">Pfann et al (1996)</a> estimates the following piecewise linear model:</p>
<p><span class="math display" id="eq:6-12">\[\begin{equation}
y=\beta_{0}+\beta_{1} x+\beta_{2}(x-\kappa)_{+}+\varepsilon
  \tag{6.11}
\end{equation}\]</span></p>
<p>Here in 6.12 the <span class="math inline">\(\kappa\)</span> denotes <em>knot</em> where the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(x\)</span> changes. (Subscript <span class="math inline">\(+\)</span> means that the term will be zero when it is not positive).</p>
<div class="sourceCode" id="cb267"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb267-1"><a href="nonparametric-estimations---basics.html#cb267-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">read.table</span>(<span class="st">&quot;irates.dat&quot;</span>, <span class="at">header =</span> <span class="cn">TRUE</span>)</span>
<span id="cb267-2"><a href="nonparametric-estimations---basics.html#cb267-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> data<span class="sc">$</span>GS10</span>
<span id="cb267-3"><a href="nonparametric-estimations---basics.html#cb267-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> data<span class="sc">$</span>TB3MS</span>
<span id="cb267-4"><a href="nonparametric-estimations---basics.html#cb267-4" aria-hidden="true" tabindex="-1"></a>xk <span class="ot">&lt;-</span> (x <span class="sc">-</span> <span class="fl">10.8</span>)<span class="sc">*</span>(x <span class="sc">&gt;</span> <span class="fl">10.8</span>)</span>
<span id="cb267-5"><a href="nonparametric-estimations---basics.html#cb267-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x <span class="sc">+</span> xk))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x + xk)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.3978 -0.9051 -0.1962  0.9584  3.2530 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.77900    0.10181   17.47  &lt; 2e-16 ***
## x            0.92489    0.01915   48.29  &lt; 2e-16 ***
## xk          -0.42910    0.08958   -4.79 2.06e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.107 on 657 degrees of freedom
## Multiple R-squared:   0.83,  Adjusted R-squared:  0.8295 
## F-statistic:  1604 on 2 and 657 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb269"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb269-1"><a href="nonparametric-estimations---basics.html#cb269-1" aria-hidden="true" tabindex="-1"></a>reorder <span class="ot">&lt;-</span> <span class="fu">order</span>(x) <span class="co"># to avoid messy lines</span></span>
<span id="cb269-2"><a href="nonparametric-estimations---basics.html#cb269-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(</span>
<span id="cb269-3"><a href="nonparametric-estimations---basics.html#cb269-3" aria-hidden="true" tabindex="-1"></a>  x[reorder],</span>
<span id="cb269-4"><a href="nonparametric-estimations---basics.html#cb269-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">fitted</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x <span class="sc">+</span> xk))[reorder],</span>
<span id="cb269-5"><a href="nonparametric-estimations---basics.html#cb269-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">&quot;l&quot;</span>,</span>
<span id="cb269-6"><a href="nonparametric-estimations---basics.html#cb269-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">col =</span> <span class="st">&quot;red&quot;</span>,</span>
<span id="cb269-7"><a href="nonparametric-estimations---basics.html#cb269-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">cex.main =</span> <span class="fl">0.80</span>,</span>
<span id="cb269-8"><a href="nonparametric-estimations---basics.html#cb269-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">cex.axis =</span> <span class="fl">0.75</span>,</span>
<span id="cb269-9"><a href="nonparametric-estimations---basics.html#cb269-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">xlab =</span> <span class="st">&quot;TB3MS&quot;</span>,</span>
<span id="cb269-10"><a href="nonparametric-estimations---basics.html#cb269-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">ylab =</span> <span class="st">&quot;GS10&quot;</span></span>
<span id="cb269-11"><a href="nonparametric-estimations---basics.html#cb269-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb269-12"><a href="nonparametric-estimations---basics.html#cb269-12" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x, y, <span class="at">col =</span> <span class="st">&quot;grey&quot;</span>)</span>
<span id="cb269-13"><a href="nonparametric-estimations---basics.html#cb269-13" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="fl">10.8</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;darkgreen&quot;</span>)</span></code></pre></div>
<p><img src="06-Basics_files/figure-html/bn7-1.png" width="672" /></p>
<p>When the model is extended to <span class="math inline">\(q\)</span> knots, it becomes a piecewise regression:</p>
<p><span class="math display" id="eq:6-13">\[\begin{equation}
y=\beta_{0}+\beta_{1} x+\sum_{j=1}^{q} \beta_{1 j}\left(x-\kappa_{j}\right)_{+}+\varepsilon
  \tag{6.12}
\end{equation}\]</span></p>
<div class="sourceCode" id="cb270"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb270-1"><a href="nonparametric-estimations---basics.html#cb270-1" aria-hidden="true" tabindex="-1"></a><span class="co">#5 knots</span></span>
<span id="cb270-2"><a href="nonparametric-estimations---basics.html#cb270-2" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">2.8</span>, <span class="fl">4.8</span>, <span class="fl">6.8</span>, <span class="fl">8.8</span>, <span class="fl">10.8</span>)</span>
<span id="cb270-3"><a href="nonparametric-estimations---basics.html#cb270-3" aria-hidden="true" tabindex="-1"></a>Xk <span class="ot">&lt;-</span> x <span class="sc">-</span> <span class="fu">matrix</span>(k, <span class="fu">length</span>(x), <span class="fu">length</span>(k), <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb270-4"><a href="nonparametric-estimations---basics.html#cb270-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb270-5"><a href="nonparametric-estimations---basics.html#cb270-5" aria-hidden="true" tabindex="-1"></a>Xk <span class="ot">&lt;-</span> Xk<span class="sc">*</span>(Xk <span class="sc">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb270-6"><a href="nonparametric-estimations---basics.html#cb270-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb270-7"><a href="nonparametric-estimations---basics.html#cb270-7" aria-hidden="true" tabindex="-1"></a>reorder <span class="ot">&lt;-</span> <span class="fu">order</span>(x) <span class="co"># to avoid messy lines</span></span>
<span id="cb270-8"><a href="nonparametric-estimations---basics.html#cb270-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(</span>
<span id="cb270-9"><a href="nonparametric-estimations---basics.html#cb270-9" aria-hidden="true" tabindex="-1"></a>  x,</span>
<span id="cb270-10"><a href="nonparametric-estimations---basics.html#cb270-10" aria-hidden="true" tabindex="-1"></a>  y,</span>
<span id="cb270-11"><a href="nonparametric-estimations---basics.html#cb270-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>,</span>
<span id="cb270-12"><a href="nonparametric-estimations---basics.html#cb270-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">ylim =</span> <span class="fu">c</span>(<span class="fl">2.5</span>, <span class="fl">14.5</span>),</span>
<span id="cb270-13"><a href="nonparametric-estimations---basics.html#cb270-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">cex.main =</span> <span class="fl">0.80</span>,</span>
<span id="cb270-14"><a href="nonparametric-estimations---basics.html#cb270-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">cex.axis =</span> <span class="fl">0.75</span>,</span>
<span id="cb270-15"><a href="nonparametric-estimations---basics.html#cb270-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">xlab =</span> <span class="st">&quot;TB3MS&quot;</span>,</span>
<span id="cb270-16"><a href="nonparametric-estimations---basics.html#cb270-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">ylab =</span> <span class="st">&quot;GS10&quot;</span></span>
<span id="cb270-17"><a href="nonparametric-estimations---basics.html#cb270-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb270-18"><a href="nonparametric-estimations---basics.html#cb270-18" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x[reorder], <span class="fu">fitted</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x <span class="sc">+</span> Xk))[reorder], <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb270-19"><a href="nonparametric-estimations---basics.html#cb270-19" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> k, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;darkgreen&quot;</span>)</span></code></pre></div>
<p><img src="06-Basics_files/figure-html/bn8-1.png" width="672" /></p>
<p>The piecewise linear model is not smooth at the knots. To get a smooth estimator, we replace the basis of linear functions by a basis of spline functions, which is defined as</p>
<p><span class="math display">\[
b_{0}(x, \kappa), \ldots, b_{r}(x, \kappa)
\]</span>
Hence, regression spline is defined as:</p>
<p><span class="math display" id="eq:6-14">\[\begin{equation}
y=\sum_{j=0}^{r} \beta_{j} b_{j}(x, \kappa)+\varepsilon
  \tag{6.13}
\end{equation}\]</span></p>
<p>Once the knots are fixed, it becomes essentially a parametric regression. For example, the following spline regression model,</p>
<p><span class="math display">\[
y=\beta_{0}+\beta_{1} x+\cdots+\beta_{p} x^{p}+\sum_{j=1}^{q} \beta_{p j}\left(x-\kappa_{j}\right)_{+}^{p}+\varepsilon,
\]</span></p>
<p>can be estimated as a cubic spline:</p>
<p><span class="math display">\[
\hat{m}(x)=2+x-2 x^{2}+x^{3}+(x-0.4)_{+}^{3}-(x-0.8)_{+}^{3}
\]</span></p>
<p>which can be rewritten as</p>
<p><span class="math display">\[
\hat{m}(x)=\left\{\begin{aligned} 2+x-2 x^{2}+x^{3} &amp; \text { if } &amp; x&lt;0.4 \\ 2+x-2 x^{2}+x^{3}+(x-0.4)^{3} &amp; \text { if } &amp; 0.4 \leq x&lt;0.8 \\ 2+x-2 x^{2}+x^{3}+(x-0.4)^{3}-(x-0.8)^{3} &amp; \text { if } &amp; x \geq 0.8 \end{aligned}\right.
\]</span></p>
<p>In short <strong>a spline is a piecewise polynomial function</strong>.</p>
<p>Now, the question is how we are supposed to choose the basis and the knots? Spline estimation is sensitive to the choice of the number of knots and their position. A knot can have an economic interpretation such as a specific date or structural change in the data, thus some information is required. There are two common approaches for choosing the position of the knots: <strong>quantiles</strong> - intervals with the same number of observations; <strong>equidistant</strong> - intervals with the same width. As for the number of knots, if it’s too small, the potential bias can be large in the estimator, so a larger number is preferred.</p>
<p>Let’s use the same data and apply regression spline. Here is the example for equidistant knots:</p>
<div class="sourceCode" id="cb271"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb271-1"><a href="nonparametric-estimations---basics.html#cb271-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(splines)</span>
<span id="cb271-2"><a href="nonparametric-estimations---basics.html#cb271-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb271-3"><a href="nonparametric-estimations---basics.html#cb271-3" aria-hidden="true" tabindex="-1"></a><span class="co">#equidistant knots</span></span>
<span id="cb271-4"><a href="nonparametric-estimations---basics.html#cb271-4" aria-hidden="true" tabindex="-1"></a>nknots <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb271-5"><a href="nonparametric-estimations---basics.html#cb271-5" aria-hidden="true" tabindex="-1"></a>k <span class="ot">=</span> <span class="fu">seq</span>(<span class="fu">min</span>(x), <span class="fu">max</span>(x), <span class="at">length.out =</span> nknots <span class="sc">+</span> <span class="dv">2</span>)[<span class="dv">2</span><span class="sc">:</span>(nknots <span class="sc">+</span> <span class="dv">1</span>)]</span>
<span id="cb271-6"><a href="nonparametric-estimations---basics.html#cb271-6" aria-hidden="true" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">bs</span>(x, <span class="at">degree =</span> <span class="dv">3</span>, <span class="at">knots =</span> k)) <span class="co">#check ?bs</span></span>
<span id="cb271-7"><a href="nonparametric-estimations---basics.html#cb271-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb271-8"><a href="nonparametric-estimations---basics.html#cb271-8" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ bs(x, degree = 3, knots = k))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.5190 -0.8537 -0.1889  0.8841  3.2169 
## 
## Coefficients:
##                               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                     3.1855     0.3160  10.082  &lt; 2e-16 ***
## bs(x, degree = 3, knots = k)1   0.4081     0.5722   0.713  0.47596    
## bs(x, degree = 3, knots = k)2   0.9630     0.3163   3.045  0.00242 ** 
## bs(x, degree = 3, knots = k)3   4.2239     0.4311   9.798  &lt; 2e-16 ***
## bs(x, degree = 3, knots = k)4   6.2233     0.3869  16.084  &lt; 2e-16 ***
## bs(x, degree = 3, knots = k)5   9.9021     0.6620  14.957  &lt; 2e-16 ***
## bs(x, degree = 3, knots = k)6   9.8107     0.8370  11.722  &lt; 2e-16 ***
## bs(x, degree = 3, knots = k)7  10.8604     0.9400  11.553  &lt; 2e-16 ***
## bs(x, degree = 3, knots = k)8  10.6991     1.0866   9.847  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.086 on 651 degrees of freedom
## Multiple R-squared:  0.8378, Adjusted R-squared:  0.8358 
## F-statistic: 420.3 on 8 and 651 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb273"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb273-1"><a href="nonparametric-estimations---basics.html#cb273-1" aria-hidden="true" tabindex="-1"></a>u <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(x), <span class="fu">max</span>(x), <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb273-2"><a href="nonparametric-estimations---basics.html#cb273-2" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(model1, <span class="at">newdata =</span> <span class="fu">list</span>(<span class="at">x =</span> u), <span class="at">se =</span> <span class="cn">TRUE</span>)</span>
<span id="cb273-3"><a href="nonparametric-estimations---basics.html#cb273-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb273-4"><a href="nonparametric-estimations---basics.html#cb273-4" aria-hidden="true" tabindex="-1"></a>reorder <span class="ot">&lt;-</span> <span class="fu">order</span>(u) <span class="co"># to avoid messy lines</span></span>
<span id="cb273-5"><a href="nonparametric-estimations---basics.html#cb273-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(</span>
<span id="cb273-6"><a href="nonparametric-estimations---basics.html#cb273-6" aria-hidden="true" tabindex="-1"></a>  x,</span>
<span id="cb273-7"><a href="nonparametric-estimations---basics.html#cb273-7" aria-hidden="true" tabindex="-1"></a>  y,</span>
<span id="cb273-8"><a href="nonparametric-estimations---basics.html#cb273-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>,</span>
<span id="cb273-9"><a href="nonparametric-estimations---basics.html#cb273-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">ylim =</span> <span class="fu">c</span>(<span class="fl">2.5</span>, <span class="fl">14.5</span>),</span>
<span id="cb273-10"><a href="nonparametric-estimations---basics.html#cb273-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">cex.main =</span> <span class="fl">0.80</span>,</span>
<span id="cb273-11"><a href="nonparametric-estimations---basics.html#cb273-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">cex.axis =</span> <span class="fl">0.75</span></span>
<span id="cb273-12"><a href="nonparametric-estimations---basics.html#cb273-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb273-13"><a href="nonparametric-estimations---basics.html#cb273-13" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(u, pred<span class="sc">$</span>fit, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb273-14"><a href="nonparametric-estimations---basics.html#cb273-14" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(u[reorder], pred<span class="sc">$</span>fit[reorder] <span class="sc">+</span> <span class="fl">1.96</span> <span class="sc">*</span> pred<span class="sc">$</span>se, <span class="at">lty =</span> <span class="st">&quot;dashed&quot;</span>)</span>
<span id="cb273-15"><a href="nonparametric-estimations---basics.html#cb273-15" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(u[reorder], pred<span class="sc">$</span>fit[reorder] <span class="sc">-</span> <span class="fl">1.96</span> <span class="sc">*</span> pred<span class="sc">$</span>se, <span class="at">lty =</span> <span class="st">&quot;dashed&quot;</span>)</span>
<span id="cb273-16"><a href="nonparametric-estimations---basics.html#cb273-16" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> k, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;darkgreen&quot;</span>)</span></code></pre></div>
<p><img src="06-Basics_files/figure-html/bn9-1.png" width="672" /></p>
<p>And, an example for quantile knots:</p>
<div class="sourceCode" id="cb274"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb274-1"><a href="nonparametric-estimations---basics.html#cb274-1" aria-hidden="true" tabindex="-1"></a>model2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">bs</span>(x, <span class="at">degree =</span> <span class="dv">3</span>, <span class="at">df =</span> <span class="dv">8</span>))</span>
<span id="cb274-2"><a href="nonparametric-estimations---basics.html#cb274-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb274-3"><a href="nonparametric-estimations---basics.html#cb274-3" aria-hidden="true" tabindex="-1"></a>u <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(x), <span class="fu">max</span>(x), <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb274-4"><a href="nonparametric-estimations---basics.html#cb274-4" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(model2, <span class="at">newdata =</span> <span class="fu">list</span>(<span class="at">x =</span> u), <span class="at">se =</span> <span class="cn">TRUE</span>)</span>
<span id="cb274-5"><a href="nonparametric-estimations---basics.html#cb274-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb274-6"><a href="nonparametric-estimations---basics.html#cb274-6" aria-hidden="true" tabindex="-1"></a>reorder <span class="ot">&lt;-</span> <span class="fu">order</span>(u) <span class="co"># to avoid messy lines</span></span>
<span id="cb274-7"><a href="nonparametric-estimations---basics.html#cb274-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(</span>
<span id="cb274-8"><a href="nonparametric-estimations---basics.html#cb274-8" aria-hidden="true" tabindex="-1"></a>  x,</span>
<span id="cb274-9"><a href="nonparametric-estimations---basics.html#cb274-9" aria-hidden="true" tabindex="-1"></a>  y,</span>
<span id="cb274-10"><a href="nonparametric-estimations---basics.html#cb274-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>,</span>
<span id="cb274-11"><a href="nonparametric-estimations---basics.html#cb274-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">ylim =</span> <span class="fu">c</span>(<span class="fl">2.5</span>, <span class="fl">14.5</span>),</span>
<span id="cb274-12"><a href="nonparametric-estimations---basics.html#cb274-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">cex.main =</span> <span class="fl">0.80</span>,</span>
<span id="cb274-13"><a href="nonparametric-estimations---basics.html#cb274-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">cex.axis =</span> <span class="fl">0.75</span></span>
<span id="cb274-14"><a href="nonparametric-estimations---basics.html#cb274-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb274-15"><a href="nonparametric-estimations---basics.html#cb274-15" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(u, pred<span class="sc">$</span>fit, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb274-16"><a href="nonparametric-estimations---basics.html#cb274-16" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(u[reorder], pred<span class="sc">$</span>fit[reorder] <span class="sc">+</span> <span class="fl">1.96</span> <span class="sc">*</span> pred<span class="sc">$</span>se, <span class="at">lty =</span> <span class="st">&quot;dashed&quot;</span>)</span>
<span id="cb274-17"><a href="nonparametric-estimations---basics.html#cb274-17" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(u[reorder], pred<span class="sc">$</span>fit[reorder] <span class="sc">-</span> <span class="fl">1.96</span> <span class="sc">*</span> pred<span class="sc">$</span>se, <span class="at">lty =</span> <span class="st">&quot;dashed&quot;</span>)</span>
<span id="cb274-18"><a href="nonparametric-estimations---basics.html#cb274-18" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="fu">attr</span>(<span class="fu">bs</span>(x, <span class="at">degree =</span> <span class="dv">3</span>, <span class="at">df =</span> <span class="dv">8</span>), <span class="st">&quot;knots&quot;</span>)</span>
<span id="cb274-19"><a href="nonparametric-estimations---basics.html#cb274-19" aria-hidden="true" tabindex="-1"></a><span class="co">#These functions provide access to a single attribute of an object.</span></span>
<span id="cb274-20"><a href="nonparametric-estimations---basics.html#cb274-20" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> k, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;darkgreen&quot;</span>)</span></code></pre></div>
<p><img src="06-Basics_files/figure-html/bn10-1.png" width="672" /></p>
<p>Note that, in regression spline, <code>df</code> (degree of freedom) is the number of components of the basis. Thus, with cubic spline, <code>df</code>=6 defines 3 knots (quartiles q:25; q:50; q:75). Since, <code>df</code>= 8, we have 5 knots, so is the quantile of 20%.</p>
<p>Is bad or good for prediction to have a very large number of knots? See <a href="https://freakonometrics.hypotheses.org/47681" class="uri">https://freakonometrics.hypotheses.org/47681</a> <span class="citation">(<a href="#ref-Charpentier_SplineReg" role="doc-biblioref"><strong>Charpentier_SplineReg?</strong></a>)</span>, for the argument about the number of knots. Here is Arthur Charpentier’s conclusion:</p>
<blockquote>
<p>So, it looks like having a lot of non-significant components in a spline regression is not a major issue. And reducing the degrees of freedom is clearly a bad option.</p>
</blockquote>
<p>Let’s see how sensitive the results to the number of knots <a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>:</p>
<div class="sourceCode" id="cb275"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb275-1"><a href="nonparametric-estimations---basics.html#cb275-1" aria-hidden="true" tabindex="-1"></a>pred1 <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">bs</span>(x, <span class="at">degree =</span> <span class="dv">3</span>, <span class="at">df =</span> <span class="dv">6</span>)))   <span class="co">#quartiles</span></span>
<span id="cb275-2"><a href="nonparametric-estimations---basics.html#cb275-2" aria-hidden="true" tabindex="-1"></a>pred2 <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">bs</span>(x, <span class="at">degree =</span> <span class="dv">3</span>, <span class="at">df =</span> <span class="dv">12</span>)))  <span class="co">#deciles</span></span>
<span id="cb275-3"><a href="nonparametric-estimations---basics.html#cb275-3" aria-hidden="true" tabindex="-1"></a>pred3 <span class="ot">&lt;-</span> <span class="fu">predict</span>(<span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">bs</span>(x, <span class="at">degree =</span> <span class="dv">3</span>, <span class="at">df =</span> <span class="dv">102</span>))) <span class="co">#percentile</span></span>
<span id="cb275-4"><a href="nonparametric-estimations---basics.html#cb275-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb275-5"><a href="nonparametric-estimations---basics.html#cb275-5" aria-hidden="true" tabindex="-1"></a>reorder <span class="ot">&lt;-</span> <span class="fu">order</span>(x) </span>
<span id="cb275-6"><a href="nonparametric-estimations---basics.html#cb275-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="fl">2.5</span>, <span class="fl">14.5</span>),</span>
<span id="cb275-7"><a href="nonparametric-estimations---basics.html#cb275-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">cex.main =</span> <span class="fl">0.80</span>, <span class="at">cex.axis =</span> <span class="fl">0.75</span>)</span>
<span id="cb275-8"><a href="nonparametric-estimations---basics.html#cb275-8" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x[reorder], pred1[reorder], <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb275-9"><a href="nonparametric-estimations---basics.html#cb275-9" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x[reorder], pred2[reorder], <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span>
<span id="cb275-10"><a href="nonparametric-estimations---basics.html#cb275-10" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x[reorder], pred3[reorder], <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;green&quot;</span>)</span></code></pre></div>
<p><img src="06-Basics_files/figure-html/bn11-1.png" width="672" /></p>
<p>There is a method called as <strong>smoothing spline</strong>, a spline basis method that avoids the knot selection problem. It uses a maximal set of knots, at the unique values of the each <span class="math inline">\(X\)</span> values and control for the fit by <strong>regularization</strong>. Similar to OLS, it selects <span class="math inline">\(\beta_j\)</span> to minimize the residual sum of squares but with a penalization on the curvature in the function:</p>
<p><span class="math display" id="eq:6-15">\[\begin{equation}
\sum_{i=1}^{n}\left[y_{i}-m\left(x_{i}\right)\right]^{2}+\lambda \int\left[m^{\prime \prime}(x)\right]^{2} d x
  \tag{6.14}
\end{equation}\]</span></p>
<p>The first term minimizes the closeness to the data with a constraint (the second term) on the curvature in the function. If <span class="math inline">\(\lambda=0\)</span>, <span class="math inline">\(m(x_i)\)</span> could be any function that fits the data very closely (interpolates the data). If <span class="math inline">\(\lambda &gt; 0\)</span> and goes infinity, it makes the penalization so high that the algorithm fits a simple least squares line without any curvature. The penalty term, or bandwidth <span class="math inline">\(\lambda\)</span>, restricts fluctuations of <span class="math inline">\(\hat{m}\)</span> and the optimum <span class="math inline">\(\lambda\)</span> minimizes the distance between <span class="math inline">\(m\)</span>, which is unknown, and <span class="math inline">\(\hat{m}\)</span>. The method used to find the optimal <span class="math inline">\(\lambda\)</span> is called as <em>generalized cross-validation</em>. Here is a simulation:</p>
<div class="sourceCode" id="cb276"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb276-1"><a href="nonparametric-estimations---basics.html#cb276-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb276-2"><a href="nonparametric-estimations---basics.html#cb276-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">200</span></span>
<span id="cb276-3"><a href="nonparametric-estimations---basics.html#cb276-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(n)</span>
<span id="cb276-4"><a href="nonparametric-estimations---basics.html#cb276-4" aria-hidden="true" tabindex="-1"></a>dgm <span class="ot">&lt;-</span> <span class="fu">sin</span>(<span class="dv">12</span><span class="sc">*</span>(x <span class="sc">+</span> <span class="fl">0.2</span>))<span class="sc">/</span>(x <span class="sc">+</span> <span class="fl">0.2</span>) <span class="co"># our dgm</span></span>
<span id="cb276-5"><a href="nonparametric-estimations---basics.html#cb276-5" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> dgm <span class="sc">+</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb276-6"><a href="nonparametric-estimations---basics.html#cb276-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">cex.main =</span> <span class="fl">0.80</span>, <span class="at">cex.axis =</span> <span class="fl">0.75</span>)</span>
<span id="cb276-7"><a href="nonparametric-estimations---basics.html#cb276-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb276-8"><a href="nonparametric-estimations---basics.html#cb276-8" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x[<span class="fu">order</span>(x)], dgm[<span class="fu">order</span>(x)], <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;black&quot;</span>) <span class="co"># DGM</span></span>
<span id="cb276-9"><a href="nonparametric-estimations---basics.html#cb276-9" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">smooth.spline</span>(x,y, <span class="at">df =</span> <span class="dv">20</span>), <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb276-10"><a href="nonparametric-estimations---basics.html#cb276-10" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">smooth.spline</span>(x,y, <span class="at">df =</span> <span class="dv">40</span>), <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span></code></pre></div>
<p><img src="06-Basics_files/figure-html/bn12-1.png" width="672" /></p>
<p>And when we use automated selection of knots:</p>
<div class="sourceCode" id="cb277"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb277-1"><a href="nonparametric-estimations---basics.html#cb277-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">cex.main =</span> <span class="fl">0.80</span>, <span class="at">cex.axis =</span> <span class="fl">0.75</span>)</span>
<span id="cb277-2"><a href="nonparametric-estimations---basics.html#cb277-2" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x[<span class="fu">order</span>(x)], dgm[<span class="fu">order</span>(x)], <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;black&quot;</span>) <span class="co"># DGM</span></span>
<span id="cb277-3"><a href="nonparametric-estimations---basics.html#cb277-3" aria-hidden="true" tabindex="-1"></a><span class="co">#lines(smooth.spline(x,y, cv = FALSE), lwd = 2, col = &quot;blue&quot;) # With GCV</span></span>
<span id="cb277-4"><a href="nonparametric-estimations---basics.html#cb277-4" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">smooth.spline</span>(x,y), <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>) <span class="co"># With LOOCV</span></span></code></pre></div>
<p><img src="06-Basics_files/figure-html/bn13-1.png" width="672" /></p>
<p>Note that there are other packages for smoothing splines like <code>npreg</code> that uses <code>ss()</code><a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>.</p>
<p>In theory, nonparametric regression estimations can be easily extended to several regressors but in practice, good precision may require a huge number of observations without any graphical tools to make interpretation. Because it is difficult to fit the nonparametric regression model when there are many predictors, more restrictive models have been developed. One such model is the additive regression model,</p>
<p><span class="math display" id="eq:6-9">\[\begin{equation}
y=\beta_{0}+m_{1}\left(x_{1}\right)+m_{2}\left(x_{2}\right)+\cdots+m_{p}\left(x_{p}\right)+\varepsilon
  \tag{6.15}
\end{equation}\]</span></p>
<p>Variations on the additive regression model include semiparametric models, in which some of the predictors enter linearly or interactively. There are two common methods that have been used in multivariable settings: GAM (generalized additive regression splines) and MARS (multivariate adaptive regression splines).</p>
</div>
<div id="mars---multivariate-adaptive-regression-splines" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> MARS - Multivariate Adaptive Regression Splines<a href="nonparametric-estimations---basics.html#mars---multivariate-adaptive-regression-splines" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Linear models can incorporate nonlinear patterns in the data by manually adding squared terms and interaction effects, if we know the specific nature of the nonlinearity in advance. Although we can extend linear models to capture nonlinear relationships by including polynomial terms, it is generally unusual to use degree greater than 3 or 4. Even if we use higher degrees, with multiple interactions and polynomials, the dimension of the model goes out of control. Although useful, the typical implementation of polynomial regression requires the user to explicitly identify and incorporate which variables should have what specific degree polynomials and interactions. With data sets that can easily contain 50, 100, or more variables today, this would require an enormous time to determine the explicit structure of nonlinear nature of the model.</p>
<p>Multivariate adaptive regression splines (MARS) can be a solution to capture the nonlinearity aspect of polynomial regression by assessing cutpoints (knots) like in a piecewise regression model. For example,in a simple one-variable model, the procedure will first look for the single point across the range of <span class="math inline">\(X\)</span> values where two different linear relationships between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> achieve the smallest error. The results is known as a hinge function <span class="math inline">\(\text{max}(0,x−a)\)</span> where <span class="math inline">\(a\)</span> is the cutpoint value. Once the first knot has been found, the search continues for a second knot, which results in three linear models. This procedure can continue until many knots are found, producing a highly nonlinear pattern. Once the full set of knots have been created, we can sequentially remove knots that do not contribute significantly to predictive accuracy. This process is known as <em>pruning</em> and can be done by cross-validation.</p>
<p>Here is a simple application with the Longley dataset (in the <code>datasets</code> package) that describes seven economic variables observed from 1947 to 1962 used to predict the number of people employed yearly.</p>
<div class="sourceCode" id="cb278"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb278-1"><a href="nonparametric-estimations---basics.html#cb278-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(earth)</span>
<span id="cb278-2"><a href="nonparametric-estimations---basics.html#cb278-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb278-3"><a href="nonparametric-estimations---basics.html#cb278-3" aria-hidden="true" tabindex="-1"></a><span class="co"># load data</span></span>
<span id="cb278-4"><a href="nonparametric-estimations---basics.html#cb278-4" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(longley)</span>
<span id="cb278-5"><a href="nonparametric-estimations---basics.html#cb278-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(longley)</span></code></pre></div>
<pre><code>##   GNP.deflator         GNP          Unemployed     Armed.Forces  
##  Min.   : 83.00   Min.   :234.3   Min.   :187.0   Min.   :145.6  
##  1st Qu.: 94.53   1st Qu.:317.9   1st Qu.:234.8   1st Qu.:229.8  
##  Median :100.60   Median :381.4   Median :314.4   Median :271.8  
##  Mean   :101.68   Mean   :387.7   Mean   :319.3   Mean   :260.7  
##  3rd Qu.:111.25   3rd Qu.:454.1   3rd Qu.:384.2   3rd Qu.:306.1  
##  Max.   :116.90   Max.   :554.9   Max.   :480.6   Max.   :359.4  
##    Population         Year         Employed    
##  Min.   :107.6   Min.   :1947   Min.   :60.17  
##  1st Qu.:111.8   1st Qu.:1951   1st Qu.:62.71  
##  Median :116.8   Median :1954   Median :65.50  
##  Mean   :117.4   Mean   :1954   Mean   :65.32  
##  3rd Qu.:122.3   3rd Qu.:1958   3rd Qu.:68.29  
##  Max.   :130.1   Max.   :1962   Max.   :70.55</code></pre>
<div class="sourceCode" id="cb280"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb280-1"><a href="nonparametric-estimations---basics.html#cb280-1" aria-hidden="true" tabindex="-1"></a><span class="co"># fit model</span></span>
<span id="cb280-2"><a href="nonparametric-estimations---basics.html#cb280-2" aria-hidden="true" tabindex="-1"></a>fit1 <span class="ot">&lt;-</span> <span class="fu">earth</span>(Employed <span class="sc">~</span> ., longley)</span>
<span id="cb280-3"><a href="nonparametric-estimations---basics.html#cb280-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit1)</span></code></pre></div>
<pre><code>## Call: earth(formula=Employed~., data=longley)
## 
##                       coefficients
## (Intercept)            -1682.60259
## Year                       0.89475
## h(293.6-Unemployed)        0.01226
## h(Unemployed-293.6)       -0.01596
## h(Armed.Forces-263.7)     -0.01470
## 
## Selected 5 of 8 terms, and 3 of 6 predictors
## Termination condition: GRSq -Inf at 8 terms
## Importance: Year, Unemployed, Armed.Forces, GNP.deflator-unused, ...
## Number of terms at each degree of interaction: 1 4 (additive model)
## GCV 0.2389853    RSS 0.7318924    GRSq 0.9818348    RSq 0.996044</code></pre>
<div class="sourceCode" id="cb282"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb282-1"><a href="nonparametric-estimations---basics.html#cb282-1" aria-hidden="true" tabindex="-1"></a><span class="co"># summarize the importance of input variables</span></span>
<span id="cb282-2"><a href="nonparametric-estimations---basics.html#cb282-2" aria-hidden="true" tabindex="-1"></a><span class="fu">evimp</span>(fit1)</span></code></pre></div>
<pre><code>##              nsubsets   gcv    rss
## Year                4 100.0  100.0
## Unemployed          3  24.1   23.0
## Armed.Forces        2  10.4   10.8</code></pre>
<div class="sourceCode" id="cb284"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb284-1"><a href="nonparametric-estimations---basics.html#cb284-1" aria-hidden="true" tabindex="-1"></a><span class="co">#plot</span></span>
<span id="cb284-2"><a href="nonparametric-estimations---basics.html#cb284-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit1, <span class="at">which =</span> <span class="dv">1</span>, <span class="at">cex.main =</span> <span class="fl">0.80</span>, <span class="at">cex.axis =</span> <span class="fl">0.75</span>)</span></code></pre></div>
<p><img src="06-Basics_files/figure-html/bn14-1.png" width="672" /></p>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb285-1"><a href="nonparametric-estimations---basics.html#cb285-1" aria-hidden="true" tabindex="-1"></a><span class="co"># make predictions</span></span>
<span id="cb285-2"><a href="nonparametric-estimations---basics.html#cb285-2" aria-hidden="true" tabindex="-1"></a>predictions1 <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit1, longley)</span>
<span id="cb285-3"><a href="nonparametric-estimations---basics.html#cb285-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb285-4"><a href="nonparametric-estimations---basics.html#cb285-4" aria-hidden="true" tabindex="-1"></a><span class="co"># summarize accuracy for fit1</span></span>
<span id="cb285-5"><a href="nonparametric-estimations---basics.html#cb285-5" aria-hidden="true" tabindex="-1"></a>mse <span class="ot">&lt;-</span> <span class="fu">mean</span>((longley<span class="sc">$</span>Employed <span class="sc">-</span> predictions1)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb285-6"><a href="nonparametric-estimations---basics.html#cb285-6" aria-hidden="true" tabindex="-1"></a><span class="co">#Remember this an in-sample fit. </span></span>
<span id="cb285-7"><a href="nonparametric-estimations---basics.html#cb285-7" aria-hidden="true" tabindex="-1"></a>mse</span></code></pre></div>
<pre><code>## [1] 0.04574327</code></pre>
<p>The figure illustrates the model selection with GCV (generalized cross-validation) <span class="math inline">\(R^2\)</span> based on the number of terms retained in the model. These retained terms are constructed from original predictors (right-hand y-axis). The vertical dashed line at 5 indicates the optimal number of non-intercept terms retained where marginal increases in GCV R2 are less than 0.001.</p>
<p>Let’s use another data, the Ames Housing data, which is available by <code>AmesHousing</code> package.</p>
<div class="sourceCode" id="cb287"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb287-1"><a href="nonparametric-estimations---basics.html#cb287-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(AmesHousing)</span>
<span id="cb287-2"><a href="nonparametric-estimations---basics.html#cb287-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb287-3"><a href="nonparametric-estimations---basics.html#cb287-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a basic MARS model</span></span>
<span id="cb287-4"><a href="nonparametric-estimations---basics.html#cb287-4" aria-hidden="true" tabindex="-1"></a>amesdata <span class="ot">&lt;-</span> <span class="fu">make_ames</span>()</span>
<span id="cb287-5"><a href="nonparametric-estimations---basics.html#cb287-5" aria-hidden="true" tabindex="-1"></a>ames1 <span class="ot">&lt;-</span> <span class="fu">earth</span>(Sale_Price <span class="sc">~</span> .,  <span class="at">data =</span> amesdata)</span>
<span id="cb287-6"><a href="nonparametric-estimations---basics.html#cb287-6" aria-hidden="true" tabindex="-1"></a>ames2 <span class="ot">&lt;-</span> <span class="fu">earth</span>(Sale_Price <span class="sc">~</span> .,  <span class="at">data =</span> amesdata, <span class="at">degree =</span> <span class="dv">2</span>)</span>
<span id="cb287-7"><a href="nonparametric-estimations---basics.html#cb287-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb287-8"><a href="nonparametric-estimations---basics.html#cb287-8" aria-hidden="true" tabindex="-1"></a><span class="co"># In addition to pruning the number of knots,</span></span>
<span id="cb287-9"><a href="nonparametric-estimations---basics.html#cb287-9" aria-hidden="true" tabindex="-1"></a><span class="co"># we can also assess potential interactions between different hinge functions.</span></span>
<span id="cb287-10"><a href="nonparametric-estimations---basics.html#cb287-10" aria-hidden="true" tabindex="-1"></a><span class="co"># In the 2nd model, &quot;degree = 2&quot; argument allows level-2 interactions. </span></span>
<span id="cb287-11"><a href="nonparametric-estimations---basics.html#cb287-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb287-12"><a href="nonparametric-estimations---basics.html#cb287-12" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ames1)</span></code></pre></div>
<pre><code>## Call: earth(formula=Sale_Price~., data=amesdata)
## 
##                                coefficients
## (Intercept)                      279490.795
## NeighborhoodNorthridge_Heights    16810.853
## NeighborhoodCrawford              24189.424
## NeighborhoodNorthridge            27618.337
## NeighborhoodStone_Brook           31410.387
## NeighborhoodGreen_Hills          109592.758
## Condition_2PosN                  -96442.914
## Overall_QualGood                  12990.668
## Overall_QualVery_Good             34907.970
## Overall_QualExcellent             84380.868
## Overall_QualVery_Excellent       125196.226
## Overall_CondFair                 -23679.636
## Overall_CondGood                  11521.886
## Overall_CondVery_Good             14138.461
## Bsmt_ExposureGd                   11893.023
## FunctionalTyp                     17341.390
## h(15431-Lot_Area)                    -1.749
## h(Lot_Area-15431)                     0.301
## h(2003-Year_Built)                 -426.978
## h(Year_Built-2003)                 4212.701
## h(1972-Year_Remod_Add)              253.232
## h(Year_Remod_Add-1972)              486.266
## h(1869-Bsmt_Unf_SF)                  19.399
## h(Bsmt_Unf_SF-1869)                -121.684
## h(Total_Bsmt_SF-1822)               125.954
## h(2452-Total_Bsmt_SF)               -31.670
## h(Total_Bsmt_SF-2452)              -221.022
## h(Second_Flr_SF-1540)               320.816
## h(Gr_Liv_Area-3005)                 237.824
## h(3228-Gr_Liv_Area)                 -50.647
## h(Gr_Liv_Area-3228)                -316.547
## h(Kitchen_AbvGr-1)               -22620.827
## h(1-Fireplaces)                   -5701.130
## h(Fireplaces-1)                    8654.214
## h(2-Garage_Cars)                  -5290.463
## h(Garage_Cars-2)                  11400.346
## h(210-Screen_Porch)                 -55.241
## 
## Selected 37 of 40 terms, and 26 of 308 predictors
## Termination condition: RSq changed by less than 0.001 at 40 terms
## Importance: Gr_Liv_Area, Year_Built, Total_Bsmt_SF, Overall_QualExcellent, ...
## Number of terms at each degree of interaction: 1 36 (additive model)
## GCV 506531262    RSS 1.411104e+12    GRSq 0.9206569    RSq 0.9245098</code></pre>
<div class="sourceCode" id="cb289"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb289-1"><a href="nonparametric-estimations---basics.html#cb289-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ames2)</span></code></pre></div>
<pre><code>## Call: earth(formula=Sale_Price~., data=amesdata, degree=2)
## 
##                                                   coefficients
## (Intercept)                                         304004.163
## NeighborhoodGreen_Hills                             107542.815
## Overall_QualGood                                     28295.297
## Overall_QualVery_Good                                50500.728
## Overall_QualExcellent                                80054.922
## Overall_QualVery_Excellent                          115273.427
## Bsmt_ExposureGd                                      11761.126
## h(5400-Lot_Area)                                        -4.428
## h(Lot_Area-5400)                                         3.752
## h(2003-Year_Built)                                    -497.006
## h(Year_Built-2003)                                    7976.946
## h(Year_Remod_Add-1974)                                 957.791
## h(2452-Total_Bsmt_SF)                                  -54.823
## h(Total_Bsmt_SF-2452)                                   49.902
## h(3228-Gr_Liv_Area)                                    -44.151
## h(Gr_Liv_Area-3228)                                    197.513
## h(2-Fireplaces)                                      -6761.928
## h(Lot_Area-5400) * Overall_CondFair                     -2.710
## NeighborhoodCrawford * h(2003-Year_Built)              399.860
## Overall_QualAverage * h(2452-Total_Bsmt_SF)              6.310
## Overall_QualAbove_Average * h(2452-Total_Bsmt_SF)       11.542
## Overall_QualVery_Good * h(Bsmt_Full_Bath-1)          49827.988
## Overall_QualVery_Good * h(1-Bsmt_Full_Bath)         -12863.190
## Overall_CondGood * h(3228-Gr_Liv_Area)                   4.782
## Mas_Vnr_TypeStone * h(Gr_Liv_Area-3228)               -512.416
## h(Lot_Area-19645) * h(2452-Total_Bsmt_SF)               -0.001
## h(Lot_Area-5400) * h(Half_Bath-1)                       -3.867
## h(Lot_Area-5400) * h(1-Half_Bath)                       -0.397
## h(Lot_Area-5400) * h(Open_Porch_SF-195)                 -0.011
## h(Lot_Area-5400) * h(195-Open_Porch_SF)                 -0.005
## h(Lot_Area-5400) * h(192-Screen_Porch)                  -0.008
## h(2003-Year_Built) * h(Total_Bsmt_SF-1117)              -0.729
## h(2003-Year_Built) * h(1117-Total_Bsmt_SF)               0.368
## h(Year_Built-2003) * h(2439-Gr_Liv_Area)                -5.516
## h(Year_Remod_Add-1974) * h(Mas_Vnr_Area-14)              1.167
## h(Year_Remod_Add-1974) * h(14-Mas_Vnr_Area)             17.544
## h(Year_Remod_Add-1974) * h(Gr_Liv_Area-1627)             1.067
## h(Year_Remod_Add-1974) * h(932-Garage_Area)             -1.132
## h(Year_Remod_Add-1974) * h(Longitude- -93.6278)     -19755.291
## h(Year_Remod_Add-1974) * h(-93.6278-Longitude)       -7450.926
## h(1191-Bsmt_Unf_SF) * h(3228-Gr_Liv_Area)                0.009
## h(Bsmt_Unf_SF-1191) * h(3228-Gr_Liv_Area)               -0.028
## 
## Selected 42 of 49 terms, and 26 of 308 predictors
## Termination condition: RSq changed by less than 0.001 at 49 terms
## Importance: Gr_Liv_Area, Year_Built, Total_Bsmt_SF, Overall_QualExcellent, ...
## Number of terms at each degree of interaction: 1 16 25
## GCV 415202608    RSS 1.132115e+12    GRSq 0.9349626    RSq 0.9394349</code></pre>
<div class="sourceCode" id="cb291"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb291-1"><a href="nonparametric-estimations---basics.html#cb291-1" aria-hidden="true" tabindex="-1"></a><span class="co"># predictions</span></span>
<span id="cb291-2"><a href="nonparametric-estimations---basics.html#cb291-2" aria-hidden="true" tabindex="-1"></a>predictions1 <span class="ot">&lt;-</span> <span class="fu">predict</span>(ames1, amesdata)</span>
<span id="cb291-3"><a href="nonparametric-estimations---basics.html#cb291-3" aria-hidden="true" tabindex="-1"></a>predictions2 <span class="ot">&lt;-</span> <span class="fu">predict</span>(ames2, amesdata)</span>
<span id="cb291-4"><a href="nonparametric-estimations---basics.html#cb291-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb291-5"><a href="nonparametric-estimations---basics.html#cb291-5" aria-hidden="true" tabindex="-1"></a><span class="co"># summarize accuracy for ames1 and ames2</span></span>
<span id="cb291-6"><a href="nonparametric-estimations---basics.html#cb291-6" aria-hidden="true" tabindex="-1"></a>mse <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">sqrt</span>((amesdata<span class="sc">$</span>Sale_Price <span class="sc">-</span> predictions1)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb291-7"><a href="nonparametric-estimations---basics.html#cb291-7" aria-hidden="true" tabindex="-1"></a>mse</span></code></pre></div>
<pre><code>## [1] 15345.66</code></pre>
<div class="sourceCode" id="cb293"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb293-1"><a href="nonparametric-estimations---basics.html#cb293-1" aria-hidden="true" tabindex="-1"></a>mse <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">sqrt</span>((amesdata<span class="sc">$</span>Sale_Price <span class="sc">-</span> predictions2)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb293-2"><a href="nonparametric-estimations---basics.html#cb293-2" aria-hidden="true" tabindex="-1"></a>mse</span></code></pre></div>
<pre><code>## [1] 13910.27</code></pre>
<p>Now the second model includes interaction terms between multiple hinge functions. For example, <span class="math inline">\(h(Year\_Built-2003) \times h(Gr\_Liv\_Area-2274)\)</span> is an interaction effect for those houses built prior to 2003 and have less than 2,274 square feet of living space above ground.</p>
<p>There are two tuning parameters with a MARS model: the <code>degree</code> of interactions and <code>nprune</code> - the number of retained terms. These parameters are called <em>hyperparameters</em> and we need to perform a grid search to find the best combination that maximizes the prediction accuracy. We will have chapter on this subject with examples later. For now, we will have a simple grid search with the <code>caret</code> package, which provides the most comprehensive machine learning library in R.</p>
<div class="sourceCode" id="cb295"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb295-1"><a href="nonparametric-estimations---basics.html#cb295-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb295-2"><a href="nonparametric-estimations---basics.html#cb295-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb295-3"><a href="nonparametric-estimations---basics.html#cb295-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(vip)</span>
<span id="cb295-4"><a href="nonparametric-estimations---basics.html#cb295-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb295-5"><a href="nonparametric-estimations---basics.html#cb295-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Grid to search</span></span>
<span id="cb295-6"><a href="nonparametric-estimations---basics.html#cb295-6" aria-hidden="true" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(</span>
<span id="cb295-7"><a href="nonparametric-estimations---basics.html#cb295-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">degree =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>,</span>
<span id="cb295-8"><a href="nonparametric-estimations---basics.html#cb295-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">nprune =</span> <span class="fu">seq</span>(<span class="dv">2</span>, <span class="dv">100</span>, <span class="at">length.out =</span> <span class="dv">10</span>) <span class="sc">%&gt;%</span> <span class="fu">floor</span>())</span>
<span id="cb295-9"><a href="nonparametric-estimations---basics.html#cb295-9" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(grid)</span></code></pre></div>
<pre><code>##   degree nprune
## 1      1      2
## 2      2      2
## 3      3      2
## 4      1     12
## 5      2     12
## 6      3     12</code></pre>
<div class="sourceCode" id="cb297"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb297-1"><a href="nonparametric-estimations---basics.html#cb297-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Training</span></span>
<span id="cb297-2"><a href="nonparametric-estimations---basics.html#cb297-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb297-3"><a href="nonparametric-estimations---basics.html#cb297-3" aria-hidden="true" tabindex="-1"></a>mars <span class="ot">&lt;-</span> <span class="fu">train</span>(</span>
<span id="cb297-4"><a href="nonparametric-estimations---basics.html#cb297-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">subset</span>(amesdata, <span class="at">select =</span> <span class="sc">-</span>Sale_Price),</span>
<span id="cb297-5"><a href="nonparametric-estimations---basics.html#cb297-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> amesdata<span class="sc">$</span>Sale_Price,</span>
<span id="cb297-6"><a href="nonparametric-estimations---basics.html#cb297-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;earth&quot;</span>,</span>
<span id="cb297-7"><a href="nonparametric-estimations---basics.html#cb297-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">metric =</span> <span class="st">&quot;RMSE&quot;</span>,</span>
<span id="cb297-8"><a href="nonparametric-estimations---basics.html#cb297-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span>),</span>
<span id="cb297-9"><a href="nonparametric-estimations---basics.html#cb297-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">tuneGrid =</span> grid</span>
<span id="cb297-10"><a href="nonparametric-estimations---basics.html#cb297-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb297-11"><a href="nonparametric-estimations---basics.html#cb297-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb297-12"><a href="nonparametric-estimations---basics.html#cb297-12" aria-hidden="true" tabindex="-1"></a>mars<span class="sc">$</span>bestTune</span></code></pre></div>
<pre><code>##   nprune degree
## 5     45      1</code></pre>
<div class="sourceCode" id="cb299"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb299-1"><a href="nonparametric-estimations---basics.html#cb299-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(mars)</span></code></pre></div>
<p><img src="06-Basics_files/figure-html/bn16-1.png" width="672" /></p>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb300-1"><a href="nonparametric-estimations---basics.html#cb300-1" aria-hidden="true" tabindex="-1"></a><span class="fu">vip</span>(mars,</span>
<span id="cb300-2"><a href="nonparametric-estimations---basics.html#cb300-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">num_features =</span> <span class="dv">40</span>,</span>
<span id="cb300-3"><a href="nonparametric-estimations---basics.html#cb300-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">bar =</span> <span class="cn">FALSE</span>,</span>
<span id="cb300-4"><a href="nonparametric-estimations---basics.html#cb300-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">value =</span> <span class="st">&quot;gcv&quot;</span>) <span class="sc">+</span> <span class="fu">ggtitle</span>(<span class="st">&quot;GCV&quot;</span>)</span></code></pre></div>
<p><img src="06-Basics_files/figure-html/bn16-2.png" width="672" /></p>
<p>How does this compare to some other linear models for the Ames housing data? That’s the main question that we will ask in related chapters covering other machine learning models.</p>
</div>
<div id="gam---generalized-additive-model" class="section level2 hasAnchor" number="6.5">
<h2><span class="header-section-number">6.5</span> GAM - Generalized Additive Model<a href="nonparametric-estimations---basics.html#gam---generalized-additive-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Generalized Additive Model (GAM) is another method for discovering non-linear relationships in a multivariate setting. The performance difference between MARS and GAM is well explained by Leathwich, Elith, and Hastie <span class="citation">(<a href="#ref-Leath_2006" role="doc-biblioref"><strong>Leath_2006?</strong></a>)</span>. Here is an excerpt from their paper (Page 189):</p>
<blockquote>
<p>Two other commonly used techniques capable of fitting non-linear relationships (…) are neural nets and classification and regression trees. A third alternative, multivariate adaptive regression splines (MARS), has shown promise in recent comparative studies. This technique combines the strengths of regression trees and spline fitting by replacing the step functions normally associated with regression trees with piecewise linear basis functions. This allows the modelling of complex relationships between a response variable and its predictors. In practical terms, MARS has exceptional analytical speed, and its simple rule-based basis functions facilitate the prediction of species distributions using independent data.</p>
</blockquote>
<p>And from their abstract:</p>
<blockquote>
<p>Results indicate little difference between the performance of GAM and MARS models, even when MARS models included interaction terms between predictor variables. Results from MARS models are much more easily incorporated into other analyses than those from GAM models. The strong performance of a MARS multiresponse model, particularly for species of low prevalence, suggests that it may have distinct advantages for the analysis of large datasets.</p>
</blockquote>
<p>GAM uses an iterative estimation process to the following generalized additive model by assuming <span class="math inline">\(m\)</span> can be decompose as a sum of several functions of dimension one or two (or more):<br />
<span class="math display" id="eq:6-16">\[\begin{equation}
y=m_{1}\left(x_{1}\right)+m_{2}\left(x_{2}\right)+\cdots+m_{k}\left(x_{k}\right)+\varepsilon
  \tag{6.16}
\end{equation}\]</span></p>
<p>The estimation to this 2-variable additive model <span class="math inline">\(y=m_{1}\left(x_{1}\right)+m_{2}\left(x_{2}\right)+\varepsilon\)</span> can be done by the following iterative procedure:</p>
<ol style="list-style-type: decimal">
<li>Select initial estimates <span class="math inline">\(m_1^{(0)}\)</span> and <span class="math inline">\(m_2^{(0)}\)</span></li>
<li>Obtain <span class="math inline">\(\hat{m}_1^{(i)}\)</span> by regressing <span class="math inline">\(y-\hat{m}_2^{(i-1)}\)</span> on <span class="math inline">\(x_1\)</span></li>
<li>Obtain <span class="math inline">\(\hat{m}_2^{(i)}\)</span> by regressing <span class="math inline">\(y-\hat{m}_1^{(i-1)}\)</span> on <span class="math inline">\(x_2\)</span></li>
<li>Repeat steps 2 an 3 until no significant changes</li>
</ol>
<p>Initial estimates can be equal to 0 or obtained by OLS. An important advantage of GAM is that an extension to more than two functions, which kernel or spline methods could be used for <span class="math inline">\(m(x)\)</span>, does not lead to the curse of dimensionality problem. Let’s consider the following estimation using a housing data set described below:</p>
<p><span class="math display">\[
\log (\text { price })=X \beta+m_{1}(\text { green })+m_{2}(\text { coord } 1)+m_{3}(\text { coord } 2)+\varepsilon,
\]</span></p>
<p>where <code>price</code> is the housing price - 1135 observations, for 1995 in Brest; <span class="math inline">\(X\)</span> dummies are <code>Studio</code>, <code>T1</code>, <code>T2</code>, <code>T3</code>, <code>T4</code>, <code>T5</code>, <code>house</code>, <code>parking</code> defining the type of building and whether the parking lot exits or not; <code>green</code> is the distance to the closest green park; and <code>coord1</code>, <code>coord2</code> are geographical coordinates (location).</p>
<div class="sourceCode" id="cb301"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb301-1"><a href="nonparametric-estimations---basics.html#cb301-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">read.table</span>(<span class="st">&quot;hedonic.dat&quot;</span>, <span class="at">header =</span> <span class="cn">TRUE</span>)</span>
<span id="cb301-2"><a href="nonparametric-estimations---basics.html#cb301-2" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(data) <span class="co"># note that this is not advisable but I use it in this example</span></span>
<span id="cb301-3"><a href="nonparametric-estimations---basics.html#cb301-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mgcv)</span>
<span id="cb301-4"><a href="nonparametric-estimations---basics.html#cb301-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb301-5"><a href="nonparametric-estimations---basics.html#cb301-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Note that a nonlinear transformation of a dummy variable is still a dummy.</span></span>
<span id="cb301-6"><a href="nonparametric-estimations---basics.html#cb301-6" aria-hidden="true" tabindex="-1"></a><span class="co"># let&#39;s add them in X vector</span></span>
<span id="cb301-7"><a href="nonparametric-estimations---basics.html#cb301-7" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(T1, T2, T3, T4, T5, HOUSE, PARKING)</span>
<span id="cb301-8"><a href="nonparametric-estimations---basics.html#cb301-8" aria-hidden="true" tabindex="-1"></a>gam1 <span class="ot">&lt;-</span> <span class="fu">gam</span>(LPRIX <span class="sc">~</span> X <span class="sc">+</span> <span class="fu">s</span>(GREEN) <span class="sc">+</span> <span class="fu">s</span>(COORD1) <span class="sc">+</span> <span class="fu">s</span>(COORD2))</span>
<span id="cb301-9"><a href="nonparametric-estimations---basics.html#cb301-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb301-10"><a href="nonparametric-estimations---basics.html#cb301-10" aria-hidden="true" tabindex="-1"></a><span class="co"># s() defines smooths in GAM formulae</span></span>
<span id="cb301-11"><a href="nonparametric-estimations---basics.html#cb301-11" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(gam1)</span></code></pre></div>
<pre><code>## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## LPRIX ~ X + s(GREEN) + s(COORD1) + s(COORD2)
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  4.79133    0.04698 101.997  &lt; 2e-16 ***
## XT1          0.06974    0.05737   1.216    0.224    
## XT2          0.38394    0.05173   7.421 2.41e-13 ***
## XT3          0.75105    0.05025  14.946  &lt; 2e-16 ***
## XT4          0.97310    0.05138  18.939  &lt; 2e-16 ***
## XT5          1.13707    0.05666  20.070  &lt; 2e-16 ***
## XHOUSE       0.23965    0.03273   7.321 4.91e-13 ***
## XPARKING     0.22890    0.02400   9.538  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Approximate significance of smooth terms:
##             edf Ref.df     F p-value    
## s(GREEN)  5.539  6.745 2.552  0.0111 *  
## s(COORD1) 8.289  8.848 7.523  &lt;2e-16 ***
## s(COORD2) 8.323  8.869 7.182  &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## R-sq.(adj) =  0.672   Deviance explained = 68.1%
## GCV = 0.10301  Scale est. = 0.10011   n = 1070</code></pre>
<div class="sourceCode" id="cb303"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb303-1"><a href="nonparametric-estimations---basics.html#cb303-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot nonparametric components </span></span>
<span id="cb303-2"><a href="nonparametric-estimations---basics.html#cb303-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb303-3"><a href="nonparametric-estimations---basics.html#cb303-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(gam1, <span class="at">shade =</span> <span class="cn">TRUE</span>, <span class="at">shade.col =</span> <span class="st">&quot;pink&quot;</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.5</span>, <span class="fl">0.7</span>))</span></code></pre></div>
<p><img src="06-Basics_files/figure-html/bn17-1.png" width="672" /></p>
<p>These figures do not suggest how a simple parametric modeling would be misleading. From the results, we can make standard interpretations: with similar other characteristics: (1) on average, a studio costs 120.421 francs (<span class="math inline">\(e^{4.791}\)</span>); (2) a T2 is expected to cost 38.4% more than a studio; (3) a house is expected to cost 23.9% more than an apartment. We also have results on the nonparametric components. The <span class="math inline">\(p\)</span>-values correspond to test <span class="math inline">\(H_0\)</span> : linear vs. <span class="math inline">\(H_1\)</span> : nonlinear relationship.</p>
<p>If geographical location is assumed highly nonlinear, we should consider a more flexible model:</p>
<p><span class="math display">\[
\log (\text { price })=X \beta+m_{1}(\text { green })+m_{2}(\text { coord } 1, \text { coord } 2)+\varepsilon,
\]</span>
where the spatial dependence is specified fully nonparametrically.</p>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb304-1"><a href="nonparametric-estimations---basics.html#cb304-1" aria-hidden="true" tabindex="-1"></a>gam2 <span class="ot">&lt;-</span> <span class="fu">gam</span>(LPRIX <span class="sc">~</span> X <span class="sc">+</span> <span class="fu">s</span>(GREEN) <span class="sc">+</span> <span class="fu">s</span>(COORD1, COORD2))</span>
<span id="cb304-2"><a href="nonparametric-estimations---basics.html#cb304-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(gam2)</span></code></pre></div>
<pre><code>## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## LPRIX ~ X + s(GREEN) + s(COORD1, COORD2)
## 
## Parametric coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  4.77597    0.04641 102.916  &lt; 2e-16 ***
## XT1          0.08030    0.05628   1.427    0.154    
## XT2          0.38691    0.05102   7.583 7.50e-14 ***
## XT3          0.76278    0.04959  15.383  &lt; 2e-16 ***
## XT4          0.99325    0.05079  19.555  &lt; 2e-16 ***
## XT5          1.13897    0.05594  20.361  &lt; 2e-16 ***
## XHOUSE       0.23827    0.03247   7.339 4.36e-13 ***
## XPARKING     0.24428    0.02426  10.069  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Approximate significance of smooth terms:
##                     edf Ref.df     F p-value    
## s(GREEN)          6.487  7.626 2.074  0.0445 *  
## s(COORD1,COORD2) 24.063 27.395 6.714  &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## R-sq.(adj) =  0.685   Deviance explained = 69.6%
## GCV = 0.099791  Scale est. = 0.096196  n = 1070</code></pre>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="nonparametric-estimations---basics.html#cb306-1" aria-hidden="true" tabindex="-1"></a><span class="fu">vis.gam</span>(gam2, <span class="at">view =</span><span class="fu">c</span>(<span class="st">&quot;COORD1&quot;</span>, <span class="st">&quot;COORD2&quot;</span>), <span class="at">phi =</span> <span class="dv">20</span>,</span>
<span id="cb306-2"><a href="nonparametric-estimations---basics.html#cb306-2" aria-hidden="true" tabindex="-1"></a>        <span class="at">main =</span>  <span class="fu">bquote</span>(m[<span class="dv">2</span>](coord1) <span class="sc">+</span> m[<span class="dv">3</span>](coord2)))</span></code></pre></div>
<p><img src="06-Basics_files/figure-html/bn18-1.png" width="672" /></p>
<p>If we define them linearly,</p>
<div class="sourceCode" id="cb307"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb307-1"><a href="nonparametric-estimations---basics.html#cb307-1" aria-hidden="true" tabindex="-1"></a>gam3 <span class="ot">&lt;-</span> <span class="fu">gam</span>(LPRIX <span class="sc">~</span> X <span class="sc">+</span> <span class="fu">s</span>(GREEN) <span class="sc">+</span> COORD1 <span class="sc">+</span> COORD2)</span>
<span id="cb307-2"><a href="nonparametric-estimations---basics.html#cb307-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(gam3)</span></code></pre></div>
<pre><code>## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## LPRIX ~ X + s(GREEN) + COORD1 + COORD2
## 
## Parametric coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  6.707e+00  1.101e+00   6.093 1.55e-09 ***
## XT1          5.972e-02  6.003e-02   0.995 0.320062    
## XT2          3.742e-01  5.426e-02   6.896 9.22e-12 ***
## XT3          7.450e-01  5.222e-02  14.267  &lt; 2e-16 ***
## XT4          9.475e-01  5.293e-02  17.901  &lt; 2e-16 ***
## XT5          1.127e+00  5.845e-02  19.285  &lt; 2e-16 ***
## XHOUSE       2.683e-01  3.254e-02   8.245 4.86e-16 ***
## XPARKING     2.397e-01  2.442e-02   9.817  &lt; 2e-16 ***
## COORD1       2.046e-05  7.256e-06   2.819 0.004903 ** 
## COORD2      -3.854e-05  1.150e-05  -3.351 0.000834 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Approximate significance of smooth terms:
##            edf Ref.df     F p-value   
## s(GREEN) 6.895  8.028 2.809 0.00469 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## R-sq.(adj) =  0.637   Deviance explained = 64.2%
## GCV = 0.11277  Scale est. = 0.11099   n = 1070</code></pre>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb309-1"><a href="nonparametric-estimations---basics.html#cb309-1" aria-hidden="true" tabindex="-1"></a><span class="fu">vis.gam</span>(gam3, <span class="at">view =</span><span class="fu">c</span>(<span class="st">&quot;COORD1&quot;</span>, <span class="st">&quot;COORD2&quot;</span>), <span class="at">phi =</span> <span class="dv">20</span>,</span>
<span id="cb309-2"><a href="nonparametric-estimations---basics.html#cb309-2" aria-hidden="true" tabindex="-1"></a>        <span class="at">main =</span> <span class="fu">bquote</span>(beta[<span class="dv">1</span>] <span class="sc">*</span> <span class="st">&#39;coord1&#39;</span> <span class="sc">+</span> beta[<span class="dv">2</span>] <span class="sc">*</span> <span class="st">&#39;coord2&#39;</span>))</span></code></pre></div>
<p><img src="06-Basics_files/figure-html/bn19-1.png" width="672" /></p>
<p>GAMs provide a useful compromise between linear and fully nonparametric models. Here is the linear specification with GAM:</p>
<div class="sourceCode" id="cb310"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb310-1"><a href="nonparametric-estimations---basics.html#cb310-1" aria-hidden="true" tabindex="-1"></a>gam4 <span class="ot">&lt;-</span> <span class="fu">gam</span>(LPRIX <span class="sc">~</span> X <span class="sc">+</span> GREEN <span class="sc">+</span> COORD1 <span class="sc">+</span> COORD2)</span>
<span id="cb310-2"><a href="nonparametric-estimations---basics.html#cb310-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(gam4)</span></code></pre></div>
<pre><code>## 
## Family: gaussian 
## Link function: identity 
## 
## Formula:
## LPRIX ~ X + GREEN + COORD1 + COORD2
## 
## Parametric coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  7.211e+00  1.096e+00   6.577 7.54e-11 ***
## XT1          6.455e-02  6.042e-02   1.068 0.285597    
## XT2          3.859e-01  5.444e-02   7.089 2.47e-12 ***
## XT3          7.548e-01  5.235e-02  14.418  &lt; 2e-16 ***
## XT4          9.576e-01  5.309e-02  18.037  &lt; 2e-16 ***
## XT5          1.141e+00  5.860e-02  19.462  &lt; 2e-16 ***
## XHOUSE       2.741e-01  3.274e-02   8.371  &lt; 2e-16 ***
## XPARKING     2.417e-01  2.454e-02   9.850  &lt; 2e-16 ***
## GREEN       -4.758e-05  4.470e-05  -1.064 0.287435    
## COORD1       1.909e-05  7.111e-06   2.685 0.007365 ** 
## COORD2      -4.219e-05  1.148e-05  -3.674 0.000251 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## 
## R-sq.(adj) =   0.63   Deviance explained = 63.3%
## GCV = 0.11418  Scale est. = 0.113     n = 1070</code></pre>
<div class="sourceCode" id="cb312"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb312-1"><a href="nonparametric-estimations---basics.html#cb312-1" aria-hidden="true" tabindex="-1"></a><span class="fu">detach</span>(data)</span></code></pre></div>
<p>GAM becomes an important tool to understand whether model should be estimated as a linear function or a complex nonlinear function. We can even see and test if a specific variable should have a nonlinear part. For example, if we want to know coordinates should be included additively, linear, or nonparametrically, we can use GAM and then compare performances of those different models and make a decision.</p>
<p>GAMs are based on a hypothesis of additive separability. They are helpful for reducing the dimension of the model without facing the curse of dimensionality problem. But, unlike MARS, <strong>they miss possible important interactions</strong> (we can add them manually, though).</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="2">
<li id="fn2"><p>see <a href="https://www.rdocumentation.org/packages/freeknotsplines/versions/1.0.1/topics/fit.search.numknots" class="uri">https://www.rdocumentation.org/packages/freeknotsplines/versions/1.0.1/topics/fit.search.numknots</a> for knot location selection<a href="nonparametric-estimations---basics.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>see: <a href="http://users.stat.umn.edu/~helwig/notes/smooth-spline-notes.html" class="uri">http://users.stat.umn.edu/~helwig/notes/smooth-spline-notes.html</a>.<a href="nonparametric-estimations---basics.html#fnref3" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="parametric-estimations.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="smoothing.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/yaydede/toolbox/edit/master/06-Basics.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["YA_TextBook.pdf", "YA_TextBook.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
