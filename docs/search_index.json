[["index.html", "Toolbox for Social Scientists and Policy Analysts Applied Predictive Analytics with Machine Learning and R Preface Who Acknowledgements License", " Toolbox for Social Scientists and Policy Analysts Applied Predictive Analytics with Machine Learning and R Yigit Aydede This version: 2023-02-16 Preface Our lives are shaped by predictions that we make daily. The process of predicting is not static. We want to improve our predictions to avoid catastrophes in our lives … We learn from our mistakes. We are all self-learning walking machines with a very limited processing capacity. Can we develop a self-learning algorithm for a high-capacity machine that can make prediction more efficiently and accurately for us? Yes, we can! With the well-developed statistical models programmed in very effective algorithms run by high-capacity computers. This book takes on the first part, statistical models, without too much abstraction. It doesn’t teach all aspects of programming but enough coding skills that you can find your way in building predictive algorithms with R. All gets into a computer. So, we also need to know enough about the “machines” with which we can facilitate a better efficiency. We have enough of it too … According to Leo Breiman (Breiman_2001?), Statistical Modeling: The Two Cultures, there are two goals in analyzing the data: Prediction: To be able to predict what the responses are going to be to future input variables; Information: To extract some information about how nature is associating the response variables to the input variables. And there are two approaches towards those goals: The Data Modeling Culture : One assumes that the data are generated by a given stochastic data model (econometrics) … Algorithmic Modeling Culture: One uses algorithmic models and treats the data mechanism as unknown (machine learning) … And he describes the current state: …With the insistence on data models, multivariate analysis tools in statistics are frozen at discriminant analysis and logistic regression in classification and multiple linear regression in regression. Nobody really believes that multivariate data is multivariate normal, but that data model occupies a large number of pages in every graduate text book on multivariate statistical analysis… Broadly speaking, many social scientists look at a statistical analysis from the window of causal inference. Most courses in their training have been (are) based on inferential statistics covering regression-based parametric approaches using interface-based statistical packages (like, EViews, Stata, SPSS, SAS). Since the demand for broader, more inclusive “data analytics” courses has been rising in the last decade, most departments (Economics, Finance, and Business fields) are looking for a data analytics course, which is less foreign to their traditional curriculum. This integration is important because of two reasons: first, “predictive” and nonparametric methods are never given the front seat in conventional curricula. We have “forecasting” courses, but they mostly cover conventional parametric time-series using ARIMA/GARCH type applications. Second, interface-based statistical packages are not enough anymore for unconventional nonparametric applications. R and Python are the new languages that students increasingly demand in all Business schools. This should not be surprising: first, machine learning is new for many fields. And not only the concept is relatively new, but its “language” is different: hyperparameters, classification, features, variance-bias trade-off, tuning, and so on. Further, the culture in our quantitative courses is different. We do not “understand” how a high prediction accuracy itself would be a focal point in data analytics: even if “ice-cream sales predict the crime rates very well, the result would be useless”. This is what many policy analysts think. The structure of the book is different from many other books on machine learning. First, it is not all about Machine Learning written mostly for practitioners. For example, the initial chapters are positioned for a smooth transition from inferential statistics and the “parametric world” to predictive models with the help of a section that covers nonparametric methods. Even at the PhD level, we rarely teach nonparametric methods as those methods have been less applied in inferential statistics. Nonparametric econometrics, however, makes the link between these two cultures (data modeling and algorithmic modeling) as machine learning is an extension of nonparametric econometrics. The order of chapters offers this transition to have our first nonparametric application, kNN in Chapter 8. After traditional tree-based models, the book also covers Support Vector Machines and Neural Network. These chapters can be skipped but they are self-contained so that even students with a weak background in linear algebra can understand the “black-box” models. The book covers the concepts with applications. However, the most applications use “toy data” to make the book more generalizable for other fields with a similar curriculum in inferential statistics. We will have supplementary books covering field specific data with applications. I do not believe that we need a book for each field (accounting, management, finance, economics, political science, sociology, and so on). The first supplementary online book using real data (from economic and financial sources) will be ready very soon. After well-known predictive algorithms collected only in two sections, the book proceeds into five new sections: Penalized Regressions, which are the well-known high-dimensional methods in economics and finance used in model selection and sparsity. The next section gives a new look at time-series applications by showing how time series data can be used in predictions beyond traditional parametric models. The following section covers Dimension Reduction Methods. They are important tools in “noise” reduction in almost all fields in social science. The last section covers Network Analysis. This concept is also very prevalent in many business fields and specially in economics and finance. This section summarizes all new developments in Graphical Network Analysis. Finally, the chapters in Appendix provide enough information on two important subjects: Algorithmic Optimization (including gradient descent applications) and the discussion on classification with imbalanced data. Moreover, there will be some additions to several chapters covering Conditional Inference Trees, General CV, and Causal Random Forest. I hope that this book provides a good starting point to give predictive analytics a well-deserved place in curricula of social science and business fields. I also hope that this book will “grow” by the time to keep up with the fast changing world. Hence, I guess, it will always be a “draft” in a humble way … Who This book is targeted at motivated students and researchers who have a background in inferential statistics using parametric models. It is applied because I skip many theoretical proofs and justifications that can easily be found elsewhere. I do not assume a previous experience with R but some familiarity with coding. Acknowledgements This book was made possible by Mutlu Yuksel, Tolga Kaya, Mehmet Caner, Juri Marcucci, Atul Dar and, Andrea Guisto. This work is greatly inspired by following books and people: Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. Introduction to Data Science by Rafael A. Irizarry. Applied Statistics with R by David Dalpiaz R for Statistical Learning by David Dalpiaz I’ve also greatly benefited from my participation in the Summer School of SiDe in 2019 on Machine Learning Algorithms for Econometricians by Arthur Carpentier and Emmanuel Flachaire and in 2017 on High-Dimensional Econometrics by Anders Bredahl Kock and Mehmet Caner. I never stop learning from these people. I also thank my research assistant Kyle Morton. Without him, this book wouldn’t be possible. Finally, my wife and my son, Isik and Ege, you are always my compass finding my purpose … License This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["intro.html", "Chapter 1 How we define Machine Learning", " Chapter 1 How we define Machine Learning The demand for skilled data science practitioners in industry, academia, and government is rapidly growing. This book introduces concepts and skills that can help you develop a foundation that is missing in many educational platforms for Machine Learning. This book is a collection of my lecture notes that I have developed in the past 10 years. Therefore, its language is not formal and it leaves most theoretical proofs to carefully selected cited sources. It feels like a written transcript of lectures more than a reference textbook that covers from A to Z. As of today (August 29, 2022), more than 103,942 people have been enrolled in the Machine Learning course offered online by Stanford University at Coursera. The course is offered multiple times in a month and can be completed in approximately 61 hours. I had a hard time for finding a good title for the book on a field where the level of interest is jaw-dropping. Even finding a good definition for Machine Learning has became a subtle job as “machine learning” seems increasingly an overloaded term implying that a robot-like machine predicts things by learning itself without being explicitly programmed. Ethem Alpaydin, who is a professor of computer engineering, defines machine learning in the \\(3^{rd}\\) edition of his book, Introduction to Machine Learning (alpaydin_2014?) as follows: Machine learning is programming computers to optimize a performance criterion using example data or past experience. We have a model defined up to some parameters, and learning is the execution of a computer program to optimize the parameters of the model using the training data of past experience. (…) Machine learning uses the theory of statistics in building mathematical models, because the core task is making inference from sample. The role of computer science is twofold: First, in training, we need efficient algorithms to solve the optimization problem, as well as to store and process the massive amount of data we generally have. Second, once the model is learned, its representation and algorithmic solution for inference needs to be efficient as well. Hence, there are no “mysterious” machines that are learning and acting alone. Rather, there are well-defined statistical models for predictions that are optimized by efficient algorithms, and executed by powerful machines that we know as computers. "],["preliminaries.html", "Chapter 2 Preliminaries 2.1 Data and dataset types 2.2 Plots 2.3 Probability Distributions with R 2.4 Regressions 2.5 BLUE 2.6 Modeling the data 2.7 Causal vs. Predictive Models 2.8 Simulation", " Chapter 2 Preliminaries We will start with reviewing some basic concepts in statistics. For now, don’t stress about understanding the code chunks. As you study and practice R more (See R Labs at the end of the book), the codes will become clearer. 2.1 Data and dataset types R has a number of basic data types. Numeric : Also known as Double. The default type when dealing with numbers. 1,1.0,42.5 Integer: 1L,2L,42L Complex: 4 + 2i Logical: Two possible values: TRUE and FALSE. NA is also considered logical. Character: “a”,“Statistics”,“1plus2.” Data can also be classified as numeric (what’s your age?) and categorical (Do you have a car?) R also has a number of basic data (“container”) structures. A data structure is either homogeneous (all elements are of the same data type) or heterogeneous (elements can be of more than one data type): You can think each data structure as data container where your data are stored. Here are the main “container” or data structures: Vector: One dimension (column or row) and homogeneous. That is, every element of a vector has to be the same type. Each vector can be thought of as a variable. Matrix: Two dimensions (columns and rows) and homogeneous. That is, every element of a matrix has to be the same type. Data Frame: Two dimensions (columns and rows) and heterogeneous. That is, every column of a data frame doesn’t have to be the same type. This is the main difference between a matrix and a data frame. Data frames are the most common data structures in any data analysis. List: One dimension and heterogeneous. A list can have multiple data structures. Array: 3+ dimensions and homogeneous. In this book, we most frequently work with data frames. When using data, there are three things we like to do: Look at the raw data. Understand the data. (What are the variables and their types?) Visualize the data. To look at the data, we have two useful commands: head() (tail()) and str(). As seen in the following examples, head() and tail() allow us to see the first few data points in a dataset. str() allows us to see the structure of the data, including what data types are used. Using str() we can identify the mtcars dataset, which includes only numeric variables and is structured in a data frame. Cross-Sectional In cross-sectional data, we have one dimension: subjects. Since the order of observations are not important, we can shuffle the data. library(datasets) head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... summary(mtcars) ## mpg cyl disp hp ## Min. :10.40 Min. :4.000 Min. : 71.1 Min. : 52.0 ## 1st Qu.:15.43 1st Qu.:4.000 1st Qu.:120.8 1st Qu.: 96.5 ## Median :19.20 Median :6.000 Median :196.3 Median :123.0 ## Mean :20.09 Mean :6.188 Mean :230.7 Mean :146.7 ## 3rd Qu.:22.80 3rd Qu.:8.000 3rd Qu.:326.0 3rd Qu.:180.0 ## Max. :33.90 Max. :8.000 Max. :472.0 Max. :335.0 ## drat wt qsec vs ## Min. :2.760 Min. :1.513 Min. :14.50 Min. :0.0000 ## 1st Qu.:3.080 1st Qu.:2.581 1st Qu.:16.89 1st Qu.:0.0000 ## Median :3.695 Median :3.325 Median :17.71 Median :0.0000 ## Mean :3.597 Mean :3.217 Mean :17.85 Mean :0.4375 ## 3rd Qu.:3.920 3rd Qu.:3.610 3rd Qu.:18.90 3rd Qu.:1.0000 ## Max. :4.930 Max. :5.424 Max. :22.90 Max. :1.0000 ## am gear carb ## Min. :0.0000 Min. :3.000 Min. :1.000 ## 1st Qu.:0.0000 1st Qu.:3.000 1st Qu.:2.000 ## Median :0.0000 Median :4.000 Median :2.000 ## Mean :0.4062 Mean :3.688 Mean :2.812 ## 3rd Qu.:1.0000 3rd Qu.:4.000 3rd Qu.:4.000 ## Max. :1.0000 Max. :5.000 Max. :8.000 plot(mtcars[,c(1, 3, 4)]) Time-series Time-series data have also one dimension with subject followed through the time. We cannot shuffle the data, as it follows a sequential order. head(airquality) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 str(airquality) ## &#39;data.frame&#39;: 153 obs. of 6 variables: ## $ Ozone : int 41 36 12 18 NA 28 23 19 8 NA ... ## $ Solar.R: int 190 118 149 313 NA NA 299 99 19 194 ... ## $ Wind : num 7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ... ## $ Temp : int 67 72 74 62 56 66 65 59 61 69 ... ## $ Month : int 5 5 5 5 5 5 5 5 5 5 ... ## $ Day : int 1 2 3 4 5 6 7 8 9 10 ... summary(airquality) ## Ozone Solar.R Wind Temp ## Min. : 1.00 Min. : 7.0 Min. : 1.700 Min. :56.00 ## 1st Qu.: 18.00 1st Qu.:115.8 1st Qu.: 7.400 1st Qu.:72.00 ## Median : 31.50 Median :205.0 Median : 9.700 Median :79.00 ## Mean : 42.13 Mean :185.9 Mean : 9.958 Mean :77.88 ## 3rd Qu.: 63.25 3rd Qu.:258.8 3rd Qu.:11.500 3rd Qu.:85.00 ## Max. :168.00 Max. :334.0 Max. :20.700 Max. :97.00 ## NA&#39;s :37 NA&#39;s :7 ## Month Day ## Min. :5.000 Min. : 1.0 ## 1st Qu.:6.000 1st Qu.: 8.0 ## Median :7.000 Median :16.0 ## Mean :6.993 Mean :15.8 ## 3rd Qu.:8.000 3rd Qu.:23.0 ## Max. :9.000 Max. :31.0 ## airquality$date &lt;- airquality$Month*10+airquality$Day plot(airquality$date, airquality$Ozone) This was just a simple time-series data presentation. In Part VII, we will see more advance ways to handle time-series data. Moreover, dates and times can be handled by various packages/functions in R, like lubridate (see https://lubridate.tidyverse.org). Here are some examples: library(lubridate) # get current system date Sys.Date() ## [1] &quot;2023-02-16&quot; # get current system time Sys.time() ## [1] &quot;2023-02-16 12:25:24 AST&quot; #lubridate now() ## [1] &quot;2023-02-16 12:25:24 AST&quot; dates &lt;- c(&quot;2022-07-11&quot;, &quot;2012-04-19&quot;, &quot;2017-03-08&quot;) # extract years from dates year(dates) ## [1] 2022 2012 2017 # extract months from dates month(dates) ## [1] 7 4 3 # extract days from dates mday(dates) ## [1] 11 19 8 Panel Panel data (also known as longitudinal data) is the richest data type as it includes information fro multiple entities observed across time. These entities could be people, households, countries, firms, etc. Note in the example below, we use the plm package which has several data sets. The first one, EmplUK, is summarized below: library(foreign) library(plm) data(&quot;EmplUK&quot;, package=&quot;plm&quot;) head(EmplUK, 15) ## firm year sector emp wage capital output ## 1 1 1977 7 5.041 13.1516 0.5894 95.7072 ## 2 1 1978 7 5.600 12.3018 0.6318 97.3569 ## 3 1 1979 7 5.015 12.8395 0.6771 99.6083 ## 4 1 1980 7 4.715 13.8039 0.6171 100.5501 ## 5 1 1981 7 4.093 14.2897 0.5076 99.5581 ## 6 1 1982 7 3.166 14.8681 0.4229 98.6151 ## 7 1 1983 7 2.936 13.7784 0.3920 100.0301 ## 8 2 1977 7 71.319 14.7909 16.9363 95.7072 ## 9 2 1978 7 70.643 14.1036 17.2422 97.3569 ## 10 2 1979 7 70.918 14.9534 17.5413 99.6083 ## 11 2 1980 7 72.031 15.4910 17.6574 100.5501 ## 12 2 1981 7 73.689 16.1969 16.7133 99.5581 ## 13 2 1982 7 72.419 16.1314 16.2469 98.6151 ## 14 2 1983 7 68.518 16.3051 17.3696 100.0301 ## 15 3 1977 7 19.156 22.6920 7.0975 95.7072 length(unique(EmplUK$firm)) ## [1] 140 table(EmplUK$year) ## ## 1976 1977 1978 1979 1980 1981 1982 1983 1984 ## 80 138 140 140 140 140 140 78 35 As you can see, we have 140 unique subjects (firms) each of which is observed between 1977 and 1983. However, there are some firms with missing years. This type of panel data is called as “unbalanced panel”. 2.2 Plots Often, a proper visualization can illuminate features of the data that can inform further analysis. We will look at four methods of visualizing data that we will use throughout the book: Histograms Barplots Boxplots Scatterplots We can use the data mpg provided by the ggplot2 package. To begin, we can get a sense of the data by looking at the first few data points and some summary statistics. library(ggplot2) head(mpg, 5) ## # A tibble: 5 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto(l5) f 18 29 p compa… ## 2 audi a4 1.8 1999 4 manual(m5) f 21 29 p compa… ## 3 audi a4 2 2008 4 manual(m6) f 20 31 p compa… ## 4 audi a4 2 2008 4 auto(av) f 21 30 p compa… ## 5 audi a4 2.8 1999 6 auto(l5) f 16 26 p compa… tail(mpg, 5) ## # A tibble: 5 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 volkswagen passat 2 2008 4 auto(s6) f 19 28 p mids… ## 2 volkswagen passat 2 2008 4 manual(m6) f 21 29 p mids… ## 3 volkswagen passat 2.8 1999 6 auto(l5) f 16 26 p mids… ## 4 volkswagen passat 2.8 1999 6 manual(m5) f 18 26 p mids… ## 5 volkswagen passat 3.6 2008 6 auto(s6) f 17 26 p mids… When visualizing a single numerical variable, a histogram would be very handy: hist(mpg$cty, xlab = &quot;Miles Per Gallon (City)&quot;, main = &quot;Histogram of MPG (City)&quot;, breaks = 12, col = &quot;dodgerblue&quot;,cex.main=1, cex.lab=.75, cex.axis=0.75, border = &quot;darkorange&quot;) Similar to a histogram, a barplot provides a visual summary of a categorical variable, or a numeric variable with a finite number of values, like a ranking from 1 to 10. barplot(table(mpg$drv), xlab = &quot;Drivetrain (f = FWD, r = RWD, 4 = 4WD)&quot;, ylab = &quot;Frequency&quot;, main = &quot;Drivetrains&quot;, col = &quot;dodgerblue&quot;,cex.main=1, cex.lab=.75, cex.axis=0.75, border = &quot;darkorange&quot;) To visualize the relationship between a numerical and categorical variable, we will use a boxplot. In the mpg dataset, the drv few categories: front-wheel drive, 4-wheel drive, or rear-wheel drive. boxplot(hwy ~ drv, data = mpg, xlab = &quot;Drivetrain (f = FWD, r = RWD, 4 = 4WD)&quot;, ylab = &quot;Miles Per Gallon (Highway)&quot;, main = &quot;MPG (Highway) vs Drivetrain&quot;, pch = 20, cex =2,cex.main=1, cex.lab=.75, cex.axis=0.75, col = &quot;darkorange&quot;, border = &quot;dodgerblue&quot;) Finally, to visualize the relationship between two numeric variables we will use a scatterplot. plot(hwy ~ displ, data = mpg, xlab = &quot;Engine Displacement (in Liters)&quot;, ylab = &quot;Miles Per Gallon (Highway)&quot;, main = &quot;MPG (Highway) vs Engine Displacement&quot;, pch = 20, cex = 2, cex.main=1, cex.lab=.75, cex.axis=0.75, col = &quot;dodgerblue&quot;) While visualization is not enough to draw definitive conclusions, it can help us identify insights about data. R is well-know for its graphical capabilities. The package ggplot is the main tool for more advance graphical representations that we will see later. 2.3 Probability Distributions with R We often want to make probabilistic statements based on a distribution. Typically, we will want to know one of four things: The probability density function (pdf) at a particular value. The cumulative probability distribution (cdf) at a particular value. The quantile value corresponding to a particular probability. A random draw of values from a particular distribution. The general naming structure of the relevant R functions is: dname calculates density (pdf) at input \\(x\\). pname calculates distribution (cdf) at input \\(x\\). qname calculates the quantile at an input probability. rname generates a random draw from a particular distribution, where name represents the name of the given distribution, like rnorm for a random draw from a normal distribution For example, consider a random variable \\(X\\): \\[ X \\sim N\\left(\\mu=2, \\sigma^{2}=25\\right) \\] To calculate the value of the pdf at \\(x = 4\\), we use dnorm(): dnorm(x = 4, mean = 2, sd = 5) ## [1] 0.07365403 Note that R uses the standard deviation. To calculate the value of the cdf at \\(x = 4\\), that is, \\(P(X \\leq{4})\\), the probability that \\(X\\) is less than or equal to 4, we use pnorm(): pnorm(q = 4, mean = 2, sd = 5) ## [1] 0.6554217 Or, to calculate the quantile for probability 0.975, we use qnorm(): qnorm(p = 0.975, mean = 2, sd = 5) ## [1] 11.79982 Lastly, to generate a random sample of size n = 10, we use rnorm() rnorm(n = 10, mean = 2, sd = 5) ## [1] 3.1964037 4.6978267 3.9209005 -1.4238047 -5.1275255 0.9107116 ## [7] 7.1060835 -0.4340366 -2.0364064 -2.3286135 These functions exist for many other distributions such as: binom (Binomial), t (Student’s t), pois (Poisson), f (F), chisq (Chi-Squared) and so on. 2.4 Regressions Regressions allow us to make estimations of the relationships between variables. Let’s consider a simple example of how the speed of a car affects its stopping distance. To examine this relationship, we will use the cars data set from the datasets package. The data give the speed of cars and the distances taken to stop. Note that the data were recorded in the 1920s. str(cars) ## &#39;data.frame&#39;: 50 obs. of 2 variables: ## $ speed: num 4 4 7 7 8 9 10 10 10 11 ... ## $ dist : num 2 10 4 22 16 10 18 26 34 17 ... summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 We can plot the speed and stopping distance in a scatterplot to get a sense of their relationship before proceeding with a formal regression. plot(dist ~ speed, data = cars, xlab = &quot;Speed (in Miles Per Hour)&quot;, ylab = &quot;Stopping Distance (in Feet)&quot;, main = &quot;Stopping Distance vs Speed&quot;, pch = 20, cex = 2,cex.main=1, cex.lab=.75, cex.axis=0.75, col = &quot;grey&quot;) This visualization suggests there could be a relationship. In this example, we are interested in using the predictor variable speed to predict and explain the response variable dist. We could express the relationship between \\(X\\) and \\(Y\\) using the following “Data Generating Process” (DGP): \\[\\begin{equation} Y=f(X)+\\epsilon \\tag{2.1} \\end{equation}\\] Here \\(Y\\) from 2.1 is the outcome determined by two parts: \\(f(X)\\), which is the deterministic part (a.k.a Data Generating Model - DGM) and \\(\\epsilon\\) the random part that makes the outcome different for the same \\(X\\) for each observation. What’s \\(f(X)\\)? We will see later that this question is very important. For now, however, we assume that \\(f(X)\\) is linear as \\[\\begin{equation} f(X)=\\beta_{0}+\\beta_{1} x_{i}. \\tag{2.2} \\end{equation}\\] And, \\[ \\begin{array}{l}{\\qquad Y_{i}=\\beta_{0}+\\beta_{1} x_{i}+\\epsilon_{i}} \\ {\\text { where }} \\ {\\qquad \\epsilon_{i} \\sim N\\left(0, \\sigma^{2}\\right)}\\end{array} \\] We could think that \\(Y\\) has different distribution for each value of \\(X\\). Hence, \\(f(X)\\) becomes the conditional mean of \\(Y\\) given \\(X\\). \\[ f(X) = \\mathrm{E}\\left[Y | X=x_{i}\\right]=\\beta_{0}+\\beta_{1} x_{i}, \\] which means that \\(\\mathrm{E}\\left[\\epsilon | X=x_{i}\\right]=0\\). This model, which is also called as the population regression function (PRF), has three parameters to be estimated: \\(\\beta_{0}\\), \\(\\beta_{1}\\), and \\(\\sigma^{2}\\), which are fixed but unknown constants. The coefficient \\(\\beta_{1}\\) defines the relationship between \\(X\\) and \\(Y\\). Inferential statistics deals with estimating these population parameters using a sample drawn from the population. The statistical inference requires an estimator of a population parameter to be BLUE (Best Linear Unbiased Estimator) which is usually challenging to satisfy. A BLU estimator also requires several assumptions on PRF. These include that: The errors are independent (no serial correlation). The errors are identically distributed (constant variance of \\(Y\\) for different values of \\(X\\)) . How do we actually find a line that represents the best relationship between \\(X\\) and \\(Y\\) best? One way to find a line is to find a set of parameters that minimize the sum of squared “errors”. This is called as the Ordinary Least Squares (OLS) method: \\[\\begin{equation} \\underset{\\beta_{0}, \\beta_{1}}{\\operatorname{argmin}} \\sum_{i=1}^{n}\\left(y_{i}-\\left(\\beta_{0}+\\beta_{1} x_{i}\\right)\\right)^{2} \\tag{2.3} \\end{equation}\\] Using R, we can apply this method very simply with a bit of code. model &lt;- lm(dist ~ speed, data = cars) b &lt;- coef(model) plot(cars, col = &quot;blue&quot;, pch = 20) abline(b, col = &quot;red&quot;, lty = 5) Although we can see the red line which seems to minimize the sum of squared errors, we can understand this problem better mathematically. 2.4.1 Ordinary Least Squares (OLS) The solution to this problem starts with defining the loss (cost) function, finding the first order conditions (F.O.C.) and solving them through normal equations. \\[\\begin{equation} f\\left(\\beta_{0}, \\beta_{1}\\right)=\\sum_{i=1}^{n}\\left(y_{i}-\\left(\\beta_{0}+\\beta_{1} x_{i}\\right)\\right)^{2}=\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\beta_{1} x_{i}\\right)^{2} \\tag{2.4} \\end{equation}\\] \\[ \\begin{aligned} \\frac{\\partial f}{\\partial \\beta_{0}} &amp;=-2 \\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\beta_{1} x_{i}\\right) \\\\ \\frac{\\partial f}{\\partial \\beta_{1}} &amp;=-2 \\sum_{i=1}^{n}\\left(x_{i}\\right)\\left(y_{i}-\\beta_{0}-\\beta_{1} x_{i}\\right) \\end{aligned} \\] Here, we have two equations and two unknowns: \\[ \\begin{array}{c}{\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\beta_{1} x_{i}\\right)=0} \\end{array} \\] \\[ \\begin{array}{c}{\\sum_{i=1}^{n}\\left(x_{i}\\right)\\left(y_{i}-\\beta_{0}-\\beta_{1} x_{i}\\right)=0}\\end{array} \\] Which can be expressed: \\[\\begin{equation} \\begin{array}{c}{n \\beta_{0}+\\beta_{1} \\sum_{i=1}^{n} x_{i}=\\sum_{i=1}^{n} y_{i}} \\end{array} \\tag{2.5} \\end{equation}\\] \\[\\begin{equation} \\begin{array}{c}{\\beta_{0} \\sum_{i=1}^{n} x_{i}+\\beta_{1} \\sum_{i=1}^{n} x_{i}^{2}=\\sum_{i=1}^{n} x_{i} y_{i}}\\end{array} \\tag{2.6} \\end{equation}\\] These functions (2.5 and 2.6) are also called normal equations. Solving them gives us: \\[\\begin{equation} \\beta_{1}=\\frac{\\text{cov}(Y,X)}{\\text{var}(X)} \\tag{2.7} \\end{equation}\\] \\[\\begin{equation} \\beta_{0}=\\overline{y}-\\beta_{1} \\overline{x} \\tag{2.8} \\end{equation}\\] As this is a simple review, we will not cover the OLS method in more depth here. Let’s use these variance/covariance values to get the parameters. x &lt;- cars$speed y &lt;- cars$dist Sxy &lt;- sum((x - mean(x)) * (y - mean(y))) Sxx = sum((x - mean(x)) ^ 2) #Here to show, &quot;=&quot; would work as well Syy &lt;- sum((y - mean(y)) ^ 2) beta_1 &lt;- Sxy / Sxx beta_0 &lt;- mean(y) - beta_1 * mean(x) c(beta_0, beta_1) ## [1] -17.579095 3.932409 Instead of coding each of the steps ourselves, we can also use the lm() function to achieve the same thing. model &lt;- lm(dist ~ speed, data = cars) model ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Coefficients: ## (Intercept) speed ## -17.579 3.932 The slope parameter (\\(\\beta_1\\)) tells us that the stopping distance is predicted to increase by \\(3.93\\) feet on average for an increase in speed of one mile per hour. The intercept parameter tells us that we have “modelling” issues: when the car’s speed is zero, it moves backwards. This indicates a modeling problem. One way to handle it to remove the intercept from the model that starts from the origin. x &lt;- cars$speed y &lt;- cars$dist beta_1 &lt;- sum(x*y) / sum(x^2) beta_1 ## [1] 2.909132 model &lt;- lm(dist ~ speed - 1, data = cars) model ## ## Call: ## lm(formula = dist ~ speed - 1, data = cars) ## ## Coefficients: ## speed ## 2.909 As we can see changing the model affects the prediction. Unfortunately the single-variable case is usually not a realistic model to capture the determination of the output. Let’s use a better dataset, mtcars from the same library, datasets: head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... We may want to model the fuel efficiency (mpg) of a car as a function of its weight (wt) and horse power (hp). We can do this using our method of normal equations. \\[ Y_{i}=\\beta_{0}+\\beta_{1} x_{i 1}+\\beta_{2} x_{i 2}+\\epsilon_{i}, \\quad i=1,2, \\ldots, n \\] \\[ f\\left(\\beta_{0}, \\beta_{1}, \\beta_{2}\\right)=\\sum_{i=1}^{n}\\left(y_{i}-\\left(\\beta_{0}+\\beta_{1} x_{i 1}+\\beta_{2} x_{i 2}\\right)\\right)^{2} \\] \\[ \\begin{aligned} \\frac{\\partial f}{\\partial \\beta_{0}} &amp;=0 \\\\ \\frac{\\partial f}{\\partial \\beta_{1}} &amp;=0 \\\\ \\frac{\\partial f}{\\partial \\beta_{2}} &amp;=0 \\end{aligned} \\] \\[ \\begin{array}{c}{n \\beta_{0}+\\beta_{1} \\sum_{i=1}^{n} x_{i 1}+\\beta_{2} \\sum_{i=1}^{n} x_{i 2}=\\sum_{i=1}^{n} y_{i}} \\end{array} \\] \\[ \\begin{array}{c}{\\beta_{0} \\sum_{i=1}^{n} x_{i 1}+\\beta_{1} \\sum_{i=1}^{n} x_{i 1}^{2}+\\beta_{2} \\sum_{i=1}^{n} x_{i 1} x_{i 2}=\\sum_{i=1}^{n} x_{i 1} y_{i}} \\end{array} \\] \\[ \\begin{array}{c}{\\beta_{0} \\sum_{i=1}^{n} x_{i 2}+\\beta_{1} \\sum_{i=1}^{n} x_{i 1} x_{i 2}+\\beta_{2} \\sum_{i=1}^{n} x_{i 2}^{2}=\\sum_{i=1}^{n} x_{i 2} y_{i}}\\end{array} \\] We now have three equations and three variables. While we could solve them by scalar algebra, it becomes increasingly cumbersome. Although we can apply linear algebra to see the analytical solutions, we just let R solve it for us: mpg_model = lm(mpg ~ wt + hp, data = mtcars) coef(mpg_model) ## (Intercept) wt hp ## 37.22727012 -3.87783074 -0.03177295 Up to this point we used OLS that finds the parameters minimizing the residual sum of squares (RSS - or the sum of squared errors). Instead of OLS, there are other methods that we can use. One of them is called Maximum likelihood Estimator or MLE. 2.4.2 Maximum Likelihood Estimators Understanding the MLE method starts with probability density functions (pdf), which characterize the distribution of a continuous random variable. Recall that pdf of a random variable \\(X \\sim N\\left(\\mu, \\sigma^{2}\\right)\\) is given by: \\[ f_{x}\\left(x ; \\mu, \\sigma^{2}\\right)=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} \\exp \\left[-\\frac{1}{2}\\left(\\frac{x_i-\\mu}{\\sigma}\\right)^{2}\\right] \\] In R, you can use dnorm(x, mean, sd) to calculate the pdf of normal distribution. The argument \\(x\\) represent the location(s) at which to compute the pdf. The arguments \\(\\mu\\) and \\(\\sigma\\) represent the mean and standard deviation of the normal distribution, respectively. For example, dnorm (0, mean=1, sd=2) computes the pdf at location 0 of \\(N(1,4)\\), normal distribution with mean 1 and variance 4. Let’s see examples of computing the pdf at 2 locations for. dnorm(0, mean = 1, sd = 2) ## [1] 0.1760327 dnorm(1, mean = 1, sd = 2) ## [1] 0.1994711 In addition to computing the pdf at one location for a single normal distribution, dnorm also accepts vectors with more than one elements in all three arguments. For example, suppose that we have the following data, \\(x\\). We can now compute pdf values for each \\(x\\). x &lt;- seq(from = -10, to = +22, length.out = 100) pdfs &lt;- dnorm(x, mean = 1, sd = 2) plot(x, pdfs) There are two implicit assumptions made here: (1) \\(x\\) is normally distributed, i.e. \\(X \\sim N\\left(\\mu = 1, \\sigma^{2}=4\\right)\\). As you can see from the code it’s a wrong assumption, but ignore it for now; (2) the distribution is defined by mean = 1 and sd = 2. The main goal in defining the likelihood function is to find the distribution parameters (mean and sd in a normal distribution) that fit the observed data best. Let’s have an example. Pretend that we do not see and know the following data creation: x &lt;- rnorm(1000, 2, 7) Of course we can plot the data and calculate the parameters of its distribution (the mean and standard deviation of \\(x\\)). However, how can we do it with likelihood function? Let’s plot three different pdf’s. Which one is the best distribution, representing the true distribution of the data? How can we find the parameters of the last plot? This is the idea behind Maximum likelihood method. pdfs1 &lt;- dnorm(x, mean = 1, sd=2) pdfs2 &lt;- dnorm(x, mean = 5, sd=7) pdfs3 &lt;- dnorm(x, mean = 2, sd=7) par(mfrow=c(1,3)) plot(x,pdfs1) plot(x,pdfs2) plot(x,pdfs3) It seems reasonable that a good estimate of the unknown parameter \\(\\mu\\) would be the value that maximizes the the likelihood (not the probability) of getting the data we observed. The probability density (or mass) function of each \\(x_{i}\\) is \\(f\\left(x_{i} ; \\mu, \\sigma^2\\right)\\). Then, the joint probability density function of \\(x_{1}, x_{2}, \\cdots, x_{n}\\), which we’ll call \\(L(\\mu, \\sigma^2)\\) is: \\[ L(\\mu, \\sigma^2)=P\\left(X_{1}=x_{1}, X_{2}=x_{2}, \\ldots, X_{n}=x_{n}\\right)=\\\\ f\\left(x_{1} ; \\mu,\\sigma^2\\right) \\cdot f\\left(x_{2} ; \\mu,\\sigma^2\\right) \\cdots f\\left(x_{n} ; \\mu, \\sigma^2\\right)=\\\\ \\prod_{i=1}^{n} f\\left(x_{i} ; \\mu, \\sigma^2\\right) \\] The first equality is just the definition of the joint probability density function. The second equality comes from that fact that we have a random sample, \\(x_i\\), that are independent and identically distributed. Hence, the likelihood function is: \\[ L(\\mu, \\sigma)=\\sigma^{-n}(2 \\pi)^{-n / 2} \\exp \\left[-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n}\\left(x_{i}-\\mu\\right)^{2}\\right] \\] and therefore the log of the likelihood function: \\[ \\log L\\left(\\mu, \\sigma\\right)=-n \\log \\sigma^{2}-\\frac{n}{2} \\log (2 \\pi)-\\frac{\\sum\\left(x_{i}-\\mu\\right)^{2}}{2 \\sigma^{2}} \\] Now, upon taking the partial derivative of the log likelihood with respect to \\(\\mu\\) and setting to 0, we see that: \\[ \\frac{\\partial \\log L\\left(\\mu, \\sigma\\right)}{\\partial \\mu}=\\frac{-\\not 2 \\sum\\left(x_{i}-\\mu\\right)(-\\not1)}{\\not2 \\sigma^{2}} \\stackrel{\\mathrm{SET}}{=} 0 \\] and we get \\[ \\sum x_{i}-n \\mu=0 \\] \\[ \\hat{\\mu}=\\frac{\\sum x_{i}}{n}=\\bar{x} \\] We can solve for \\(\\sigma^2\\) by the same way. As for the regression, since, \\[ Y_{i} | X_{i} \\sim N\\left(\\beta_{0}+\\beta_{1} x_{i}, \\sigma^{2}\\right) \\] \\[ f_{y_{i}}\\left(y_{i} ; x_{i}, \\beta_{0}, \\beta_{1}, \\sigma^{2}\\right)=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} \\exp \\left[-\\frac{1}{2}\\left(\\frac{y_{i}-\\left(\\beta_{0}+\\beta_{1} x_{i}\\right)}{\\sigma}\\right)^{2}\\right] \\] Given \\(n\\) data points \\((x_i,y_i)\\) we can write the likelihood as a function of the three parameters \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\). \\[\\begin{equation} L\\left(\\beta_{0}, \\beta_{1}, \\sigma^{2}\\right)=\\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} \\exp \\left[-\\frac{1}{2}\\left(\\frac{y_{i}-\\beta_{0}-\\beta_{1} x_{i}}{\\sigma}\\right)^{2}\\right] \\tag{2.9} \\end{equation}\\] Our goal is to find values of \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\) which maximize function 2.9. It is a straightforward multivariate calculus problem. First, let’s re-write 2.9 as follows: \\[\\begin{equation} L\\left(\\beta_{0}, \\beta_{1}, \\sigma^{2}\\right)=\\left(\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}}\\right)^{n} \\exp \\left[-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\beta_{1} x_{i}\\right)^{2}\\right] \\tag{2.10} \\end{equation}\\] We can make 2.10 linear by taking the log of this function, which is called as the log-likelihood function. \\[\\begin{equation} \\log L\\left(\\beta_{0}, \\beta_{1}, \\sigma^{2}\\right)=-\\frac{n}{2} \\log (2 \\pi)-\\frac{n}{2} \\log \\left(\\sigma^{2}\\right)-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\beta_{1} x_{i}\\right)^{2} \\tag{2.11} \\end{equation}\\] The rest would be simple calculus: \\[ \\frac{\\partial \\log L(\\beta_{0}, \\beta_{1}, \\sigma^{2})}{\\partial \\beta_{0}}=\\frac{1}{\\sigma^{2}} \\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\beta_{1} x_{i}\\right), \\] \\[ \\frac{\\partial \\log L\\left(\\beta_{0}, \\beta_{1}, \\sigma^{2}\\right)}{\\partial \\beta_{1}}=\\frac{1}{\\sigma^{2}} \\sum_{i=1}^{n}\\left(x_{i}\\right)\\left(y_{i}-\\beta_{0}-\\beta_{1} x_{i}\\right), \\] and, \\[ \\frac{\\partial \\log L\\left(\\beta_{0}, \\beta_{1}, \\sigma^{2}\\right)}{\\partial \\sigma^{2}}=-\\frac{n}{2 \\sigma^{2}}+\\frac{1}{2\\left(\\sigma^{2}\\right)^{2}} \\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\beta_{1} x_{i}\\right)^{2} \\] These first order conditions yield the following three equations with three unknown parameters: \\[ \\begin{aligned} \\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\beta_{1} x_{i}\\right) &amp;=0 \\\\ \\sum_{i=1}^{n}\\left(x_{i}\\right)\\left(y_{i}-\\beta_{0}-\\beta_{1} x_{i}\\right) &amp;=0 \\\\-\\frac{n}{2 \\sigma^{2}}+\\frac{1}{2\\left(\\sigma^{2}\\right)^{2}} \\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\beta_{1} x_{i}\\right)^{2} &amp;=0 \\end{aligned} \\] We call these estimates the maximum likelihood estimates. They are exactly the same as OLS parameters, except for the variance. So, we now have two different estimates of \\(\\sigma^{2}\\). \\[ \\begin{aligned} s_{e}^{2} &amp;=\\frac{1}{n-2} \\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}=\\frac{1}{n-2} \\sum_{i=1}^{n} e_{i}^{2} \\quad \\text { Least Squares } \\end{aligned} \\] \\[ \\begin{aligned} \\hat{\\sigma}^{2} &amp;=\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}=\\frac{1}{n} \\sum_{i=1}^{n} e_{i}^{2} \\quad \\text {MLE}\\end{aligned} \\] That’s why MLS is only an efficient estimator for large samples. 2.4.3 Estimating MLE with R How can we make estimations with MLE? It is good to look at a simple example. Suppose the observations \\(X_1,X_2,...,X_n\\) are from \\(N(\\mu,\\sigma^{2})\\) distribution (two parameters: mean and variance). The likelihood function is: \\[\\begin{equation} L(x)=\\prod_{i=1}^{i=n} \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{-\\frac{(x-\\mu)^{2}}{2 \\sigma^{2}}} \\tag{2.12} \\end{equation}\\] The objective is to find out the mean and the variance of the sample. Of course this a silly example: instead of using MLE to calculate them, we can use our middle-school algebra and find them right away. But the point here is to show how MLE works. And more importantly, we now have a different way to estimate the mean and the variance. The question here is, given the data, what parameters (mean and variance) would give us the maximum joint density. Hence, the likelihood function is a function of the parameter only, with the data held as a fixed constant, which gives us an idea of how well the data summarizes these parameters. Because we are interested in observing all the data points jointly, it can be calculated as a product of marginal densities of each observation assuming that observations are independent and identically distributed. Here is an example: #Let&#39;s create a sample of normal variables set.seed(2019) x &lt;- rnorm(100) # And the likelihood of these x&#39;s is prod(dnorm(x)) ## [1] 2.23626e-58 What’s happening here? One issue with the MLE method is that, as probability densities are often smaller than 1, the value of \\(L(x)\\) would be very small. Or very high, if the variance is very high. This could be a worse problem for large samples and create a problem for computers in terms of storage and precision. The solution would be the log-likelihood: \\[\\begin{equation} \\log (\\mathcal{L}(\\mu, \\sigma))=-\\frac{n}{2} \\log \\left(2 \\pi \\sigma^{2}\\right)-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n}\\left(x_{i}-\\mu\\right)^{2} \\tag{2.13} \\end{equation}\\] In a more realistic case we can only observe a sample of some data points and assume how it is distributed. With this assumption, we can have a log-likelihood function. Hence, if it’s a wrong assumption, our estimations will be wrong as well. The fact that we have to make assumptions about pdf’s will be very important issue when we cover nonparamateric estimations. Let’s assume that we have 100 \\(x\\)’s with \\(x\\sim N\\left(\\mu, \\sigma^{2}\\right)\\). We can now compute the derivatives of this log-likelihood and calculate the parameters. However, instead of this manual analytic optimization procedure, we can use R packages or algorithmic/numerical optimization methods. In fact, except for trivial models, the analytic methods cannot be applied to solve for the parameters. R has two packages optim() and nlm() that use algorithmic optimization methods, which we will see in the Appendix. For these optimization methods, it really does not matter how complex or simple the function is, as they will treat it as a black box. Here we can re-write function 2.13 : \\[\\begin{equation} -\\sum\\left(\\frac{\\left(x_{i}-\\mu\\right)^{2}}{2 \\sigma^{2}}+1 / 2 \\log 2 \\pi+1 / 2 \\log \\sigma^{2}\\right), \\tag{2.14} \\end{equation}\\] Instead of finding the parameters that minimize this negative function, we can find the maximum of the negative of this function. We can omit the term \\(-1/2\\log2\\pi\\) and define the function to R as follows: #Here is our function f(x) fn &lt;- function(prmt){ sum(0.5*(x - prmt[1])^2/prmt[2] + 0.5*log(prmt[2])) } #We have two packages nlm() and optim() to solve it #We arbitrarily pick starting points for the parameters sol1 &lt;- nlm(fn, prmt &lt;- c(1,2), hessian=TRUE) sol2 &lt;- optim(prmt &lt;- c(0,1), fn, hessian=TRUE) sol1 ## $minimum ## [1] 39.56555 ## ## $estimate ## [1] -0.07333445 0.81164723 ## ## $gradient ## [1] 5.478284e-06 4.384049e-06 ## ## $hessian ## [,1] [,2] ## [1,] 123.206236680 -0.007519674 ## [2,] -0.007519674 75.861573379 ## ## $code ## [1] 1 ## ## $iterations ## [1] 10 sol2 ## $par ## [1] -0.07328781 0.81161672 ## ## $value ## [1] 39.56555 ## ## $counts ## function gradient ## 51 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL ## ## $hessian ## [,1] [,2] ## [1,] 123.210867601 -0.007012263 ## [2,] -0.007012263 75.911070194 Let’s check if these estimates are correct: #mean mean(x) ## [1] -0.073334 #sd sum((x-mean(x))^2 )/length(x) ## [1] 0.8116477 This is nice. But we need to know little bit more about how optim() and nlp() works. More specifically, what’s an algorithmic optimization? We leave it to Algorithmic Optimization in Appendix. 2.5 BLUE There two universes in inferential statistics: the population and a sample. Statistical inference makes propositions about unknown population parameters using the sample data randomly drawn from the same population. For example, if we want to estimate the population mean of \\(X\\), \\(\\mu_X\\), we use \\(\\bar{x} =n^{-1}\\Sigma x_i\\) as an estimator on the sample. The choice of \\(n^{-1}\\Sigma x_i\\) as an estimator of \\(\\mu_X\\) seems commonsense, but why? What’s the criteria for a “good” estimator? The answer to this question is the key subject in econometrics and causal analysis. The chosen estimator must be the best (B) linear (L) unbiased (U) estimator (E) of the population parameter for a proper statistical inference. The Gauss–Markov theorem states that \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) in the sample regression function \\((Y_{i}=\\hat{\\beta}_{0}+\\hat{\\beta}_{1} x_{i}+\\hat{\\epsilon}_{i})\\) are BLU estimators of \\(\\beta_{0}\\) and \\(\\beta_{1}\\) provided that certain assumptions on the population regression function \\((Y_{i}=\\beta_{0}+\\beta_{1} x_{i}+\\epsilon_{i})\\) are satisfied. The property of unbiasedness requires that the expected value of the estimator is equal to the true value of the parameter being estimated. In the case of a regression, \\[ \\begin{array}{l}{\\mathrm{E}\\left[\\hat{\\beta}_{0}\\right]=\\beta_{0}} \\\\ {\\mathrm{E}\\left[\\hat{\\beta}_{1}\\right]=\\beta_{1}}\\end{array} \\] Recall the sampling distribution you’ve learned in statistics. The idea is simple. You have a population, but it is not accessible usually because it is too large. Thus, we use a random sample to estimate the parameters of interest for the population. This generalization from the sample to the population requires the concept of sampling distribution. Suppose we want to estimate the average age of the local population. You calculate the average age as 23 from a sample of 200 people randomly selected from the population. But, if you keep sampling 1000 times (1000 samples, 200 people in each), each sample will give you a different estimate. Which one should be used for the generalization (population)? None of them. We know that the average of all average ages calculated from 1000 samples will be the most correct average of the population, although only if the estimator is unbiased. For a simple average, the proof is easy. Let’s create our own sampling distribution for \\(\\bar{x}\\). We will draw 1000 samples from \\(X \\sim N\\left(5, 1\\right)\\). Each sample will have 200 \\(x\\)’s. Thus, we will calculate 1000 \\(\\bar{x}\\)’s. The objective is to see if \\[ \\mathrm{E}\\left(\\bar{x}\\right)=\\mu_x \\] There are multiple ways to do this simulation. # Population (1 million x&#39;s) pop_x &lt;- rnorm(1000000, mean = 5, sd = 1) # Random Sampling n &lt;- 200 # number of x&#39;s in each sample mcn &lt;- 1000 # number of samples in the simulation samp &lt;- matrix(0, nrow = n, ncol = mcn) # a Container: matrix by 200 x 1000 for(i in 1: mcn){ samp[,i] &lt;- sample(pop_x, n, replace = TRUE) } xbar &lt;- colMeans(samp) # We calculate the column means mxbar &lt;- mean(xbar) # the mean of xbars round(mxbar, 2) ## [1] 5 hist(xbar, breaks=20) This is our sampling distribution of \\(\\bar{x}\\) and \\(\\bar{x}\\) is an unbiased estimator of \\(\\mu_x\\). But is that enough? We may have another estimator, like \\(\\tilde{x} = (x_1 + x_{200})/2\\), which could be an unbiased estimator as well. xtilde &lt;- apply(samp, 2, function(x) (head(x,1)+ tail(x,1))/2) mtilde &lt;- mean(xtilde) # the mean of xbars round(mtilde, 2) ## [1] 5.03 hist(xtilde, breaks=20) Now, if we are happy with our unbiased estimators, how do we choose one estimator among all other unbiased estimators? How do we define the best estimator? The answer is simple: we choose the one with the minimum sampling variance. var(xbar) ## [1] 0.004704642 var(xtilde) ## [1] 0.4781128 Why do we need the minimum variance? Remember more variance means higher differences in \\(\\hat{\\beta}_{0} \\text { and } \\hat{\\beta}_{1}\\) from sample to sample. That means a very large confidence interval for the \\(\\mu_x\\). Since we have only one sample in practice, the high sampling variance results in greater likelihood that we will get results further away from the mean of \\(\\bar{x}\\), which is captured by the confidence interval. First, note that it is very easy to create an estimator for \\(\\beta_{1}\\) that has very low variance, but is not unbiased. For example, define \\(\\hat{\\theta}_{B A D}=5\\). Since \\(\\hat{\\theta}_{B A D}\\) is constant, \\[ \\begin{array}{r}{\\mathbf{Var}\\left[\\hat{\\theta}_{B A D}\\right]=0} \\end{array} \\] However since, \\(\\mathbf{E}\\left[\\hat{\\theta}_{B A D}\\right]=5\\), we can say that \\(\\hat{\\theta}_{B A D}\\) is not a good estimator even though it has the smallest possible variance. Hence two conditions, unbiasedness and minimum variance, have an order: we look for an estimator with the minimum variance among unbiased estimators. Omitted Variable Bias (OVB) A regression analysis requires a correctly specified regression model. When the estimated model is misspecified by “omitting” some necessary variables, the resulting sample regression function (SRF) will be a biased estimator of the true data generating model (DGM). While the solution seems simple - “correctly specify your SRF by including all necessary variables as in the true DGM”, we often have to assume what the true DGM is. In other words, we do our best to control all the confounding variables, variables that are correlated with the outcome (\\(y_i\\)) and the explanatory variables. When we miss (omit) one of those “control” variables, perhaps due to our ignorance on the true DGM, the effect of \\(x_i\\) on \\(y_i\\) will be misleading and biased. One gross example is the significant and positive relationship between ice-cream sales and the crime rates. When we include the confounding factor, the hot weather, which is strongly and positively related to both ice-cream sales and the crime rates, the effect disappears. What we are looking for is the isolated effect of \\(x_i\\) on \\(y_i\\), after controlling for all the other possible reasons of the variation in \\(y_i\\). Let’s illustrate it with Venn diagrams. library(broom) library(faux) ## ## ************ ## Welcome to faux. For support and examples visit: ## https://debruine.github.io/faux/ ## - Get and set global package options with: faux_options() ## ************ library(eulerr) #For Euler and Venn diagrams library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ## ── ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ✔ purrr 1.0.0 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ purrr::%||%() masks faux::%||%() ## ✖ lubridate::as.difftime() masks base::as.difftime() ## ✖ dplyr::between() masks plm::between() ## ✖ lubridate::date() masks base::date() ## ✖ dplyr::filter() masks stats::filter() ## ✖ lubridate::intersect() masks base::intersect() ## ✖ dplyr::lag() masks plm::lag(), stats::lag() ## ✖ dplyr::lead() masks plm::lead() ## ✖ lubridate::setdiff() masks base::setdiff() ## ✖ lubridate::union() masks base::union() # Correlated variables and PRF set.seed(1234) df &lt;- rnorm_multi(n = 100, mu = c(10, 9, 9), sd = c(2, 1.7, 1.3), r = c(0.5, 0.4, 0.8), varnames = c(&quot;Y&quot;, &quot;X1&quot;, &quot;X2&quot;), empirical = FALSE) Here is our simulated data and the SRF: head(df, 6) ## Y X1 X2 ## 1 12.510529 10.266830 10.180426 ## 2 9.058085 8.682856 9.391148 ## 3 8.223374 7.284975 7.968759 ## 4 13.509343 12.554053 12.030183 ## 5 8.419466 8.878325 9.273916 ## 6 9.337398 7.832841 8.773532 cor(df) ## Y X1 X2 ## Y 1.0000000 0.4739343 0.3878172 ## X1 0.4739343 1.0000000 0.8107924 ## X2 0.3878172 0.8107924 1.0000000 lm(Y ~., data = df) # true model ## ## Call: ## lm(formula = Y ~ ., data = df) ## ## Coefficients: ## (Intercept) X1 X2 ## 5.15034 0.54801 0.01717 Here is the Venn diagram reflecting the effect of \\(X_1\\), \\(\\mathbf{B}\\), when we omit \\(X_2\\) in the SRF. The sizes also reflect the decomposition of the variances. Thus, \\(\\mathbf{B}/(\\mathbf{A+D})\\) is % of \\(y\\) is explained by \\(X_1\\): Now, after controlling for the effect of \\(X_2\\), \\(\\mathbf{B}\\), the isolated effect of \\(X_1\\) is reduced to \\(\\mathbf{D}\\): Hence, when we omit \\(X_2\\), the effect of \\(X_1\\) is the area of \\(\\mathbf{D+G}\\), while the true effect is \\(\\mathbf{D}\\). Of course, if \\(X_1\\) and \\(X_2\\) are independent, omitting \\(X_2\\) would not be a problem. It is the same thing to say that, if we have independent regressors, running separate regressions each regressor would give us the same results as the one when we combine them. However, when omitted variables are correlated with the regressors (\\(X\\)), the conditional expectation on the error term will be non-zero, \\(\\text{E}(\\epsilon|x)\\neq0\\). An excellent demonstration of regressions with Venn diagrams can be found in the Andrew Heiss’ post. Here is a more realistic example from Stock and Watson (2015, p. 196): suppose we want to understand the relationship between test score and class size. If we run a regression without considering possible confounding factors, we may face the same problem we described above. This is because the percentage of English learners in the school district might be correlated with both test score and class size. Hence, the “true” model should look like equation 2.15: \\[\\begin{equation} Test Score =\\beta_{0}+\\beta_{1}S T R+\\beta_{2}PctEL+\\epsilon_{i}, \\tag{2.15} \\end{equation}\\] where STR and PctEL are correlated, that is \\(\\rho_{str,~ pctel} \\neq 0\\). We can omit PctEL in 2.15 and estimate it as \\[\\begin{equation} Test Score =\\hat{\\beta}_{0}+\\hat{\\beta}_{1}STR+v_{i}. \\tag{2.16} \\end{equation}\\] Intuitively, the errors are the residuals in regressions as they collect all “other” factors that are not explicitly modeled in the regrassion but affect the outcome randomly. They are supposed to be independent from all the regressors in the model. As the omitted variable, PctEL, joins to the residual (\\(v_i\\)), \\(\\hat{\\beta}_{1}\\) will not reflect the true effect of changes in STR on the test score. We can formally see the result of this omission as follows: \\[ \\hat{\\beta}_{1}=\\frac{\\mathrm{Cov}\\left(STR_{i},TestScore_{i}\\right)}{\\mathrm{Var}\\left(STR_{i}\\right)}=\\frac{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)^{2}}=\\frac{\\sum_{i}\\left(x_{i}-\\bar{x}\\right) y_{i}}{\\sum_{i}\\left(x_{i}-\\bar{x}\\right) x_{i}} \\] We can substitute \\(y_i\\) into the last term and simplify: \\[ \\begin{aligned} \\hat{\\beta}_{1} &amp;=\\frac{\\sum_{i}\\left(x_{i}-\\bar{x}\\right)\\left(\\beta_{0}+\\beta_{1} x_{i}+\\beta_{2} z_{i}+\\epsilon_{i}\\right)}{\\sum_{i}\\left(x_{i}-\\bar{x}\\right) x_{i}} =\\beta_{1}+\\beta_{2} \\frac{\\sum_{i}\\left(x_{i}-\\bar{x}\\right) z_{i}}{\\sum_{i}\\left(x_{i}-\\bar{x}\\right) x_{i}}+\\frac{\\sum_{i}\\left(x_{i}-\\bar{x}\\right) \\epsilon_{i}}{\\sum_{i}\\left(x_{i}-\\bar{x}\\right) x_{i}}, \\end{aligned} \\] where \\(z\\) is \\(PctEL\\) (omitted variable), \\(x\\) is \\(STR\\), \\(y\\) is \\(TestScore\\). The second term is a result of our omission of variable PctEL (\\(z\\)). If we take the expectation of the last line: \\[ \\begin{aligned} \\mathrm{E}\\left[\\hat{\\beta}_{1}\\right] &amp;=\\mathrm{E}\\left[\\beta_{1}+\\beta_{2} \\frac{\\sum_{i}\\left(x_{i}-\\bar{x}\\right) z_{i}}{\\sum_{i}\\left(x_{i}-\\bar{x}\\right) x_{i}}+\\frac{\\sum_{i}\\left(x_{i}-\\bar{x}\\right) \\epsilon_{i}}{\\sum_{i}\\left(x_{i}-\\bar{x}\\right) x_{i}}\\right] \\\\ &amp;=\\beta_{1}+\\beta_{2} \\mathrm{E}\\left[\\frac{\\sum_{i}\\left(x_{i}-\\bar{x}\\right) z_{i}}{\\sum_{i}\\left(x_{i}-\\bar{x}\\right) x_{i}}\\right]+\\mathrm{E}\\left[\\frac{\\sum_{i}\\left(x_{i}-\\bar{x}\\right) \\epsilon_{i}}{\\sum_{i}\\left(x_{i}-\\bar{x}\\right) x_{i}}\\right] \\\\ &amp;=\\beta_{1}+\\beta_{2} [\\mathrm{Cov}(x_i,z_i)/\\mathrm{Var}(x_i)] \\end{aligned} \\] What this means is that on average, our regression estimate is going to miss the true population parameter by the second term. Here is the OVB in action: # load the AER package library(AER) # load the data set data(CASchools) str(CASchools) ## &#39;data.frame&#39;: 420 obs. of 14 variables: ## $ district : chr &quot;75119&quot; &quot;61499&quot; &quot;61549&quot; &quot;61457&quot; ... ## $ school : chr &quot;Sunol Glen Unified&quot; &quot;Manzanita Elementary&quot; &quot;Thermalito Union Elementary&quot; &quot;Golden Feather Union Elementary&quot; ... ## $ county : Factor w/ 45 levels &quot;Alameda&quot;,&quot;Butte&quot;,..: 1 2 2 2 2 6 29 11 6 25 ... ## $ grades : Factor w/ 2 levels &quot;KK-06&quot;,&quot;KK-08&quot;: 2 2 2 2 2 2 2 2 2 1 ... ## $ students : num 195 240 1550 243 1335 ... ## $ teachers : num 10.9 11.1 82.9 14 71.5 ... ## $ calworks : num 0.51 15.42 55.03 36.48 33.11 ... ## $ lunch : num 2.04 47.92 76.32 77.05 78.43 ... ## $ computer : num 67 101 169 85 171 25 28 66 35 0 ... ## $ expenditure: num 6385 5099 5502 7102 5236 ... ## $ income : num 22.69 9.82 8.98 8.98 9.08 ... ## $ english : num 0 4.58 30 0 13.86 ... ## $ read : num 692 660 636 652 642 ... ## $ math : num 690 662 651 644 640 ... # define variables CASchools$STR &lt;- CASchools$students/CASchools$teachers CASchools$score &lt;- (CASchools$read + CASchools$math)/2 Let’s estimate both regression models and compare. # Estimate both regressions model1 &lt;- lm(score ~ STR, data = CASchools) # Underfitted model model2 &lt;- lm(score ~ STR + english, data = CASchools) # True model model1 ## ## Call: ## lm(formula = score ~ STR, data = CASchools) ## ## Coefficients: ## (Intercept) STR ## 698.93 -2.28 model2 ## ## Call: ## lm(formula = score ~ STR + english, data = CASchools) ## ## Coefficients: ## (Intercept) STR english ## 686.0322 -1.1013 -0.6498 Is the magnitude of the bias, \\(-1.1787 = -2.28 - (-1.1013)\\), consistent with the formula, \\(\\beta_{2} [\\mathrm{Cov}(x_i,z_i)/\\mathrm{Var}(x_i)]\\)? cov(CASchools$STR, CASchools$english)/var(CASchools$STR)*model2$coefficients[3] ## english ## -1.178512 2.6 Modeling the data When modeling data, there are a number of choices that need to be made. What family of models will be considered? In linear regression, we specified models with parameters (\\(\\beta{j}\\)) and fit the model by finding the best values of these parameters. This is a parametric approach. A non-parametric approach skips the step of specifying a model with parameters and are often described as more of an algorithm. Non-parametric models are often used in machine learning, which we will see in Part II. Here is an example x &lt;- CASchools$STR y &lt;- CASchools$score xt &lt;- seq(min(x), max(x), length.out = length(x)) plot(x, y, col = &quot;grey&quot;, main = &quot;Parametric (red) vs Nonparametric (blue)&quot;) lines(x, predict(lm(y ~ x)), col = &quot;red&quot;, lwd = 2) lines(xt, predict(loess(y ~ x, degree = 2, span =0.2), xt), col = &quot;blue&quot;, lwd = 2) If we define a parametric model, what form of the model will be used for \\(f(.)\\) shown below? \\[ y =f\\left(x_{1}, x_{2}, x_{3}, \\ldots, x_{p}\\right)+\\epsilon \\] Would it be linear or polynomial? pfit &lt;- lm(y ~ x + I(x^2)+I(x^3)+I(x^4)+I(x^5)) dt &lt;- data.frame(yhat = pfit$fitted.values, x = x) dt &lt;- dt[order(dt$x), ] plot(x, y, col = &quot;grey&quot;, main = &quot;Linear (red) vs Polynomial (blue)&quot;) lines(x, predict(lm(y ~ x)), col = &quot;red&quot;, lwd = 2) lines(dt$x, dt$yhat, col = &quot;blue&quot;, lwd = 2) If we are going to add non-linear terms, which variables would be selected with what degree of polynomial terms? Moreover, if there are interaction between the predictors, the effect of a regressor on the response will change depending on the values of the other predictors. Hence, we need to assume what the “true” population model (DGM) would be when searching for a model. How will the model be fit? Although we have seen two of the most common techniques, OLS and MLE, there are more techniques in the literature. Addressing these three questions are the fundamental steps in defining the relationships between variables and could be different in causal and predictive analyses. 2.7 Causal vs. Predictive Models What is the purpose of fitting a model to data? Usually it is to accomplish one of two goals. We can use a model to explain the causal relationship between the response and the explanatory variables. And, we can also use a model to predict the outocme variable. 2.7.1 Causal Models If the goal of a model is to explain the causal relationship between the response and one or more of the explanatory variables, we are looking for a model that is small and interpretable, but still fits the data well. Suppose we would like to identify the factors that explain fuel efficiency (mpg - miles per gallon) based on a car’s attributes, weight, year, hp, etc. If we are trying to understand how fuel efficiency is determined by a car’s attributes, we may want to have a less complex and interpretable model. Note that parametric models, particularly linear models of any size, are the most interpretable models to begin with. If our objective is to predict if a car would be classified as efficient or not given its attributes, we may give up interpretablity and use more complicated methods that may have better prediction accuracy. We will see later many examples of this trade-off. To find small and interpretable models, we use inferential techniques with additional assumptions about the error terms in a model: \\[ \\epsilon \\sim N\\left(0, \\sigma^{2}\\right) \\] This assumption states that the the error is normally distributed with some common variance. Also, this assumption states that the expected value of the error term is zero. In order words, the model has to be correctly specified without any omitted variable. One very important issue to understand a causal relationship is to distinguish two terms often used to describe a relationship between two variables: causation and correlation, both of which explain the relationship between \\(Y\\) and \\(X\\). Correlation is often also referred to as association. One good example is the empirical relationship between ice cream sales and the crime rate in a given region. It has been shown that (Pearl_Mackenzie_2018?) the correlation between these two measures are strong and positive. Just because these two variables are correlated does not necessarily mean that one causes the other (as people eat more ice cream, they commit more crimes?). Perhaps there is a third variable that explains both! And, it is known that very hot weather is that third missing factor that causes both ice cream sales and crime rates to go up. You can see many more absurd examples on the Spurious Correlations website. Causation is distinct from correlation, because it reflects a relationship in which one variable directly effects another. Rather than just an association between variables that may be caused by a third hidden variable, causation implies a direct link between the two. Continuing the example from earlier, the very hot weather has a causal connection with both ice cream sales and crime, even though those two outcomes only share a correlation with each other. 2.7.2 Predictive Models If the goal of a model is to predict the response, then the only consideration is how well the model fits the data. Hence we do not need to have distributional assumptions as stated above. More specifically, correlation and causation are not an issue here. If a predictor is correlated with the response, it would be useful for prediction. For example, ice-cream sales are perfectly fine if they predict the crime rates. However, as we see later, a word of caution is needed when using a model to predict an outcome. Mud on the ground would predict that we had a rain. Or if a person has been hospitalized for the last 3 months, we can predict that the person was sick. These types of predictions are useless and called usually model or data leaking in machine learning. What happens if we use a model built for causal analysis to predict the outcome? We will answer this question later. Keep this question in mind, because it will be a fundamental question to understand how statistical learning would be different than a model that seeks a causation between \\(Y\\) and \\(X\\). Since we are not performing inference, life is relatively easier with predictive models. Therefore, the extra assumptions about the model specifications and the distributional aspects of the estimators are not needed. We only care about prediction error or the prediction accuracy. Although we may have a secondary objective in predictive models, which is to identify the most important predictors, they would be useless for causal inference in most cases. The best predictive model minimizes the prediction error, which is the following root-mean-squared-prediction-error for numerical outcomes: \\[ \\text { RMSPE }=\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}}, \\] where \\(y_i\\) are the actual values of the response for the “given data” and \\(\\hat{y}\\) are the predicted values using the fitted model and the predictors from the data. Note that RMSPE has the same unit as the response variable. Later, we will see more performance metrics in predictive models. An important issue in calculating RMSPE is which \\(y\\)’s are supposed to be predicted. If we use the same” \\(y\\)’s that we also use to calculate \\(\\hat{y}\\)’s, RMSPE will tend to be lower for a larger and more complex models. However, a model becomes too specific for a sample as it gets more complex, which is called overfitting. In other words, when a model overfits, it will be less “generalizable” for another sample. Consequently, these overly specific models would have very poor predictions for “out-of-sample” data. This topic will be covered in the next section. But before that, lets have an example that shows an overfitting model: # Simple OLS model_ols &lt;- lm(dist ~ speed, data = cars) b &lt;- coef(model_ols) # A complex model model_cmpx &lt;- smooth.spline(cars$speed, cars$dist, df=19) plot(cars, col = &quot;blue&quot;, pch = 1, main = &quot;Complex Model vs. Simple OLS&quot;) abline(b, col = &quot;red&quot;) lines(model_cmpx,col=&#39;green&#39;, lwd=2) The figure above shows two models to predict the stopping distance a car by its speed. The “complex” model uses a non-parametric method (green line), which has the minimum RMSPE relative to the red dashed line representing a simple linear model. We have only one sample and the “complex” model is the winner with the smallest RMSPE. But if we use these two models on “unseen” (out-of-sample) data, would the winner change? Would it be possible to have the following results (consider only the order of the numbers)? Type of Model In-Sample RMSPE Out-Sample RMSPE Simple model 1.71 1.45 Complex model 1.41 2.07 We will answer it in coming chapters in details but, for now, let use our first simulation exercise. 2.8 Simulation Simulations are tools to see whether statistical arguments are true or not. In simulations, we know the data generating process (DGP) because we define them by a selected model and a set of parameters. On of the biggest strengths of R is its ability to carry out simulations with a simple design. We will see more examples on simulations in Chapter 37. We are going to generate a sample of observations on \\(Y\\) from a data generation model (DGM): set.seed(1) X &lt;- seq(from = 0, to = 20, by = 0.1) dgm &lt;- 500 + 20*X - 90*sin(X) #This is our DGM y = dgm + rnorm(length(X), mean = 10, sd = 100) #This is our DGP data = data.frame(y, X) plot(X, y, col=&#39;deepskyblue4&#39;, xlab=&#39;X&#39;, main=&#39;Observed data &amp; DGM&#39;) lines(X, dgm, col=&#39;firebrick1&#39;, lwd=2) This is the plot of our simulated data. The simulated data points are the blue dots while the red line is the DGM or the systematic part. Now we have the data (\\(X\\) and \\(Y\\)) and we also know the underlying data generating procedure (DGP) that produces these observations. Let’s pretend that we do not know DGP. Our job is to estimate DGM from the data we have. We will use three alternative models to estimate the true DGM. # Linear model model1 &lt;- lm(y ~ X) plot(X, y, col=&#39;deepskyblue4&#39;, xlab=&#39;X&#39;, main=&#39;Linear&#39;) lines(X, model1$fitted.values, col = &quot;blue&quot;, lwd=2) # Polynomial model (there is an easier way!) model2 &lt;- lm(y ~ X + I(X^2) + I(X^3) + I(X^4) + I(X^5) + I(X^6) + I(X^7) + I(X^8) + I(X^9) + I(X^10) + I(X^11) + I(X^12) + I(X^13) + I(X^14) + I(X^15) + I(X^16) + I(X^17) + I(X^18), data=data) plot(X, y, col= &#39;deepskyblue4&#39;, xlab=&#39;X&#39;, main=&#39;Polynomial&#39;) lines(X, fitted(model2), col=&#39;darkgreen&#39;, lwd=2) #Nonparametric model3 &lt;- smooth.spline(X, y, df=200) plot(X, y, col=&#39;deepskyblue4&#39;, xlab=&#39;X&#39;, main=&#39;Spline&#39;) lines(model3, col=&#39;orange&#39;, lwd=2) As obvious from the plots, the nonparametric spline model (we’ll see later what it is) should have the minimum RMSPE. # Let create a function for RMSPE rmse = function(actual, predicted) { sqrt(mean((actual - predicted) ^ 2)) } # Predicted values by the 3 models using the &quot;seen&quot; data predicted1 &lt;- fitted(model1) predicted2 &lt;- fitted(model2) predicted3 &lt;- predict(model3, X) # Note that the actual y is the same for all models rmse1_s &lt;- rmse(predicted1, y) rmse2_s &lt;- rmse(predicted2, y) rmse3_s &lt;- rmse(predicted3$y, y) seen &lt;- c(&quot;RMSPE for model1 (linear)&quot; = rmse1_s, &quot;RMSPE for model2 (polynomial)&quot; = rmse2_s, &quot;RMSPE for model3 (nonparametric)&quot; = rmse3_s ) seen ## RMSPE for model1 (linear) RMSPE for model2 (polynomial) ## 119.46405 88.87396 ## RMSPE for model3 (nonparametric) ## 67.72450 Now we will test them on another sample from the same DGP that we haven’t seen before: # Since DGM is the same the only difference is the random error # in this sample set.seed(2) y2 = dgm + rnorm(length(X), mean = 10, sd = 100) plot(X, y2, col=&#39;deepskyblue4&#39;, xlab=&#39;X&#39;, main = &#39;The &quot;Unseen&quot; 2nd Sample&#39;) # Since DGM is the same X&#39;s are the same rmse1_us &lt;- rmse(predicted1, y2) rmse2_us&lt;- rmse(predicted2, y2) rmse3_us &lt;- rmse(predicted3$y, y2) unseen &lt;- c(&quot;RMSPE for model1 (linear)&quot; = rmse1_us, &quot;RMSPE for model2 (polynomial)&quot; = rmse2_us, &quot;RMSPE for model3 (nonparametric)&quot; = rmse3_us) Let’s put them together: table &lt;- matrix(NA, 2, 3) row.names(table) &lt;- c(&quot;Seen-data&quot;, &quot;Unseen-data&quot;) colnames(table) &lt;- c(&quot;Linear&quot;, &quot;Polynomial&quot;, &quot;Spline&quot;) table[1,1] &lt;- seen[1] table[1,2] &lt;- seen[2] table[1,3] &lt;- seen[3] table[2,1] &lt;- unseen[1] table[2,2] &lt;- unseen[2] table[2,3] &lt;- unseen[3] table ## Linear Polynomial Spline ## Seen-data 119.4640 88.87396 67.7245 ## Unseen-data 123.4378 109.99681 122.3018 The last model estimated by Spline has the minimum RMSPE using the seen data. It fits very well when we use the seen data but it is not so good at predicting the outcomes in the unseen data. A better fitting model using only the seen data could be worse in prediction. This is called overfitting, which is what we will see in the next chapter. "],["learning-systems.html", "Learning Systems", " Learning Systems A process of learning in a learning system can be summarized in several steps: The learner has a sample of observations. This is an arbitrary (random) set of objects or instances each of which has a set of features (\\(\\mathbf{X}\\) - features vector) and labels/outcomes (\\(y\\)). We call this sequence of pairs as a training set: \\(S=\\left(\\left(\\mathbf{X}_{1}, y_{1}\\right) \\ldots\\left(\\mathbf{X}_{m}, y_{m}\\right)\\right)\\). We ask the learner to produce a prediction rule (a predictor or a classifier), so that we can use it to predict the outcome of new observations (instances). We assume that the training dataset \\(S\\) is generated by a data-generating model (DGM) or a labeling function, \\(f(x)\\). The learner does not know about \\(f(x)\\). In fact, we ask the learner to discover it. The learner will come up with a prediction rule, \\(\\hat{f}(x)\\), by using \\(S\\), which will be different than \\(f(x)\\). Hence, we can measure the learning system’s performance by a loss function: \\(L_{(S, f)}(\\hat{f})\\), which is a sort of rule (or a function) that defines the difference between \\(\\hat{f}(x)\\) and \\(f(x)\\). This is also called as the generalization error or the risk. The goal of the algorithm is to find \\(\\hat{f}(x)\\) that minimizes the difference from the unknown \\(f(x)\\). The key point here is that, since the learner does not know \\(f(x)\\), it cannot quantify the gap. However, it calculates the prediction error also called as the empirical error or the empirical risk, which is a function that defines the difference between \\(\\hat{f}(x)\\) and \\(y_i\\). Hence, the learning process can be defined as coming up with a predictor \\(\\hat{f}(x)\\) that minimizes the empirical error. This process is called Empirical Risk Minimization (ERM). The question now becomes what sort of conditions would lead to bad or good ERM? If we use the training data (in-sample data points) to minimize the empirical risk, the process can lead to \\(L_{(S, f)}(\\hat{f}) = 0\\). This problem is called overfitting and the only way to rectify it is to restrict the sample that the learning model can access. The common way to do this is to “train” the model over a subsection of the data (training or in-sample data points) and apply ERM by using the test data (out-sample data points). Since this process restrict the learning model by limiting the number of observations in it, this procedure is also called inductive bias in the process of learning. There are always two “universes” in a statistical analysis: the population and the sample. The population is usually unknown or inaccessible to us. We consider the sample as a random subset of the population. Whatever the statistical analysis we apply almost always uses that sample dataset, which could be very large or very small. Although the sample we have is randomly drawn from the population, it may not always be representative of the population. There is always some risk that the sampled data happen to be very unrepresentative of the population. Intuitively, the sample is a window through which we have partial information about the population. We use the sample to estimate an unknown parameter of the population, which is the main task of inferential statistics. In predictive systems, we also use the sample, but we develop a prediction rule to predict unknown population outcomes. Can we use an estimator as a predictor? Could a “good” estimator also be a “good” predictor. We had some simulations in the previous chapter showing that the best estimator could be the worst predictor. Why? In this section we will try to delve deeper into these questions to find answers. The starting point will be to define these two different but similar processes. "],["bias-variance-tradeoff.html", "Chapter 3 Bias-Variance Tradeoff 3.1 Estimator and MSE 3.2 Prediction - MSPE 3.3 Biased estimator as a predictor 3.4 Dropping a variable in a regression 3.5 Uncertainty in estimations and predictions 3.6 Prediction interval for unbiased OLS predictor", " Chapter 3 Bias-Variance Tradeoff The intuition behind the subject formally covered in the following chapters is simple: if a “thing” is more specific to a certain “environment”, it would be less generalizable for other environments. The “thing” is our predictive model, the “environment” is the data, our sample. If we build a model that fits (predicts) our sample very well, it would be too specific for that data but less generalizable for other samples. What’s the “sweet spot” between being “specific” and “generalizable” when we build a predictive model? 3.1 Estimator and MSE The task is to estimate an unknown population parameter, say \\(\\theta\\), which could be a simple mean of \\(X\\), \\(\\mu_x\\), or a more complex slope coefficient, \\(\\beta\\), of an unknown DGM. We use a random sample from the population and \\(\\hat{\\theta}\\) as an estimator of \\(\\theta\\). We need to choose the best estimator to estimate \\(\\theta\\) among many possible estimators. For example, if we want to estimate \\(\\mu_x\\), we could use, \\[ \\bar{X}=\\frac{1}{n} \\sum_{i=1}^{n} x_{i} \\] or alternatively, \\[ \\hat{X}=0.5 x_{1}+0.5x_{n} \\] Therefore, we need to define what makes an estimator the “best” among others. The sampling distribution, which is the probability distribution of all possible estimates obtained from repeated sampling, would help us develop some principles. The first and the most important criteria should be that the expected mean of all estimates obtained from repeated samples should be equal to \\(\\mu_x\\). Any estimator satisfying this condition is called as an unbiased estimator. However, if \\(x\\)’s are independently and identically distributed (i.i.d), it can be shown that those two estimators, \\(\\bar{X}\\) and \\(\\hat{X}\\), are both unbiased. That is, \\(\\mathbf{E}(\\bar{X})= \\mathbf{E}(\\hat{X})=\\mu_x\\). Although, it would be easy to obtain an algebraic proof, a simulation exercise can help us visualize it. In fact, we can add a third estimator, \\(\\tilde{X}=x_3\\), which is also unbiased. # Population populationX &lt;- c(0, 3, 12) #Container to have repeated samples samples &lt;- matrix(0, 2000, 3) colnames(samples) &lt;- c(&quot;FirstX&quot;, &quot;SecondX&quot;, &quot;ThirdX&quot;) # Random samples from population set.seed(123) for (i in 1:nrow(samples)) { samples[i, ] &lt;- sample(populationX, 3, replace = TRUE) } head(samples) ## FirstX SecondX ThirdX ## [1,] 12 12 12 ## [2,] 3 12 3 ## [3,] 3 3 12 ## [4,] 0 3 3 ## [5,] 0 3 12 ## [6,] 0 12 12 In this simulation the population has only 3 values (0, 3, 12) but our sample can draw the same number multiple times. Each row is displaying the first few samples of 2000 random samples drawn from the population. Each column shows the order of observations, or random draws, \\(x_1, x_2, x_3\\). This example may seem strange because of its population size, but for the sake of simplicity, it works fine in our experiment. We know the population \\(\\mu_x\\) is 5, which is the mean of our three values (0, 3, 12) in the population. Knowing this, we can test the following points: Is \\(X\\) i.i.d? An identical distribution requires \\(\\mathbf{E}(x_1)=\\mathbf{E}(x_2)=\\mathbf{E}(x_3)\\) and \\(\\mathbf{Var}(x_1)=\\mathbf{Var}(x_2)=\\mathbf{Var}(x_3)\\). And an independent distribution requires \\(\\mathbf{Corr}(x_i,x_j)=0\\) where \\(i\\neq{j}\\). Are the three estimators unbiased. That is, whether \\(\\mathbf{E}(\\bar{X})= \\mathbf{E}(\\hat{X})= \\mathbf{E}(\\tilde{X}) = \\mu_x\\). Let’s see: library(corrplot) # Check if E(x_1)=E(x_2)=E(x_3) round(colMeans(samples),2) ## FirstX SecondX ThirdX ## 5.07 5.00 5.13 # Check if Var(x_1)=Var(x_2)=Var(x_3) apply(samples, 2, var) ## FirstX SecondX ThirdX ## 26.42172 25.39669 26.31403 # Check correlation cor(samples) ## FirstX SecondX ThirdX ## FirstX 1.000000000 -0.025157827 -0.005805772 ## SecondX -0.025157827 1.000000000 -0.002157489 ## ThirdX -0.005805772 -0.002157489 1.000000000 # Note that if we use only unique set of samples uniqsam &lt;- unique(samples) colMeans(uniqsam) ## FirstX SecondX ThirdX ## 5 5 5 apply(uniqsam, 2, var) ## FirstX SecondX ThirdX ## 27 27 27 cor(uniqsam) ## FirstX SecondX ThirdX ## FirstX 1 0 0 ## SecondX 0 1 0 ## ThirdX 0 0 1 It seems that the i.i.d condition is satisfied. Now, we need to answer the second question, whether the estimators are unbiased or not. For this, we need to apply each estimator to each sample: # Xbar X_bar &lt;- c() for(i in 1:nrow(samples)){ X_bar[i] &lt;- sum(samples[i, ]) / ncol(samples) } mean(X_bar) ## [1] 5.064 # Xhat X_hat &lt;- c() for(i in 1:nrow(samples)){ X_hat[i] &lt;- 0.5*samples[i, 1] + 0.5*samples[i, 3] } mean(X_hat) ## [1] 5.097 # Xtilde X_tilde &lt;- c() for(i in 1:nrow(samples)){ X_tilde[i] &lt;- samples[i,3] } mean(X_tilde) ## [1] 5.127 Yes, they are unbiased because \\(\\mathbf{E}(\\bar{X})\\approx \\mathbf{E}(\\hat{X}) \\approx \\mathbf{E}(\\tilde{X}) \\approx \\mu_x \\approx 5\\). But this is not enough. Which estimator is better? The answer is the one with the smallest variance. We call it an “efficient” estimator. The reason is that the interval estimation of \\(\\mu_x\\) will be narrower when the sampling distribution has a smaller variance. Let’s see which one has the smallest variance: var(X_bar) ## [1] 8.490149 var(X_hat) ## [1] 13.10739 var(X_tilde) ## [1] 26.31403 The \\(\\bar{X}\\) has the smallest variance among the unbiased estimators, \\(\\bar{X}=\\frac{1}{n} \\sum_{i=1}^{n} x_{i}\\), \\(\\hat{X}=0.5 x_{1}+0.5x_{n}\\), and \\(\\tilde{X}=x_3\\). In practice we have only one sample. We know that if the sample size is big enough (more than 50, for example), the sampling distribution would be normal according to the Central Limit Theorem (CLT). In other words, if the number of observations in each sample large enough, \\(\\bar{X} \\sim N(\\mu_x, \\sigma^{2}/n)\\) or when population variance is not known \\(\\bar{X} \\sim \\mathcal{T}\\left(\\mu, S^{2}\\right)\\) where \\(S\\) is the standard deviation of the sample and \\(\\mathcal{T}\\) is the Student’s \\(t\\)-distribution. Why is this important? Because it works like a magic: with only one sample, we can generalize the results for the population. We will not cover the details of interval estimation here, but by knowing \\(\\bar{X}\\) and the sample variance \\(S\\), we can have the following interval for the \\(\\mu_{x}\\): \\[ \\left(\\bar{x}-t^{*} \\frac{s}{\\sqrt{n}}, \\bar{x}+t^{*} \\frac{s}{\\sqrt{n}}\\right) \\] where \\(t^*\\), the critical values in \\(t\\)-distribution, are usually around 1.96 for samples more than 100 observations and for the 95% confidence level. This interval would be completely wrong or misleading if \\(\\mathbf{E}(\\bar{X}) \\neq \\mu_x\\) and would be useless if it is very wide, which is caused by a large variance. That’s the reason why we don’t like large variances. Let’s summarize the important steps in estimations: The main task is to estimate the population parameter from a sample. The requirement for a (linear) estimator is unbiasedness. An unbiased estimator is called as the Best Linear Unbiased Estimator (BLUE) of a population parameter if it has the minimum variance among all other unbiased estimators. These steps are also observed when we use Mean Squared Error (MSE) to evaluate each estimator’s performance. The MSE of an estimator \\(\\hat{\\theta}\\) with respect to an unknown population parameter \\(\\theta\\) is defined as \\[ \\mathbf{MSE}(\\hat{\\theta})=\\mathbf{E}_{\\hat{\\theta}}\\left[(\\hat{\\theta}-\\theta)^{2}\\right]=\\mathbf{E}_{\\hat{\\theta}}\\left[(\\hat{\\theta}-\\mathbf{E}(\\hat{\\theta}))^{2}\\right] \\] Since we choose only unbiased estimators, \\(\\mathbf{E}(\\hat{\\theta})=\\theta\\), this expression becomes \\(\\mathbf{Var}(\\hat{\\theta})\\). Hence, evaluating the performance of all alternative unbiased estimators by MSE is actually comparing their variances and picking up the smallest one. More specifically, \\[\\begin{equation} \\mathbf{MSE}\\left(\\hat{\\theta}\\right)=\\mathbf{E}\\left[\\left(\\hat{\\theta}-\\theta\\right)^{2}\\right]=\\mathbf{E}\\left\\{\\left(\\hat{\\theta}-\\mathbf{E}\\left(\\hat{\\theta}\\right)+\\mathbf{E}\\left(\\hat{\\theta}\\right)-\\theta\\right)^{2}\\right\\} \\tag{3.1} \\end{equation}\\] \\[ =\\mathbf{E}\\left\\{\\left(\\left[\\hat{\\theta}-\\mathbf{E}\\left(\\hat{\\theta}\\right)\\right]+\\left[\\mathbf{E}\\left(\\hat{\\theta}\\right)-\\theta\\right]\\right)^{2}\\right\\} \\] \\[\\begin{equation} \\begin{aligned} \\mathbf{MSE}(\\hat{\\theta})=&amp; \\mathbf{E}\\left\\{\\left[\\hat{\\theta}-\\mathbf{E}\\left(\\hat{\\theta}\\right)\\right]^{2}\\right\\}+\\mathbf{E}\\left\\{\\left[\\mathbf{E}\\left(\\hat{\\theta}\\right)-\\theta\\right]^{2}\\right\\} \\\\ &amp;+2 \\mathbf{E}\\left\\{\\left[\\hat{\\theta}-\\mathbf{E}\\left(\\hat{\\theta}\\right)\\right]\\left[\\mathbf{E}\\left(\\hat{\\theta}\\right)-\\theta\\right]\\right\\} \\end{aligned} \\tag{3.2} \\end{equation}\\] The first term in 3.2 is the variance. The second term is outside of expectation, as \\([\\mathbf{E}(\\hat{\\theta})-\\theta]\\) is not random, which represents the bias. The last term is zero. This is because \\([\\mathbf{E}(\\hat{\\theta})-\\theta]\\) is not random, therefore it is again outside of expectations: \\[ 2\\left[\\mathbf{E}\\left(\\hat{\\theta}\\right)-\\theta\\right] \\mathbf{E}\\left\\{\\left[\\hat{\\theta}-\\mathbf{E}\\left(\\hat{\\theta}\\right)\\right]\\right\\}, \\] and the last term is zero since \\(\\mathbf{E}(\\hat{\\theta})-\\mathbf{E}(\\hat{\\theta}) = 0\\). Hence, \\[ \\mathbf{MSE}\\left(\\hat{\\theta}\\right)=\\mathbf{Var}\\left(\\hat{\\theta}\\right)+\\left[\\mathbf{bias}\\left(\\hat{\\theta}\\right)\\right]^{2} \\] Because we choose only unbiased estimators, \\(\\mathbf{E}(\\hat{\\theta})=\\theta\\), this expression becomes \\(\\mathbf{Var}(\\hat{\\theta})\\). In our case, the estimator can be \\(\\hat{\\theta}=\\bar{X}\\) and what we try to estimate is \\(\\theta = \\mu_x\\). 3.2 Prediction - MSPE Let’s follow the same example. Our task is now different. We want to predict the unobserved value of \\(X\\) rather than to estimate \\(\\mu_x\\). Therefore, we need a predictor, not an estimator. What makes a good predictor? Is unbiasedness one of them? If we use a biased estimator such as \\[ X^*=\\frac{1}{n-4} \\sum_{i=1}^{n} x_{i} \\] to predict \\(x_0\\), would being a biased estimator make it automatically a bad predictor? To answer these questions, we need to look at MSE again. Since our task is prediction, we (usually) change its name to mean square prediction error (MSPE). \\[\\begin{equation} \\mathbf{MSPE}=\\mathbf{E}\\left[(x_0-\\hat{f})^{2}\\right]=\\mathbf{E}\\left[(f+\\varepsilon_0-\\hat{f})^{2}\\right] \\tag{3.3} \\end{equation}\\] Similar to the best estimator, a predictor with the smallest MSPE will be our choice among other alternative predictors. Let’s summarize some important facts about our MSPE here: \\(x_0\\) is the number we want to predict and \\(\\hat{f}\\) is the predictor, which could be \\(\\mathbf{E}(\\bar{X})\\), \\(\\mathbf{E}(\\hat{X})\\), or \\(\\mathbf{E}(\\tilde{X})\\) or any other predictor. \\(x_0 = \\mu_x + \\varepsilon_0\\), where \\(f = \\mu_x\\). Hence, \\(\\mathbf{E}[x_0]=f\\) so that \\(\\mathbf{E}[\\varepsilon_0]=0\\). \\(\\mathbf{E}[f]=f\\). In other words, the expected value of a constant is a constant: \\(\\mathbf{E}[\\mu_x]=\\mu_x\\). \\(\\mathbf{Var}[x_0]=\\mathbf{E}\\left[(x_0-\\mathbf{E}[x_0])^{2}\\right]=\\mathbf{E}\\left[(x_0-f)^{2}\\right]=\\mathbf{E}\\left[(f+\\varepsilon_0-f)^{2}\\right]=\\mathbf{E}\\left[\\varepsilon_0^{2}\\right]=\\mathbf{Var}[\\varepsilon_0]=\\sigma^{2}.\\) Note that we can use MSPE here because our example is not a classification problem. When we have a binary outcome to predict, the loss function would have a different structure. We will see the performance evaluation in classification problems later. Before running a simulation, let’s look at MSPE closer. We will drop the subscript \\(0\\) to keep the notation simple. With a trick, adding and subtracting \\(\\mathbf{E}(\\hat{f})\\), MSPE becomes \\[ \\mathbf{MSPE}=\\mathbf{E}\\left[(x-\\hat{f})^{2}\\right]=\\mathbf{E}\\left[(f+\\varepsilon-\\hat{f})^{2}\\right]=\\mathbf{E}\\left[(f+\\varepsilon-\\hat{f}+\\mathbf{E}[\\hat{f}]-\\mathbf{E}[\\hat{f}])^{2}\\right] \\] \\[ =\\mathbf{E}\\left[(f-\\mathbf{E}[\\hat{f}])^{2}\\right]+\\mathbf{E}\\left[\\varepsilon^{2}\\right]+\\mathbf{E}\\left[(\\mathbf{E}[\\hat{f}]-\\hat{f})^{2}\\right]+2 \\mathbf{E}[(f-\\mathbf{E}[\\hat{f}]) \\varepsilon]+2 \\mathbf{E}[\\varepsilon(\\mathbf{E}[\\hat{f}]-\\hat{f})]+\\\\2 \\mathbf{E}[(\\mathbf{E}[\\hat{f}]-\\hat{f})(f-\\mathbf{E}[\\hat{f}])], \\] which can be simplified with the following few steps: The first term, \\(\\mathbf{E}\\left[(f-\\mathbf{E}[\\hat{f}])^{2}\\right]\\), is \\((f-\\mathbf{E}[\\hat{f}])^{2}\\), because \\((f-\\mathbf{E}[\\hat{f}])^{2}\\) is a constant. Similarly, the same term, \\((f-\\mathbf{E}[\\hat{f}])^{2}\\) is in the \\(4^{th}\\) term. Hence, \\(2 \\mathbf{E}[(f-\\mathbf{E}[\\hat{f}]) \\varepsilon]\\) can be written as \\(2(f-\\mathbf{E}[\\hat{f}]) \\mathbf{E}[\\varepsilon]\\). Finally, the \\(5^{th}\\) term, \\(2 \\mathbf{E}[\\varepsilon(\\mathbf{E}[\\hat{f}]-\\hat{f})]\\), can be written as \\(2 \\mathbf{E}[\\varepsilon] \\mathbf{E}[\\mathbf{E}[\\hat{f}]-\\hat{f}]\\). (Note that \\(\\varepsilon\\) and \\(\\hat{f}\\) are independent) As a result we have: \\[ \\mathbf{MSPE}=(f-\\mathbf{E}[\\hat{f}])^{2}+\\mathbf{E}\\left[\\varepsilon^{2}\\right]+\\mathbf{E}\\left[(\\mathbf{E}[\\hat{f}]-\\hat{f})^{2}\\right]+2(f-\\mathbf{E}[\\hat{f}]) \\mathbf{E}[\\varepsilon]+2 \\mathbf{E}[\\varepsilon] \\mathbf{E}[\\mathbf{E}[\\hat{f}]-\\hat{f}]+\\\\2 \\mathbf{E}[\\mathbf{E}[\\hat{f}]-\\hat{f}](f-\\mathbf{E}[\\hat{f}]) \\] The \\(4^{th}\\) and the \\(5^{th}\\) terms are zero because \\(\\mathbf{E}[\\varepsilon]=0\\). The last term is also zero because \\(\\mathbf{E}[\\mathbf{E}[\\hat{f}]-\\hat{f}]\\) is \\(\\mathbf{E}[\\hat{f}]-\\mathbf{E}[\\hat{f}]\\). Hence, we have: \\[ \\mathbf{MSPE}=(f-\\mathbf{E}[\\hat{f}])^{2}+\\mathbf{E}\\left[\\varepsilon^{2}\\right]+\\mathbf{E}\\left[(\\mathbf{E}[\\hat{f}]-\\hat{f})^{2}\\right] \\] Let’s look at the second term first. It’s called as “irreducible error” because it comes with the data. Thus, we can write: \\[\\begin{equation} \\mathbf{MSPE}=(\\mu_x-\\mathbf{E}[\\hat{f}])^{2}+\\mathbf{E}\\left[(\\mathbf{E}[\\hat{f}]-\\hat{f})^{2}\\right]+\\mathbf{Var}\\left[x\\right] \\tag{3.4} \\end{equation}\\] The first term of 3.4 is the bias squared. It would be zero for an unbiased estimator, that is, if \\(\\mathbf{E}[\\hat{f}]=\\mu_x.\\) The second term is the variance of the estimator. For example, if the predictor is \\(\\bar{X}\\) it would be \\(\\mathbf{E}\\left[(\\bar{X} -\\mathbf{E}[\\bar{X}])^{2}\\right]\\). Hence, the variance comes from the sampling distribution. \\[ \\mathbf{MSPE}=\\mathbf{Bias}[\\hat{f}]^{2}+\\mathbf{Var}[\\hat{f}]+\\sigma^{2} \\] These two terms together, the bias-squared and the variance of \\(\\hat{f}\\), is called reducible error. Hence, the MSPE can be written as \\[ \\mathbf{MSPE}=\\mathbf{Reducible~Error}+\\mathbf{Irreducible~Error} \\] Now, our job is to pick a predictor that will have the minimum MSPE among alternatives. Obviously, we can pick \\(\\bar{X}\\) because it has a zero-bias. But now, unlike an estimator, we can accept some bias as long as the MSPE is lower. More specifically, we can allow a predictor to have a bias if it reduces the variance more than the bias itself. In predictions, we can have a reduction in MSPE by allowing a trade-off between variance and bias. How can we achieve it? For example, our predictor would be a constant, say 4, which, although it’s a biased estimator, has a zero variance. However, the MSPE would probably increase because the bias would be much larger than the reduction in the variance. Trade-off Although conceptually the variance-bias trade-off seems intuitive, at least mathematically, we need to ask another practical question: how can we calculate MSPE? How can we see the segments of MSPE in a simulations? We will use the same example. We have a population with three numbers: 0, 3, and 12. We sample from this “population” multiple times. Now the task is to use each sample and come up with a predictor (a prediction rule) to predict a number or multiple numbers drawn from the same population. # The same example here again populationX &lt;- c(0, 3, 12) # A container to have 2000 samples Ms &lt;- 2000 samples &lt;- matrix(0, Ms, 3) colnames(samples) &lt;- c(&quot;FirstX&quot;, &quot;SecondX&quot;, &quot;ThirdX&quot;) # All samples (with replacement always) set.seed(123) for (i in 1:nrow(samples)) { samples[i, ] &lt;- sample(populationX, 3, replace = TRUE) } head(samples) ## FirstX SecondX ThirdX ## [1,] 12 12 12 ## [2,] 3 12 3 ## [3,] 3 3 12 ## [4,] 0 3 3 ## [5,] 0 3 12 ## [6,] 0 12 12 Now suppose that we come up with two predictors: \\(\\hat{f}_1 = 9\\) and \\(\\hat{f}_2 = \\bar{X}\\): # Container to record all predictions predictions &lt;- matrix(0, Ms, 2) # fhat_1 = 9 for (i in 1:Ms) { predictions[i,1] &lt;- 9 } # fhat_2 - mean for (i in 1:Ms) { predictions[i,2] &lt;- sum(samples[i,])/length(samples[i,]) } head(predictions) ## [,1] [,2] ## [1,] 9 12 ## [2,] 9 6 ## [3,] 9 6 ## [4,] 9 2 ## [5,] 9 5 ## [6,] 9 8 Now let’s have our MSPE decomposition: # MSPE MSPE &lt;- matrix(0, Ms, 2) for (i in 1:Ms) { MSPE[i,1] &lt;- mean((populationX-predictions[i,1])^2) MSPE[i,2] &lt;- mean((populationX-predictions[i,2])^2) } head(MSPE) ## [,1] [,2] ## [1,] 42 75 ## [2,] 42 27 ## [3,] 42 27 ## [4,] 42 35 ## [5,] 42 26 ## [6,] 42 35 # Bias bias1 &lt;- mean(populationX)-mean(predictions[,1]) bias2 &lt;- mean(populationX)-mean(predictions[,2]) # Variance (predictor) var1 &lt;- var(predictions[,1]) var2 &lt;- var(predictions[,2]) # Variance (epsilon) var_eps &lt;- mean((populationX-mean(populationX))^2) Let’s put them in a table: VBtradeoff &lt;- matrix(0, 2, 4) rownames(VBtradeoff) &lt;- c(&quot;fhat_1&quot;, &quot;fhat_2&quot;) colnames(VBtradeoff) &lt;- c(&quot;Bias&quot;, &quot;Var(fhat)&quot;, &quot;Var(eps)&quot;, &quot;MSPE&quot;) VBtradeoff[1,1] &lt;- bias1^2 VBtradeoff[2,1] &lt;- bias2^2 VBtradeoff[1,2] &lt;- var1 VBtradeoff[2,2] &lt;- var2 VBtradeoff[1,3] &lt;- var_eps VBtradeoff[2,3] &lt;- var_eps VBtradeoff[1,4] &lt;- mean(MSPE[,1]) VBtradeoff[2,4] &lt;- mean(MSPE[,2]) round(VBtradeoff, 3) ## Bias Var(fhat) Var(eps) MSPE ## fhat_1 16.000 0.00 26 42.00 ## fhat_2 0.004 8.49 26 34.49 This table shows the decomposition of MSPE. The first column is the contribution to the MSPE from the bias; the second column is the contribution from the variance of the predictor. These together make up the reducible error. The third column is the variance that comes from the data, the irreducible error. The last column is, of course, the total MSPE, and we can see that \\(\\hat{f}_2\\) is the better predictor because of its lower MSPE. This decomposition would be checked with colMeans(MSPE) ## [1] 42.00 34.49 3.3 Biased estimator as a predictor We saw earlier that \\(\\bar{X}\\) is a better estimator. But if we have some bias in our predictor, can we reduce MSPE? Let’s define a biased estimator of \\(\\mu_x\\): \\[ \\hat{X}_{biased} = \\hat{\\mu}_x=\\alpha \\bar{X} \\] The sample mean \\(\\bar{X}\\) is an unbiased estimator of \\(\\mu_x\\). The magnitude of the bias is \\(\\alpha\\) and as it goes to 1, the bias becomes zero. As before, we are given one sample with three observations from the same distribution (population). We want to guess the value of a new data point from the same distribution. We will make the prediction with the best predictor which has the minimum MSPE. By using the same decomposition we can show that: \\[ \\hat{\\mu}_x=\\alpha \\bar{X} \\] \\[ \\mathbf{E}[\\hat{\\mu}_x]=\\alpha \\mu_x \\] \\[ \\mathbf{MSPE}=[(1-\\alpha) \\mu_x]^{2}+\\frac{1}{n} \\alpha^{2} \\sigma_{\\varepsilon}^{2}+\\sigma_{\\varepsilon}^{2} \\] Our first observation is that when \\(\\alpha\\) is one, the bias will be zero. Since it seems that MSPE is a convex function of \\(\\alpha\\), we can search for \\(\\alpha\\) that minimizes MSPE. The first-order-condition would give us the solution: \\[ \\frac{\\partial \\mathbf{MSPE}}{\\partial \\alpha} =0 \\rightarrow ~~ \\alpha = \\frac{\\mu^2_x}{\\mu^2_x+\\sigma^2_\\varepsilon/n}&lt;1 \\] Let’s see if this level of bias would improve MSPE that we found earlier: pred &lt;- c() # The magnitude of bias alpha &lt;- (mean(populationX))^2/((mean(populationX)^2 + var_eps/3)) alpha ## [1] 0.7425743 # Biased predictor for (i in 1:Ms) { pred[i] &lt;- alpha*predictions[i, 2] } # Check if E(alpha*Xbar) = alpha*mu_x mean(pred) ## [1] 3.760396 alpha*mean(populationX) ## [1] 3.712871 # MSPE MSPE_biased &lt;- c() for (i in 1:Ms) { MSPE_biased[i] &lt;- mean((populationX-pred[i])^2) } mean(MSPE_biased) ## [1] 32.21589 Let’s add this predictor into our table: VBtradeoff &lt;- matrix(0, 3, 4) rownames(VBtradeoff) &lt;- c(&quot;fhat_1&quot;, &quot;fhat_2&quot;, &quot;fhat_3&quot;) colnames(VBtradeoff) &lt;- c(&quot;Bias&quot;, &quot;Var(fhat)&quot;, &quot;Var(eps)&quot;, &quot;MSPE&quot;) VBtradeoff[1,1] &lt;- bias1^2 VBtradeoff[2,1] &lt;- bias2^2 VBtradeoff[3,1] &lt;- (mean(populationX)-mean(pred))^2 VBtradeoff[1,2] &lt;- var1 VBtradeoff[2,2] &lt;- var2 VBtradeoff[3,2] &lt;- var(pred) VBtradeoff[1,3] &lt;- var_eps VBtradeoff[2,3] &lt;- var_eps VBtradeoff[3,3] &lt;- var_eps VBtradeoff[1,4] &lt;- mean(MSPE[,1]) VBtradeoff[2,4] &lt;- mean(MSPE[,2]) VBtradeoff[3,4] &lt;- mean(MSPE_biased) round(VBtradeoff, 3) ## Bias Var(fhat) Var(eps) MSPE ## fhat_1 16.000 0.000 26 42.000 ## fhat_2 0.004 8.490 26 34.490 ## fhat_3 1.537 4.682 26 32.216 When we allow some bias in our predictor the variance drops from 8.490 to 4.682. Since the decrease in variance is bigger than the increase in bias, the MSPE goes down. This example shows the difference between estimation and prediction for a simplest predictor, the mean of \\(X.\\) We will see a more complex example when we have a regression later. Before moving on to the next section, let’s ask a question: what if we use in-sample data points to calculate MSPE. In our simple case, suppose you have the \\(3^{rd}\\) sample, {3, 3, 12}. Would you still choose \\(\\bar{X}\\) as your predictor? Suppose you calculate MSPE by in-sample data points using the \\(3^{rd}\\) sample. Would \\(\\hat{f}(x) = \\bar{X}\\) be still your choice of predictor? If we use it, the MSPE would be 18, which is not bad and may be much lower than that of some arbitrary number, say 9, as a predictor. In search of a better predictor, however, \\(\\hat{f}(x) = {x_i}\\), will give us a lower MSPE, which will be zero. In other words, it interpolates the data. This is called the overfitting problem because the predictor could have the worst MSPE if it’s tested on out-sample data points. 3.4 Dropping a variable in a regression We can assume that the outcome \\(y_i\\) is determined by the following function: \\[ y_{i}=\\beta_0+\\beta_1 x_{i}+\\varepsilon_{i}, ~~~~ i=1, \\ldots, n \\] where \\(\\varepsilon_{i} \\sim N\\left(0, \\sigma^{2}\\right)\\), \\(\\mathrm{Cov}\\left(\\varepsilon_{i}, \\varepsilon_{j}\\right)=0\\) for \\(i\\neq j.\\) Although unrealistic, for now we assume that \\(x_i\\) is fixed (non-stochastic) for simplicity in notations. That means in each sample we have same \\(x_i\\). We can write this function as \\[ y_{i}=f(x_i)+\\varepsilon_{i}, ~~~~ i=1, \\ldots, n \\] As usual, \\(f(x_i)\\) is the deterministic part (DGM) and \\(\\varepsilon_i\\) is the random part in the function that together determine the value of \\(y_i\\). Again, we are living in two universes: the population and a sample. Since none of the elements in population is known to us, we can only assume what \\(f(x)\\) would be. Based on a sample and the assumption about DGM, we choose an estimator of \\(f(x)\\), \\[ \\hat{f}(x) = \\hat{\\beta}_0+\\hat{\\beta}_1 x_{i}, \\] which is BLUE of \\(f(x)\\), when it is estimated with OLS given the assumptions about \\(\\varepsilon_i\\) stated above. Since the task of this estimation is to satisfy the unbiasedness condition, i.e. \\(\\mathrm{E}[\\hat{f}(x)]=f(x)\\), it can be achieved only if \\(\\mathrm{E}[\\hat{\\beta_0}]=\\beta_0\\) and \\(\\mathrm{E}[\\hat{\\beta_1}]=\\beta_1\\). At the end of this process, we can understand the effect of \\(x\\) on \\(y\\), signified by the unbiased slope coefficient \\(\\hat{\\beta_1}\\). This is not as an easy job as it sounds in this simple example. Finding an unbiased estimator of \\(\\beta\\) is the main challenge in the field of econometrics. In prediction, on the other hand, our main task is not to find unbiased estimator of \\(f(x)\\). We just want to predict \\(y_0\\) given \\(x_0\\). The subscript \\(0\\) tells us that we want to predict \\(y\\) for a specific value of \\(x\\). Hence we can write it as, \\[ y_{0}=\\beta_0+\\beta_1 x_{0}+\\varepsilon_{0}, \\] In other words, when \\(x_0=5\\), for example, \\(y_0\\) will be determined by \\(f(x_0)\\) and the random error, \\(\\varepsilon_0\\), which has the same variance, \\(\\sigma^2\\), as \\(\\varepsilon_i\\). Hence, when \\(x_0=5\\), although \\(f(x_0)\\) is fixed, \\(y_0\\) will vary because of its random part, \\(\\varepsilon_0\\). This in an irreducible uncertainty in predicting \\(y_0\\) given \\(f(x_0)\\). We do not know about the population. Therefore, we do not know what \\(f(x_0)\\) is. We can have a sample from the population and build a model \\(\\hat{f}(x)\\) so that \\(\\hat{f}(x_0)\\) would be as close to \\(f(x_0)\\) as possible. But this introduces another layer of uncertainty in predicting \\(y_0\\). Since each sample is random and different, \\(\\hat{f}(x_0)\\) will be a function of the sample: \\(\\hat{f}(x_0, S_m)\\). Of course, we will have one sample in practice. However, if this variation is high, it would be highly likely that our predictions, \\(\\hat{f}(x_0, S_m)\\), would be far off from \\(f(x_0)\\). We can use an unbiased estimator for prediction, but as we have seen before, we may be able to improve MSPE if we allow some bias in \\(\\hat{f}(x)\\). To see this potential trade-off, we look at the decomposition of MSPE with a simplified notation: \\[ \\mathbf{MSPE}=\\mathrm{E}\\left[(y_0-\\hat{f})^{2}\\right]=\\mathrm{E}\\left[(f+\\varepsilon-\\hat{f})^{2}\\right] \\] \\[ \\mathbf{MSPE}=\\mathrm{E}\\left[(f+\\varepsilon-\\hat{f}+\\mathrm{E}[\\hat{f}]-\\mathrm{E}[\\hat{f}])^{2}\\right] \\] We have seen this before. Since we calculate MSPE for \\(x_i = x_0\\), we call it the conditional MSPE, which can be expressed as \\(\\mathbf{MSPE}=\\mathbf{E}\\left[(y_0-\\hat{f})^{2}|x=x_0\\right]\\). We will see unconditional MSPE, which is the average of all possible data points later in last two sections. The simplification will follow the same steps, and we will have: \\[ \\mathbf{MSPE}=(f-\\mathrm{E}[\\hat{f}])^{2}+\\mathrm{E}\\left[(\\mathrm{E}[\\hat{f}]-\\hat{f})^{2}\\right]+\\mathrm{E}\\left[\\varepsilon^{2}\\right] \\] Let’s look at the first term first: \\[ \\left(f-\\mathrm{E}[\\hat{f}]\\right)^{2}=\\left(\\beta_0+\\beta_1 x_{0}-\\mathrm{E}[\\hat{\\beta}_0]-x_{0}\\mathrm{E}[\\hat{\\beta}_1]\\right)^2=\\left((\\beta_0-\\mathrm{E}[\\hat{\\beta}_0])+x_{0}(\\beta_1-\\mathrm{E}[\\hat{\\beta}_1])\\right)^2. \\] Hence, it shows the bias (squared) in parameters. The second term is the variance of \\(\\hat{f}(x)\\): \\[ \\mathrm{E}\\left[(\\mathrm{E}[\\hat{f}]-\\hat{f})^{2}\\right]=\\mathrm{Var}[\\hat{f}(x)]=\\mathrm{Var}[\\hat{\\beta}_0+\\hat{\\beta}_1 x_{0}]=\\mathrm{Var}[\\hat{\\beta}_0]+x_{0}^2\\mathrm{Var}[\\hat{\\beta}_1]+2x_{0}\\mathrm{Cov}[\\hat{\\beta}_0,\\hat{\\beta}_1] \\] As expected, the model’s variance is the sum of the variances of estimators and their covariance. Again, the variance can be thought of variation of \\(\\hat{f}(x)\\) from sample to sample. With the irreducible prediction error \\(\\mathrm{E}[\\varepsilon^{2}]=\\sigma^2\\), \\[ \\mathbf{MSPE}=(\\mathbf{bias})^{2}+\\mathbf{Var}(\\hat{f})+\\sigma^2. \\] Suppose that our OLS estimators are unbiased and that \\(\\mathrm{Cov}[\\hat{\\beta}_0,\\hat{\\beta}_1]=0\\). In that case, \\[ \\mathbf{MSPE}_{OLS} =\\mathrm{Var}(\\hat{\\beta}_{0})+x_{0}^2\\mathrm{Var}(\\hat{\\beta}_{1})+\\sigma^2 \\] Before going further, let’s summarize the meaning of this measure. The mean squared prediction error of unbiased \\(\\hat{f}(x_0)\\), or how much \\(\\hat{f}(x_0)\\) deviates from \\(y_0\\) is defined by two factors: First, \\(y_0\\) itself varies around \\(f(x_0)\\) by \\(\\sigma^2\\). This is irreducible. Second, \\(\\hat{f}(x_0)\\) varies from sample to sample. The model’s variance is the sum of variations in estimated coefficients from sample to sample, which can be reducible. Suppose that \\(\\hat{\\beta}_{1}\\) has a large variance. Hence, we can ask what would happen if we dropped the variable: \\[ \\mathbf{MSPE}_{Biased~OLS} = \\mathbf{Bias}^2+\\mathbf{Var}(\\hat{\\beta}_{0})+\\sigma^2 \\] When we take the difference: \\[ \\mathbf{MSPE}_{OLS} -\\mathbf{MSPE}_{Biased~OLS} =x_{0}^2\\mathbf{Var}(\\hat{\\beta}_{1}) - \\mathbf{Bias}^2 \\] This expression shows that dropping a variable would decrease the expected prediction error if: \\[ x_{0}^2\\mathbf{Var}(\\hat{\\beta}_{1}) &gt; \\mathbf{Bias}^2 ~~\\Rightarrow~~ \\mathbf{MSPE}_{Biased~OLS} &lt; \\mathbf{MSPE}_{OLS} \\] This option, omitting a variable, is unthinkable if our task is to obtain an unbiased estimator of \\({f}(x)\\), but improves the prediction accuracy if the condition above is satisfied. Let’s expand this example into a two-variable case: \\[ y_{i}=\\beta_0+\\beta_1 x_{1i}+\\beta_2 x_{2i}+\\varepsilon_{i}, ~~~~ i=1, \\ldots, n. \\] Thus, the bias term becomes \\[ \\left(f-\\mathrm{E}[\\hat{f}]\\right)^{2}=\\left((\\beta_0-\\mathrm{E}[\\hat{\\beta}_0])+x_{10}(\\beta_1-\\mathrm{E}[\\hat{\\beta}_1])+x_{20}(\\beta_2-\\mathrm{E}[\\hat{\\beta}_2])\\right)^2. \\] And let’s assume that \\(\\mathrm{Cov}[\\hat{\\beta}_0,\\hat{\\beta}_1]=\\mathrm{Cov}[\\hat{\\beta}_0,\\hat{\\beta}_2]=0\\), but \\(\\mathrm{Cov}[\\hat{\\beta}_1,\\hat{\\beta}_2] \\neq 0\\). Hence, the variance of \\(\\hat{f}(x)\\): \\[ \\mathrm{Var}[\\hat{f}(x)]=\\mathrm{Var}[\\hat{\\beta}_0+\\hat{\\beta}_1 x_{10}+\\hat{\\beta}_2 x_{20}]=\\mathrm{Var}[\\hat{\\beta}_0]+x_{10}^2\\mathrm{Var}[\\hat{\\beta}_1]+x_{20}^2\\mathrm{Var}[\\hat{\\beta}_2]+\\\\2x_{10}x_{20}\\mathrm{Cov}[\\hat{\\beta}_1,\\hat{\\beta}_2]. \\] This two-variable example shows that as the number of variables rises, the covariance between variables inflates the model’s variance further. This fact captured by Variance Inflation Factor (VIF) in econometrics is a key point in high-dimensional models for two reasons: First, dropping a variable highly correlated with other variables would reduce the model’s variance substantially. Second, a highly correlated variable also has limited new information among other variables. Hence dropping a highly correlated variable (with a high variance) would have a less significant effect on the prediction accuracy while reducing the model’s variance substantially. Suppose that we want to predict \\(y_0\\) for \\(\\left[x_{10},~ x_{20}\\right]\\) and \\(\\mathrm{Var}[\\hat{\\beta}_2] \\approx 10~\\text{x}~\\mathrm{Var}[\\hat{\\beta}_1]\\). Hence, we consider dropping \\(x_2\\). To evaluate the effect of this decision on MSPE, we take the difference between two MSPE’s: \\[ \\mathbf{MSPE}_{OLS} -\\mathbf{MSPE}_{Biased~OLS} =x_{20}^2\\mathrm{Var}(\\hat{\\beta}_{2}) + 2x_{10}x_{20}\\mathrm{Cov}[\\hat{\\beta}_1,\\hat{\\beta}_2] - \\mathbf{Bias}^2 \\] Thus, dropping \\(x_2\\) would decrease the prediction error if \\[ x_{20}^2\\mathrm{Var}(\\hat{\\beta}_{2}) + 2x_{10}x_{20}\\mathrm{Cov}[\\hat{\\beta}_1,\\hat{\\beta}_2]&gt; \\mathbf{Bias}^2 ~~\\Rightarrow~~ \\mathbf{MSPE}_{Biased~OLS} &lt; \\mathbf{MSPE}_{OLS} \\] We know from Elementary Econometrics that \\(\\mathrm{Var}(\\hat{\\beta}_j)\\) increases by \\(\\sigma^2\\), decreases by the \\(\\mathrm{Var}(x_j)\\), and rises by the correlation between \\(x_j\\) and other \\(x\\)’s. Let’s look at \\(\\mathrm{Var}(\\hat{\\beta}_j)\\) closer: \\[ \\mathrm{Var}\\left(\\hat{\\beta}_{j}\\right)=\\frac{\\sigma^{2}}{\\mathrm{Var}\\left(x_{j}\\right)} \\cdot \\frac{1}{1-R_{j}^{2}}, \\] where \\(R_j^2\\) is \\(R^2\\) in the regression on \\(x_j\\) on the remaining \\((k-2)\\) regressors (\\(x\\)’s). The second term is called the variance-inflating factor (VIF). As usual, a higher variability in a particular \\(x\\) leads to proportionately less variance in the corresponding coefficient estimate. Note that, however, as \\(R_j^2\\) get closer to one, that is, as the correlation between \\(x_j\\) with other regressors approaches to unity, \\(\\mathrm{Var}(\\hat{\\beta}_j)\\) goes to infinity. The variance of \\(\\varepsilon_i\\), \\(\\sigma^2\\), indicates how much \\(y_i\\)’s deviate from the \\(f(x)\\). Since \\(\\sigma^2\\) is typically unknown, we estimate it from the sample as \\[ \\widehat{\\sigma}^{2}=\\frac{1}{(n-k+1)} \\sum_{i=1}^{n}\\left(y_{i}-\\hat{f}(x)\\right)^{2} \\] Remember that we have multiple samples, hence if our estimator is unbiased, we can prove that \\(\\mathbf{E}(\\hat{\\sigma}^2)=\\sigma^2\\). The proof is not important now. However, \\(\\mathrm{Var}(\\hat{\\beta}_j)\\) becomes \\[ \\mathbf{Var}\\left(\\hat{\\beta}_{j}\\right)=\\frac{\\sum_{i=1}^{n}\\left(y_{i}-\\hat{f}(x)\\right)^{2}}{(n-k+1)\\mathrm{Var}\\left(x_{j}\\right)} \\cdot \\frac{1}{1-R_{j}^{2}}, \\] It is clear now that a greater sample size, \\(n\\), results in a proportionately less variance in the coefficient estimates. On the other hand, as the number of regressors, \\(k\\), goes up, the variance goes up. In large \\(n\\) and small \\(k\\), the trade-off by dropping a variable would be insignificant. However, as \\(k/n\\) rises, the trade-off becomes more important. Let’s have a simulation example to conclude this section. Here are the steps for our simulation: There is a random variable, \\(y\\), that we want to predict. \\(y_{i}=f(x_i)+\\varepsilon_{i}\\). DGM is \\(f(x_i)=\\beta_0+\\beta_1 x_{1i}+\\beta_2 x_{2i}\\) \\(\\varepsilon_{i} \\sim N(0, \\sigma^2)\\). The steps above define the population. We will withdraw \\(M\\) number of samples from this population. Using each sample (\\(S_m\\), where \\(m=1, \\ldots, M\\)), we will estimate two models: unbiased \\(\\hat{f}(x)_{OLS}\\) and biased \\(\\hat{f}(x)_{Biased~OLS}\\) Using these models we will predict \\(y&#39;_i\\) from a different sample (\\(T\\)) drawn from the same population. We can call it the “unseen” dataset or the “test” dataset, which contains out-of-sample data points, \\((y&#39;_i, x_{1i}, x_{2i})\\)., where \\(i=1, \\ldots, n\\). Before we start, we need to be clear how we define MSPE in our simulation. Since we will predict every \\(y&#39;_i\\) with corresponding predictors \\((x_{1i}, x_{2i})\\) in test set \\(T\\) by each \\(\\hat{f}(x_{1i}, x_{2i}, S_m))\\) estimated by each sample, we calculate the following unconditional MSPE: \\[ \\mathbf{MSPE}=\\mathbf{E}_{S}\\mathbf{E}_{S_{m}}\\left[(y&#39;_i-\\hat{f}(x_{1i}, x_{2i}, S_m))^{2}\\right]=\\\\\\mathbf{E}_S\\left[\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}^{\\prime}-\\hat{f}(x_{1i}, x_{2i}, S_m)\\right)^{2}\\right],\\\\m=1, \\ldots, M \\] We first calculate MSPE for all data points in the test set using \\(\\hat{f}(x_{1T}, x_{2T}, S_m)\\), and then take the average of \\(M\\) samples. We will show the sensitivity of trade-off by the size of irreducible error. The simulation below plots \\(diff= \\mathbf{MSPE}_{OLS}-\\mathbf{MSPE}_{Biased~OLS}\\) against \\(\\sigma\\). # Function for X - fixed at repeated samples # Argument l is used for correlation and with 0.01 # Correlation between x_1 and x_2 is 0.7494 xfunc &lt;- function(n, l){ set.seed(123) x_1 &lt;- rnorm(n, 0, 25) x_2 &lt;- l*x_1+rnorm(n, 0, 0.2) X &lt;- data.frame(&quot;x_1&quot; = x_1, &quot;x_2&quot; = x_2) return(X) } # Note that we can model dependencies with copulas in R # More specifically by using mvrnorn() function. However, here # We want one variable with a higher variance. which is easier to do manaully # More: https://datascienceplus.com/modelling-dependence-with-copulas/ # Function for test set - with different X&#39;s but same distribution. unseen &lt;- function(n, sigma, l){ set.seed(1) x_11 &lt;- rnorm(n, 0, 25) x_22 &lt;- l*x_11+rnorm(n, 0, 0.2) f &lt;- 0 + 2*x_11 + 2*x_22 y_u &lt;- f + rnorm(n, 0, sigma) un &lt;- data.frame(&quot;y&quot; = y_u, &quot;x_1&quot; = x_11, &quot;x_2&quot; = x_22) return(un) } # Function for simulation (M - number of samples) sim &lt;- function(M, n, sigma, l){ X &lt;- xfunc(n, l) # Repeated X&#39;s in each sample un &lt;- unseen(n, sigma, l) # Out-of sample (y, x_1, x_2) # containers MSPE_ols &lt;- rep(0, M) MSPE_b &lt;- rep(0, M) coeff &lt;- matrix(0, M, 3) coeff_b &lt;- matrix(0, M, 2) yhat &lt;- matrix(0, M, n) yhat_b &lt;- matrix(0, M, n) # loop for samples for (i in 1:M) { f &lt;- 0 + 2*X$x_1 + 2*X$x_2 # DGM y &lt;- f + rnorm(n, 0, sigma) samp &lt;- data.frame(&quot;y&quot; = y, X) ols &lt;- lm(y~., samp) # Unbaised OLS ols_b &lt;- lm(y~x_1, samp) #Biased OLS coeff[i,] &lt;- ols$coefficients coeff_b[i,] &lt;- ols_b$coefficients yhat[i,] &lt;- predict(ols, un) yhat_b[i,] &lt;- predict(ols_b, un) MSPE_ols[i] &lt;- mean((un$y-yhat[i])^2) MSPE_b[i] &lt;- mean((un$y-yhat_b[i])^2) } d = mean(MSPE_ols)-mean(MSPE_b) output &lt;- list(d, MSPE_b, MSPE_ols, coeff, coeff_b, yhat, yhat_b) return(output) } # Sensitivity of (MSPE_biased)-(MSPE_ols) # different sigma for the irreducible error sigma &lt;- seq(1, 20, 1) MSPE_dif &lt;- rep(0, length(sigma)) for (i in 1: length(sigma)) { MSPE_dif[i] &lt;- sim(1000, 100, sigma[i], 0.01)[[1]] } plot(sigma, MSPE_dif, col=&quot;red&quot;, main = &quot;Difference in MSPE vs. sigma&quot;, cex = 0.9, cex.main= 0.8, cex.lab = 0.7, cex.axis = 0.8) The simulation shows that the biased \\(\\hat{f}(x)\\) has a better predcition accuracy as the “noise” in the data gets higher. The reason can be understood if we look at \\(\\mathrm{Var}(\\hat{\\beta}_{2}) + 2\\mathrm{Cov}[\\hat{\\beta}_1,\\hat{\\beta}_2]\\) closer: \\[ \\mathbf{Var}\\left(\\hat{\\beta}_{2}\\right)=\\frac{\\sigma^{2}}{\\mathrm{Var}\\left(x_{2}\\right)} \\cdot \\frac{1}{1-r_{1,2}^{2}}, \\] where \\(r_{1,2}^{2}\\) is the coefficient of correlation between \\(x_1\\) and \\(x_2\\). And, \\[ \\mathbf{Cov}\\left(\\hat{\\beta}_{1},\\hat{\\beta}_{2}\\right)=\\frac{-r_{1,2}^{2}\\sigma^{2}}{\\sqrt{\\mathrm{Var}\\left(x_{1}\\right)\\mathrm{Var}\\left(x_{2}\\right)}} \\cdot \\frac{1}{1-r_{1,2}^{2}}, \\] Hence, \\[ \\begin{aligned} \\mathbf{MSPE}_{O L S}-&amp; \\mathbf{MSPE}_{Biased~OLS}=\\mathrm{Var}\\left(\\hat{\\beta}_{2}\\right)+2 \\mathrm{Cov}\\left[\\hat{\\beta}_{1}, \\hat{\\beta}_{2}\\right]-\\mathbf{Bias}^{2}=\\\\ &amp; \\frac{\\sigma^{2}}{1-r_{1,2}^{2}}\\left(\\frac{1}{\\mathrm{V a r}\\left(x_{2}\\right)}+\\frac{-2 r_{1,2}^{2}}{\\sqrt{\\mathrm{V a r}\\left(x_{1}\\right) \\mathrm{V a r}\\left(x_{2}\\right)}}\\right)-\\mathbf{Bias}^{2} \\end{aligned} \\] Given the bias due to the omitted variable \\(x_2\\), this expression shows the difference as a function of \\(\\sigma^2\\) and \\(r_{1,2}^{2}\\) and explains why the biased-OLS estimator have increasingly better predictions. As a final experiment, let’s have the same simulation that shows the relationship between correlation and trade-off. To create different correlations between \\(x_1\\) and \\(x_2\\), we use the xfunc() we created earlier. The argument \\(l\\) is used to change the the correlation and can be seen below. In our case, when \\(l=0.01\\) \\(r_{1,2}^{2}=0.7494\\). # Function for X for correlation X &lt;- xfunc(100, 0.001) cor(X) ## x_1 x_2 ## x_1 1.00000000 0.06838898 ## x_2 0.06838898 1.00000000 X &lt;- xfunc(100, 0.0011) cor(X) ## x_1 x_2 ## x_1 1.00000000 0.08010547 ## x_2 0.08010547 1.00000000 # We use this in our simulation X &lt;- xfunc(100, 0.01) cor(X) ## x_1 x_2 ## x_1 1.0000000 0.7494025 ## x_2 0.7494025 1.0000000 Now the simulation with different levels of correlation: # Sensitivity of (MSPE_biased)-(MSPE_ols) # different levels of correlation when sigma^2=7 l &lt;- seq(0.001, 0.011, 0.0001) MSPE_dif &lt;- rep(0, length(l)) for (i in 1: length(l)) { MSPE_dif[i] &lt;- sim(1000, 100, 7, l[i])[[1]] } plot(l, MSPE_dif, col=&quot;red&quot;, main= &quot;Difference in MSPE vs Correlation b/w X&#39;s&quot;, cex=0.9, cex.main= 0.8, cex.lab = 0.7, cex.axis = 0.8) As the correlation between \\(x\\)’s goes up, \\(\\mathbf{MSPE}_{OLS}-\\mathbf{MSPE}_{Biased~OLS}\\) rises. Later we will have a high-dimensional dataset (large \\(k\\)) to show the importance of correlation. 3.5 Uncertainty in estimations and predictions When we look back how we built our estimation and prediction simulations, we can see one thing: we had withdrawn 2000 samples and applied our estimators and predictors to each sample. More specifically, we had 2000 estimates from the estimator \\(\\bar{X}\\) and 2000 predictions by \\(\\hat{f}\\). Hence, we have a sampling uncertainty that is captured by the variance of the distribution of estimates, which is also known as the sampling distribution. Sampling distributions are probability distributions that provide the set of possible values for the estimates and will inform us of how appropriately our current estimator is able to explain the population data. And if the estimator is BLUE of \\(\\mu_x\\), the sampling distribution of \\(\\bar{X}\\) can be defined as \\(\\bar{X}\\sim \\mathcal{T}\\left(\\mu, S^{2}\\right)\\) where \\(S\\) is the standard deviation of the sample and \\(\\mathcal{T}\\) is the Student’s \\(t\\)-distribution. This concept is the key point in inferential statistics as it helps us build the interval estimation of the true parameter, \\(\\mu_x\\). The variation of \\(\\bar{X}\\) from sample to sample is important as it makes the interval wider or narrower. Similar to estimation, we make predictions in each sample by the best \\(\\hat{f}\\). Since each sample is a random pick from the population, the prediction would be different from sample to sample. Unlike estimations, however, we allow bias in predictors in exchange with a reduction in variance, which captures the variation of predictions across samples. Although it was easy to calculate the variance of our predictions across samples with simulations, in practice, we have only one sample to calculate our prediction. While we may consider developing a theoretical concept similar to sampling distribution to have an interval prediction, since we allow a variance-bias trade-off in predictions, it would not be as simple as before to develop a confidence interval around our predictions. Although capturing the uncertainty is not simple task and an active research area, we will see in the following chapters how we can create bootstrapping confidence intervals for our predictive models. It is tempting to come to an idea that, when we are able to use an unbiased estimator as a predictor, perhaps due to an insignificant difference between their MSPEs, we may have a more reliable interval prediction, which quantifies the uncertainty in predictions. However, although machine learning predictions are subject to a lack of reliable interval predictions, finding an unbiased estimator specifically in regression-based models is not a simple task either. There are many reasons that the condition of unbiasedness, \\(\\mathrm{E}(\\hat{\\theta})=\\theta\\), may be easily violated. Reverse causality, simultaneity, endogeneity, unobserved heterogeneity, selection bias, model misspecification, measurement errors in covariates are some of the well-known and very common reasons for biased estimations in the empirical world and the major challenges in the field of econometrics today. This section will summarize the forecast error, F, and the prediction interval when we use an unbiased estimator as a predictor. Here is the definition of forecast error, which is the difference between \\(x_0\\) and the predicted \\(\\hat{x}_0\\) in our case: \\[ F=x_0-\\hat{x}_0=\\mu_x+\\varepsilon_0-\\bar{X} \\] If we construct a standard normal variable from \\(F\\): \\[ z= \\frac{F-\\mathrm{E}[F]}{\\sqrt{\\mathrm{Var}(F)}}=\\frac{F}{\\sqrt{\\mathrm{Var}(F)}}=\\frac{x_0-\\hat{x}_0}{\\sqrt{\\mathrm{Var}(F)}}\\sim N(0,1) \\] where \\(\\mathrm{E}[F]=0\\) because \\(\\mathrm{E}[\\bar{X}]=\\mu_x\\) and \\(\\mathrm{E}[\\varepsilon]=0\\). We know that approximately 95% observations of any standard normal variable can be between \\(\\pm{1.96}\\mathbf{sd}\\) Since the standard deviation is 1: \\[ \\mathbf{Pr} = (-1.96 \\leqslant z \\leqslant 1.96) = 0.95. \\] Or, \\[ \\mathbf{Pr} = \\left(-1.96 \\leqslant \\frac{x_0-\\hat{x}_0}{\\mathbf{sd}(F)} \\leqslant 1.96\\right) = 0.95. \\] With a simple algebra this becomes, \\[ \\mathbf{Pr} \\left(\\hat{x}_0-1.96\\mathbf{sd}(F) \\leqslant x_0 \\leqslant \\hat{x}_0+1.96\\mathbf{sd}(F)\\right) = 0.95. \\] This is called a 95% confidence interval or prediction interval for \\(x_0\\). We need to calculate \\(\\mathbf{sd}(F)\\). We have derived it before, but let’s repeat it here again: \\[ \\mathbf{Var}(F) = \\mathrm{Var}\\left(\\mu_x+\\varepsilon_0-\\bar{X}\\right)=\\mathrm{Var}\\left(\\mu_x\\right)+\\mathrm{Var}\\left(\\varepsilon_0\\right)+\\mathrm{Var}\\left(\\bar{X}\\right)\\\\ = \\mathrm{Var}\\left(\\varepsilon_0\\right)+\\mathrm{Var}\\left(\\bar{X}\\right)\\\\=\\sigma^2+\\mathrm{Var}\\left(\\bar{X}\\right) \\] What’s \\(\\mathbf{Var}(\\bar{X})\\)? With the assumption of i.i.d. \\[ \\mathbf{Var}(\\bar{X}) = \\mathrm{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n} x_{i}\\right) =\\frac{1}{n^2} \\sum_{i=1}^{n}\\mathrm{Var}(x_{i})=\\frac{1}{n^2} \\sum_{i=1}^{n}\\sigma^2=\\frac{1}{n^2} n\\sigma^2=\\frac{\\sigma^2}{n}. \\] We do not know \\(\\sigma^2\\) but we can approximate it by \\(\\hat{\\sigma}^2\\), which is the variance of the sample. \\[ \\mathbf{Var}(\\bar{X}) = \\frac{\\hat{\\sigma}^2}{n}~~\\Rightarrow~~ \\mathbf{se}(\\bar{X}) = \\frac{\\hat{\\sigma}}{\\sqrt{n}} \\] Note that the terms, standard deviation and standard error, often lead to confusion about their interchangeability. We use the term standard error for the sampling distribution (standard error of the mean - SEM): the standard error measures how far the sample mean is likely to be from the population mean. Whereas the standard deviation of the sample (population) is the degree to which individuals within the sample (population) differ from the sample (population) mean. Now we can get \\(\\mathbf{sd}(F)\\): \\[ \\mathbf{sd}(F) =\\hat{\\sigma}+\\frac{\\hat{\\sigma}}{\\sqrt{n}}=\\hat{\\sigma}\\left(1+\\frac{1}{\\sqrt{n}}\\right) \\] Therefore, \\(\\mathbf{se}(\\bar{X})\\) changes from sample to sample, as \\(\\hat{\\sigma}\\) will be different in each sample. As we discussed earlier, when we use \\(\\hat{\\sigma}\\) we should use \\(t\\)-distribution, instead of standard normal distribution. Although they have the same critical values for 95% intervals, which is closed to 1.96 when the sample size larger than 100, we usually use critical \\(t\\)-values for the interval estimations. Note that when \\(\\mathrm{E}[\\bar{X}]\\neq\\mu_x\\) the whole process of building a prediction interval collapses at the beginning. Moreover, confidence or prediction intervals require that data must follow a normal distribution. If the sample size is large enough (more than 35, roughly) the central limit theorem makes sure that the sampling distribution would be normal regardless of how the population is distributed. In our example, since our sample sizes 3, the CLT does not hold. Let’s have a more realistic case in which we have a large population and multiple samples with \\(n=100\\). # Better example set.seed(123) popx &lt;- floor(rnorm(10000, 10, 2)) summary(popx) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.000 8.000 9.000 9.489 11.000 17.000 samples &lt;- matrix(0, 1000, 200) set.seed(1) for (i in 1:nrow(samples)) { samples[i,] &lt;- sample(popx, ncol(samples), replace = TRUE) } head(samples[, 1:10]) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 10 10 9 10 8 9 11 8 13 10 ## [2,] 11 13 5 11 6 9 9 9 10 9 ## [3,] 12 12 11 9 7 7 5 10 9 6 ## [4,] 9 8 12 8 10 11 8 8 10 10 ## [5,] 15 9 10 10 10 10 10 11 9 7 ## [6,] 8 9 11 9 10 10 10 13 11 15 hist(rowMeans(samples), breaks = 20, cex.main=0.8, cex.lab = 0.8, main = &quot;Histogram of X_bar&#39;s&quot;, xlab = &quot;X_bar&quot;) summary(rowMeans(samples)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 9.005 9.390 9.485 9.486 9.581 9.940 mean(popx) ## [1] 9.4895 As you can see, the sampling distribution of \\(\\bar{X}\\) is almost normal ranging from 9 to 9.94 with mean 9.486. We can see also that it’s an unbiased estimator of \\(\\mu_x\\). When we use \\(\\bar{X}\\) from a sample to predict \\(x\\), we can quantify the uncertainty in this prediction by building a 95% confidence interval. Let’s use sample 201 to show the interval. # Our sample sample_0 &lt;- samples[201,] mean(sample_0) ## [1] 9.35 # sd(F) sdF &lt;- sqrt(var(sample_0))*(1+1/sqrt(length(sample_0))) upper &lt;- mean(sample_0) + 1.96*sdF lower &lt;- mean(sample_0) - 1.96*sdF c(lower, upper) ## [1] 5.387422 13.312578 The range of this 95% prediction interval quantifies the prediction accuracy when we use 9.35 as a predictor, which implies that the value of a randomly picked \\(x\\) from the same population could be predicted to be between those numbers. When we change the sample, the interval changes due to differences in the mean and the variance of the sample. 3.6 Prediction interval for unbiased OLS predictor We will end this chapter by setting up a confidence interval for predictions made by an unbiased \\(\\hat{f}(x)\\). We follow the same steps as in section 4. Note that the definition of the forecast error, \\[ F=y_0-\\hat{f}(x_0)=f(x_0)+\\varepsilon_0-\\hat{f}(x_0), \\] is the base in MSPE. We will have here a simple textbook example to identify some important elements in prediction interval. Our model is, \\[ y_{i}=\\beta_0+\\beta_1 x_{1i}+\\varepsilon_{i}, ~~~~ i=1, \\ldots, n \\] where \\(\\varepsilon_{i} \\sim N\\left(0, \\sigma^{2}\\right)\\), \\(\\mathrm{Cov}\\left(\\varepsilon_{i}, \\varepsilon_{j}\\right)=0\\) for \\(i\\neq j\\). We can write this function as \\[ y_{i}=f(x_i)+\\varepsilon_{i}, ~~~~ i=1, \\ldots, n \\] Based on a sample and the assumption about DGM, we choose an estimator of \\(f(x)\\), \\[ \\hat{f}(x) = \\hat{\\beta}_0+\\hat{\\beta}_1 x_{1i}, \\] which is BLUE of \\(f(x)\\), when it is estimated with OLS given the assumptions about \\(\\varepsilon_i\\) stated above. Then, the forecast error is \\[ F=y_0-\\hat{f}(x_0)=\\beta_0+\\beta_1 x_{0}+\\varepsilon_{0}-\\hat{\\beta}_0+\\hat{\\beta}_1 x_{0}, \\] Since our \\(\\hat{f}(x)\\) is an unbiased estimator of \\(f(x)\\), \\(\\mathrm{E}(F)=0\\). And, given that \\(\\varepsilon_{0}\\) is independent of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) and \\(\\beta_0\\) as well as \\(\\beta_1 x_{0}\\) are non-stochastic (i.e. they have zero variance), then \\[ \\mathbf{Var}(F)=\\mathrm{Var}\\left(\\varepsilon_{0}\\right)+\\mathrm{Var}\\left(\\hat{\\beta_0}+\\hat{\\beta_1} x_{0}\\right), \\] which is \\[ \\mathbf{Var}(F)=\\sigma^{2}+\\mathrm{Var}(\\hat{\\beta}_0)+x_{0}^{2} \\mathrm{Var}(\\hat{\\beta}_1)+2 x_{0} \\mathrm{Cov}(\\hat{\\beta}_0, \\hat{\\beta}_1). \\] More specifically, \\[ \\mathbf{Var}(F)=\\sigma^{2}+\\sigma^{2}\\left(\\frac{1}{n}+\\frac{\\bar{x}^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)+x_{0}^{2}\\left( \\frac{\\sigma^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right)-2 x_{0}\\left( \\sigma^{2} \\frac{\\bar{x}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right). \\] After simplifying it, we get the textbook expression of the forecast variance: \\[ \\mathbf{Var}(F)=\\sigma^{2}\\left(1+\\frac{1}{n}+\\frac{\\left(x_{0}-\\bar{x}\\right)^{2}}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}}\\right) \\] We have seen it before: as the noise in the data (\\(\\sigma^2\\)) goes up, the variance increases. More importantly, as \\(x_0\\) moves away from \\(\\bar{x}\\), \\(\\mathrm{Var}(F)\\) rises further. Intuitively, rare incidence in data should have more uncertainty in predicting the outcome. The rarity of \\(x_0\\) will be quantified by \\(x_0-\\bar{x}\\) and the uncertainty in prediction is captured by \\(\\mathrm{Var}(F)\\). Finally, using the fact that \\(\\varepsilon\\) is normally distributed, with \\(\\mathrm{E}(F)=0\\), we just found that \\(F \\sim N(0, \\mathrm{Var}(F))\\). Hence, the 95% prediction interval for \\(n&gt;100\\) will approximately be: \\[ \\mathbf{Pr} \\left(\\hat{f}_0-1.96\\mathbf{sd}(F) \\leqslant y_0 \\leqslant \\hat{f}_0+1.96\\mathbf{sd}(F)\\right) = 0.95. \\] When we replace \\(\\sigma^2\\) with \\(\\hat{\\sigma}^2\\), \\(F\\) will have a Student’s \\(t\\) distribution and the critical values (1.96) will be different specially if \\(n&lt;100\\). Since this interval is for \\(x_0\\), we can have a range of \\(x\\) and have a nice plot showing the conficence interval around the point predictions for each \\(x\\). Let’s have a simulation with a simple one-variable regression to see the uncertainty in prediction. We need one sample and one out-sample dataset for prediction. # Getting one-sample. set.seed(123) x_1 &lt;- rnorm(100, 0, 1) f &lt;- 1 - 2 * x_1 # DGM y &lt;- f + rnorm(100, 0, 1) inn &lt;- data.frame(y, x_1) # Getting out-of-sample data points. set.seed(321) x_1 &lt;- rnorm(100, 0, 10) # sd =10 to see the prediction of outlier X&#39;s f &lt;- 1 - 2 * x_1 # DGM y &lt;- f + rnorm(100, 0, 1) out &lt;- data.frame(y, x_1) # OLS ols &lt;- lm(y~., inn) yhat &lt;- predict(ols, out) # Let&#39;s have a Variance(f) function # since variance is not fixed and changes by x_0 v &lt;- function(xzero){ n &lt;- nrow(inn) sigma2_hat &lt;- sum((inn$y - yhat)^2) / (n - 2) #we replace it with sample variance num= (xzero - mean(inn$x_1))^2 denom = sum((inn$x_1 - mean(inn$x_1))^2) var &lt;- sigma2_hat * (1 + 1/n + num/denom) x0 &lt;- xzero outcome &lt;- c(var, x0) return(outcome) } varF &lt;- matrix(0, nrow(out), 2) for (i in 1:nrow(out)) { varF[i, ] &lt;- v(out$x_1[i]) } data &lt;- data.frame(&quot;sd&quot; = c(sqrt(varF[,1])), &quot;x0&quot; = varF[,2], &quot;yhat&quot; = yhat, &quot;upper&quot; = c(yhat + 1.96*sqrt(varF[,1])), &quot;lower&quot; = c(yhat - 1.96*sqrt(varF[,1]))) require(plotrix) plotCI(data$x0, data$yhat , ui=data$upper, li=data$lower, pch=21, pt.bg=par(&quot;bg&quot;), scol = &quot;blue&quot;, col=&quot;red&quot;, main = &quot;Prediction interval for each y_0&quot;, ylab=&quot;yhat(-)(+)1.96sd&quot;, xlab=&quot;x_0&quot;, cex.main = 0.8, cex.lab = 0.8, cex.axis = 0.7) As the \\(x_0\\) moves away from the mean, which is zero in our simulation, the prediction uncertainty captured by the range of confidence intervals becomes larger. "],["overfitting.html", "Chapter 4 Overfitting", " Chapter 4 Overfitting While overfitting seems as a prediction problem, it is also a serious problem in estimations, where the unbiasedness is the main objective. Before going further, we need to see the connection between MSPE and MSE in a regression setting: \\[\\begin{equation} \\mathbf{MSPE}=\\mathrm{E}\\left[(y_0-\\hat{f})^{2}\\right]=(f-\\mathrm{E}[\\hat{f}])^{2}+\\mathrm{E}\\left[(\\mathrm{E}[\\hat{f}]-\\hat{f})^{2}\\right]+\\mathrm{E}\\left[\\varepsilon^{2}\\right] \\tag{4.1} \\end{equation}\\] Equation 4.1 is simply an expected prediction error of predicting \\(y_0\\) using \\(\\hat{f}(x_0)\\). The estimate \\(\\hat{f}\\) is random depending on the sample we use to estimate it. Hence, it varies from sample to sample. We call the sum of the first two terms as “reducible error”, as we have seen before. The MSE of the estimator \\(\\hat{f}\\) is, on the other hand, shows the expected squared error loss of estimating \\(f(x)\\) by using \\(\\hat{f}\\) at a fixed point \\(x\\). \\[ \\mathbf{MSE}(\\hat{f})=\\mathrm{E}\\left[(\\hat{f}-f)^{2}\\right]=\\mathrm{E}\\left\\{\\left(\\hat{f}-\\mathrm{E}(\\hat{f})+\\mathrm{E}(\\hat{f})-f\\right)^{2}\\right\\} \\] \\[ =\\mathrm{E}\\left\\{\\left(\\left[\\hat{f}-\\mathrm{E}\\left(\\hat{f}\\right)\\right]+\\left[\\mathrm{E}\\left(\\hat{f}\\right)-f\\right]\\right)^{2}\\right\\} \\] \\[\\begin{equation} =\\mathrm{E}\\left\\{\\left[\\hat{f}-\\mathrm{E}(\\hat{f})\\right]^{2}\\right \\}+\\mathrm{E}\\left\\{\\left[\\mathrm{E}(\\hat{f})-f\\right]^{2}\\right\\}+2 \\mathrm{E}\\left\\{\\left[\\hat{f}-\\mathrm{E}(\\hat{f})\\right]\\left[\\mathrm{E}(\\hat{f})-f\\right]\\right\\} \\tag{4.2} \\end{equation}\\] The first term is the variance. The second term is outside of expectation, as \\([\\mathbf{E}(\\hat{f})-f]\\) is not random, which represents the bias. The last term is zero. Hence, \\[\\begin{equation} \\mathbf{MSE}(\\hat{f})=\\mathrm{E}\\left\\{\\left[\\hat{f}-\\mathrm{E}(\\hat{f})\\right]^{2}\\right\\}+\\mathrm{E}\\left\\{\\left[\\mathrm{E}(\\hat{f})-f\\right]^{2}\\right\\}=\\mathrm{Var}(\\hat{f})+\\left[\\mathbf{bias}(\\hat{f})\\right]^{2} \\tag{4.3} \\end{equation}\\] We can now see how MSPE is related to MSE. Since the estimator \\(\\hat{f}\\) is used in predicting \\(y_0\\), MSPE should include MSE: \\[ \\mathbf{MSPE}=(f-\\mathrm{E}[\\hat{f}])^{2}+\\mathrm{E}\\left[(\\mathrm{E}[\\hat{f}]-\\hat{f})^{2}\\right]+\\mathrm{E}\\left[\\varepsilon^{2}\\right]=\\mathrm{MSE}(\\hat{f})+\\mathrm{E}\\left[\\varepsilon^{2}\\right] \\] The important difference between estimation and prediction processes is the data points that we use to calculate the mean squared error loss functions. In estimations, our objective is to find the estimator that minimizes the MSE, \\(\\mathrm{E}\\left[(\\hat{f}-f)^{2}\\right]\\). However, since \\(f\\) is not known to us, we use \\(y_i\\) as a proxy for \\(f\\) and calculate MSPE using in-sample data points. Therefore, using an estimator for predictions means that we use in-sample data points to calculate MSPE in predictions, which may result in overfitting and a poor out-of-sample prediction accuracy. Let’s start with an example: # Getting one-sample. set.seed(123) x_1 &lt;- rnorm(100, 0, 1) f &lt;- 1 + 2*x_1 - 2*(x_1^2) + 3*(x_1^3) # DGM y &lt;- f + rnorm(100, 0, 8) inn &lt;- data.frame(y, x_1) # OLS ols1 &lt;- lm(y~ poly(x_1, degree = 1), inn) ols2 &lt;- lm(y~ poly(x_1, degree = 2), inn) ols3 &lt;- lm(y~ poly(x_1, degree = 3), inn) ols4 &lt;- lm(y~ poly(x_1, degree = 20), inn) ror &lt;- order(x_1) plot(x_1, y, col=&quot;darkgrey&quot;) lines(x_1[ror], predict(ols1)[ror], col=&quot;pink&quot;, lwd = 1.5) lines(x_1[ror], predict(ols2)[ror], col=&quot;blue&quot;, lwd = 1.5) lines(x_1[ror], predict(ols3)[ror], col=&quot;green&quot;, lwd = 1.5) lines(x_1[ror], predict(ols4)[ror], col=&quot;red&quot; , lwd = 1.5) legend(&quot;bottomright&quot;, c(&quot;ols1&quot;, &quot;ols2&quot;, &quot;ols3&quot;, &quot;ols4&quot;), col = c(&quot;pink&quot;, &quot;blue&quot;, &quot;green&quot;, &quot;red&quot;), lwd = 2) The “true” estimator, \\(f(x)\\), which is the “green” line, is: \\[ f(x_i)=\\beta_0+\\beta_1 x_{1i}+\\beta_2 x_{1i}^2+\\beta_2 x_{1i}^3 = 1+2x_{1i}-2x_{1i}^2+3 x_{1i}^3. \\] Now we can calculate in-sample empirical MSPE: \\[~\\] # MSE MSPE1 &lt;- mean((predict(ols1)-y)^2) # which is also mean(ols1$residuals^2) MSPE2 &lt;- mean((predict(ols2)-y)^2) MSPE3 &lt;- mean((predict(ols3)-y)^2) MSPE4 &lt;- mean((predict(ols4)-y)^2) all &lt;- c(MSPE1, MSPE2, MSPE3, MSPE4) MSPE &lt;- matrix(all, 4, 1) row.names(MSPE) &lt;- c(&quot;ols1&quot;, &quot;ols2&quot;, &quot;ols3&quot;, &quot;ols4&quot;) colnames(MSPE) &lt;- &quot;In-sample MSPE&#39;s&quot; MSPE ## In-sample MSPE&#39;s ## ols1 86.57600 ## ols2 79.56354 ## ols3 58.40780 ## ols4 48.88589 As we see, the overfitted \\(\\hat{f}(x)\\), the \\(4^{th}\\) model, has a lower empirical in-sample MSPE. If we use nonparametric models, we can even find a better fitting model with a lower empirical in-sample MSPE. We call these MSPE’s empirical because they are not calculated based on repeated samples, which would give an expected value of squared errors over all samples. In practice, however, we have only one sample. Therefore, even if our objective is to find an unbiased estimator of \\({f}(x)\\), not a prediction of \\(y\\), since we choose our estimator, \\(\\hat{f}(x)\\), by the empirical in-sample MSPE, we may end up with an overfitted \\(\\hat{f}(x)\\), such as the \\(4^{th}\\) estimator. Would an overfitted model create a biased estimator? We will see the answer in a simulation later. However, in estimations, our objective is not only to find an unbiased estimator but also to find the one that has the minimum variance. We know that our \\(3^{rd}\\) model is unbiased estimator of \\(f(x)\\) as is the overfitted \\(4^{th}\\) estimator. Which one should we choose? We have answered this question at the beginning of this chapter: the one with the minimum variance. Since overfitting would create a greater variance, our choice must be the \\(3^{rd}\\) model. That is why we do not use the empirical in-sample MSPE as a “cost” or “risk” function in finding the best estimator. This process is called a “data mining” exercise based on one sample without any theoretical justification on what the “true” model would be. This is a general problem in empirical risk minimization specially in finding unbiased estimators of population parameters. To see all these issues in actions, let’s have a simulation for the decomposition of in-sample unconditional MSPE’s. # Function for X - fixed at repeated samples xfunc &lt;- function(n){ set.seed(123) x_1 &lt;- rnorm(n, 0, 1) return(x_1) } # Function for simulation (M - number of samples) simmse &lt;- function(M, n, sigma, poldeg){ x_1 &lt;- xfunc(n) # Repeated X&#39;s in each sample # Containers MSPE &lt;- rep(0, M) yhat &lt;- matrix(0, M, n) olscoef &lt;- matrix(0, M, poldeg+1) ymat &lt;- matrix(0, M, n) # Loop for samples for (i in 1:M) { f &lt;- 1 + 2*x_1 - 2*I(x_1^2) # DGM y &lt;- f + rnorm(n, 0, sigma) samp &lt;- data.frame(&quot;y&quot; = y, x_1) # Estimator ols &lt;- lm(y ~ poly(x_1, degree = poldeg, raw=TRUE), samp) olscoef[i, ] &lt;- ols$coefficients # Yhat&#39;s yhat[i,] &lt;- predict(ols, samp) # MSPE - That is, residual sum of squares MSPE[i] &lt;- mean((ols$residuals)^2) ymat[i,] &lt;- y } output &lt;- list(MSPE, yhat, sigma, olscoef, f, ymat) return(output) } # running different fhat with different polynomial degrees output1 &lt;- simmse(2000, 100, 7, 1) output2 &lt;- simmse(2000, 100, 7, 2) #True model (i.e fhat = f) output3 &lt;- simmse(2000, 100, 7, 5) output4 &lt;- simmse(2000, 100, 7, 20) # Table tab &lt;- matrix(0, 4, 5) row.names(tab) &lt;- c(&quot;ols1&quot;, &quot;ols2&quot;, &quot;ols3&quot;, &quot;ols4&quot;) colnames(tab) &lt;- c(&quot;bias^2&quot;, &quot;var(yhat)&quot;, &quot;MSE&quot;, &quot;var(eps)&quot;, &quot;In-sample MSPE&quot;) f &lt;- output1[[5]] # Var(yhat) - We use our own function instead of &quot;var()&quot; tab[1,2] &lt;- mean(apply(output1[[2]], 2, function(x) mean((x-mean(x))^2))) tab[2,2] &lt;- mean(apply(output2[[2]], 2, function(x) mean((x-mean(x))^2))) tab[3,2] &lt;- mean(apply(output3[[2]], 2, function(x) mean((x-mean(x))^2))) tab[4,2] &lt;- mean(apply(output4[[2]], 2, function(x) mean((x-mean(x))^2))) # Bias^2 = (mean(yhat))-f)^2 tab[1,1] &lt;- mean((apply(output1[[2]], 2, mean) - f)^2) tab[2,1] &lt;- mean((apply(output2[[2]], 2, mean) - f)^2) tab[3,1] &lt;- mean((apply(output3[[2]], 2, mean) - f)^2) tab[4,1] &lt;- mean((apply(output4[[2]], 2, mean) - f)^2) # MSE fmat &lt;- matrix(f, nrow(output1[[6]]), length(f), byrow = TRUE) tab[1,3] &lt;- mean(colMeans((fmat - output1[[2]])^2)) tab[2,3] &lt;- mean(colMeans((fmat - output2[[2]])^2)) tab[3,3] &lt;- mean(colMeans((fmat - output3[[2]])^2)) tab[4,3] &lt;- mean(colMeans((fmat - output4[[2]])^2)) # # MSPE - This can be used as well, which is RSS # tab[1,5] &lt;- mean(output1[[1]]) # tab[2,5] &lt;- mean(output2[[1]]) # tab[3,5] &lt;- mean(output3[[1]]) # tab[4,5] &lt;- mean(output4[[1]]) # MSPE tab[1,5] &lt;- mean(colMeans((output1[[6]] - output1[[2]])^2)) tab[2,5] &lt;- mean(colMeans((output2[[6]] - output2[[2]])^2)) tab[3,5] &lt;- mean(colMeans((output3[[6]] - output3[[2]])^2)) tab[4,5] &lt;- mean(colMeans((output4[[6]] - output4[[2]])^2)) # Irreducable error - var(eps) = var(y) tab[1,4] &lt;- mean(apply(output1[[6]], 2, function(x) mean((x-mean(x))^2))) tab[2,4] &lt;- mean(apply(output2[[6]], 2, function(x) mean((x-mean(x))^2))) tab[3,4] &lt;- mean(apply(output3[[6]], 2, function(x) mean((x-mean(x))^2))) tab[4,4] &lt;- mean(apply(output4[[6]], 2, function(x) mean((x-mean(x))^2))) round(tab, 4) ## bias^2 var(yhat) MSE var(eps) In-sample MSPE ## ols1 4.9959 0.9467 5.9427 49.1493 53.2219 ## ols2 0.0006 1.4224 1.4230 49.1493 47.7574 ## ols3 0.0010 2.9011 2.9021 49.1493 46.2783 ## ols4 0.0098 10.2528 10.2626 49.1493 38.9179 The table verifies that \\(\\mathbf{MSE}(\\hat{f})=\\mathbf{Var}(\\hat{f})+\\left[\\mathbf{bias}(\\hat{f})\\right]^{2}.\\) However, it seems that the MSPE (in-sample) of each model is “wrong”, which is not the sum of MSE and \\(\\mathbf{Var}(\\varepsilon)\\). We will come back to this point, but before going further, to make the simulation calculations more understandable, I put here simple illustrations for each calculation. Think of a simulation as a big matrix: each row contains one sample and each column contains one observation of \\(x_i\\). For example, if we have 500 samples and each sample we have 100 observations, the “matrix” will be 500 by 100. Below, each section illustrates how the simulations are designed and each term is calculated Bias - \\((E(\\hat{f}(x))-f(x))^2\\) \\[ \\begin{array}{ccccc} &amp; x_1 &amp; x_2 &amp; \\ldots &amp; x_{100} \\\\ s_1 &amp; \\hat{f}\\left(x_1\\right) &amp; \\hat{f}\\left(x_2\\right) &amp; \\ldots &amp; \\hat{f}\\left(x_{100}\\right) \\\\ s_2 &amp; \\hat{f}\\left(x_1\\right) &amp; \\hat{f}\\left(x_2\\right) &amp; \\ldots &amp; \\vdots \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ldots &amp; \\vdots \\\\ s_{500} &amp; \\hat{f}\\left(x_1\\right) &amp; \\hat{f}\\left(x_2\\right) &amp; \\ldots &amp; \\hat{f}\\left(x_{100}\\right) \\end{array} \\] \\[ \\left[\\frac{\\Sigma \\hat{f}\\left(x_1\\right)}{500}-f(x)\\right]^2+\\cdots+\\left[\\frac{\\Sigma \\hat{f}\\left(x_{100}\\right)}{500}-f(x)\\right]^2=\\sum\\left[\\frac{\\sum \\hat{f}\\left(x_i\\right)}{500}-f(x)\\right]^2 \\] Variance - \\(\\operatorname{var}[\\hat{f}(x)]\\) \\[ \\begin{array}{ccccc} &amp; x_1 &amp; x_2 &amp; \\cdots &amp; x_{100} \\\\ s_1 &amp; \\hat{f}\\left(x_1\\right) &amp; \\hat{f}\\left(x_2\\right) &amp; \\ldots &amp; \\hat{f}\\left(x_{100}\\right) \\\\ s_2 &amp; \\hat{f}\\left(x_1\\right) &amp; \\hat{f}\\left(x_2\\right) &amp; \\ldots &amp; \\hat{f}\\left(x_{100}\\right) \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ldots &amp; \\vdots \\\\ s_{500} &amp; \\hat{f}\\left(x_1\\right) &amp; \\hat{f}\\left(x_2\\right) &amp; \\ldots &amp; \\hat{f}\\left(x_{100}\\right) \\end{array} \\] \\[ \\operatorname{var}\\left[\\hat{f}\\left(x_1\\right)\\right] +\\cdots+ \\operatorname{var}\\left[\\hat{f}\\left(x_{100}\\right)\\right]=\\frac{\\sum\\left(\\operatorname{var}\\left[\\hat{f}\\left(x_i\\right)\\right]\\right)}{100} \\] MSE - \\(E\\left[f\\left(x_i\\right)-\\hat{f}\\left(x_i\\right)\\right]^2\\) \\[ \\begin{array}{ccccc} &amp; x_1 &amp; x_2 &amp; \\ldots &amp; x_{100} \\\\ s_1 &amp; {\\left[f\\left(x_1\\right)-\\hat{f}\\left(x_1\\right)\\right]^2} &amp; {\\left[f\\left(x_2\\right)-\\hat{f}\\left(x_2\\right)\\right]^2} &amp; \\ldots &amp; {\\left[f\\left(x_{100}\\right)-\\hat{f}\\left(x_{100}\\right)\\right]^2} \\\\ s_2 &amp; {\\left[f\\left(x_1\\right)-\\hat{f}\\left(x_1\\right)\\right]^2} &amp; {\\left[f\\left(x_2\\right)-\\hat{f}\\left(x_2\\right)\\right]^2} &amp; \\ldots &amp; {\\left[f\\left(x_{100}\\right)-\\hat{f}\\left(x_{100}\\right)\\right]^2} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ldots &amp; \\vdots \\\\ s_{500} &amp; {\\left[f\\left(x_1\\right)-\\hat{f}\\left(x_1\\right)\\right]^2} &amp; {\\left[f\\left(x_2\\right)-\\hat{f}\\left(x_2\\right)\\right]^2} &amp; \\cdots &amp; {\\left[f\\left(x_{100}\\right)-\\hat{f}\\left(x_{100}\\right)\\right]^2} \\end{array} \\] \\[ \\frac{\\Sigma\\left[f\\left(x_1\\right)-\\hat{f}\\left(x_1\\right)\\right]^2}{500}+\\cdots+\\frac{\\Sigma\\left[f\\left(x_{100}\\right)-\\hat{f}\\left(x_{100}\\right)\\right]^2}{500}=\\sum\\left(\\frac{\\sum\\left(f\\left(x_1\\right)-\\hat{f}\\left(x_1\\right)\\right)^2}{500}\\right) \\] MSPE \\[ \\begin{array}{ccccc} &amp; x_1 &amp; x_2 &amp; \\cdots &amp; x_{100} \\\\ s_1 &amp; \\left(y_1-\\hat{f}\\left(x_1\\right)\\right)^2 &amp; \\left(y_2-\\hat{f}\\left(x_2\\right)\\right)^2 &amp; \\cdots &amp; \\left(y_{100}-\\hat{f}\\left(x_{100}\\right)\\right)^2 \\\\ s_2 &amp; \\left(y_1-\\hat{f}\\left(x_1\\right)\\right)^2 &amp; \\left(y_2-\\hat{f}\\left(x_2\\right)\\right)^2 &amp; \\cdots &amp; \\left(y_{100}-\\hat{f}\\left(x_{100}\\right)\\right)^2 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\cdots &amp; \\vdots \\\\ s_{500} &amp; \\left(y_1-\\hat{f}\\left(x_1\\right)\\right)^2 &amp; \\left(y_2-\\hat{f}\\left(x_2\\right)\\right)^2 &amp; \\cdots &amp; \\left(y_{100}-\\hat{f}\\left(x_{100}\\right)\\right)^2 \\end{array} \\] \\[ \\frac{\\sum\\left(y_1-\\hat{f}\\left(x_1\\right)\\right)^2}{500}+\\cdots+\\frac{\\sum\\left(y_{100}-\\hat{f}\\left(x_{100}\\right)\\right)^2}{500}=\\sum\\left(\\frac{\\sum\\left(y_i-\\hat{f}\\left(x_1\\right)\\right)^2}{500}\\right) \\] Now, back to our question: Why is the in-sample MSPE different than the sum of MSE and \\(\\sigma^2\\)? Let’s look at MSPE again but this time with different angle. We define MSPE over some data points, as we did in our simulation above, and re-write it as follows: \\[ \\mathbf{MSPE}_{out}=\\mathrm{E}\\left[\\frac{1}{n} \\sum_{i=1}^{n}\\left(y&#39;_{i}-\\hat{f}(x_i)\\right)^{2}\\right],~~~~~~\\text{where}~~y&#39;_i=f(x_i)+\\varepsilon&#39;_i \\] This type of MSPE is also called as unconditional MSPE. Inside of the brackets is the “prediction error” for a range of out-of-sample data points. The only difference here is that we distinguish \\(y&#39;_i\\) as out-of-sample data points. Likewise, we define MSPE for in-sample data points \\(y_i\\) as \\[ \\mathbf{MSPE}_{in}=\\mathrm{E}\\left[\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-\\hat{f}(x_i)\\right)^{2}\\right],~~~~~~\\text{where}~~y_i=f(x_i)+\\varepsilon_i. \\] Note that \\(\\varepsilon&#39;_i\\) and \\(\\varepsilon_i\\) are independent but identically distributed. Moreover \\(y&#39;_i\\) and \\(y_i\\) has the same distribution. Let’s look at \\(\\mathrm{E}\\left[(y&#39;_i-\\hat{f}(x_i))^{2}\\right]\\) closer. By using the definition of variance, \\[ \\begin{aligned} \\mathrm{E}\\left[(y&#39;_i-\\hat{f}(x_i))^{2}\\right] &amp;=\\mathrm{Var}\\left[y&#39;_{i}-\\hat{f}(x_i)\\right]+\\left(\\mathrm{E}\\left[y&#39;_{i}-\\hat{f}(x_i)\\right]\\right)^{2}\\\\ &amp;=\\mathrm{Var}\\left[y&#39;_{i}\\right]+\\mathrm{Var}\\left[\\hat{f}(x_i)\\right]-2 \\mathrm{Cov}\\left[y&#39;_{i}, \\hat{f}(x_i)\\right]+\\left(\\mathrm{E}\\left[y&#39;_{i}\\right]-\\mathrm{E}\\left[\\hat{f}(x_i)\\right]\\right)^{2} \\end{aligned} \\] Similarly, \\[ \\begin{aligned} \\mathrm{E}\\left[(y_i-\\hat{f}(x_i))^{2}\\right] &amp;=\\mathrm{Var}\\left[y_{i}-\\hat{f}(x_i)\\right]+\\left(\\mathrm{E}\\left[y_{i}-\\hat{f}(x_i)\\right]\\right)^{2}\\\\ &amp;=\\mathrm{Var}\\left[y_{i}\\right]+\\mathrm{Var}\\left[\\hat{f}(x_i)\\right]-2 \\mathrm{Cov}\\left[y_{i}, \\hat{f}(x_i)\\right]+\\left(\\mathrm{E}\\left[y_{i}\\right]-\\mathrm{E}\\left[\\hat{f}(x_i)\\right]\\right)^{2} \\end{aligned} \\] Remember our earlier derivation of variance-bias decomposition: When we predict out-of-sample data points, we know that \\(y_0\\) and \\(\\hat{f}(x_0)\\) are independent. We had stated it differently: \\(\\varepsilon_0\\) is independent from \\(\\hat{f}(x_0)\\). In other words, how we estimate our estimator is an independent process from \\(y&#39;_i\\). Hence, \\(\\mathrm{Cov}\\left[y&#39;_{i}, \\hat{f}(x_i)\\right]=0\\). The critical point here is that \\(\\mathrm{Cov}\\left[y_{i} \\hat{f}(x_i)\\right]\\) is not zero. This is because the estimator \\(\\hat{f}(x_i)\\) is chosen in a way that its difference from \\(y_i\\) should be minimum. Hence, our estimator is not an independent than in-sample \\(y_i\\) data points, on the contrary, we use them to estimate \\(\\hat{f}(x_i)\\). In fact, we can even choose \\(\\hat{f}(x_i) = y_i\\) where the MSPE would be zero. In that case correlation between \\(\\hat{f}(x_i)\\) and \\(y_i\\) would be 1. Using the fact that \\(\\mathrm{E}(y&#39;_i) = \\mathrm{E}(y_i)\\) and \\(\\mathrm{Var}(y&#39;_i) = \\mathrm{Var}(y_i)\\), we can now re-write \\(\\mathrm{E}\\left[(y&#39;_i-\\hat{f}(x_i))^{2}\\right]\\) as follows: \\[ \\mathrm{E}\\left[(y&#39;_i-\\hat{f}(x_i))^{2}\\right]=\\mathrm{Var}\\left[y_{i}\\right]+\\mathrm{Var}\\left[\\hat{f}(x_i)\\right]+\\left(\\mathrm{E}\\left[y_{i}\\right]-\\mathrm{E}\\left[\\hat{f}(x_i)\\right]\\right)^{2}\\\\ =\\mathrm{E}\\left[(y_i-\\hat{f}(x_i))^{2}\\right]+2 \\mathrm{Cov}\\left[y_{i}, \\hat{f}(x_i)\\right]. \\] Averaging over data points, \\[ \\mathrm{E}\\left[\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}^{\\prime}-\\hat{f}(x_i)\\right)^{2}\\right]=\\mathrm{E}\\left[\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-\\hat{f}(x_i)\\right)^{2}\\right]+\\frac{2}{n} \\sum_{i=1}^{n} \\mathrm{Cov}\\left[y_{i}, \\hat{f}(x_i)\\right]. \\] For a linear model, it can be shown that \\[ \\frac{2}{n} \\sum_{i=1}^{n} \\mathbf{Cov}\\left[y_{i}, \\hat{f}(x_i)\\right]=\\frac{2}{n} \\sigma^{2}(p+1). \\] Hence, \\[ \\mathbf{MSPE}_{out} =\\mathbf{MSPE}_{in}+\\frac{2}{n} \\sigma^{2}(p+1). \\] The last term quantifies the overfitting, the the amount by which the in-sample MSPE systematically underestimates its true MSPE, i.e. out-of-sample MSPE. Note also that the overfitting grows with the “noise” (\\(\\sigma^2\\)) in the data, shrinks with the sample size (\\(n\\)), grows with the number of variables (\\(p\\)). Hence, as we had stated earlier, the overfitting problem gets worse as \\(p/n\\) gets bigger. Minimizing the in-sample MSPE completely ignores the overfitting by picking models which are too large and with a very poor out-of-sample prediction accuracy. Now we can calculate the size of overfitting in our simulation. # New Table tabb &lt;- matrix(0, 4, 3) row.names(tabb) &lt;- c(&quot;ols1&quot;, &quot;ols2&quot;, &quot;ols3&quot;, &quot;ols4&quot;) colnames(tabb) &lt;- c(&quot;Cov(yi, yhat)&quot;,&quot;True MSPE&quot;, &quot;TrueMSPE-Cov&quot;) #COV tabb[1,1] &lt;- 2*mean(diag(cov(output1[[2]], output1[[6]]))) tabb[2,1] &lt;- 2*mean(diag(cov(output2[[2]], output2[[6]]))) tabb[3,1] &lt;- 2*mean(diag(cov(output3[[2]], output3[[6]]))) tabb[4,1] &lt;- 2*mean(diag(cov(output4[[2]], output4[[6]]))) #True MSPE tabb[1,2] &lt;- tab[1,3] + tab[1,4] tabb[2,2] &lt;- tab[2,3] + tab[2,4] tabb[3,2] &lt;- tab[3,3] + tab[3,4] tabb[4,2] &lt;- tab[4,3] + tab[4,4] #True MSPE - Cov (to compare with the measures in the earlier table) tabb[1,3] &lt;- tabb[1,2] - tabb[1,1] tabb[2,3] &lt;- tabb[2,2] - tabb[2,1] tabb[3,3] &lt;- tabb[3,2] - tabb[3,1] tabb[4,3] &lt;- tabb[4,2] - tabb[4,1] t &lt;- cbind(tab, tabb) round(t, 4) ## bias^2 var(yhat) MSE var(eps) In-sample MSPE Cov(yi, yhat) True MSPE ## ols1 4.9959 0.9467 5.9427 49.1493 53.2219 1.8944 55.0920 ## ols2 0.0006 1.4224 1.4230 49.1493 47.7574 2.8463 50.5723 ## ols3 0.0010 2.9011 2.9021 49.1493 46.2783 5.8052 52.0514 ## ols4 0.0098 10.2528 10.2626 49.1493 38.9179 20.5158 59.4119 ## TrueMSPE-Cov ## ols1 53.1976 ## ols2 47.7260 ## ols3 46.2462 ## ols4 38.8961 Let’s have a pause and look at this table: We know that the “true” model is ols2 in this simulation. However, we cannot know the true model and we have only one sample in practice. If we use the in-sample MSPE to choose a model, we pick ols4 as it has the minimum MSPE. Not only ols4 is the worst predictor among all models, it is also the worst estimator among the unbiased estimators ols1, ols2, and ols3, as it has the highest MSE. If our task is to find the best predictor, we cannot use in-sample MSPE, as it give us ols4, as the best predictor. As a side note: when we compare the models in terms their out-sample prediction accuracy, we usually use the root MSPE (RMSPE), which gives us the prediction error in original units. When we calculate empirical in-sample MSPE with one sample, we can asses its out-of-sample prediction performance by the Mallows \\(C_P\\) statistics, which just substitutes the feasible estimator of \\(\\sigma^2\\) into the overfitting penalty. That is, for a linear model with \\(p + 1\\) coefficients fit by OLS, \\[ C_{p}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-\\hat{f}(x_i)\\right)^{2}+\\frac{2 \\widehat{\\sigma}^{2}}{n}(p+1), \\] which becomes a good proxy for the our-of-sample error. That is, a small value of \\(C_p\\) means that the model is relatively precise. For comparing models, we really care about differences in empirical out-sample MSPE’s: \\[ \\Delta C_{p}=\\mathbf{MSPE}_{1}-\\mathbf{MSPE}_{2}+\\frac{2}{n} \\widehat{\\sigma}^{2}\\left(p_{1}-p_{2}\\right), \\] where we use \\(\\hat{\\sigma}^2\\) from the largest model. How are we going to find the best predictor? In addition to \\(C_p\\), we can also use Akaike Information Criterion (AIC), which also has the form of “in-sample performance plus penalty”. AIC can be applied whenever we have a likelihood function, whereas \\(C_p\\) can be used when we use squared errors. We will see later AIC and BIC (Bayesian Information Criteria) in this book. With these measures, we can indirectly estimate the test (out-of-sample) error by making an adjustment to the training (in-sample) error to account for the bias due to overfitting. Therefore, these methods are ex-post tools to penalize the overfitting. On the other hand, we can directly estimate the test error (out-sample) and choose the model that minimizes it. We can do it by directly validating the model using a cross-validation approach. Therefore, cross-validation methods provide ex-ante penalization for overfitting and are the main tools in selecting predictive models in machine learning applications as they have almost no assumptions. "],["parametric-vs.-nonparametric-methods.html", "Parametric vs. Nonparametric methods", " Parametric vs. Nonparametric methods According to Breiman, there are two “cultures”: The Data Modeling Culture : One assumes that the data are generated by a given stochastic data model (econometrics) … Leo Breiman (Breiman_2001?): Statistical Modeling: The Two Cultures: For instance, in the Journal of the American Statistical Association (JASA), virtually every article contains a statement of the form: Assume that the data are generated by the following model:… I am deeply troubled by the current and past use of data models in applications, where quantitative conclusions are drawn and perhaps policy decisions made. … assume the data is generated by independent draws from the model* \\[ y=b_{0}+\\sum_{1}^{M} b_{m} x_{m}+\\varepsilon \\] where the coefficients are to be estimated. The error term is N(0, \\(\\sigma^2\\)) and \\(\\sigma^2\\) is to be estimated. Given that the data is generated this way, elegant tests of hypotheses,confidence intervals,distributions of the residual sum-of-squares and asymptotics can be derived. This made the model attractive in terms of the mathematics involved. This theory was used both by academics statisticians and others to derive significance levels for coefficients on the basis of model (R), with little consideration as to whether the data on hand could have been generated by a linear model. Hundreds, perhaps thousands of articles were published claiming proof of something or other because the coefficient was significant at the 5% level… …With the insistence on data models, multivariate analysis tools in statistics are frozen at discriminant analysis and logistic regression in classification and multiple linear regression in regression. Nobody really believes that multivariate data is multivariate normal, but that data model occupies a large number of pages in every graduate text book on multivariate statistical analysis… Algorithmic Modeling Culture: One uses algorithmic models and treats the data mechanism as unknown (machine learning) … He argues that the focus in the statistical community on data models has: Led to irrelevant theory and questionable scientific conclusions; Kept statisticians from using more suitable algorithmic models; Prevented statisticians from working on exciting new problems. In parametric econometrics we assume that the data come from a generating process that takes the following form: \\[ y=X \\beta+\\varepsilon \\] Model (\\(X\\)’s) are determined by the researcher and probability theory is a foundation of econometrics In Machine learning we do not make any assumption on how the data have been generated: \\[ y \\approx m(X) \\] Model (\\(X\\)’s) is not selected by the researcher and probability theory is not required Nonparametric econometrics makes the link between the two: Machine Learning: an extension of nonparametric econometrics To see the difference between two “cultures”, we start with parametric modeling in classification problems. "],["parametric-estimations.html", "Chapter 5 Parametric Estimations 5.1 Linear Probability Models (LPM) 5.2 Logistic Regression", " Chapter 5 Parametric Estimations So far we have only considered models for numeric response variables. What happens if the response variable is categorical? Can we use linear models in these situations? Yes, we can. To understand how, let’s look at the ordinary least-square (OLS) regression, which is actually a specific case of the more general, generalized linear model (GLM). So, in general, GLMs relate the mean of the response to a linear combination of the predictors, \\(\\eta(x)\\), through the use of a link function, \\(g(.)\\). That is, \\[\\begin{equation} \\eta(\\mathbf{x})=g(\\mathrm{E}[Y | \\mathbf{X}=\\mathbf{x}]), \\tag{5.1} \\end{equation}\\] Or, \\[\\begin{equation} \\eta(\\mathbf{x})=\\beta_{0}+\\beta_{1} x_{1}+\\beta_{2} x_{2}+\\ldots+\\beta_{p-1} x_{p-1} = g(\\mathrm{E}[Y | \\mathbf{X}=\\mathbf{x}]) \\tag{5.2} \\end{equation}\\] In the case of a OLS, \\[ g(\\mathrm{E}[Y | \\mathbf{X}=\\mathbf{x}]) = E[Y | \\mathbf{X}=\\mathbf{x}], \\] To illustrate the use of a GLM, we will focus on the case of binary response variable coded using 0 and 1. In practice, these 0s and 1s represent two possible outcomes such as “yes” or “no”, “sick” or “healthy”, etc. \\[ Y=\\left\\{\\begin{array}{ll}{1} &amp; {\\text { yes }} \\\\ {0} &amp; {\\text { no }}\\end{array}\\right. \\] 5.1 Linear Probability Models (LPM) Let’s use the dataset Vehicles from the fueleconomy package. We we ill create a new variable a new variable, mpg, which is 1 if the car’s hightway mpg is more than the average, 0 otherwise: library(fueleconomy) data(vehicles) df &lt;- as.data.frame(vehicles) # Remove NAs dim(df) ## [1] 33442 12 data &lt;- df[complete.cases(df), ] dim(data) ## [1] 33382 12 # Binary outcome mpg = 1 if hwy &gt; mean(hwy), 0 otherwise data$mpg &lt;- ifelse(data$hwy &gt; mean(data$hwy), 1, 0) table(data$mpg) ## ## 0 1 ## 17280 16102 We are going to have a model that predicts mpg (i.e. mpg = 1) for each car depending on their attributes. If you check the data, you see that many variables are character variables. Although most functions, like lm(), accept character variables (and convert them to factor), it is a good practice to check each variable and convert them appropriate data types. for (i in 1:ncol(data)) { if(is.character(data[,i])) data[,i] &lt;- as.factor(data[,i]) } str(data) ## &#39;data.frame&#39;: 33382 obs. of 13 variables: ## $ id : num 13309 13310 13311 14038 14039 ... ## $ make : Factor w/ 124 levels &quot;Acura&quot;,&quot;Alfa Romeo&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ model: Factor w/ 3174 levels &quot;1-Ton Truck 2WD&quot;,..: 28 28 28 29 29 29 29 29 29 30 ... ## $ year : num 1997 1997 1997 1998 1998 ... ## $ class: Factor w/ 34 levels &quot;Compact Cars&quot;,..: 29 29 29 29 29 29 29 29 29 1 ... ## $ trans: Factor w/ 46 levels &quot;Auto (AV-S6)&quot;,..: 32 43 32 32 43 32 32 43 32 32 ... ## $ drive: Factor w/ 7 levels &quot;2-Wheel Drive&quot;,..: 5 5 5 5 5 5 5 5 5 5 ... ## $ cyl : num 4 4 6 4 4 6 4 4 6 5 ... ## $ displ: num 2.2 2.2 3 2.3 2.3 3 2.3 2.3 3 2.5 ... ## $ fuel : Factor w/ 12 levels &quot;CNG&quot;,&quot;Diesel&quot;,..: 11 11 11 11 11 11 11 11 11 7 ... ## $ hwy : num 26 28 26 27 29 26 27 29 26 23 ... ## $ cty : num 20 22 18 19 21 17 20 21 17 18 ... ## $ mpg : num 1 1 1 1 1 1 1 1 1 0 ... Done! We are ready to have a model to predict mpg. For now, we’ll use only fuel. model1 &lt;- lm(mpg ~ fuel + 0, data = data) #No intercept summary(model1) ## ## Call: ## lm(formula = mpg ~ fuel + 0, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.8571 -0.4832 -0.2694 0.5168 0.7306 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## fuelCNG 0.362069 0.065383 5.538 3.09e-08 *** ## fuelDiesel 0.479405 0.016843 28.463 &lt; 2e-16 *** ## fuelGasoline or E85 0.269415 0.015418 17.474 &lt; 2e-16 *** ## fuelGasoline or natural gas 0.277778 0.117366 2.367 0.0180 * ## fuelGasoline or propane 0.000000 0.176049 0.000 1.0000 ## fuelMidgrade 0.302326 0.075935 3.981 6.87e-05 *** ## fuelPremium 0.507717 0.005364 94.650 &lt; 2e-16 *** ## fuelPremium and Electricity 1.000000 0.497942 2.008 0.0446 * ## fuelPremium Gas or Electricity 0.857143 0.188205 4.554 5.27e-06 *** ## fuelPremium or E85 0.500000 0.053081 9.420 &lt; 2e-16 *** ## fuelRegular 0.483221 0.003311 145.943 &lt; 2e-16 *** ## fuelRegular Gas and Electricity 1.000000 0.176049 5.680 1.36e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4979 on 33370 degrees of freedom ## Multiple R-squared: 0.4862, Adjusted R-squared: 0.486 ## F-statistic: 2631 on 12 and 33370 DF, p-value: &lt; 2.2e-16 The estimated model is a probabilistic model since, \\[ E[Y | \\mathbf{X}=\\mathbf{Regular}]) = \\text{Pr}(Y=1|\\mathbf{X}=\\mathbf{Regular}), \\] In this context, the link function is called “identity” because it directly “links” the probability to the linear function of the predictor variables. Let’s see if we can verify this: tab &lt;- table(data$fuel, data$mpg) ftable(addmargins(tab)) ## 0 1 Sum ## ## CNG 37 21 58 ## Diesel 455 419 874 ## Gasoline or E85 762 281 1043 ## Gasoline or natural gas 13 5 18 ## Gasoline or propane 8 0 8 ## Midgrade 30 13 43 ## Premium 4242 4375 8617 ## Premium and Electricity 0 1 1 ## Premium Gas or Electricity 1 6 7 ## Premium or E85 44 44 88 ## Regular 11688 10929 22617 ## Regular Gas and Electricity 0 8 8 ## Sum 17280 16102 33382 prop.table(tab, 1) ## ## 0 1 ## CNG 0.6379310 0.3620690 ## Diesel 0.5205950 0.4794050 ## Gasoline or E85 0.7305849 0.2694151 ## Gasoline or natural gas 0.7222222 0.2777778 ## Gasoline or propane 1.0000000 0.0000000 ## Midgrade 0.6976744 0.3023256 ## Premium 0.4922827 0.5077173 ## Premium and Electricity 0.0000000 1.0000000 ## Premium Gas or Electricity 0.1428571 0.8571429 ## Premium or E85 0.5000000 0.5000000 ## Regular 0.5167794 0.4832206 ## Regular Gas and Electricity 0.0000000 1.0000000 The frequency table shows the probability of each class (MPG = 1 or 0) for each fuel type. The OLS we estimated produces exactly the same results, that is, \\[ Pr[Y = 1 | x=\\mathbf{Regular}]) = \\beta_{0}+\\beta_{1} x_{i}. \\] Since \\(Y\\) has only two possible outcomes (1 and 0), it has a specific probability distribution. First, let’s refresh our memories about Binomial and Bernoulli distributions. In general, if a random variable, \\(X\\), follows a binomial distribution with parameters \\(n \\in \\mathbb{N}\\) and \\(p \\in [0,1]\\), we write \\(X \\sim B(n, p)\\). The probability of getting exactly \\(k\\) successes in \\(n\\) trials is given by the probability mass function: \\[\\begin{equation} \\operatorname{Pr}(X=k)=\\left(\\begin{array}{l}{n} \\\\ {k}\\end{array}\\right) p^{k}(1-p)^{n-k} \\tag{5.3} \\end{equation}\\] for \\(k = 0, 1, 2, ..., n\\), where \\[ \\left(\\begin{array}{l}{n} \\\\ {k}\\end{array}\\right)=\\frac{n !}{k !(n-k) !} \\] Formula 5.3 can be understood as follows: \\(k\\) successes occur with probability \\(p^k\\) and \\(n-k\\) failures occur with probability \\((1-p)^{n−k}\\). However, the \\(k\\) successes can occur anywhere among the \\(n\\) trials, and there are \\(n!/k!(n!-k!)\\) different ways of distributing \\(k\\) successes in a sequence of \\(n\\) trials. Suppose a biased coin comes up heads with probability 0.3 when tossed. What is the probability of achieving 4 heads after 6 tosses? \\[ \\operatorname{Pr}(4 \\text { heads})=f(4)=\\operatorname{Pr}(X=4)=\\left(\\begin{array}{l}{6} \\\\ {4}\\end{array}\\right) 0.3^{4}(1-0.3)^{6-4}=0.059535 \\] The Bernoulli distribution on the other hand, is a discrete probability distribution of a random variable which takes the value 1 with probability \\(p\\) and the value 0 with probability \\(q = (1 - p)\\), that is, the probability distribution of any single experiment that asks a yes–no question. The Bernoulli distribution is a special case of the binomial distribution, where \\(n = 1\\). Symbolically, \\(X \\sim B(1, p)\\) has the same meaning as \\(X \\sim Bernoulli(p)\\). Conversely, any binomial distribution, \\(B(n, p)\\), is the distribution of the sum of \\(n\\) Bernoulli trials, \\(Bernoulli(p)\\), each with the same probability \\(p\\). \\[ \\operatorname{Pr}(X=k) =p^{k}(1-p)^{1-k} \\quad \\text { for } k \\in\\{0,1\\} \\] Formally, the outcomes \\(Y_i\\) are described as being Bernoulli-distributed data, where each outcome is determined by an unobserved probability \\(p_i\\) that is specific to the outcome at hand, but related to the explanatory variables. This can be expressed in any of the following equivalent forms: \\[\\begin{equation} \\operatorname{Pr}\\left(Y_{i}=y | x_{1, i}, \\ldots, x_{m, i}\\right)=\\left\\{\\begin{array}{ll}{p_{i}} &amp; {\\text { if } y=1} \\\\ {1-p_{i}} &amp; {\\text { if } y=0}\\end{array}\\right. \\tag{5.4} \\end{equation}\\] The expression 5.4 is the probability mass function of the Bernoulli distribution, specifying the probability of seeing each of the two possible outcomes. Similarly, this can be written as follows, which avoids having to write separate cases and is more convenient for certain types of calculations. This relies on the fact that \\(Y_{i}\\) can take only the value 0 or 1. In each case, one of the exponents will be 1, which will make the outcome either \\(p_{i}\\) or 1−\\(p_{i}\\), as in 5.4.1 \\[ \\operatorname{Pr}\\left(Y_{i}=y | x_{1, i}, \\ldots, x_{m, i}\\right)=p_{i}^{y}\\left(1-p_{i}\\right)^{(1-y)} \\] Hence this shows that \\[ \\operatorname{Pr}\\left(Y_{i}=1 | x_{1, i}, \\ldots, x_{m, i}\\right)=p_{i}=E[Y_{i} | \\mathbf{X}=\\mathbf{x}]) \\] Let’s have a more complex model: model2 &lt;- lm(mpg ~ fuel + drive + cyl, data = data) summary(model2) ## ## Call: ## lm(formula = mpg ~ fuel + drive + cyl, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.09668 -0.21869 0.01541 0.12750 0.97032 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.858047 0.049540 17.320 &lt; 2e-16 *** ## fuelDiesel 0.194540 0.047511 4.095 4.24e-05 *** ## fuelGasoline or E85 0.030228 0.047277 0.639 0.52258 ## fuelGasoline or natural gas 0.031187 0.094466 0.330 0.74129 ## fuelGasoline or propane 0.031018 0.132069 0.235 0.81432 ## fuelMidgrade 0.214471 0.070592 3.038 0.00238 ** ## fuelPremium 0.189008 0.046143 4.096 4.21e-05 *** ## fuelPremium and Electricity 0.746139 0.353119 2.113 0.03461 * ## fuelPremium Gas or Electricity 0.098336 0.140113 0.702 0.48279 ## fuelPremium or E85 0.307425 0.059412 5.174 2.30e-07 *** ## fuelRegular 0.006088 0.046062 0.132 0.89485 ## fuelRegular Gas and Electricity 0.092330 0.132082 0.699 0.48454 ## drive4-Wheel Drive 0.125323 0.020832 6.016 1.81e-09 *** ## drive4-Wheel or All-Wheel Drive -0.053057 0.016456 -3.224 0.00126 ** ## driveAll-Wheel Drive 0.333921 0.018879 17.687 &lt; 2e-16 *** ## driveFront-Wheel Drive 0.497978 0.016327 30.499 &lt; 2e-16 *** ## drivePart-time 4-Wheel Drive -0.078447 0.039258 -1.998 0.04570 * ## driveRear-Wheel Drive 0.068346 0.016265 4.202 2.65e-05 *** ## cyl -0.112089 0.001311 -85.488 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3501 on 33363 degrees of freedom ## Multiple R-squared: 0.5094, Adjusted R-squared: 0.5091 ## F-statistic: 1924 on 18 and 33363 DF, p-value: &lt; 2.2e-16 Since OLS is a “Gaussian” member of GLS family, we can also estimate it as GLS. We use glm() and define the family as “gaussian”. model3 &lt;- glm(mpg ~ fuel + drive + cyl, family = gaussian, data = data) identical(round(coef(model2),2), round(coef(model3),2)) ## [1] TRUE With this LPM model, we can now predict the classification of future cars in terms of high (mpg = 1) or low (mpg = 0), which was our objective. Let’s see how successful we are in identifying cars with mpg = 1 in our own sample. #How many cars we have with mpg = 1 and mpg = 0 in our data table(data$mpg) ## ## 0 1 ## 17280 16102 #In-sample fitted values or predicted probabilities for mpg = 1 #Remember our E(Y|X) is Pr(Y=1|X) mpg_hat &lt;- fitted(model2) #If any predicted mpg above 0.5 should be considered as 1 length(mpg_hat[mpg_hat &gt; 0.5]) ## [1] 14079 length(mpg_hat[mpg_hat &lt;= 0.5]) ## [1] 19303 Our prediction is significantly off: we predict many cars with mpg = 0 as having mpg = 1. Note that we are using 0.5 as our discriminating threshold to convert predicted probabilities to predicted “labels”. This is an arbitrary choice as we will see later Another issue with LPM can be see below: summary(mpg_hat) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.7994 0.2187 0.4429 0.4824 0.9138 1.2088 The predicted probabilities (of mpg = 1) are not bounded between 1 and 0. We will talk about these issues later. None of these problems are major drawbacks for LPM. But, by its nature, LPM defines a constant marginal effect of \\(x\\) on \\(Pr[Y = 1 | x)\\). \\[ Pr(Y = 1 | x=\\mathbf{Regular}) = \\beta_{0}+\\beta_{1} x_{i}. \\] We can see it with a different example model_n &lt;- lm(mpg ~ cyl, data = data) plot(data$cyl, data$mpg, ylim = c(-1.2, 1.2)) lines(data$cyl, model_n$fitted.values, col = &quot;red&quot;, lwd = 2) Two things we see in the plot: first predicted probabilities are not bounded between 0 and 1. Second, the effect of cyc on \\(Pr(Y = 1 | x)\\) is constant regardless of the value of cyc. We can of course add polynomial terms to LPM to deal with it. But, we may have a better model, Logistic regression, if we think that the constant marginal effect is an unrealistic assumption. 5.2 Logistic Regression First, let’s define some notation that we will use throughout. Note that many machine learning texts use \\(p\\) as the number of parameters. Here we use it to denote probability. \\[ p(\\mathbf{x})=P(Y=1 | \\mathbf{X}=\\mathbf{x}) \\] With a binary (Bernoulli) response, we will mostly focus on the case when \\(Y = 1\\), since, with only two possibilities, it is trivial to obtain probabilities when \\(Y = 0\\). \\[ \\begin{array}{c}{P(Y=0 | \\mathbf{X}=\\mathbf{x})+P(Y=1 | \\mathbf{X}=\\mathbf{x})=1} \\\\\\\\ {P(Y=0 | \\mathbf{X}=\\mathbf{x})=1-p(\\mathbf{x})}\\end{array} \\] We begin with introducing the standard logistic function, which is a sigmoid function. It takes any real input \\(z\\) and outputs a value between zero and one. The standard logistic function is defined as follows: \\[\\begin{equation} \\sigma(z)=\\frac{e^{z}}{e^{z}+1}=\\frac{1}{1+e^{-z}} \\tag{5.5} \\end{equation}\\] Here is an example: set.seed(1) n &lt;- 500 x = rnorm(n, 0,2) sigma &lt;- 1/(1+exp(-x)) plot(sigma ~ x, col =&quot;blue&quot;, cex.axis = 0.7) This logistic function is nice because: (1) whatever the \\(x\\)’s are \\(\\sigma(z)\\) is always between 0 and 1, (2) The effect of \\(x\\) on \\(\\sigma(z)\\) is not linear. That is, there is lower and upper thresholds in \\(x\\) that before and after those values (around -2 and 2 here) the marginal effect of \\(x\\) on \\(\\sigma(z)\\) is very low. Therefore, it seems that if we use a logistic function and replace \\(\\sigma(z)\\) with \\(p(x)\\), we can solve issues related to these two major drawbacks of LPM. Let us assume that \\(z = y = \\beta_{0}+\\beta_{1} x_{1}\\). Then, the logistic function can now be written as: \\[\\begin{equation} p(x)=P(Y=1|\\mathbf{X}=\\mathbf{x})=\\frac{1}{1+e^{-\\left(\\beta_{0}+\\beta_{1} x\\right)}} \\tag{5.6} \\end{equation}\\] To understand why nonlinearity would be a desirable future in some probability predictions, let’s imagine we try to predict the effect of saving (\\(x\\)) on homeownership (\\(p(x)\\)). If you have no saving now (\\(x=0\\)), additional $10K saving would not make a significant difference in your decision to buy a house (\\(P(Y=1|x)\\)). Similarly, when you have $500K (\\(x\\)) saving, additional $10K (\\(dx\\)) saving should not make a big difference in your decision to buy a house. That’s why flat lower and upper tails of \\(\\sigma(z)\\) are nice futures reflecting very low marginal effects of \\(x\\) on the probability of having a house in this case. After a simple algebra, we can also write the same function as follows, \\[\\begin{equation} \\ln \\left(\\frac{p(x)}{1-p(x)}\\right)=\\beta_{0}+\\beta_{1} x, \\tag{5.7} \\end{equation}\\] where \\(p(x)/(1-p(x))\\) is called odds, a ratio of success over failure. The natural log of this ratio is called, log odds, or Logit, usually denoted as \\(\\mathbf(L)\\). p_x &lt;- sigma Logit &lt;- log(p_x/(1-p_x)) #By defult log() calculates natural logarithms plot(Logit ~ x, col =&quot;red&quot;, cex.axis = 0.7) In many cases, researchers use a logistic function, when the outcome variable in a regression is dichotomous. Although there are situations where the linear model is clearly problematic (as described above), there are many common situations where the linear model is just fine, and even has advantages. Let’s start by comparing the two models explicitly. If the outcome \\(Y\\) is dichotomous with values 1 and 0, we define \\(P(Y=1|X) = E(Y|X)\\) as proved earlier, which is just the probability that \\(Y\\) is 1, given some value of the regressors \\(X\\). Then the linear and logistic probability models are: \\[ P(Y = 1|\\mathbf{X}=\\mathbf{x})=E(Y | \\mathbf{X}=\\mathbf{x}) = \\beta_{0}+\\beta_{1} x_{1}+\\beta_{2} x_{2}+\\ldots+\\beta_{k} x_{k}, \\] \\(~\\) \\[ \\ln \\left(\\frac{P(Y=1|\\mathbf{X})}{1-P(Y=1|\\mathbf{X})}\\right)=\\beta_{0}+\\beta_{1} x_{1}+\\ldots+\\beta_{k} x_{k} \\] \\(~\\) While LPM assumes that the probability \\(P\\) is a linear function of the regressors, the logistic model assumes that the natural log of the odds \\(P/(1-P)\\) is a linear function of the regressors. Note that applying the inverse logit transformation allows us to obtain an expression for \\(P(x)\\). Finally, LPM can be estimated easily with OLS, the Logistic model needs MLE. \\[ p(\\mathbf{x})=E(Y | \\mathbf{X}=\\mathbf{x})=P(Y=1 | \\mathbf{X}=\\mathbf{x})=\\frac{1}{1+e^{-(\\beta_{0}+\\beta_{1} x_{1}+\\cdots+\\beta_{k} x_{k})}} \\] \\(~\\) The major advantage of LPM is its interpretability. In the linear model, if \\(\\beta_{2}\\) is (say) 0.05, that means that a one-unit increase in \\(x_{2}\\) is associated with a 5-percentage point increase in the probability that \\(Y\\) is 1. Just about everyone has some understanding of what it would mean to increase by 5 percentage points their probability of, say, voting, or dying, or becoming obese. In the logistic model, however, a change in \\(x_{1}\\) changes the log odds, \\(\\text{log}(P/{(1-P)})\\). Hence, the coefficient of a logistic regression requires additional steps to understand what it means: we convert it to the odd ratio (OR) or use the above equation to calculate fitted (predicted) probabilities. When we should use the logistic model? It should be the choice if it fits the data much better than the linear model. In other words, for a logistic model to fit better than a linear model, it must be the case that the log odds are a linear function of X, but the probability is not. Lets review these concepts in a simulation exercise: #Creating random data set.seed(1) n &lt;- 500 x = rnorm(n) z = -2 + 3 * x #Probablity is defined by a logistic function #Therefore it is not a linear function of x! p = 1 / (1 + exp(-z)) #Remember Bernoulli distribution defines Y as 1 or 0 y = rbinom(n, size = 1, prob = p) #And we create our data data &lt;- data.frame(y, x) head(data) ## y x ## 1 0 -0.6264538 ## 2 0 0.1836433 ## 3 0 -0.8356286 ## 4 0 1.5952808 ## 5 0 0.3295078 ## 6 0 -0.8204684 table(y) ## y ## 0 1 ## 353 147 We know that probability is defined by a logistic function (see above). What happens if we fit it as LPM, which is \\(Pr(Y = 1 | x=\\mathbf{x}) = \\beta_{0}+\\beta_{1} x_{i}\\)? lpm &lt;- lm(y ~ x, data = data) summary(lpm) ## ## Call: ## lm(formula = y ~ x, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.76537 -0.25866 -0.08228 0.28686 0.82338 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.28746 0.01567 18.34 &lt;2e-16 *** ## x 0.28892 0.01550 18.64 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3504 on 498 degrees of freedom ## Multiple R-squared: 0.411, Adjusted R-squared: 0.4098 ## F-statistic: 347.5 on 1 and 498 DF, p-value: &lt; 2.2e-16 plot(x, p, col = &quot;green&quot;, cex.lab = 0.7, cex.axis = 0.8) abline(lpm, col = &quot;red&quot;) legend(&quot;topleft&quot;, c(&quot;Estimated Probability by LPM&quot;, &quot;Probability&quot;), lty = c(1, 1), pch = c(NA, NA), lwd = 2, col = c(&quot;red&quot;, &quot;green&quot;), cex = 0.7) How about a logistic regression? logis &lt;- glm(y ~ x, data = data, family = binomial) summary(logis) ## ## Call: ## glm(formula = y ~ x, family = binomial, data = data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.3813 -0.4785 -0.2096 0.2988 2.4274 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.8253 0.1867 -9.776 &lt;2e-16 *** ## x 2.7809 0.2615 10.635 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 605.69 on 499 degrees of freedom ## Residual deviance: 328.13 on 498 degrees of freedom ## AIC: 332.13 ## ## Number of Fisher Scoring iterations: 6 plot(x, p, col = &quot;green&quot;, cex.lab = 0.8, cex.axis = 0.8) curve(predict(logis, data.frame(x), type = &quot;response&quot;), add = TRUE, col = &quot;red&quot;, lty = 2) legend(&quot;topleft&quot;, c(&quot;Estimated Probability by GLM&quot;, &quot;Probability&quot;), lty = c(1, 1), pch = c(NA, NA), lwd = 2, col = c(&quot;red&quot;, &quot;green&quot;), cex = 0.7) As you can see, the estimated logistic regression coefficients are in line with our DGM coefficients (-2, 3). \\[ \\log \\left(\\frac{\\hat{p}(\\mathbf{x})}{1-\\hat{p}(\\mathbf{x})}\\right)=-1.8253+2.7809 x \\] 5.2.1 Estimating Logistic Regression Since Logit is a linear function: \\[\\begin{equation} Logit_i = \\log \\left(\\frac{p\\left(\\mathbf{x}_{\\mathbf{i}}\\right)}{\\left.1-p\\left(\\mathbf{x}_{\\mathbf{i}}\\right)\\right)}\\right)=\\beta_{0}+\\beta_{1} x_{i 1}+\\cdots+\\beta_{p-1} x_{i(p-1)}, \\tag{5.8} \\end{equation}\\] it seems that we can estimate it by a regular OLS. But, we only observe \\(Y=1\\) or \\(Y=0\\) not \\(p(\\mathbf{x})\\). To estimate the \\(\\beta\\) parameters, we apply the maximimum likelihood estimation method. First, we write the likelihood function, \\(L(\\beta)\\), given the observed data, which is technically a joint probability density function that can be written a product of \\(n\\) individual density functions: \\[ L(\\boldsymbol{\\beta})=\\prod_{i=1}^{n} P\\left(Y_{i}=y_{i} | \\mathbf{X}_{\\mathbf{i}}=\\mathbf{x}_{\\mathbf{i}}\\right) \\] With some rearrangement, we make it more explicit: \\[ \\begin{aligned} L(\\boldsymbol{\\beta}) &amp;=\\prod_{i=1}^{n} p\\left(\\mathbf{x}_{\\mathbf{i}}\\right)^{y_{i}}\\left(1-p\\left(\\mathbf{x}_{\\mathbf{i}}\\right)\\right)^{\\left(1-y_{i}\\right)} \\end{aligned} \\] With a logarithmic transformation of this function, it becomes a log-likelihood function, which turns products into sums. Hence, it becomes a linear function: \\[\\begin{equation} \\begin{split} \\begin{aligned} \\ell\\left(\\beta_{0}, \\beta\\right) &amp;=\\sum_{i=1}^{n} y_{i} \\log p\\left(x_{i}\\right)+\\left(1-y_{i}\\right) \\log (1-p\\left(x_{i}\\right)) \\\\ &amp;=\\sum_{i=1}^{n} \\log (1-p\\left(x_{i}\\right))+\\sum_{i=1}^{n} y_{i} \\log \\frac{p\\left(x_{i}\\right)}{1-p\\left(x_{i}\\right)} \\\\ &amp;=\\sum_{i=1}^{n} \\log (1-p\\left(x_{i}\\right))+\\sum_{i=1}^{n} y_{i}\\left(\\beta_{0}+\\beta x_{i} \\right) \\\\ &amp;=\\sum_{i=1}^{n} \\log 1/(1+e^{z_i})+\\sum_{i=1}^{n} y_{i}\\left(z_i\\right) \\\\ &amp;=\\sum_{i=1}^{n} -\\log (1+e^{z_i})+\\sum_{i=1}^{n} y_{i}\\left(z_i\\right), \\end{aligned} \\end{split} \\tag{5.9} \\end{equation}\\] where \\(z_i = \\beta_{0}+\\beta_{1} x_{1i}+\\cdots\\). Having a function for log-likelihood, we simply need to chose the values of \\(\\beta\\) that maximize it. Typically, to find them, we would differentiate the log-likelihood with respect to the parameters (\\(\\beta\\)), set the derivatives equal to zero, and solve. \\[ \\begin{aligned} \\frac{\\partial \\ell}{\\partial \\beta_{j}} &amp;=-\\sum_{i=1}^{n} \\frac{1}{1+e^{\\beta_{0}+x_{i} \\beta}} e^{\\beta_{0}+x_{i} \\beta} x_{i j}+\\sum_{i=1}^{n} y_{i} x_{i j} \\\\ &amp;=\\sum_{i=1}^{n}\\left(y_{i}-p\\left(x_{i} ; \\beta_{0}, \\beta\\right)\\right) x_{i j} \\end{aligned} \\] Unfortunately, there is no closed form for the maximum. However, we can find the best values of \\(\\beta\\) by using algorithm (numeric) optimization methods (See Appendix). 5.2.2 Cost functions The cost functions represent optimization objectives in estimations and predictions. In linear regression, it’s a simple sum of squared errors, i.e.  \\[\\begin{equation} \\mathbf{SSE}= \\sum{(\\hat{y}_i-y_i)^2} \\tag{5.10} \\end{equation}\\] If we use a similar cost function in Logistic Regression we would have a non-convex function with many local minimum points so that it would be hard to locate the global minimum. In logistic regression, as we have just seen, the log-likelihood function becomes the cost function. In the machine learning literature notation changes slightly: \\[\\begin{equation} J &amp;=\\sum_{i=1}^{n} y_{i} \\log p\\left(x_{i}\\right)+\\left(1-y_{i}\\right) \\log (1-p\\left(x_{i}\\right)), \\tag{5.11} \\end{equation}\\] where for each observation, \\[ p\\left(\\mathbf{x}_{\\mathbf{i}}\\right)=\\frac{e^{\\beta_{0}+\\beta x_{i}}}{1+e^{\\beta_{0}+\\beta x_{i}}} \\] Because it is more common to maximize a function in practice, the log likelihood function is inverted by adding a negative sign to the front. For classification problems, equation 5.11 is also called as “log loss“, “cross-entropy” and “negative log-likelihood” used interchangeably. Now that we have a cost function, we simply need to chose the values of \\(\\beta\\) that minimize it. Due to difficulties in multi-dimensional analytic solutions, we use gradient descent and some other types of algorithmic optimization methods. The same cost function can be written when \\(y_i \\in \\{+1,-1\\}\\) \\[ g_i(\\mathbf{w})= \\begin{cases}-\\log \\left(p\\left({\\mathbf{x}_i}^{T} \\mathbf{w}\\right)\\right) &amp; \\text { if } y_{i}=+1 \\\\ -\\log \\left(1-p\\left({\\mathbf{x}_i}^{T} \\mathbf{w}\\right)\\right) &amp; \\text { if } y_{i}=-1\\end{cases} \\] We can then form the Softmax cost for Logistic regression by taking an average of these Log Error costs as \\[ g(\\mathbf{w})=\\frac{1}{n} \\sum_{i=1}^{n} g_{i}(\\mathbf{w}) . \\] It is common to express the Softmax cost differently by re-writing the Log Error in a equivalent way as follows. Notice that with \\(z = \\mathbf{x}^{T} \\mathbf{w}\\) \\[ 1-p(z)=1-\\frac{1}{1+e^{-z}}=\\frac{1+e^{-z}}{1+e^{-z}}-\\frac{1}{1+e^{-z}}=\\frac{e^{-z}}{1+e^{-z}}=\\frac{1}{1+e^{z}}=p(-z) \\] Hence, the point-wise cost function can be written as \\[ g_{i}(\\mathbf{w})= \\begin{cases}-\\log \\left(p\\left({\\mathbf{x}}_{i}^{T} \\mathbf{w}\\right)\\right) &amp; \\text { if } y_{i}=+1 \\\\ -\\log \\left(p\\left(-{\\mathbf{x}}_{i}^{T} \\mathbf{w}\\right)\\right) &amp; \\text { if } y_{i}=-1\\end{cases} \\] Now notice that because we are using the label values \\(\\pm 1\\) we can move the label value in each case inside the inner most parenthesis, \\[ g_{i}(\\mathbf{w})=-\\log \\left(p\\left(y_{i} {\\mathbf{x}}_{i}^{T} \\mathbf{w}\\right)\\right) \\] Finally since \\(-\\log (x)=\\frac{1}{x}\\), we can re-write the point-wise cost above equivalently as \\[ g_{i}(\\mathbf{w})=\\log \\left(1+e^{-y_{i} \\mathbf{x}_{i}^{T} \\mathbf{w}}\\right) \\] The average of this point-wise cost over all \\(n\\) points we have the common Softmax cost for logistic regression: \\[ g(\\mathbf{w})=\\frac{1}{n} \\sum_{i=1}^{n} g_{i}(\\mathbf{w})=\\frac{1}{n} \\sum_{i=1}^{n} \\log \\left(1+e^{-y_{i} \\mathbf{x}_{i}^{T} \\mathbf{w}}\\right) \\] This will be helpful when we make the comparisons between logistic regression and support vector machines in Chapter 15. 5.2.3 Deviance You have probably noticed that the output from summary() reports the “deviance” measures for logistic regressions. The “Null deviance” is the deviance for the null model, that is, a model with no predictors. The null deviance shows how well the response variable is predicted by a model that includes only the intercept (grand mean). What is deviance? It is defined as the difference of likelihoods between the fitted model and the saturated model: \\[\\begin{equation} D=-2 \\ell(\\hat{\\beta})+2 \\ell(\\text { saturated model }) \\tag{5.12} \\end{equation}\\] This is also known as the Likelihood Ratio Test (LRT) that has been used to compare two nested models. \\[\\begin{equation} \\mathbf{L R T}=-2 \\log\\left(\\frac{L_{s}(\\hat{\\theta})}{L_{g}(\\hat{\\theta})}\\right) \\tag{5.13} \\end{equation}\\] where \\(L_s\\) in 5.13 is the likelihood for the null model and \\(L_g\\) is the likelihood for the alternative model. The perfect model, known as the saturated model, denotes an abstract model that fits perfectly the sample, that is, the model such that \\(P(Y=1 | \\mathbf{X}=\\mathbf{x})=Y_{i}\\). As the likelihood of the saturated model is exactly one, the deviance can be expressed as \\[ D=-2 \\ell(\\hat{\\beta}) \\] Therefore, the deviance is always larger than or equal to zero, which means a perfect fit. We can evaluate the magnitude of the deviance relative to the null deviance, \\[ D_{0}=-2 \\ell\\left(\\hat{\\beta}_{0}\\right), \\] reflecting the deviance of the worst model, which has no predictors. Hence, this comparison shows how much our fitted model has improved relative to the benchmark. We can develop a metric, the (Pseudo) \\(R^{2}\\) statistic: \\[\\begin{equation} R^{2}=1-\\frac{D}{D_{0}} \\tag{5.14} \\end{equation}\\] Similar to \\(R^{2}\\), the (Pseudo) \\(R^{2}\\) is a quantity between 0 and 1. If the fit is perfect, then \\(D = 0\\) and \\(R^{2}=1\\). 5.2.4 Predictive accuracy Another way of evaluating the model’s fit is to look at its predictive accuracy. When we are interested simply in prediction in classification, but not in predicting the value of \\(\\hat{p(x)}\\), such as \\[ \\hat{Y}=\\left\\{\\begin{array}{ll}{1,} &amp; {\\hat{p}\\left(x_{1}, \\ldots, x_{k}\\right)&gt;\\frac{1}{2}} \\\\ {0,} &amp; {\\hat{p}\\left(x_{1}, \\ldots, x_{k}\\right)&lt;\\frac{1}{2}}\\end{array}\\right. \\] then, the overall predictive accuracy can be summarized with a matrix, \\[ \\begin{array}{ccc}{\\text { Predicted vs. Reality}} &amp; {Y=1} &amp; {Y=0} \\\\ {\\hat{Y}=1} &amp; {\\text { TP }_{}} &amp; {\\text { FP }_{}} \\\\ {\\hat{Y}=0} &amp; {\\text { FN }_{}} &amp; {\\text { TN }_{}}\\end{array} \\] where, TP, FP, FN, TN are “True positives”, “False Positives”, “False Negatives”, “True Negatives”, respectively. This table is also know as Confusion Table. There are many metrics that can be calculated from this table to measure the accuracy of our classifier. We will spend more time on this this subject under Chapter 10 later. Intuitively, when \\(n=1\\), achieving head once (\\(k=1\\)) is \\(P(head)= p^{k}(1-p)^{1-k}=p\\) or \\(P(tail)= p^{k}(1-p)^{1-k}=1-p.\\)↩︎ "],["nonparametric-estimations---basics.html", "Chapter 6 Nonparametric Estimations - Basics 6.1 Density Estimations 6.2 Kernel regressions 6.3 Regression Splines 6.4 MARS - Multivariate Adaptive Regression Splines 6.5 GAM - Generalized Additive Model", " Chapter 6 Nonparametric Estimations - Basics The models we see in the previous chapters are parametric, which means that they have to assume a certain structure on the regression function \\(m\\) controlled by parameters before the estimations. Therefore, the results from parametric models are the best if the specification of \\(m\\) is correct. Avoiding this assumption is the strongest point of nonparametric methods, which do not require any hard-to-satisfy pre-determined regression functions. Before talking about a nonparametric estimator for the regression function \\(m\\), we should first look at a simple nonparametric density estimation of \\(X\\). We aim to estimate \\(f(x)\\) from a sample and without assuming any specific form for \\(f\\). 6.1 Density Estimations We will only look at one-variable Kernel density estimations. Let’s assume that a sample of \\(n\\) observations, \\(y_1,...,y_n\\), is drawn from a parametric distribution \\(f(y,\\theta)\\). If the data are i.i.d., the joint density function is: \\[\\begin{equation} f(y;\\theta)=\\prod_{i=1}^{n} f\\left(y_{i} ; \\theta\\right) \\tag{6.1} \\end{equation}\\] To estimate, we find the parameters that maximize this density function (“likelihood”), or its logarithmic transformation: \\[\\begin{equation} \\ell(y ; \\theta)=\\log f(y ; \\theta)=\\sum_{i=1}^{n} \\log f\\left(y_{i} ; \\theta\\right) \\tag{6.2} \\end{equation}\\] We apply the maximum likelihood estimation (MLE) method to recover \\(\\theta\\). This is called parametric estimation and if our pre-determined density model is not right, that is, if \\(f\\) is misspecified, we will have a biased estimator for \\(\\theta\\). To avoid this problem, we can use nonparametric estimation, which does not require an assumption about the distribution of the data. The starting point for a density estimation is a histogram. We define the intervals by choosing a number of bins and a starting value for the first interval. Here, we use 0 as a starting value and 10 bins: #Random integers from 1 to 100 set.seed(123) data &lt;- sample(1:100, 100, replace = TRUE) stem(data) ## ## The decimal point is 1 digit(s) to the right of the | ## ## 0 | 46777999 ## 1 | 23344456 ## 2 | 12335555677 ## 3 | 00111224456889 ## 4 | 01122337 ## 5 | 0012337 ## 6 | 003477999 ## 7 | 1222466899 ## 8 | 112366799 ## 9 | 0011123334566799 foo &lt;- hist(data, nclass = 10, col = &quot;lightblue&quot;, cex.main = 0.80, cex.axis = 0.75) foo$counts ## [1] 8 8 13 13 9 7 7 10 11 14 foo$density ## [1] 0.008 0.008 0.013 0.013 0.009 0.007 0.007 0.010 0.011 0.014 sum(foo$density) ## [1] 0.1 Not that the sum of these densities is not one. The vertical scale of a ‘frequency histogram’ shows the number of observations in each bin. From above, we know that the tallest bar has 14 observations, so this bar accounts for relative frequency 14/100=0.14 of the observations. As the relative frequency indicate probability their total would be 1. We are looking for a density function which gives the “height” of each observation. Since the width of this bar is 10, the density of each observation in the bin is 0.014. We can have a formula to calculate the density for each data point: \\[\\begin{equation} \\hat{f}(y)=\\frac{1}{n} \\times \\frac{\\text{ number of observations in the interval of } y}{\\text { width of the interval }} \\tag{6.3} \\end{equation}\\] Here is the pdf on the same data with binwidth = 4 for our example: # to put pdf and X&#39;s on the same graph, we scale the data foo &lt;- hist(data/(10*mean(data)), nclass = 25, cex.main = 0.80, cex.axis = 0.75, xlim = c(0, 0.2), main = NULL) lines(foo$mids, foo$density, col=&quot;blue&quot;, lwd = 2) #Naive The number of bins defines the degree of smoothness of the histogram. We can have the following general expression for a nonparametric density estimation: \\[\\begin{equation} f(x) \\cong \\frac{k}{n h} \\text { where }\\left\\{\\begin{array}{ll}{h} &amp; {\\text { binwidth }} \\\\ {n} &amp; {\\text { total } \\text { number of observation points }} \\\\ {k} &amp; {\\text { number of observations inside } h}\\end{array}\\right. \\tag{6.4} \\end{equation}\\] Note that, in practical density estimation problems, two basic approaches can be adopted: (1) we can fix \\(h\\) (width of the interval) and determine \\(k\\) in each bin from the data, which is the subject of this chapter and called kernel density estimation (KDE); or (2) we can fix \\(k\\) in each bin and determine \\(h\\) from the data. This gives rise to the k-nearest-neighbors (kNN) approach, which we cover in the next chapters. The global density cab be obtained with a moving window (intervals with intersections), which is also called as Naive estimator (a.k.a. Parzen windows). The naive estimator is not sensitive to the position of bins, but it is not smooth either: \\[\\begin{equation} \\hat{f}(y)=\\frac{1}{n h} \\sum_{i=1}^{n} I\\left(y-\\frac{h}{2}&lt;y_{i}&lt;y+\\frac{h}{2}\\right), \\tag{6.5} \\end{equation}\\] where \\(I(.)\\) is an indicator function, which results in value of 1 if the expression inside of the function is satisfied (0 otherwise). Thus, it counts the number of observations in a given window. The binwidth (\\(h\\)) defines the bin range by adding and subtracting \\(h/2\\) from \\(y\\). We can rearrange 6.5 differently: \\[ \\hat{f}(y)=\\frac{1}{2n h} \\sum_{i=1}^{n} I\\left(y-h&lt;y_{i}&lt;y+h\\right). \\] If we rewrite the inequality by subtracting \\(y\\) and divide it by \\(h\\): \\[ \\hat{f}(y)=\\frac{1}{2n h} \\sum_{i=1}^{n} I\\left(-1&lt;\\frac{y-y_{i}}{h}&lt;1\\right), \\] which can be written more compactly: \\[\\begin{equation} \\hat{f}(y)=\\frac{1}{2nh} \\sum_{i=1}^{n} w\\left(\\frac{y-y_{i}}{h}\\right) \\quad \\text { where } \\quad w(x)=\\left\\{\\begin{array}{ll}{1} &amp; {\\text { if }|x|&lt;1} \\\\ {0} &amp; {\\text { otherwise }}\\end{array}\\right. \\tag{6.6} \\end{equation}\\] Consider a sample \\(\\left\\{X_{i}\\right\\}_{i=1}^{10}\\), which is 4, 5, 5, 6, 12, 14, 15, 15, 16, 17. And the bin width is \\(h=4\\). What’s the density of 3, \\(\\hat{f}(3)\\)? Note that we do not have 3 in the data. \\[ \\begin{aligned} \\hat{f}(3) &amp;=\\frac{1}{2\\times10 \\times4}\\left\\{w\\left(\\frac{3-4}{4}\\right)+w\\left(\\frac{3-5}{4}\\right)+\\ldots+w\\left(\\frac{3-17}{4}\\right)\\right\\} \\\\ &amp;=\\frac{1}{80}\\left\\{1+1+1+1+0+\\ldots+0\\right\\} \\\\ &amp;=\\frac{1}{20} \\end{aligned} \\] This “naive” estimator yields density estimates that have discontinuities and weights equal at all points \\(x_i\\) regardless of their distance to the estimation point \\(x\\). In other words, in any given bin, \\(x\\)’s have a uniform distribution. That’s why, \\(w(x)\\) is commonly replaced with a smooth kernel function \\(K(x)\\). Kernel replaces it with usually, but not always, with a radially symmetric and unimodal pdf, such as the Gaussian. You can choose “gaussian”, “epanechnikov”, “rectangular”, “triangular”, “biweight”, “cosine”, “optcosine” distributions in the R’s density() function. With the Kernel density estimator replacing \\(w\\) in 6.6 by a kernel function \\(K\\): \\[\\begin{equation} \\hat{f}(y)=\\frac{1}{2n h} \\sum_{i=1}^{n} K\\left(\\frac{y-y_{i}}{h}\\right), \\tag{6.7} \\end{equation}\\] Here are the samples of kernels, \\(K(x)\\): \\[ \\text { Rectangular (Uniform): } ~~ K(x)=\\left\\{\\begin{array}{ll}{\\frac{1}{2}} &amp; {|x|&lt;1} \\\\ {0} &amp; {\\text { otherwise }}\\end{array}\\right. \\] \\[ \\text { Epanechnikov: } ~~ K(x)=\\left\\{\\begin{array}{cc}{\\frac{3}{4}\\left(1-\\frac{1}{5} x^{2}\\right) / \\sqrt{5}} &amp; {|x|&lt;\\sqrt{5}} \\\\ {0} &amp; {\\text { otherwise }}\\end{array}\\right. \\] \\[ \\text { Gaussian: } ~~ K(x)=\\frac{1}{\\sqrt{2 \\pi}} e^{(-1 / 2) x^{2}} \\] Although the kernel density estimator depends on the choices of the kernel function \\(K\\), it is very sensitive to \\(h\\), not to \\(K\\). In R, the standard kernel density estimation is obtained by density(), which uses Silverman rule-of-thumb to select the optimal bandwidth, \\(h\\), and the Gaussian kernel. Here is an example with our artificial data: X &lt;- readRDS(&quot;fes73.rds&quot;) X &lt;- X/mean(X) hist(X[X &lt; 3.5], nclass = 130, probability = TRUE, col = &quot;white&quot;, cex.axis = 0.75, cex.main = 0.8, main = NULL) lines(density(X, adjust = 1/4), col = &quot;red&quot;) # bandwith/4 lines(density(X, adjust = 1), col = &quot;blue&quot;) lines(density(X, adjust = 4), col = &quot;green&quot;) # bandwith x 4 lines(density(X, kernel = &quot;rectangular&quot;, adjust = 1/4), col = &quot;black&quot;) # bandwith x 4 #Here is the details of the last one density(X, adjust = 4) ## ## Call: ## density.default(x = X, adjust = 4) ## ## Data: X (6968 obs.); Bandwidth &#39;bw&#39; = 0.3423 ## ## x y ## Min. :-0.954 Min. :0.0000000 ## 1st Qu.: 2.352 1st Qu.:0.0000576 ## Median : 5.657 Median :0.0005510 ## Mean : 5.657 Mean :0.0755509 ## 3rd Qu.: 8.963 3rd Qu.:0.0269050 ## Max. :12.269 Max. :0.6282958 Bigger the bandwidth \\(h\\) smoother the pdf. Which one is better? There are several bandwidth selection methods to identify the best fitting \\(h\\), which are beyond the scope of this chapter. Why do we estimate pdf with KDE? Note that, when you explore our density object by str(), you’ll see that y will get you the pdf values of density for each value of \\(X\\) you have in our data. Of course pdf is a function: the values of pdf are \\(Y\\) and the input values are \\(X\\). Hence, given a new data point on \\(X\\), we may want to find the outcome of \\(Y\\) (the value of pdf for that data point) based on the function, the kernel density estimator that we have from the density() function result. When can do it with approxfun(): poo &lt;- density(X, adjust = 1/4) dens &lt;- approxfun(poo) plot(poo, col = &quot;blue&quot;) x_new &lt;- c(0.5,1.5,2.2) points(x_new, dens(x_new), col=2, lwd = 3) dens(1.832) ## [1] 0.1511783 This is a predicted value of pdf when \\(x=1.832\\) estimated by KDE without specifying the model apriori, which is like a magic! Based on the sample we have, we just predicted \\(Y\\) without explicitly modeling it. Keep in mind that our objective here is not to estimate probabilities. We can do it if we want. But then, of course we have to remember that values of a density curve are not the same as probabilities. Taking the integral of the desired section in the estimated pdf would give us the corresponding probability. integrate(dens, lower = 1, upper = 1.832) ## 0.3574063 with absolute error &lt; 5.8e-06 6.2 Kernel regressions Theoretically, nonparametric density estimation can be easily extended to several dimensions (multivariate distributions). For instance, suppose we are interested in predicting \\(Y\\) by using several predictors. We have almost no idea what functional form the predictive model could take. If we have a sufficiently large sample, we may obtain a reasonably accurate estimate of the joint probability density (by kernel density estimation or similar) of \\(Y\\) and the \\(X\\)’s. In practice, however, we rarely have enough sample to execute robust density estimations. As the dimension increases, KDE rapidly needs many more samples. Even in low dimensions, a KDE-based model has mostly no ability to generalize. In other words, if our test set has parts outside our training distribution, we cannot use our KDE-based model for forecasting. In regression functions, the outcome is the conditional mean of \\(Y\\) given \\(X\\)’s. Since nonparametric regressions are agnostic about the functional form between the outcome and the covariates, they are immune to misspecification error. The traditional regression model fits the model: \\[\\begin{equation} y=m(\\mathbf{x}, \\boldsymbol{\\beta})+\\varepsilon \\tag{6.8} \\end{equation}\\] where \\(\\beta\\) is a vector of parameters to be estimated, and x is a vector of predictors. The errors, \\(\\varepsilon\\) are assumed to be i.i.d, \\(\\varepsilon \\sim NID(0, \\sigma^2)\\). The function \\(m(\\mathbf{x},\\beta)\\), which links the conditional averages of \\(y\\) to the predictors, is specified in advance. The generic nonparametric regression model is written similarly, but the function \\(m\\) remains unspecified: \\[ \\begin{aligned} y &amp;=m(\\mathbf{x})+\\varepsilon \\\\ &amp;=m\\left(x_{1}, x_{2}, \\ldots, x_{p}\\right)+\\varepsilon, \\end{aligned} \\] where \\(\\varepsilon \\sim NID(0, \\sigma^2)\\) again. An important special case of the general model is nonparametric simple regression, where there is only one predictor: \\[ y=m(x)+\\varepsilon \\] With its definition, we can rewrite \\(m\\) as \\[\\begin{equation} \\begin{split} \\begin{aligned} m(x) &amp;=\\mathbb{E}[Y | X=x] \\\\ &amp;=\\int y f_{Y | X=x}(y) \\mathrm{d} y \\\\ &amp;=\\frac{\\int y f(x, y) \\mathrm{d} y}{f_{X}(x)} \\end{aligned} \\end{split} \\tag{6.9} \\end{equation}\\] This shows that the regression function can be computed from the joint density \\(f(x,y)\\) and the marginal \\(f(x)\\). Therefore, given a sample \\(\\left\\{\\left(X_{i}, Y_{i}\\right)\\right\\}_{i=1}^{n}\\), a nonparametric estimate of \\(m\\) may follow by replacing the these densities with their kernel density estimators, as we have see earlier in this section. A limitation of the bin smoothing approach in kernel density estimations is that we need small windows for the “approximately constant” assumptions to hold. As a result, we end up with a small number of data points to average and obtain imprecise estimates of \\(f(x)\\). Locally estimated scatter-plot smoothing (LOESS, loess) permits us to consider larger window sizes, which is a nonparametric approach that fits multiple regressions in local neighborhood. It is called local regression because, instead of assuming the function is approximately constant in a window, it fits a local regression at the “neighborhood” of \\(x_0\\). The distance from \\(x_0\\) is controlled by the span setting, which determines the width of the moving (sliding) window when smoothing the data. The parameter spanrepresents the proportion of the data (size of the sliding window) that is considered to be neighboring \\(x_0\\). For example, if N is the number of data points and span = 0.5, then for a given \\(x_0\\), loess will use the \\(0.5\\times N\\) closest points to \\(x_0\\) for the fit. Usually span should be between 0 and 1. When its larger than 1, the regression will be over-smoothed. Moreover, the weighting in the regression is proportional to \\(1-(\\text{distance}/\\text{maximum distance})^3)^3\\), which is called the Tukey tri-weight. Different than the Gaussian kernel, the Tukey tri-weight covers more points closer to the center point. We will not see the theoretical derivations of kernel regressions but an illustration of local polynomial of order 0, 1 and 2, below. (Examples from Ahamada and Flachaire) (Ibrahim_2011?). The Nadaraya–Watson estimator is a local polynomial of order 0, which estimates a local mean of \\(Y_1...Y_n\\) around \\(X=x_0\\). #Simulating our data n = 300 set.seed(1) x &lt;- sort(runif(n)*2*pi) y &lt;- sin(x) + rnorm(n)/4 plot(x, y) #Estimation loe0 &lt;- loess(y~x, degree = 0, span = 0.5) #Nadaraya-Watson loe1 &lt;- loess(y~x, degree = 1, span = 0.5) #Local linear loe2 &lt;- loess(y~x, degree = 2, span = 0.5) #Locally quadratic #To have a plot, we first calculate the fitted values on a grid, t &lt;- seq(min(x), max(x), length.out = 100) fit0 &lt;- predict(loe0, t) fit1 &lt;- predict(loe1, t) fit2 &lt;- predict(loe2, t) plot(x, y, col = &quot;gray&quot;, cex.main = 0.80, cex.axis = 0.75) lines(t, fit0, col = &quot;green&quot;, lwd = 3) lines(t, fit1, col = &quot;red&quot;) lines(t, fit2, col = &quot;blue&quot;) And, its sensitivity to the bandwidth: fit0 &lt;- predict(loess(y~x, degree=2, span = 0.05)) #minimum, 5%*300 = 14 obs. fit1 &lt;- predict(loess(y~x, degree=2, span = 0.75)) #default fit2 &lt;- predict(loess(y~x, degree=2, span = 2)) plot(x, y, col = &quot;gray&quot;, cex.main = 0.80, cex.axis = 0.75) lines(x, fit0, lwd = 2, col = &quot;green&quot;) lines(x, fit1, lwd = 2, col = &quot;red&quot;) lines(x, fit2, lwd = 2, col = &quot;blue&quot;) The bandwidth we choose will be determined by the prediction accuracy. This subject is related to cross-validation, which we will see later as a whole chapter. 6.3 Regression Splines In a model, non-linearity can be captured by estimating a linear regression through several intervals, which is called as piecewise linear model. \\[\\begin{equation} \\begin{split} \\begin{array}{ll}{y=\\alpha_{1}+\\beta_{1} x+\\varepsilon_{1}} &amp; {\\text { if } \\quad x \\in\\left[z_{0} ; z_{1}\\right]} \\\\ {y=\\alpha_{2}+\\beta_{2} x+\\varepsilon_{2}} &amp; {\\text { if } \\quad x \\in\\left[z_{1} ; z_{2}\\right]} \\\\ {\\cdots} \\\\ {y=\\alpha_{k}+\\beta_{k} x+\\varepsilon_{k}} &amp; {\\text { if } \\quad x \\in\\left[z_{k-1} ; z_{k}\\right]}\\end{array} \\end{split} \\tag{6.10} \\end{equation}\\] This function will not have a smooth transitions at the knots, \\(z.\\), which brings us to a regression spline, which is a piecewise regression model with a smooth transition at the knots. First, let’s see how a piecewise regression works with an example. To show evidence of nonlinearity between short and long-term interest rates, Pfann et al (1996) estimates the following piecewise linear model: \\[\\begin{equation} y=\\beta_{0}+\\beta_{1} x+\\beta_{2}(x-\\kappa)_{+}+\\varepsilon \\tag{6.11} \\end{equation}\\] Here in 6.12 the \\(\\kappa\\) denotes knot where the relationship between \\(Y\\) and \\(x\\) changes. (Subscript \\(+\\) means that the term will be zero when it is not positive). data &lt;- read.table(&quot;irates.dat&quot;, header = TRUE) y &lt;- data$GS10 x &lt;- data$TB3MS xk &lt;- (x - 10.8)*(x &gt; 10.8) summary(lm(y ~ x + xk)) ## ## Call: ## lm(formula = y ~ x + xk) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.3978 -0.9051 -0.1962 0.9584 3.2530 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.77900 0.10181 17.47 &lt; 2e-16 *** ## x 0.92489 0.01915 48.29 &lt; 2e-16 *** ## xk -0.42910 0.08958 -4.79 2.06e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.107 on 657 degrees of freedom ## Multiple R-squared: 0.83, Adjusted R-squared: 0.8295 ## F-statistic: 1604 on 2 and 657 DF, p-value: &lt; 2.2e-16 reorder &lt;- order(x) # to avoid messy lines plot( x[reorder], fitted(lm(y ~ x + xk))[reorder], type = &quot;l&quot;, col = &quot;red&quot;, cex.main = 0.80, cex.axis = 0.75, xlab = &quot;TB3MS&quot;, ylab = &quot;GS10&quot; ) points(x, y, col = &quot;grey&quot;) abline(v = 10.8, lty = 2, col = &quot;darkgreen&quot;) When the model is extended to \\(q\\) knots, it becomes a piecewise regression: \\[\\begin{equation} y=\\beta_{0}+\\beta_{1} x+\\sum_{j=1}^{q} \\beta_{1 j}\\left(x-\\kappa_{j}\\right)_{+}+\\varepsilon \\tag{6.12} \\end{equation}\\] #5 knots k &lt;- c(2.8, 4.8, 6.8, 8.8, 10.8) Xk &lt;- x - matrix(k, length(x), length(k), byrow = TRUE) Xk &lt;- Xk*(Xk &gt; 0) reorder &lt;- order(x) # to avoid messy lines plot( x, y, col = &quot;gray&quot;, ylim = c(2.5, 14.5), cex.main = 0.80, cex.axis = 0.75, xlab = &quot;TB3MS&quot;, ylab = &quot;GS10&quot; ) lines(x[reorder], fitted(lm(y ~ x + Xk))[reorder], lwd = 2, col = &quot;red&quot;) abline(v = k, lty = 2, col = &quot;darkgreen&quot;) The piecewise linear model is not smooth at the knots. To get a smooth estimator, we replace the basis of linear functions by a basis of spline functions, which is defined as \\[ b_{0}(x, \\kappa), \\ldots, b_{r}(x, \\kappa) \\] Hence, regression spline is defined as: \\[\\begin{equation} y=\\sum_{j=0}^{r} \\beta_{j} b_{j}(x, \\kappa)+\\varepsilon \\tag{6.13} \\end{equation}\\] Once the knots are fixed, it becomes essentially a parametric regression. For example, the following spline regression model, \\[ y=\\beta_{0}+\\beta_{1} x+\\cdots+\\beta_{p} x^{p}+\\sum_{j=1}^{q} \\beta_{p j}\\left(x-\\kappa_{j}\\right)_{+}^{p}+\\varepsilon, \\] can be estimated as a cubic spline: \\[ \\hat{m}(x)=2+x-2 x^{2}+x^{3}+(x-0.4)_{+}^{3}-(x-0.8)_{+}^{3} \\] which can be rewritten as \\[ \\hat{m}(x)=\\left\\{\\begin{aligned} 2+x-2 x^{2}+x^{3} &amp; \\text { if } &amp; x&lt;0.4 \\\\ 2+x-2 x^{2}+x^{3}+(x-0.4)^{3} &amp; \\text { if } &amp; 0.4 \\leq x&lt;0.8 \\\\ 2+x-2 x^{2}+x^{3}+(x-0.4)^{3}-(x-0.8)^{3} &amp; \\text { if } &amp; x \\geq 0.8 \\end{aligned}\\right. \\] In short a spline is a piecewise polynomial function. Now, the question is how we are supposed to choose the basis and the knots? Spline estimation is sensitive to the choice of the number of knots and their position. A knot can have an economic interpretation such as a specific date or structural change in the data, thus some information is required. There are two common approaches for choosing the position of the knots: quantiles - intervals with the same number of observations; equidistant - intervals with the same width. As for the number of knots, if it’s too small, the potential bias can be large in the estimator, so a larger number is preferred. Let’s use the same data and apply regression spline. Here is the example for equidistant knots: library(splines) #equidistant knots nknots &lt;- 5 k = seq(min(x), max(x), length.out = nknots + 2)[2:(nknots + 1)] model1 &lt;- lm(y ~ bs(x, degree = 3, knots = k)) #check ?bs summary(model1) ## ## Call: ## lm(formula = y ~ bs(x, degree = 3, knots = k)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.5190 -0.8537 -0.1889 0.8841 3.2169 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.1855 0.3160 10.082 &lt; 2e-16 *** ## bs(x, degree = 3, knots = k)1 0.4081 0.5722 0.713 0.47596 ## bs(x, degree = 3, knots = k)2 0.9630 0.3163 3.045 0.00242 ** ## bs(x, degree = 3, knots = k)3 4.2239 0.4311 9.798 &lt; 2e-16 *** ## bs(x, degree = 3, knots = k)4 6.2233 0.3869 16.084 &lt; 2e-16 *** ## bs(x, degree = 3, knots = k)5 9.9021 0.6620 14.957 &lt; 2e-16 *** ## bs(x, degree = 3, knots = k)6 9.8107 0.8370 11.722 &lt; 2e-16 *** ## bs(x, degree = 3, knots = k)7 10.8604 0.9400 11.553 &lt; 2e-16 *** ## bs(x, degree = 3, knots = k)8 10.6991 1.0866 9.847 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.086 on 651 degrees of freedom ## Multiple R-squared: 0.8378, Adjusted R-squared: 0.8358 ## F-statistic: 420.3 on 8 and 651 DF, p-value: &lt; 2.2e-16 u &lt;- seq(min(x), max(x), length.out = 100) pred &lt;- predict(model1, newdata = list(x = u), se = TRUE) reorder &lt;- order(u) # to avoid messy lines plot( x, y, col = &quot;gray&quot;, ylim = c(2.5, 14.5), cex.main = 0.80, cex.axis = 0.75 ) lines(u, pred$fit, lwd = 2, col = &quot;red&quot;) lines(u[reorder], pred$fit[reorder] + 1.96 * pred$se, lty = &quot;dashed&quot;) lines(u[reorder], pred$fit[reorder] - 1.96 * pred$se, lty = &quot;dashed&quot;) abline(v = k, lty = 2, col = &quot;darkgreen&quot;) And, an example for quantile knots: model2 &lt;- lm(y ~ bs(x, degree = 3, df = 8)) u &lt;- seq(min(x), max(x), length.out = 100) pred &lt;- predict(model2, newdata = list(x = u), se = TRUE) reorder &lt;- order(u) # to avoid messy lines plot( x, y, col = &quot;gray&quot;, ylim = c(2.5, 14.5), cex.main = 0.80, cex.axis = 0.75 ) lines(u, pred$fit, lwd = 2, col = &quot;red&quot;) lines(u[reorder], pred$fit[reorder] + 1.96 * pred$se, lty = &quot;dashed&quot;) lines(u[reorder], pred$fit[reorder] - 1.96 * pred$se, lty = &quot;dashed&quot;) k &lt;- attr(bs(x, degree = 3, df = 8), &quot;knots&quot;) #These functions provide access to a single attribute of an object. abline(v = k, lty = 2, col = &quot;darkgreen&quot;) Note that, in regression spline, df (degree of freedom) is the number of components of the basis. Thus, with cubic spline, df=6 defines 3 knots (quartiles q:25; q:50; q:75). Since, df= 8, we have 5 knots, so is the quantile of 20%. Is bad or good for prediction to have a very large number of knots? See https://freakonometrics.hypotheses.org/47681 (Charpentier_SplineReg?), for the argument about the number of knots. Here is Arthur Charpentier’s conclusion: So, it looks like having a lot of non-significant components in a spline regression is not a major issue. And reducing the degrees of freedom is clearly a bad option. Let’s see how sensitive the results to the number of knots 2: pred1 &lt;- predict(lm(y ~ bs(x, degree = 3, df = 6))) #quartiles pred2 &lt;- predict(lm(y ~ bs(x, degree = 3, df = 12))) #deciles pred3 &lt;- predict(lm(y ~ bs(x, degree = 3, df = 102))) #percentile reorder &lt;- order(x) plot(x, y, col = &quot;gray&quot;, ylim = c(2.5, 14.5), cex.main = 0.80, cex.axis = 0.75) lines(x[reorder], pred1[reorder], lwd = 2, col = &quot;red&quot;) lines(x[reorder], pred2[reorder], lwd = 2, col = &quot;blue&quot;) lines(x[reorder], pred3[reorder], lwd = 2, col = &quot;green&quot;) There is a method called as smoothing spline, a spline basis method that avoids the knot selection problem. It uses a maximal set of knots, at the unique values of the each \\(X\\) values and control for the fit by regularization. Similar to OLS, it selects \\(\\beta_j\\) to minimize the residual sum of squares but with a penalization on the curvature in the function: \\[\\begin{equation} \\sum_{i=1}^{n}\\left[y_{i}-m\\left(x_{i}\\right)\\right]^{2}+\\lambda \\int\\left[m^{\\prime \\prime}(x)\\right]^{2} d x \\tag{6.14} \\end{equation}\\] The first term minimizes the closeness to the data with a constraint (the second term) on the curvature in the function. If \\(\\lambda=0\\), \\(m(x_i)\\) could be any function that fits the data very closely (interpolates the data). If \\(\\lambda &gt; 0\\) and goes infinity, it makes the penalization so high that the algorithm fits a simple least squares line without any curvature. The penalty term, or bandwidth \\(\\lambda\\), restricts fluctuations of \\(\\hat{m}\\) and the optimum \\(\\lambda\\) minimizes the distance between \\(m\\), which is unknown, and \\(\\hat{m}\\). The method used to find the optimal \\(\\lambda\\) is called as generalized cross-validation. Here is a simulation: set.seed(1) n &lt;- 200 x &lt;- runif(n) dgm &lt;- sin(12*(x + 0.2))/(x + 0.2) # our dgm y &lt;- dgm + rnorm(n) plot(x, y, col = &quot;gray&quot;, cex.main = 0.80, cex.axis = 0.75) lines(x[order(x)], dgm[order(x)], lwd = 2, col = &quot;black&quot;) # DGM lines(smooth.spline(x,y, df = 20), lwd = 2, col = &quot;red&quot;) lines(smooth.spline(x,y, df = 40), lwd = 2, col = &quot;blue&quot;) And when we use automated selection of knots: plot(x, y, col = &quot;gray&quot;, cex.main = 0.80, cex.axis = 0.75) lines(x[order(x)], dgm[order(x)], lwd = 2, col = &quot;black&quot;) # DGM #lines(smooth.spline(x,y, cv = FALSE), lwd = 2, col = &quot;blue&quot;) # With GCV lines(smooth.spline(x,y), lwd = 2, col = &quot;red&quot;) # With LOOCV Note that there are other packages for smoothing splines like npreg that uses ss()3. In theory, nonparametric regression estimations can be easily extended to several regressors but in practice, good precision may require a huge number of observations without any graphical tools to make interpretation. Because it is difficult to fit the nonparametric regression model when there are many predictors, more restrictive models have been developed. One such model is the additive regression model, \\[\\begin{equation} y=\\beta_{0}+m_{1}\\left(x_{1}\\right)+m_{2}\\left(x_{2}\\right)+\\cdots+m_{p}\\left(x_{p}\\right)+\\varepsilon \\tag{6.15} \\end{equation}\\] Variations on the additive regression model include semiparametric models, in which some of the predictors enter linearly or interactively. There are two common methods that have been used in multivariable settings: GAM (generalized additive regression splines) and MARS (multivariate adaptive regression splines). 6.4 MARS - Multivariate Adaptive Regression Splines Linear models can incorporate nonlinear patterns in the data by manually adding squared terms and interaction effects, if we know the specific nature of the nonlinearity in advance. Although we can extend linear models to capture nonlinear relationships by including polynomial terms, it is generally unusual to use degree greater than 3 or 4. Even if we use higher degrees, with multiple interactions and polynomials, the dimension of the model goes out of control. Although useful, the typical implementation of polynomial regression requires the user to explicitly identify and incorporate which variables should have what specific degree polynomials and interactions. With data sets that can easily contain 50, 100, or more variables today, this would require an enormous time to determine the explicit structure of nonlinear nature of the model. Multivariate adaptive regression splines (MARS) can be a solution to capture the nonlinearity aspect of polynomial regression by assessing cutpoints (knots) like in a piecewise regression model. For example,in a simple one-variable model, the procedure will first look for the single point across the range of \\(X\\) values where two different linear relationships between \\(Y\\) and \\(X\\) achieve the smallest error. The results is known as a hinge function \\(\\text{max}(0,x−a)\\) where \\(a\\) is the cutpoint value. Once the first knot has been found, the search continues for a second knot, which results in three linear models. This procedure can continue until many knots are found, producing a highly nonlinear pattern. Once the full set of knots have been created, we can sequentially remove knots that do not contribute significantly to predictive accuracy. This process is known as pruning and can be done by cross-validation. Here is a simple application with the Longley dataset (in the datasets package) that describes seven economic variables observed from 1947 to 1962 used to predict the number of people employed yearly. library(earth) # load data data(longley) summary(longley) ## GNP.deflator GNP Unemployed Armed.Forces ## Min. : 83.00 Min. :234.3 Min. :187.0 Min. :145.6 ## 1st Qu.: 94.53 1st Qu.:317.9 1st Qu.:234.8 1st Qu.:229.8 ## Median :100.60 Median :381.4 Median :314.4 Median :271.8 ## Mean :101.68 Mean :387.7 Mean :319.3 Mean :260.7 ## 3rd Qu.:111.25 3rd Qu.:454.1 3rd Qu.:384.2 3rd Qu.:306.1 ## Max. :116.90 Max. :554.9 Max. :480.6 Max. :359.4 ## Population Year Employed ## Min. :107.6 Min. :1947 Min. :60.17 ## 1st Qu.:111.8 1st Qu.:1951 1st Qu.:62.71 ## Median :116.8 Median :1954 Median :65.50 ## Mean :117.4 Mean :1954 Mean :65.32 ## 3rd Qu.:122.3 3rd Qu.:1958 3rd Qu.:68.29 ## Max. :130.1 Max. :1962 Max. :70.55 # fit model fit1 &lt;- earth(Employed ~ ., longley) summary(fit1) ## Call: earth(formula=Employed~., data=longley) ## ## coefficients ## (Intercept) -1682.60259 ## Year 0.89475 ## h(293.6-Unemployed) 0.01226 ## h(Unemployed-293.6) -0.01596 ## h(Armed.Forces-263.7) -0.01470 ## ## Selected 5 of 8 terms, and 3 of 6 predictors ## Termination condition: GRSq -Inf at 8 terms ## Importance: Year, Unemployed, Armed.Forces, GNP.deflator-unused, ... ## Number of terms at each degree of interaction: 1 4 (additive model) ## GCV 0.2389853 RSS 0.7318924 GRSq 0.9818348 RSq 0.996044 # summarize the importance of input variables evimp(fit1) ## nsubsets gcv rss ## Year 4 100.0 100.0 ## Unemployed 3 24.1 23.0 ## Armed.Forces 2 10.4 10.8 #plot plot(fit1, which = 1, cex.main = 0.80, cex.axis = 0.75) # make predictions predictions1 &lt;- predict(fit1, longley) # summarize accuracy for fit1 mse &lt;- mean((longley$Employed - predictions1)^2) #Remember this an in-sample fit. mse ## [1] 0.04574327 The figure illustrates the model selection with GCV (generalized cross-validation) \\(R^2\\) based on the number of terms retained in the model. These retained terms are constructed from original predictors (right-hand y-axis). The vertical dashed line at 5 indicates the optimal number of non-intercept terms retained where marginal increases in GCV R2 are less than 0.001. Let’s use another data, the Ames Housing data, which is available by AmesHousing package. library(AmesHousing) # Fit a basic MARS model amesdata &lt;- make_ames() ames1 &lt;- earth(Sale_Price ~ ., data = amesdata) ames2 &lt;- earth(Sale_Price ~ ., data = amesdata, degree = 2) # In addition to pruning the number of knots, # we can also assess potential interactions between different hinge functions. # In the 2nd model, &quot;degree = 2&quot; argument allows level-2 interactions. summary(ames1) ## Call: earth(formula=Sale_Price~., data=amesdata) ## ## coefficients ## (Intercept) 279490.795 ## NeighborhoodNorthridge_Heights 16810.853 ## NeighborhoodCrawford 24189.424 ## NeighborhoodNorthridge 27618.337 ## NeighborhoodStone_Brook 31410.387 ## NeighborhoodGreen_Hills 109592.758 ## Condition_2PosN -96442.914 ## Overall_QualGood 12990.668 ## Overall_QualVery_Good 34907.970 ## Overall_QualExcellent 84380.868 ## Overall_QualVery_Excellent 125196.226 ## Overall_CondFair -23679.636 ## Overall_CondGood 11521.886 ## Overall_CondVery_Good 14138.461 ## Bsmt_ExposureGd 11893.023 ## FunctionalTyp 17341.390 ## h(15431-Lot_Area) -1.749 ## h(Lot_Area-15431) 0.301 ## h(2003-Year_Built) -426.978 ## h(Year_Built-2003) 4212.701 ## h(1972-Year_Remod_Add) 253.232 ## h(Year_Remod_Add-1972) 486.266 ## h(1869-Bsmt_Unf_SF) 19.399 ## h(Bsmt_Unf_SF-1869) -121.684 ## h(Total_Bsmt_SF-1822) 125.954 ## h(2452-Total_Bsmt_SF) -31.670 ## h(Total_Bsmt_SF-2452) -221.022 ## h(Second_Flr_SF-1540) 320.816 ## h(Gr_Liv_Area-3005) 237.824 ## h(3228-Gr_Liv_Area) -50.647 ## h(Gr_Liv_Area-3228) -316.547 ## h(Kitchen_AbvGr-1) -22620.827 ## h(1-Fireplaces) -5701.130 ## h(Fireplaces-1) 8654.214 ## h(2-Garage_Cars) -5290.463 ## h(Garage_Cars-2) 11400.346 ## h(210-Screen_Porch) -55.241 ## ## Selected 37 of 40 terms, and 26 of 308 predictors ## Termination condition: RSq changed by less than 0.001 at 40 terms ## Importance: Gr_Liv_Area, Year_Built, Total_Bsmt_SF, Overall_QualExcellent, ... ## Number of terms at each degree of interaction: 1 36 (additive model) ## GCV 506531262 RSS 1.411104e+12 GRSq 0.9206569 RSq 0.9245098 summary(ames2) ## Call: earth(formula=Sale_Price~., data=amesdata, degree=2) ## ## coefficients ## (Intercept) 304004.163 ## NeighborhoodGreen_Hills 107542.815 ## Overall_QualGood 28295.297 ## Overall_QualVery_Good 50500.728 ## Overall_QualExcellent 80054.922 ## Overall_QualVery_Excellent 115273.427 ## Bsmt_ExposureGd 11761.126 ## h(5400-Lot_Area) -4.428 ## h(Lot_Area-5400) 3.752 ## h(2003-Year_Built) -497.006 ## h(Year_Built-2003) 7976.946 ## h(Year_Remod_Add-1974) 957.791 ## h(2452-Total_Bsmt_SF) -54.823 ## h(Total_Bsmt_SF-2452) 49.902 ## h(3228-Gr_Liv_Area) -44.151 ## h(Gr_Liv_Area-3228) 197.513 ## h(2-Fireplaces) -6761.928 ## h(Lot_Area-5400) * Overall_CondFair -2.710 ## NeighborhoodCrawford * h(2003-Year_Built) 399.860 ## Overall_QualAverage * h(2452-Total_Bsmt_SF) 6.310 ## Overall_QualAbove_Average * h(2452-Total_Bsmt_SF) 11.542 ## Overall_QualVery_Good * h(Bsmt_Full_Bath-1) 49827.988 ## Overall_QualVery_Good * h(1-Bsmt_Full_Bath) -12863.190 ## Overall_CondGood * h(3228-Gr_Liv_Area) 4.782 ## Mas_Vnr_TypeStone * h(Gr_Liv_Area-3228) -512.416 ## h(Lot_Area-19645) * h(2452-Total_Bsmt_SF) -0.001 ## h(Lot_Area-5400) * h(Half_Bath-1) -3.867 ## h(Lot_Area-5400) * h(1-Half_Bath) -0.397 ## h(Lot_Area-5400) * h(Open_Porch_SF-195) -0.011 ## h(Lot_Area-5400) * h(195-Open_Porch_SF) -0.005 ## h(Lot_Area-5400) * h(192-Screen_Porch) -0.008 ## h(2003-Year_Built) * h(Total_Bsmt_SF-1117) -0.729 ## h(2003-Year_Built) * h(1117-Total_Bsmt_SF) 0.368 ## h(Year_Built-2003) * h(2439-Gr_Liv_Area) -5.516 ## h(Year_Remod_Add-1974) * h(Mas_Vnr_Area-14) 1.167 ## h(Year_Remod_Add-1974) * h(14-Mas_Vnr_Area) 17.544 ## h(Year_Remod_Add-1974) * h(Gr_Liv_Area-1627) 1.067 ## h(Year_Remod_Add-1974) * h(932-Garage_Area) -1.132 ## h(Year_Remod_Add-1974) * h(Longitude- -93.6278) -19755.291 ## h(Year_Remod_Add-1974) * h(-93.6278-Longitude) -7450.926 ## h(1191-Bsmt_Unf_SF) * h(3228-Gr_Liv_Area) 0.009 ## h(Bsmt_Unf_SF-1191) * h(3228-Gr_Liv_Area) -0.028 ## ## Selected 42 of 49 terms, and 26 of 308 predictors ## Termination condition: RSq changed by less than 0.001 at 49 terms ## Importance: Gr_Liv_Area, Year_Built, Total_Bsmt_SF, Overall_QualExcellent, ... ## Number of terms at each degree of interaction: 1 16 25 ## GCV 415202608 RSS 1.132115e+12 GRSq 0.9349626 RSq 0.9394349 # predictions predictions1 &lt;- predict(ames1, amesdata) predictions2 &lt;- predict(ames2, amesdata) # summarize accuracy for ames1 and ames2 mse &lt;- mean(sqrt((amesdata$Sale_Price - predictions1)^2)) mse ## [1] 15345.66 mse &lt;- mean(sqrt((amesdata$Sale_Price - predictions2)^2)) mse ## [1] 13910.27 Now the second model includes interaction terms between multiple hinge functions. For example, \\(h(Year\\_Built-2003) \\times h(Gr\\_Liv\\_Area-2274)\\) is an interaction effect for those houses built prior to 2003 and have less than 2,274 square feet of living space above ground. There are two tuning parameters with a MARS model: the degree of interactions and nprune - the number of retained terms. These parameters are called hyperparameters and we need to perform a grid search to find the best combination that maximizes the prediction accuracy. We will have chapter on this subject with examples later. For now, we will have a simple grid search with the caret package, which provides the most comprehensive machine learning library in R. library(caret) library(ggplot2) library(vip) # Grid to search grid &lt;- expand.grid( degree = 1:3, nprune = seq(2, 100, length.out = 10) %&gt;% floor()) head(grid) ## degree nprune ## 1 1 2 ## 2 2 2 ## 3 3 2 ## 4 1 12 ## 5 2 12 ## 6 3 12 # Training set.seed(123) mars &lt;- train( x = subset(amesdata, select = -Sale_Price), y = amesdata$Sale_Price, method = &quot;earth&quot;, metric = &quot;RMSE&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10), tuneGrid = grid ) mars$bestTune ## nprune degree ## 5 45 1 ggplot(mars) vip(mars, num_features = 40, bar = FALSE, value = &quot;gcv&quot;) + ggtitle(&quot;GCV&quot;) How does this compare to some other linear models for the Ames housing data? That’s the main question that we will ask in related chapters covering other machine learning models. 6.5 GAM - Generalized Additive Model Generalized Additive Model (GAM) is another method for discovering non-linear relationships in a multivariate setting. The performance difference between MARS and GAM is well explained by Leathwich, Elith, and Hastie (Leath_2006?). Here is an excerpt from their paper (Page 189): Two other commonly used techniques capable of fitting non-linear relationships (…) are neural nets and classification and regression trees. A third alternative, multivariate adaptive regression splines (MARS), has shown promise in recent comparative studies. This technique combines the strengths of regression trees and spline fitting by replacing the step functions normally associated with regression trees with piecewise linear basis functions. This allows the modelling of complex relationships between a response variable and its predictors. In practical terms, MARS has exceptional analytical speed, and its simple rule-based basis functions facilitate the prediction of species distributions using independent data. And from their abstract: Results indicate little difference between the performance of GAM and MARS models, even when MARS models included interaction terms between predictor variables. Results from MARS models are much more easily incorporated into other analyses than those from GAM models. The strong performance of a MARS multiresponse model, particularly for species of low prevalence, suggests that it may have distinct advantages for the analysis of large datasets. GAM uses an iterative estimation process to the following generalized additive model by assuming \\(m\\) can be decompose as a sum of several functions of dimension one or two (or more): \\[\\begin{equation} y=m_{1}\\left(x_{1}\\right)+m_{2}\\left(x_{2}\\right)+\\cdots+m_{k}\\left(x_{k}\\right)+\\varepsilon \\tag{6.16} \\end{equation}\\] The estimation to this 2-variable additive model \\(y=m_{1}\\left(x_{1}\\right)+m_{2}\\left(x_{2}\\right)+\\varepsilon\\) can be done by the following iterative procedure: Select initial estimates \\(m_1^{(0)}\\) and \\(m_2^{(0)}\\) Obtain \\(\\hat{m}_1^{(i)}\\) by regressing \\(y-\\hat{m}_2^{(i-1)}\\) on \\(x_1\\) Obtain \\(\\hat{m}_2^{(i)}\\) by regressing \\(y-\\hat{m}_1^{(i-1)}\\) on \\(x_2\\) Repeat steps 2 an 3 until no significant changes Initial estimates can be equal to 0 or obtained by OLS. An important advantage of GAM is that an extension to more than two functions, which kernel or spline methods could be used for \\(m(x)\\), does not lead to the curse of dimensionality problem. Let’s consider the following estimation using a housing data set described below: \\[ \\log (\\text { price })=X \\beta+m_{1}(\\text { green })+m_{2}(\\text { coord } 1)+m_{3}(\\text { coord } 2)+\\varepsilon, \\] where price is the housing price - 1135 observations, for 1995 in Brest; \\(X\\) dummies are Studio, T1, T2, T3, T4, T5, house, parking defining the type of building and whether the parking lot exits or not; green is the distance to the closest green park; and coord1, coord2 are geographical coordinates (location). data &lt;- read.table(&quot;hedonic.dat&quot;, header = TRUE) attach(data) # note that this is not advisable but I use it in this example library(mgcv) # Note that a nonlinear transformation of a dummy variable is still a dummy. # let&#39;s add them in X vector X &lt;- cbind(T1, T2, T3, T4, T5, HOUSE, PARKING) gam1 &lt;- gam(LPRIX ~ X + s(GREEN) + s(COORD1) + s(COORD2)) # s() defines smooths in GAM formulae summary(gam1) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## LPRIX ~ X + s(GREEN) + s(COORD1) + s(COORD2) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.79133 0.04698 101.997 &lt; 2e-16 *** ## XT1 0.06974 0.05737 1.216 0.224 ## XT2 0.38394 0.05173 7.421 2.41e-13 *** ## XT3 0.75105 0.05025 14.946 &lt; 2e-16 *** ## XT4 0.97310 0.05138 18.939 &lt; 2e-16 *** ## XT5 1.13707 0.05666 20.070 &lt; 2e-16 *** ## XHOUSE 0.23965 0.03273 7.321 4.91e-13 *** ## XPARKING 0.22890 0.02400 9.538 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(GREEN) 5.539 6.745 2.552 0.0111 * ## s(COORD1) 8.289 8.848 7.523 &lt;2e-16 *** ## s(COORD2) 8.323 8.869 7.182 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.672 Deviance explained = 68.1% ## GCV = 0.10301 Scale est. = 0.10011 n = 1070 # plot nonparametric components par(mfrow = c(1, 3)) plot(gam1, shade = TRUE, shade.col = &quot;pink&quot;, ylim = c(-0.5, 0.7)) These figures do not suggest how a simple parametric modeling would be misleading. From the results, we can make standard interpretations: with similar other characteristics: (1) on average, a studio costs 120.421 francs (\\(e^{4.791}\\)); (2) a T2 is expected to cost 38.4% more than a studio; (3) a house is expected to cost 23.9% more than an apartment. We also have results on the nonparametric components. The \\(p\\)-values correspond to test \\(H_0\\) : linear vs. \\(H_1\\) : nonlinear relationship. If geographical location is assumed highly nonlinear, we should consider a more flexible model: \\[ \\log (\\text { price })=X \\beta+m_{1}(\\text { green })+m_{2}(\\text { coord } 1, \\text { coord } 2)+\\varepsilon, \\] where the spatial dependence is specified fully nonparametrically. gam2 &lt;- gam(LPRIX ~ X + s(GREEN) + s(COORD1, COORD2)) summary(gam2) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## LPRIX ~ X + s(GREEN) + s(COORD1, COORD2) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.77597 0.04641 102.916 &lt; 2e-16 *** ## XT1 0.08030 0.05628 1.427 0.154 ## XT2 0.38691 0.05102 7.583 7.50e-14 *** ## XT3 0.76278 0.04959 15.383 &lt; 2e-16 *** ## XT4 0.99325 0.05079 19.555 &lt; 2e-16 *** ## XT5 1.13897 0.05594 20.361 &lt; 2e-16 *** ## XHOUSE 0.23827 0.03247 7.339 4.36e-13 *** ## XPARKING 0.24428 0.02426 10.069 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(GREEN) 6.487 7.626 2.074 0.0445 * ## s(COORD1,COORD2) 24.063 27.395 6.714 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.685 Deviance explained = 69.6% ## GCV = 0.099791 Scale est. = 0.096196 n = 1070 vis.gam(gam2, view =c(&quot;COORD1&quot;, &quot;COORD2&quot;), phi = 20, main = bquote(m[2](coord1) + m[3](coord2))) If we define them linearly, gam3 &lt;- gam(LPRIX ~ X + s(GREEN) + COORD1 + COORD2) summary(gam3) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## LPRIX ~ X + s(GREEN) + COORD1 + COORD2 ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.707e+00 1.101e+00 6.093 1.55e-09 *** ## XT1 5.972e-02 6.003e-02 0.995 0.320062 ## XT2 3.742e-01 5.426e-02 6.896 9.22e-12 *** ## XT3 7.450e-01 5.222e-02 14.267 &lt; 2e-16 *** ## XT4 9.475e-01 5.293e-02 17.901 &lt; 2e-16 *** ## XT5 1.127e+00 5.845e-02 19.285 &lt; 2e-16 *** ## XHOUSE 2.683e-01 3.254e-02 8.245 4.86e-16 *** ## XPARKING 2.397e-01 2.442e-02 9.817 &lt; 2e-16 *** ## COORD1 2.046e-05 7.256e-06 2.819 0.004903 ** ## COORD2 -3.854e-05 1.150e-05 -3.351 0.000834 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(GREEN) 6.895 8.028 2.809 0.00469 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.637 Deviance explained = 64.2% ## GCV = 0.11277 Scale est. = 0.11099 n = 1070 vis.gam(gam3, view =c(&quot;COORD1&quot;, &quot;COORD2&quot;), phi = 20, main = bquote(beta[1] * &#39;coord1&#39; + beta[2] * &#39;coord2&#39;)) GAMs provide a useful compromise between linear and fully nonparametric models. Here is the linear specification with GAM: gam4 &lt;- gam(LPRIX ~ X + GREEN + COORD1 + COORD2) summary(gam4) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## LPRIX ~ X + GREEN + COORD1 + COORD2 ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.211e+00 1.096e+00 6.577 7.54e-11 *** ## XT1 6.455e-02 6.042e-02 1.068 0.285597 ## XT2 3.859e-01 5.444e-02 7.089 2.47e-12 *** ## XT3 7.548e-01 5.235e-02 14.418 &lt; 2e-16 *** ## XT4 9.576e-01 5.309e-02 18.037 &lt; 2e-16 *** ## XT5 1.141e+00 5.860e-02 19.462 &lt; 2e-16 *** ## XHOUSE 2.741e-01 3.274e-02 8.371 &lt; 2e-16 *** ## XPARKING 2.417e-01 2.454e-02 9.850 &lt; 2e-16 *** ## GREEN -4.758e-05 4.470e-05 -1.064 0.287435 ## COORD1 1.909e-05 7.111e-06 2.685 0.007365 ** ## COORD2 -4.219e-05 1.148e-05 -3.674 0.000251 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## R-sq.(adj) = 0.63 Deviance explained = 63.3% ## GCV = 0.11418 Scale est. = 0.113 n = 1070 detach(data) GAM becomes an important tool to understand whether model should be estimated as a linear function or a complex nonlinear function. We can even see and test if a specific variable should have a nonlinear part. For example, if we want to know coordinates should be included additively, linear, or nonparametrically, we can use GAM and then compare performances of those different models and make a decision. GAMs are based on a hypothesis of additive separability. They are helpful for reducing the dimension of the model without facing the curse of dimensionality problem. But, unlike MARS, they miss possible important interactions (we can add them manually, though). see https://www.rdocumentation.org/packages/freeknotsplines/versions/1.0.1/topics/fit.search.numknots for knot location selection↩︎ see: http://users.stat.umn.edu/~helwig/notes/smooth-spline-notes.html.↩︎ "],["smoothing.html", "Chapter 7 Smoothing 7.1 Using bins 7.2 Kernel smoothing 7.3 Locally weighted regression loess() 7.4 Smooth Spline Regression 7.5 Multivariate Loess", " Chapter 7 Smoothing The main reason for using smoothing methods is noise reduction, which makes patterns and trends in the data more noticeable and easier to analyze for the improved accuracy of predictions made from the data. You can think of smoothing as a process that reduces the effect of noise in the data. We can define \\(Y_i\\) with a following model: \\[ Y_{i}=f\\left(x_{i}\\right)+\\varepsilon_{i} \\] We do not want to (and cannot) predict \\(Y_i\\) as we do not know the random part, \\(\\epsilon_i\\), the “noise”. If we predict \\(f(x)\\) well, it would give us a good approximation about \\(Y_i\\). Nonparametric estimations can be helpful for recovering \\(f(x)\\). In general, the purposes of smoothing is two-fold: building a forecasting model by smoothing and learning the shape of the trend embedded in the data (\\(Y\\)). The mcycle dataset from the MASS package contains \\(n=133\\) pairs of time points (in ms - milliseconds) and observed head accelerations (in g - acceleration of gravity) that were recorded in a simulated motorcycle accident. We will have several smoothing methods to explore the relationship between time and acceleration. First, let’s visualize the relationship between time \\((X)\\) and acceleration \\((Y)\\) and see if we can assume that \\(f(x_i)\\) is a linear function of time: library(tidyverse) library(MASS) data(mcycle) head(mcycle) ## times accel ## 1 2.4 0.0 ## 2 2.6 -1.3 ## 3 3.2 -2.7 ## 4 3.6 0.0 ## 5 4.0 -2.7 ## 6 6.2 -2.7 plot(mcycle$times, mcycle$accel, cex.axis = 0.75, cex.main = 0.8) # linear regression lines(mcycle$times, predict(lm(accel ~ times, mcycle)), lwd = 2, col = &quot;red&quot;) The line does not appear to describe the trend very well. 7.1 Using bins As we have seen before, the main idea is to group data points into bins (equal size) in which the value of \\(f(x)\\) can be assumed to be constant. This assumption could be realistic because if we consider \\(f(x)\\) is almost constant in small windows of time. After deciding the window size (say, 10ms), we find out how many observations (\\(Y_i\\)) we have in those 10-ms windows. We can calculate the number of observations in a 10-ms window centered around \\(x_i\\) satisfying the following condition: \\[ \\left(x-\\frac{10}{2}&lt;x_{i}&lt;x+\\frac{10}{2}\\right) \\] When we apply condition such as this for each observation of \\(x_i\\), we create a moving 10-ms window. Note that the window is established by adding half of 10ms to \\(x_i\\) to get the forward half and subtracting it from \\(x_i\\) to get the backward half. When we identify all the observations in each window, we estimate \\(f(x)\\) as the average of the \\(Y_i\\) values in that window. If we define \\(A_0\\) as the set of indexes in each window and \\(N_0\\) as the number of observation in each window, with our data, computing \\(f(x)\\) can be expressed as \\[\\begin{equation} \\hat{f}\\left(x_{0}\\right)=\\frac{1}{N_{0}} \\sum_{i \\in A_{0}} Y_{i} \\tag{7.1} \\end{equation}\\] Here is its application to our data: #With ksmooth() Pay attention to &quot;box&quot; fit1 &lt;- with(mcycle, ksmooth(times, accel, kernel = &quot;box&quot;, bandwidth = 7)) fit2 &lt;- with(mcycle, ksmooth(times, accel, kernel = &quot;box&quot;, bandwidth = 10)) fit3 &lt;- with(mcycle, ksmooth(times, accel, kernel = &quot;box&quot;, bandwidth = 21)) plot(mcycle$times, mcycle$accel, xlab = &quot;Time (ms)&quot;, ylab = &quot;Acceleration (g)&quot;) lines(mcycle$times, fit1$y, lwd = 2, col = &quot;blue&quot;) lines(mcycle$times, fit2$y, lwd = 2, col = &quot;red&quot;) lines(mcycle$times, fit3$y, lwd = 2, col = &quot;green&quot;) As you can see, even if we use a shorter bandwidth, the lines are quite wiggly. 7.2 Kernel smoothing We can take care of this by taking weighted averages that give the center points more weight than far away points. #With ksmooth() Pay attention to &quot;box&quot; fit1 &lt;- with(mcycle, ksmooth(times, accel, kernel = &quot;normal&quot;, bandwidth = 7)) fit2 &lt;- with(mcycle, ksmooth(times, accel, kernel = &quot;normal&quot;, bandwidth = 10)) fit3 &lt;- with(mcycle, ksmooth(times, accel, kernel = &quot;normal&quot;, bandwidth = 21)) plot(mcycle$times, mcycle$accel, xlab = &quot;Time (ms)&quot;, ylab = &quot;Acceleration (g)&quot;) lines(mcycle$times, fit1$y, lwd = 2, col = &quot;blue&quot;) lines(mcycle$times, fit2$y, lwd = 2, col = &quot;red&quot;) lines(mcycle$times, fit3$y, lwd = 2, col = &quot;green&quot;) Now, they look smoother. There are several functions in R that implement bin smoothing. One example is ksmooth, shown above. As we have seen before, however, we typically prefer methods such as loess that improves on these methods fitting a constant. 7.3 Locally weighted regression loess() A limitation of the bin smoother approach by ksmooth() is that we need small windows for the approximately constant assumptions to hold. Now loess() permits us to consider larger window sizes. #With loess() fit1 &lt;- loess(accel ~ times, degree = 1, span = 0.1, mcycle) fit2 &lt;-loess(accel ~ times, degree = 1, span = 0.9, mcycle) summary(fit1) ## Call: ## loess(formula = accel ~ times, data = mcycle, span = 0.1, degree = 1) ## ## Number of Observations: 133 ## Equivalent Number of Parameters: 18.57 ## Residual Standard Error: 22.93 ## Trace of smoother matrix: 22.01 (exact) ## ## Control settings: ## span : 0.1 ## degree : 1 ## family : gaussian ## surface : interpolate cell = 0.2 ## normalize: TRUE ## parametric: FALSE ## drop.square: FALSE plot(mcycle$times, mcycle$accel, xlab = &quot;Time (ms)&quot;, ylab = &quot;Acceleration (g)&quot;) lines(mcycle$times, fit1$fitted, lwd = 2, col = &quot;blue&quot;) lines(mcycle$times, fit2$fitted, lwd = 2, col = &quot;red&quot;) It seems the “red” line is underfitting the data. We can make our windows even larger by fitting parabolas instead of lines. fit1 &lt;- loess(accel ~ times, degree = 1, span = 0.1, data = mcycle) fit2 &lt;-loess(accel ~ times, degree = 2, span = 0.1, data = mcycle) plot(mcycle$times, mcycle$accel, xlab = &quot;Time (ms)&quot;, ylab = &quot;Acceleration (g)&quot;) lines(mcycle$times, fit1$fitted, lwd = 2, col = &quot;blue&quot;) lines(mcycle$times, fit2$fitted, lwd = 2, col = &quot;green&quot;) 7.4 Smooth Spline Regression We can also use npreg package with ss() function for automated smooth splines library(npreg) fit3 &lt;- with(mcycle, npreg::ss(times, accel)) fit3 ## ## Call: ## npreg::ss(x = times, y = accel) ## ## Smoothing Parameter spar = 0.1585867 lambda = 8.337283e-07 ## Equivalent Degrees of Freedom (Df) 12.20781 ## Penalized Criterion (RSS) 62034.66 ## Generalized Cross-Validation (GCV) 565.4684 plot(fit3, xlab = &quot;Time (ms)&quot;, ylab = &quot;Acceleration (g)&quot;, col = &quot;orange&quot;) rug(mcycle$times) # add rug to plot for the precise location of each point The gray shaded area denotes a 95% Bayesian “confidence interval” for the unknown function. 7.5 Multivariate Loess When there are more than two predictors, it is always advisable to use additive models either GAM or MARS. Nevertheless, let’s try loess() with several variables. This dataset is from from St.Louis Federal Reserve. It has 6 variables (observed monthly): psavert, personal savings rate; pce, personal consumption expenditures (in billions of dollars); unemploy, number of unemployed (in thousands), uempmed median duration of unemployment (weeks), and pop total population (in thousands). Although we have a time variable, date, we create an index for time. data(economics, package = &quot;ggplot2&quot;) str(economics) ## spc_tbl_ [574 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ date : Date[1:574], format: &quot;1967-07-01&quot; &quot;1967-08-01&quot; ... ## $ pce : num [1:574] 507 510 516 512 517 ... ## $ pop : num [1:574] 198712 198911 199113 199311 199498 ... ## $ psavert : num [1:574] 12.6 12.6 11.9 12.9 12.8 11.8 11.7 12.3 11.7 12.3 ... ## $ uempmed : num [1:574] 4.5 4.7 4.6 4.9 4.7 4.8 5.1 4.5 4.1 4.6 ... ## $ unemploy: num [1:574] 2944 2945 2958 3143 3066 ... economics$index &lt;- 1:nrow(economics) fit1 &lt;- loess(uempmed ~ index, data = economics, span = 0.25) # 25% smoothing span RRSS_1 &lt;- sqrt(mean((fit1$residuals) ^ 2)) RRSS_1 ## [1] 1.192171 plot(economics$index, economics$uempmed, cex.axis = 0.75, cex.main = 0.8, xlab = &quot;Time index - months&quot;, ylab = &quot;Unemployment duration - weeks&quot;, col = &quot;grey&quot;) lines(economics$index, fit1$fitted, lwd = 1, col = &quot;red&quot;) #Now more predictors fit2 &lt;- loess(uempmed ~ pce + psavert + pop + index, data = economics, span = 2) RRSS_2 &lt;- sqrt(mean((fit2$residuals) ^ 2)) RRSS_2 ## [1] 58.61336 "],["nonparametric-classifier---knn.html", "Chapter 8 Nonparametric Classifier - kNN 8.1 mnist Dataset 8.2 Linear classifiers (again) 8.3 k-Nearest Neighbors 8.4 kNN with caret", " Chapter 8 Nonparametric Classifier - kNN We complete this section, Nonparametric Estimations, with a nonparametric classifier and compare its performance with parametric classifiers, LPM and Logistic. 8.1 mnist Dataset Reading hand-written letters and numbers is not a big deal nowadays. For example, In Canada Post, computers read postal codes and robots sorts them for each postal code groups. This application is mainly achieved by machine learning algorithms. In order to understand how, let’s use a real dataset, Mnist. Here is the description of the dataset by Wikipedia: The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The MNIST database contains 60,000 training images and 10,000 testing images. Half of the training set and half of the test set were taken from NIST’s training dataset, while the other half of the training set and the other half of the test set were taken from NIST’s testing dataset. There have been a number of scientific papers on attempts to achieve the lowest error rate; one paper, using a hierarchical system of convolutional neural networks, manages to get an error rate on the MNIST database of 0.23%. These images are converted into \\(28 \\times 28 = 784\\) pixels and, for each pixel, there is a measure that scales the darkness in that pixel between 0 (white) and 255 (black). Hence, for each digitized image, we have an indicator variable \\(Y\\) between 0 and 9, and we have 784 variables that identifies each pixel in the digitized image. Let’s download the data. (More details about the data). #loading the data library(tidyverse) library(dslabs) #Download the data to your directory. It&#39;s big! #mnist &lt;- read_mnist() #save(mnist, file = &quot;mnist.Rdata&quot;) load(&quot;mnist.Rdata&quot;) str(mnist) ## List of 2 ## $ train:List of 2 ## ..$ images: int [1:60000, 1:784] 0 0 0 0 0 0 0 0 0 0 ... ## ..$ labels: int [1:60000] 5 0 4 1 9 2 1 3 1 4 ... ## $ test :List of 2 ## ..$ images: int [1:10000, 1:784] 0 0 0 0 0 0 0 0 0 0 ... ## ..$ labels: int [1:10000] 7 2 1 0 4 1 4 9 5 9 ... The data is given as a list and already divided into train and test sets. We have 60,000 images in the train set and 10,000 images in the test set. For the train set, we have two nested sets: images, which contains all 784 features for 60,000 images. Hence, it’s a \\(60000 \\times 784\\) matrix. And, labels contains the labes (from 0 to 9) for each image. The digitizing can be understood from this image better: Each image has \\(28 \\times 28\\) = 784 pixels. For each image, the pixels are features with a label that shows the true number between 0 and 9. This methods is called as “flattening”, which is a technique that is used to convert multi-dimensional image into a one-dimension array (vector). For now, we will use a smaller version of this data set given in the dslabs package, which is a random sample of 1,000 images (only for 2 and 7 digits), 800 in the training set and 200 in the test set, with only two features: the proportion of dark pixels that are in the upper left quadrant, x_1, and the lower right quadrant, x_2. data(&quot;mnist_27&quot;) str(mnist_27) ## List of 5 ## $ train :&#39;data.frame&#39;: 800 obs. of 3 variables: ## ..$ y : Factor w/ 2 levels &quot;2&quot;,&quot;7&quot;: 1 2 1 1 2 1 2 2 2 1 ... ## ..$ x_1: num [1:800] 0.0395 0.1607 0.0213 0.1358 0.3902 ... ## ..$ x_2: num [1:800] 0.1842 0.0893 0.2766 0.2222 0.3659 ... ## $ test :&#39;data.frame&#39;: 200 obs. of 3 variables: ## ..$ y : Factor w/ 2 levels &quot;2&quot;,&quot;7&quot;: 1 2 2 2 2 1 1 1 1 2 ... ## ..$ x_1: num [1:200] 0.148 0.283 0.29 0.195 0.218 ... ## ..$ x_2: num [1:200] 0.261 0.348 0.435 0.115 0.397 ... ## $ index_train: int [1:800] 40334 33996 3200 38360 36239 38816 8085 9098 15470 5096 ... ## $ index_test : int [1:200] 46218 35939 23443 30466 2677 54248 5909 13402 11031 47308 ... ## $ true_p :&#39;data.frame&#39;: 22500 obs. of 3 variables: ## ..$ x_1: num [1:22500] 0 0.00352 0.00703 0.01055 0.01406 ... ## ..$ x_2: num [1:22500] 0 0 0 0 0 0 0 0 0 0 ... ## ..$ p : num [1:22500] 0.703 0.711 0.719 0.727 0.734 ... ## ..- attr(*, &quot;out.attrs&quot;)=List of 2 ## .. ..$ dim : Named int [1:2] 150 150 ## .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;x_1&quot; &quot;x_2&quot; ## .. ..$ dimnames:List of 2 ## .. .. ..$ x_1: chr [1:150] &quot;x_1=0.0000000&quot; &quot;x_1=0.0035155&quot; &quot;x_1=0.0070310&quot; &quot;x_1=0.0105465&quot; ... ## .. .. ..$ x_2: chr [1:150] &quot;x_2=0.000000000&quot; &quot;x_2=0.004101417&quot; &quot;x_2=0.008202834&quot; &quot;x_2=0.012304251&quot; ... 8.2 Linear classifiers (again) A linear classifier (like LPM and Logistic) is one where a “hyperplane” is formed by taking a linear combination of the features. Hyperplane represents a decision boundary chosen by our classifier to separate the data points in different class labels. let’s start with LPM: \\[\\begin{equation} \\operatorname{Pr}\\left(Y=1 | X_{1}=x_{1}, X_{2}=x_{2}\\right)=\\beta_{0}+\\beta_{1} x_{1}+\\beta_{2} x_{2} \\tag{8.1} \\end{equation}\\] # LPM requires numerical 1 and 0 y10 = ifelse(mnist_27$train$y == 7, 1, 0) train &lt;- data.frame(mnist_27$train, y10) plot(train$x_1, train$x_2, col = train$y10 + 1, cex = 0.5) Here, the black dots are 2 and red dots are 7. Note that if we use 0.5 as a decision rule such that it separates pairs (\\(x_1\\), \\(x_2\\)) for which \\(\\operatorname{Pr}\\left(Y=1 | X_{1}=x_{1}, X_{2}=x_{2}\\right) &lt; 0.5\\) then we can have a hyperplane as \\[ \\hat{\\beta}_{0}+\\hat{\\beta}_{1} x_{1}+\\hat{\\beta}_{2} x_{2}=0.5 \\Longrightarrow x_{2}=\\left(0.5-\\hat{\\beta}_{0}\\right) / \\hat{\\beta}_{2}-\\hat{\\beta}_{1} / \\hat{\\beta}_{2} x_{1}. \\] If we incorporate this into our plot for the train data: model &lt;- lm(y10 ~ x_1 + x_2, train) tr &lt;- 0.5 a &lt;- tr - model$coefficients[1] a &lt;- a / model$coefficients[3] b &lt;- -model$coefficients[2] / model$coefficients[3] plot(train$x_1, train$x_2, col = train$y10 + 1, cex = 0.72) abline(a, b, col = &quot;blue&quot;, lwd = 2.8) Play with the (discriminating) threshold and see how the hyperplane moves. When we change it to different numbers between 0 and 1, the number of correct and wrong predictions, a separation of red and black dots located in different sides, changes as well. Moreover the decision boundary is linear. That’s why LPM is called a linear classifier. Would including interactions and polynomials (nonlinear parts) would place the line such a way that separation of these dots (2s and 7s) would be better? Let’s see if adding a polynomial to our LPM improves this. model2 &lt;- lm(y10 ~ x_1 + I(x_1 ^ 2) + x_2, train) summary(model2) ## ## Call: ## lm(formula = y10 ~ x_1 + I(x_1^2) + x_2, data = train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.14744 -0.28816 0.03999 0.28431 1.06759 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.09328 0.06571 1.419 0.1562 ## x_1 4.81884 0.55310 8.712 &lt; 2e-16 *** ## I(x_1^2) -2.75520 1.40760 -1.957 0.0507 . ## x_2 -1.18864 0.17252 -6.890 1.14e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3891 on 796 degrees of freedom ## Multiple R-squared: 0.3956, Adjusted R-squared: 0.3933 ## F-statistic: 173.7 on 3 and 796 DF, p-value: &lt; 2.2e-16 tr &lt;- 0.5 s &lt;- model2$coefficients a = tr / s[3] b = s[1] / s[3] d = s[2] / s[3] e = s[4] / s[3] x22 = a - b - d * train$x_1 - e * (train$x_1 ^ 2) plot(train$x_1, train$x_2, col = train$y10 + 1, cex = 0.72) lines(train$x_1[order(x22)], x22[order(x22)], lwd = 2.8) The coefficient of the polynomial is barely significant and very negligible in magnitude. And in fact the classification seems worse than the previous one. Would a logistic regression give us a better line? We don’t need to estimate it, but we can obtain the decision boundary for the logistic regression. Remember, \\[ P(Y=1 | x)=\\frac{\\exp \\left(w_{0}+\\sum_{i} w_{i} x_{i}\\right)}{1+\\exp \\left(w_{0}+\\sum_{i} w_{i} x_{i}\\right)} \\] And, \\[ P(Y=0 | x)=1-P(Y=1 | x)= \\frac{1}{1+\\exp \\left(w_{0}+\\sum_{i} w_{i} x_{i}\\right)} \\] if we take the ratio of success over failure, \\(P/1-P\\), \\[ \\frac{P}{1-P}=\\exp \\left(w_{0}+\\sum_{i} w_{i} x_{i}\\right) \\] If this ratio is higher than 1, we think that the probability for \\(Y=1\\) is higher than the probability for \\(Y=0\\). And this only happens when \\(P&gt;0.5\\). Hence, the condition to classify the observation as \\(Y=1\\) is: \\[ \\frac{P}{1-P}=\\exp \\left(w_{0}+\\sum_{i} w_{i} x_{i}\\right) &gt; 1 \\] If we take the log of both sides, \\[ w_{0}+\\sum_{i} w_{i} X_{i}&gt;0 \\] From here, the hyperplane function in our case becomes, \\[ \\hat{\\beta}_{0}+\\hat{\\beta}_{1} x_{1}+\\hat{\\beta}_{2} x_{2}=0 \\Longrightarrow x_{2}=-\\hat{\\beta}_{0} / \\hat{\\beta}_{2}-\\hat{\\beta}_{1} / \\hat{\\beta}_{2} x_{1}. \\] We see that the decision boundary is again linear. Therefore, LPM and logistic regressions are called as linear classifiers, which are good only if the problem on hand is linearly separable. Would it be possible to have a nonlinear boundary condition so that we can get a better classification for our predicted probabilities? 8.3 k-Nearest Neighbors k-nearest neighbors (kNN) is a nonparametric method used for classification (or regression), which estimate \\(p(x_1, x_2)\\) by using a method similar to bin smoothing. In kNN classification, the output is a class membership. An object is assigned to the class most common among its k-nearest neighbors. In kNN regressions, the output is the average of the values of k-nearest neighbors, which we’ve seen in bin smoothing applications. Suppose we have to classify (identify) the red dot as 7 or 2. Since it’s a nonparametric approach, we have to define bins. If the number of observations in bins set to 1 (\\(k = 1\\)), then we need to find one observation that is nearest to the red dot. How? Since we know to coordinates (\\(x_1, x_2\\)) of that red dot, we can find its nearest neighbors by some distance functions among all points (observations) in the data. A popular choice is the Euclidean distance given by \\[ d\\left(x, x^{\\prime}\\right)=\\sqrt{\\left(x_{1}-x_{1}^{\\prime}\\right)^{2}+\\ldots+\\left(x_{n}-x_{n}^{\\prime}\\right)^{2}}. \\] Other measures are also available and can be more suitable in different settings including the Manhattan, Chebyshev and Hamming distance. The last one is used if the features are binary. In our case the features are continuous so we can use the Euclidean distance. We now have to calculate this measure for every point (observation) in our data. In our graph we have 10 points, and we have to have 10 distance measures from the red dot. Usually, in practice, we calculate all distance measures between each point, which becomes a symmetric matrix with \\(n\\)x\\(n\\) dimensions. For example, for two dimensional space, we can calculate the distances as follows x1 &lt;- c(2, 2.1, 4, 4.3) x2 &lt;- c(3, 3.3, 5, 5.1) EDistance &lt;- function(x, y){ dx &lt;- matrix(0, length(x), length(x)) dy &lt;- matrix(0, length(x), length(x)) for (i in 1:length(x)) { dx[i,] &lt;- (x[i] - x)^2 dy[i,] &lt;- (y[i] - y)^2 dd &lt;- sqrt(dx^2 + dy^2) } return(dd) } EDistance(x1, x2) ## [,1] [,2] [,3] [,4] ## [1,] 0.00000000 0.09055385 5.65685425 6.88710389 ## [2,] 0.09055385 0.00000000 4.62430535 5.82436263 ## [3,] 5.65685425 4.62430535 0.00000000 0.09055385 ## [4,] 6.88710389 5.82436263 0.09055385 0.00000000 plot(x1, x2, col = &quot;red&quot;, lwd = 3) #segments(x1[1], x2[1], x1[2:4], x2[2:4], col = &quot;blue&quot; ) #segments(x1[2], x2[2], x1[c(1, 3:4)], x2[c(1, 3:4)], col = &quot;green&quot; ) #segments(x1[3], x2[3], x1[c(1:2, 4)], x2[c(1:2, 4)], col = &quot;orange&quot; ) segments(x1[4], x2[4], x1[1:3], x2[1:3], col = &quot;darkgreen&quot; ) The matrix shows all distances for four points and, as we expect, it is symmetric. The green lines show the distance from the last point (\\(x = 4.3,~ y = 5.1\\)) to all other points. Using this matrix, we can easily find the k-nearest neighbors for any point. When \\(k=1\\), the observation that has the shortest distance is going to be the one to predict what the red dot could be. This is shown in the figure below: If we define the bin as \\(k=3\\), we look for the 3 nearest points to the red dot and then take an average of the 1s (7s) and 0s (2s) associated with these points. Here is an example: Using \\(k\\) neighbors to estimate the probability of \\(Y=1\\) (the dot is 7), that is \\[\\begin{equation} \\hat{P}_{k}(Y=1 | X=x)=\\frac{1}{k} \\sum_{i \\in \\mathcal{N}_{k}(x, D)} I\\left(y_{i}=1\\right) \\tag{8.2} \\end{equation}\\] With this predicted probability, we classify the red dot to the class with the most observations in the \\(k\\) nearest neighbors (we assign a class at random to one of the classes tied for highest). Here is the rule in our case: \\[ \\hat{C}_{k}(x)=\\left\\{\\begin{array}{ll}{1} &amp; {\\hat{p}_{k 0}(x)&gt;0.5} \\\\ {0} &amp; {\\hat{p}_{k 1}(x)&lt;0.5}\\end{array}\\right. \\] Suppose our red dot has \\(x=(x_1,x_2)=(4,3)\\) \\[ \\begin{aligned} \\hat{P}\\left(Y=\\text { Seven } | X_{1}=4, X_{2}=3\\right)=\\frac{2}{3} \\\\ \\hat{P}\\left(Y=\\text { Two} | X_{1}=4, X_{2}=3\\right)=\\frac{1}{3} \\end{aligned} \\] Hence, \\[ \\hat{C}_{k=4}\\left(x_{1}=4, x_{2}=3\\right)=\\text { Seven } \\] As it’s clear from this application, \\(k\\) is our hyperparameter and we need to tune it as to have the best predictive kNN algorithm. The following section will show its application. But before that, we need to understand how decision boundaries can be found in kNN set.seed(1) x1 &lt;- runif(50) x2 &lt;- runif(50) library(deldir) tesselation &lt;- deldir(x1, x2) tiles &lt;- tile.list(tesselation) plot(tiles, pch = 19, close = TRUE, fillcol = hcl.colors(4, &quot;Sunset&quot;), xlim = c(-0.2:1.1)) These are called Voronoi cells associated with 1-NN, which is the set of polygons whose edges are the perpendicular bisectors of the lines joining the neighboring points. Thus, the decision boundary is the result of fusing adjacent Voronoi cells that are associated with same class. In the example above, it’s the boundary of unions of each colors. Finding the boundaries that trace each adjacent Vorono regions can be done with additional several steps. To see all in an application, we will use knn3() from the Caret package. We will not train a model but only see how the separation between classes will be nonlinear and different for different \\(k\\). library(tidyverse) library(caret) library(dslabs) #With k = 50 model1 &lt;- knn3(y ~ ., data = mnist_27$train, k = 2) x_1 &lt;- mnist_27$true_p$x_1 x_2 &lt;- mnist_27$true_p$x_2 df &lt;- data.frame(x_1, x_2) #This is whole data 22500 obs. p_hat &lt;- predict(model1, df, type = &quot;prob&quot;) # Predicting probabilities in each bin p_7 &lt;- p_hat[,2] #Selecting the p_hat for 7 df &lt;- data.frame(x_1, x_2, p_7) my_colors &lt;- c(&quot;black&quot;, &quot;red&quot;) p1 &lt;- ggplot() + geom_point(data = mnist_27$train, aes(x = x_1, y = x_2, colour = factor(y)), shape = 21, size = 1, stroke = 1) + stat_contour(data = df, aes(x = x_1, y = x_2, z = p_7), breaks=c(0.5), color=&quot;blue&quot;) + scale_color_manual(values = my_colors) plot(p1) #With k = 400 model2 &lt;- knn3(y ~ ., data = mnist_27$train, k = 400) p_hat &lt;- predict(model2, df, type = &quot;prob&quot;) # Prediciting probabilities in each bin p_7 &lt;- p_hat[,2] #Selecting the p_hat for 7 df &lt;- data.frame(x_1, x_2, p_7) p1 &lt;- ggplot() + geom_point(data = mnist_27$train, aes(x = x_1, y = x_2, colour = factor(y)), shape = 21, size = 1, stroke = 1) + stat_contour(data = df, aes(x = x_1, y = x_2, z = p_7), breaks=c(0.5), color=&quot;blue&quot;) + scale_color_manual(values = my_colors) plot(p1) One with \\(k=2\\) shows signs for overfitting, the other one with \\(k=400\\) indicates oversmoothing or underfitting. We need to tune \\(k\\) such a way that it will be best in terms of prediction accuracy. 8.4 kNN with caret There are many different learning algorithms developed by different authors and often with different parametric structures. The caret, Classification And Regression Training package tries to consolidate these differences and provide consistency. It currently includes 237 (and growing) different methods which are summarized in the caret package manual (Kuhn_2019?). Here, we will use mnset_27 to illustrate how we can use caret for kNN. For now, we will use the caret’s train() function to find the optimal k in kNN, which is basically an automated version of cross-validation that we will see in the next chapter. 8.4.1 mnist_27 Since, our dataset, mnist_27, is already split into train and test sets, we do not need to do it again. Here is the starting point: library(caret) #Training/Model building model_knn &lt;- train(y ~ ., method = &quot;knn&quot;, data = mnist_27$train) model_knn ## k-Nearest Neighbors ## ## 800 samples ## 2 predictor ## 2 classes: &#39;2&#39;, &#39;7&#39; ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 800, 800, 800, 800, 800, 800, ... ## Resampling results across tuning parameters: ## ## k Accuracy Kappa ## 5 0.8075980 0.6135168 ## 7 0.8157975 0.6300494 ## 9 0.8205824 0.6396302 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was k = 9. By default, the cross validation is performed by taking 25 bootstrap samples comprised of 25% of the observations. Moreover, the default is to try \\(k=5,7,9\\). We can to expand it: #Training/Model building with our own grid set.seed(2008) model_knn1 &lt;- train( y ~ ., method = &quot;knn&quot;, data = mnist_27$train, tuneGrid = data.frame(k = seq(9, 71, 2)) ) ggplot(model_knn1, highlight = TRUE) model_knn1$bestTune ## k ## 10 27 model_knn1$finalModel ## 27-nearest neighbor model ## Training set outcome distribution: ## ## 2 7 ## 379 421 We can change its tuning to cross-validation: #Training/Model building with 10-k cross validation cv &lt;- trainControl(method = &quot;cv&quot;, number = 10, p = 0.9) model_knn2 &lt;- train(y ~ ., method = &quot;knn&quot;, data = mnist_27$train, tuneGrid = data.frame(k=seq(9,71,2)), trControl = cv) ggplot(model_knn2, highlight = TRUE) model_knn2$bestTune ## k ## 11 29 It seems like \\(k=27\\) (\\(k=29\\) with CV) gives us the best performing prediction model. We can see their prediction performance on the test set: caret::confusionMatrix(predict(model_knn1, mnist_27$test, type = &quot;raw&quot;), mnist_27$test$y) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 2 7 ## 2 92 19 ## 7 14 75 ## ## Accuracy : 0.835 ## 95% CI : (0.7762, 0.8836) ## No Information Rate : 0.53 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.6678 ## ## Mcnemar&#39;s Test P-Value : 0.4862 ## ## Sensitivity : 0.8679 ## Specificity : 0.7979 ## Pos Pred Value : 0.8288 ## Neg Pred Value : 0.8427 ## Prevalence : 0.5300 ## Detection Rate : 0.4600 ## Detection Prevalence : 0.5550 ## Balanced Accuracy : 0.8329 ## ## &#39;Positive&#39; Class : 2 ## caret::confusionMatrix(predict(model_knn2, mnist_27$test, type = &quot;raw&quot;), mnist_27$test$y) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 2 7 ## 2 91 18 ## 7 15 76 ## ## Accuracy : 0.835 ## 95% CI : (0.7762, 0.8836) ## No Information Rate : 0.53 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.6682 ## ## Mcnemar&#39;s Test P-Value : 0.7277 ## ## Sensitivity : 0.8585 ## Specificity : 0.8085 ## Pos Pred Value : 0.8349 ## Neg Pred Value : 0.8352 ## Prevalence : 0.5300 ## Detection Rate : 0.4550 ## Detection Prevalence : 0.5450 ## Balanced Accuracy : 0.8335 ## ## &#39;Positive&#39; Class : 2 ## What are these measures? What is a “Confusion Matrix”? We will see them in the next section. But for now, let’s use another example. 8.4.2 Adult dataset This dataset provides information on income earning and attributes that may effect it. Information on the dataset is given at its website (Kohavi_1996?): Extraction from 1994 US. Census database. A set of reasonably clean records was extracted using the following conditions: ((AAGE&gt;16) &amp;&amp; (AGI&gt;100) &amp;&amp; (AFNLWGT&gt;1)&amp;&amp; (HRSWK&gt;0)). The prediction task is to determine whether a person makes over 50K a year. # Download adult income data # SET YOUR WORKING DIRECTORY FIRST # url.train &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data&quot; # url.test &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test&quot; # url.names &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names&quot; # download.file(url.train, destfile = &quot;adult_train.csv&quot;) # download.file(url.test, destfile = &quot;adult_test.csv&quot;) # download.file(url.names, destfile = &quot;adult_names.txt&quot;) # Read the training set into memory train &lt;- read.csv(&quot;adult_train.csv&quot;, header = FALSE) str(train) ## &#39;data.frame&#39;: 32561 obs. of 15 variables: ## $ V1 : int 39 50 38 53 28 37 49 52 31 42 ... ## $ V2 : chr &quot; State-gov&quot; &quot; Self-emp-not-inc&quot; &quot; Private&quot; &quot; Private&quot; ... ## $ V3 : int 77516 83311 215646 234721 338409 284582 160187 209642 45781 159449 ... ## $ V4 : chr &quot; Bachelors&quot; &quot; Bachelors&quot; &quot; HS-grad&quot; &quot; 11th&quot; ... ## $ V5 : int 13 13 9 7 13 14 5 9 14 13 ... ## $ V6 : chr &quot; Never-married&quot; &quot; Married-civ-spouse&quot; &quot; Divorced&quot; &quot; Married-civ-spouse&quot; ... ## $ V7 : chr &quot; Adm-clerical&quot; &quot; Exec-managerial&quot; &quot; Handlers-cleaners&quot; &quot; Handlers-cleaners&quot; ... ## $ V8 : chr &quot; Not-in-family&quot; &quot; Husband&quot; &quot; Not-in-family&quot; &quot; Husband&quot; ... ## $ V9 : chr &quot; White&quot; &quot; White&quot; &quot; White&quot; &quot; Black&quot; ... ## $ V10: chr &quot; Male&quot; &quot; Male&quot; &quot; Male&quot; &quot; Male&quot; ... ## $ V11: int 2174 0 0 0 0 0 0 0 14084 5178 ... ## $ V12: int 0 0 0 0 0 0 0 0 0 0 ... ## $ V13: int 40 13 40 40 40 40 16 45 50 40 ... ## $ V14: chr &quot; United-States&quot; &quot; United-States&quot; &quot; United-States&quot; &quot; United-States&quot; ... ## $ V15: chr &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; ... # Read the test set into memory test &lt;- read.csv(&quot;adult_test.csv&quot;, header = FALSE) The data doesn’t have the variable names. That’s bad because we don’t know which one is which. Check the adult_names.txt file. The list of variables is given in that file. Thanks to Matthew Baumer (Baumer_2015?), we can write them manually: varNames &lt;- c(&quot;Age&quot;, &quot;WorkClass&quot;, &quot;fnlwgt&quot;, &quot;Education&quot;, &quot;EducationNum&quot;, &quot;MaritalStatus&quot;, &quot;Occupation&quot;, &quot;Relationship&quot;, &quot;Race&quot;, &quot;Sex&quot;, &quot;CapitalGain&quot;, &quot;CapitalLoss&quot;, &quot;HoursPerWeek&quot;, &quot;NativeCountry&quot;, &quot;IncomeLevel&quot;) names(train) &lt;- varNames names(test) &lt;- varNames str(train) ## &#39;data.frame&#39;: 32561 obs. of 15 variables: ## $ Age : int 39 50 38 53 28 37 49 52 31 42 ... ## $ WorkClass : chr &quot; State-gov&quot; &quot; Self-emp-not-inc&quot; &quot; Private&quot; &quot; Private&quot; ... ## $ fnlwgt : int 77516 83311 215646 234721 338409 284582 160187 209642 45781 159449 ... ## $ Education : chr &quot; Bachelors&quot; &quot; Bachelors&quot; &quot; HS-grad&quot; &quot; 11th&quot; ... ## $ EducationNum : int 13 13 9 7 13 14 5 9 14 13 ... ## $ MaritalStatus: chr &quot; Never-married&quot; &quot; Married-civ-spouse&quot; &quot; Divorced&quot; &quot; Married-civ-spouse&quot; ... ## $ Occupation : chr &quot; Adm-clerical&quot; &quot; Exec-managerial&quot; &quot; Handlers-cleaners&quot; &quot; Handlers-cleaners&quot; ... ## $ Relationship : chr &quot; Not-in-family&quot; &quot; Husband&quot; &quot; Not-in-family&quot; &quot; Husband&quot; ... ## $ Race : chr &quot; White&quot; &quot; White&quot; &quot; White&quot; &quot; Black&quot; ... ## $ Sex : chr &quot; Male&quot; &quot; Male&quot; &quot; Male&quot; &quot; Male&quot; ... ## $ CapitalGain : int 2174 0 0 0 0 0 0 0 14084 5178 ... ## $ CapitalLoss : int 0 0 0 0 0 0 0 0 0 0 ... ## $ HoursPerWeek : int 40 13 40 40 40 40 16 45 50 40 ... ## $ NativeCountry: chr &quot; United-States&quot; &quot; United-States&quot; &quot; United-States&quot; &quot; United-States&quot; ... ## $ IncomeLevel : chr &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; ... Since the dataset is large we are not going to use the test set but split the train set into our own test and train sets. Note that, however, if we had used the original test set, we would have had to make some adjustments/cleaning before using it. For example, if you look at Age variable, it seems as a factor variable. It is an integer in the training set. We have to change it first. Moreover, our \\(Y\\) has two levels in the train set, it has 3 levels in the test set. We have to go over each variable and make sure that the test and train sets have the same features and class types. This task is left to you if you want to use the original train and test sets. A final tip: remove the first row in the original test set! #Caret needs some preparations! table(train$IncomeLevel) ## ## &lt;=50K &gt;50K ## 24720 7841 # This is b/c we will use the same data for LPM train$Y &lt;- ifelse(train$IncomeLevel == &quot; &lt;=50K&quot;, 0, 1) train &lt;- train[,-15] # kNN needs Y to be a factor variable train$Y &lt;- as.factor(train$Y) levels(train$Y)[levels(train$Y) == &quot;0&quot;] &lt;- &quot;Less&quot; levels(train$Y)[levels(train$Y) == &quot;1&quot;] &lt;- &quot;More&quot; levels(train$Y) ## [1] &quot;Less&quot; &quot;More&quot; #kNN set.seed(3033) train_df &lt;- caret::createDataPartition(y = train$Y, p = 0.7, list = FALSE) training &lt;- train[train_df, ] testing &lt;- train[-train_df, ] #Training/Model building with 10-k cross validation cv &lt;- caret::trainControl(method = &quot;cv&quot;, number = 10, p = 0.9) model_knn3 &lt;- caret::train( Y ~ ., method = &quot;knn&quot;, data = training, tuneGrid = data.frame(k = seq(9, 41 , 2)), trControl = cv ) ggplot(model_knn3, highlight = TRUE) Now we are going to use the test set to see the model’s performance. caret::confusionMatrix(predict(model_knn3, testing, type = &quot;raw&quot;), testing$Y) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Less More ## Less 7311 1871 ## More 105 481 ## ## Accuracy : 0.7977 ## 95% CI : (0.7896, 0.8056) ## No Information Rate : 0.7592 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.256 ## ## Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 ## ## Sensitivity : 0.9858 ## Specificity : 0.2045 ## Pos Pred Value : 0.7962 ## Neg Pred Value : 0.8208 ## Prevalence : 0.7592 ## Detection Rate : 0.7485 ## Detection Prevalence : 0.9400 ## Balanced Accuracy : 0.5952 ## ## &#39;Positive&#39; Class : Less ## Next, as you can guess, we will delve into these performance measures. Learning algorithm may not be evaluated only by its predictive capacity. We may want to interpret the results by identifying the important predictors and their importance. There is always a trade-off between interpretability and predictive accuracy. Here is a an illustration. We will talk about this later in the book. "],["hyperparameter-tuning.html", "Chapter 9 Hyperparameter Tuning 9.1 Training, validation, and test datasets 9.2 Splitting the data randomly 9.3 k-fold cross validation 9.4 Cross-validated grid search 9.5 Bootstrapped grid search 9.6 When the data is time-series 9.7 Speed", " Chapter 9 Hyperparameter Tuning In general, there are multiple tuning parameters or so-called hyperparameters associated with each prediction method. The value of the hyperparameter has to be set before the learning process begins as those tuning parameters are external to the model and their value cannot be estimated from data. Therefore, we usually need to perform a grid search to identify the optimal combination of these parameters that minimizes the prediction error. For example, \\(k\\) in kNN, the number of hidden layers in Neural Networks, even the degree of of polynomials in a linear regression have to be tuned before the learning process starts. In contrast, a parameter (in parametric models) is an internal characteristic of the model and its value can be estimated from data for any given hyperparameter. For example, \\(\\lambda\\), the penalty parameter that shrinks the number of variables in Lasso, which we will see in Section 6, is a hyperparameter and has to be set before the estimation. When it’s set, the coefficients of Lasso are estimated from the process. We start with k-fold cross validation process and perform a cross-validated grid search to identify the optimal mix of tuning parameters. We will learn the rules with simple applications about how to set up a grid search that evaluates many different combinations of hyperparameters. This chapter covers the key concept in modern machine learning applications and many learning algorithms. 9.1 Training, validation, and test datasets Before learning how to split the data into subsections randomly, we need to know what these sets are for and how we define them properly. This section is inspired by the article, What is the Difference Between Test and Validation Datasets?, by Jason Brownlee (Brownlee_2017?). The article clarifies how validation and test datasets are different, which can be confusing in practice. Each machine learning model needs a training, which is a process of tuning their hyperparameters and selecting their features (variables) for the best predictive performance. Therefore, this process requires two different datasets: training and validation datasets. The intuition behind this split is very simple: the prediction is an out-of-sample problem. If we use the same sample that we use to fit the model for assessing the prediction accuracy of our model, we face the infamous overfitting problem. Since we usually don’t have another unseen dataset available to us, we split the data and leave one part out of our original dataset. We literally pretend that one that is left out is “unseen” by us. Now the question is how we do this split. Would it be 50-50?. The general approach is k-fold cross validation with a grid search. Here are the main steps: Suppose Model1 requires to pick a value for \\(\\lambda\\), perhaps it is a degree of polynomials in the model. We establish a grid, a set of sequential numbers, that is a set of possible values of \\(\\lambda\\). We split the data into \\(k\\) random sections, let’s say 10 proportionally equal sections. We leave one section out and use 9 sections. The combination of these 9 sections is our training set. The one that is left out is our validation set. We fit the model using each value in the set of possible values of \\(\\lambda\\). For example, if we have 100 values of \\(\\lambda\\), we fit the model to the training set 100 times, once for each possible value of \\(\\lambda\\). We evaluate each of 100 models by using their predictive accuracy on the validation set, the one that is left out. We pick a \\(\\lambda\\) that gives the highest prediction accuracy. We do this process 10 times (if it is 10-fold cross validation) with each time using a different section of the data as the validation set. So, note that each time our training and validation sets are going to be different. At the end, in total we will have 10 best \\(\\lambda\\)s. We pick the average or modal value of \\(\\lambda\\) as our optimal hyperparameter that tunes our predictive model for its best performance. Note that the term validation is sometimes is mixed-up with test for the dataset we left out from our sample. This point often confuses practitioners. So what is the test set? We have now Model1 tuned with the optimal \\(\\lambda\\). This is a model among several alternative models (there are more than 300 predictive models and growing in practice). Besides, the steps above we followed provides a limited answer whether if Model1 has a good or “acceptable” prediction accuracy or not. In other words, tuning Model1 doesn’t mean that it does a good or a bad job in prediction. How do we know and measure the tuned model’s performance in prediction? Usually, if the outcome that we try to predict is quantitative variable, we use root mean squared prediction error (RMSPE). There are several other metrics that we will see later. If it’s an indicator outcome, we have to apply some other methods, one of which is called as Receiver Operating Curve (ROC). We will see and learn all of them all in detail shortly. But, for now, let’s pretend that we know a metric that measures the prediction accuracy of Model1 as well as other alternative models. The only sensible way to do it would be to test the “tuned” model on a new dataset. In other words, we need to use the trained model on a real and new dataset and calculate the prediction accuracy of Model1 by RMSPE or ROC. Therefore, we have to go back to the start and create a split before starting the training process: training and test datasets. We use the training data for the feature selection and tuning the parameter. After we “trained” the model by validation, we can use the test set to see the performance of the tuned model. Finally, you follow the same steps for other alternative learning algorithms and then pick the winner. Having trained each model using the training set, and chosen the best model using the validation set, the test set tells you how good your final choice of model is. Here is a visualization of the split: Before seeing every step with an application in this chapter, let’s have a more intuitive and simpler explanation about “training” a model. First, what’s learning? We can summarize it this way: observe the facts, do some generalizations, use these generalizations to predict previously unseen facts, evaluate your predictions, and adjust your generalizations (knowledge) for better predictions. It’s an infinite loop. Here is the basic paradigm: Observe the facts (training data), Make generalizations (build prediction models), Adjust your prediction to make them better (train your model with validation data), Test your predictions on unseen data to see how they hold up (test data) As the distinction between validation and test datasets is now clear, we can conclude that, even if we have the best possible predictive model given the training dataset, our generalization of the seen data for prediction of unseen facts would be fruitless in practice. In fact, we may learn nothing at the end of this process and remain unknowledgeable about the unseen facts. Why would this happen? The main reason would be the lack of enough information in training set. If we do not have enough data to make and test models that are applicable to real life, our predictions may not be valid. The second reason would be modeling inefficiencies in a sense that it requires a very large computing power and storage capacity. This subject is also getting more interesting everyday. The Google’s quantum computers are one of them. 9.2 Splitting the data randomly We already know how to sample a set of observation by using sample(). We can use this function again to sort the data into k-fold sections. Here is an example with just 2 sections: #We can create a simple dataset with X and Y using a DGM set.seed(2) n &lt;- 10000 x &lt;- rnorm(n, 3, 6) y &lt;- 2 + 13*x + rnorm(n, 0, 1) data &lt;- data.frame(y, x) #We need to shuffle it random &lt;- sample(n, n, replace = FALSE) data &lt;- data[random, ] #The order of data is now completely random, #we can divide it as many slices as we wish k &lt;- 2 #2-fold (slices-sections) nslice &lt;- floor(n/k) #number of observations in each fold/slice #Since we have only 2 slices of data #we can call one slice as a &quot;validation set&quot; the other one as a &quot;training set&quot; train &lt;- data[1:nslice, ] str(train) ## &#39;data.frame&#39;: 5000 obs. of 2 variables: ## $ y: num -92.7 52.35 -114 133.6 7.39 ... ## $ x: num -7.344 3.868 -9.006 9.978 0.468 ... val &lt;- data[(nslice+1):n,] str(val) ## &#39;data.frame&#39;: 5000 obs. of 2 variables: ## $ y: num -49.1 -53.7 -25 -46.2 135.2 ... ## $ x: num -3.9 -4.22 -1.99 -3.67 10.37 ... How can we use this method to tune a model? Let’s use Kernel regressions applied by loess() that we have seen before. Our validation and train sets are ready. We are going to use the train set to train our models with different values of span in each one. Then, we will validate each model by looking at the RMSPE of the model results against our validation set. The winner will be the one with the lowest RMSPE. That’s the plan. Let’s use the set of span = 0.02, 0.1, and 1. #Estimation with degree = 2 (locally quadratic) by training set loe0 &lt;- loess(y ~ x, degree = 2, span = 0.02, data = train) loe1 &lt;- loess(y ~ x, degree = 2, span = 0.1, data = train) loe2 &lt;- loess(y ~ x, degree = 2, span = 1, data = train) #Predicting by using validation set fit0 &lt;- predict(loe0, val$x) fit1 &lt;- predict(loe1, val$x) fit2 &lt;- predict(loe2, val$x) We must also create our performance metric, RMSPE; rmspe0 &lt;- sqrt(mean((val$y-fit0)^2)) rmspe1 &lt;- sqrt(mean((val$y-fit1)^2)) rmspe2 &lt;- sqrt(mean((val$y-fit2)^2)) c(paste(&quot;With span = 0.02&quot;, &quot;rmspe is &quot;, rmspe0), paste(&quot;With span = 0.1&quot;, &quot;rmspe is &quot;, rmspe1), paste(&quot;With span = 1&quot;, &quot;rmspe is &quot;, rmspe2)) ## [1] &quot;With span = 0.02 rmspe is 1.01967570649964&quot; ## [2] &quot;With span = 0.1 rmspe is 1.00901093799357&quot; ## [3] &quot;With span = 1 rmspe is 1.0056119864882&quot; We have several problems with this algorithm. First, we only use three arbitrary values for span. If we use 0.11, for example, we don’t know if its RMSPE could be better or not. Second, we only see the differences across RMSPE’s by manually comparing them. If we had tested for a large set of span values, this would have been difficult. Third, we have used only one set of validation and training sets. If we do it multiple times, we may have different results and different rankings of the models. How are we going to address these issues? Let’s address this last issue, about using more than one set of training and validation sets, first: 9.3 k-fold cross validation We can start with the following figure about k-fold cross-validation. It shows 5-fold cross validation. It splits the data into k-folds, then trains the data on k-1 folds and validation on the one fold that was left out. Although, this type cross validation is the most common one, there are also several different cross validation methods, such as leave-one-out (LOOCV), leave-one-group-out, and time-series cross validation methods, which we will see later. This figure illustrates 5-k CV. We need to create a loop that does the slicing 10 times. If we repeat the same loess() example with 10-k cross validation, we will have 10 RMSPE’s for each span value. To evaluate which one is the lowest, we take the average of those 10 RSMPE’s for each model. #Let&#39;s simulate our data n = 10000 set.seed(1) x &lt;- sort(runif(n)*2*pi) y &lt;- sin(x) + rnorm(n)/4 data &lt;- data.frame(y, x) #Shuffle the order of observations by their index mysample &lt;- sample(n, n, replace = FALSE) k &lt;- 10 #10-k CV nvalidate &lt;- round(n / k) RMSPE &lt;- c() # we need an empty container to store RMSPE&#39;s #loop for (i in 1:k) { if (i &lt; k) { ind_val &lt;- mysample[((i - 1) * nvalidate + 1):(i * nvalidate)] } else{ ind_val &lt;- mysample[((i - 1) * nvalidate + 1):n] } cat(&quot;K-fold loop: &quot;, i, &quot;\\r&quot;) # Counter, no need it in practice data_val &lt;- data[ind_val, ] data_train &lt;- data[-ind_val, ] model &lt;- loess( y ~ x, control = loess.control(surface = &quot;direct&quot;), degree = 2, span = 0.1, data = data_train ) fit &lt;- predict(model, data_val$x) RMSPE[i] &lt;- sqrt(mean((data_val$y - fit) ^ 2)) } ## K-fold loop: 1 K-fold loop: 2 K-fold loop: 3 K-fold loop: 4 K-fold loop: 5 K-fold loop: 6 K-fold loop: 7 K-fold loop: 8 K-fold loop: 9 K-fold loop: 10 RMSPE ## [1] 0.2427814 0.2469909 0.2387873 0.2472059 0.2489808 0.2510570 0.2553914 ## [8] 0.2517241 0.2521053 0.2429688 mean(RMSPE) ## [1] 0.2477993 Note that loess.control() is used for the adjustment for \\(x\\) that are outside of \\(x\\) values used in training set. How can we use this k-fold cross validation in selecting the best span for a range of possible values? 9.4 Cross-validated grid search The traditional way of performing hyperparameter optimization has been a grid search, or a parameter sweep, which is simply an exhaustive search through a manually specified subset of the hyperparameter space of a learning algorithm. This is how grid search is defined by Wikipedia. Although we did not do a grid search, we have already completed the major process in the last example. What we need to add is a hyperparameter grid and a search loop. In each hyperparameter, we need to know the maximum and the minimum values of this subset. For example, span in loess() sets the size of the neighborhood, which ranges between 0 to 1. This controls the degree of smoothing. So, the greater the value of span, smoother the fitted curve is. Additionally the degree argument in loess() is defined as the degree of the polynomials to be used, normally 1 or 2. (Degree 0 is also allowed, but see the ‘Note’. in ?loess). For each learning algorithm, the number of tuning parameters and their ranges will be different. Before running any grid search, therefore, we need to understand their function and range. #Using the same data with reduced size n = 1000 set.seed(1) x &lt;- sort(runif(n) * 2 * pi) y &lt;- sin(x) + rnorm(n) / 4 data &lt;- data.frame(y, x) #Setting CV ind &lt;- sample(n, n, replace = FALSE) k &lt;- 10 #10-fold CV nval &lt;- round(n / k) # Grid grid &lt;- expand.grid(seq(from = 0.01, to = 1, by = 0.02), c(1, 2)) head(grid) ## Var1 Var2 ## 1 0.01 1 ## 2 0.03 1 ## 3 0.05 1 ## 4 0.07 1 ## 5 0.09 1 ## 6 0.11 1 #loops OPT &lt;- c() for (i in 1:k) { if (i &lt; k) { ind_val &lt;- ind[((i - 1) * nval + 1):(i * nval)] } else{ ind_val &lt;- ind[((i - 1) * nval + 1):n] } data_val &lt;- data[ind_val, ] data_train &lt;- data[-ind_val, ] #we need a vector to store RMSPE of each row in the grid RMSPE &lt;- c() #we need to have another loop running each row in grid: for (s in 1:nrow(grid)) { model &lt;- loess( y ~ x, control = loess.control(surface = &quot;direct&quot;), degree = grid[s, 2], span = grid[s, 1], data = data_train ) fit &lt;- predict(model, data_val$x) RMSPE[s] &lt;- sqrt(mean((data_val$y - fit) ^ 2)) } OPT[i] &lt;- which(RMSPE == min(RMSPE), arr.ind = TRUE) } opgrid &lt;- grid[OPT, ] colnames(opgrid) &lt;- c(&quot;span&quot;, &quot;degree&quot;) rownames(opgrid) &lt;- c(1:10) opgrid ## span degree ## 1 0.09 2 ## 2 0.63 2 ## 3 0.23 2 ## 4 0.03 2 ## 5 0.33 2 ## 6 0.59 2 ## 7 0.75 2 ## 8 0.57 2 ## 9 0.25 1 ## 10 0.21 1 These results are good but how are we going to pick one set, the coordinates of the optimal span and degree? It seems that most folds agree that we should use degree = 2, but which span value is the optimal? If the hyperparameter is a discrete value, we can use majority rule with the modal value, which is just the highest number of occurrences in the set. This would be appropriate for degree but not for span. We may use the mean of all 10 optimal span values, each of which is calculated from each fold. This would be also a problem because the range of selected span is from 0.09 to 0.75 and we have only 10 span values. One solution would be to run the same algorithm multiple times so that we can have a better base for averaging the selected spans. Before running the same algorithm multiple times, however, remember, we used the whole sample to tune our hyperparameters. At the outset, we said that this type of application should be avoided. Therefore, we need to create a test set and put a side for reporting accuracy of tuned models. Here is an illustration about this process: Let’s do it: # Using the same data n = 1000 set.seed(1) x &lt;- sort(runif(n) * 2 * pi) y &lt;- sin(x) + rnorm(n) / 4 data &lt;- data.frame(y, x) # Grid grid &lt;- expand.grid(seq(from = 0.01, to = 1, by = 0.02), c(1, 2)) # Train - Test Split set.seed(321) sh_ind &lt;- sample(nrow(data), 0.20 * nrow(data), replace = FALSE) testset &lt;- data[sh_ind,] #20% of data set a side trainset &lt;- data[-sh_ind, ] # k-CV, which is the same as before set.seed(3) ind &lt;- sample(nrow(trainset), nrow(trainset), replace = FALSE) k = 10 nval &lt;- round(nrow(trainset) / k) # CV loop OPT &lt;- c() for (i in 1:k) { if (i &lt; k) { ind_val &lt;- ind[((i - 1) * nval + 1):(i * nval)] } else{ ind_val &lt;- ind[((i - 1) * nval + 1):length(ind)] } data_val &lt;- trainset[ind_val, ] data_train &lt;- trainset[-ind_val, ] RMSPE &lt;- c() for (s in 1:nrow(grid)) { model &lt;- loess( y ~ x, control = loess.control(surface = &quot;direct&quot;), degree = grid[s, 2], span = grid[s, 1], data = data_train ) fit &lt;- predict(model, data_val$x) RMSPE[s] &lt;- sqrt(mean((data_val$y - fit) ^ 2)) } OPT[i] &lt;- which.min(RMSPE) } # Hyperparameters opgrid &lt;- grid[OPT, ] colnames(opgrid) &lt;- c(&quot;span&quot;, &quot;degree&quot;) rownames(opgrid) &lt;- c(1:10) opt_degree &lt;- raster::modal(opgrid[, 2]) opt_degree ## [1] 2 opt_span &lt;- mean(opgrid[, 1]) opt_span ## [1] 0.358 # **** Using the test set for final evaluation ****** model &lt;- loess( y ~ x, control = loess.control(surface = &quot;direct&quot;), degree = opt_degree, span = opt_span, data = trainset ) fit &lt;- predict(model, testset$x) RMSPE_test &lt;- sqrt(mean((testset$y - fit) ^ 2)) RMSPE_test ## [1] 0.2598607 What we have built is an algorithm that learns by trial-and-error. However, we need one more step to finalize this process: instead of doing only one 90%-10% train split, we need to do it multiple times and use the average RMSPE_test and the uncertainty (its variation) associated with it as our final performance metrics. Here again: #Using the same data n = 1000 set.seed(1) x &lt;- sort(runif(n) * 2 * pi) y &lt;- sin(x) + rnorm(n) / 4 data &lt;- data.frame(y, x) # Grid grid &lt;- expand.grid(seq(from = 0.01, to = 1, by = 0.02), c(1, 2)) t = 100 # number of times we loop RMSPE_test &lt;- c() # container for 100 RMSPE&#39;s for (l in 1:t) { # Training-test split set.seed(10 + l) sh_ind &lt;- sample(nrow(data), 0.20 * nrow(data), replace = FALSE) testset &lt;- data[sh_ind, ] #20% of data set a side trainset &lt;- data[-sh_ind,] # k-CV, which is the same as before set.seed(100+l) ind &lt;- sample(nrow(trainset), nrow(trainset), replace = FALSE) k = 10 nval &lt;- round(nrow(trainset) / k) # CV loop OPT &lt;- c() for (i in 1:k) { if (i &lt; k) { ind_val &lt;- ind[((i - 1) * nval + 1):(i * nval)] } else{ ind_val &lt;- ind[((i - 1) * nval + 1):length(ind)] } data_val &lt;- trainset[ind_val,] data_train &lt;- trainset[-ind_val,] RMSPE &lt;- c() for (s in 1:nrow(grid)) { model &lt;- loess( y ~ x, control = loess.control(surface = &quot;direct&quot;), degree = grid[s, 2], span = grid[s, 1], data = data_train ) fit &lt;- predict(model, data_val$x) RMSPE[s] &lt;- sqrt(mean((data_val$y - fit) ^ 2)) } OPT[i] &lt;- which.min(RMSPE) } # Hyperparameters opgrid &lt;- grid[OPT, ] colnames(opgrid) &lt;- c(&quot;span&quot;, &quot;degree&quot;) rownames(opgrid) &lt;- c(1:10) opt_degree &lt;- raster::modal(opgrid[, 2]) opt_span &lt;- mean(opgrid[, 1]) # **** Using the test set for final evaluation ****** model &lt;- loess( y ~ x, control = loess.control(surface = &quot;direct&quot;), degree = opt_degree, span = opt_span, data = trainset ) fit &lt;- predict(model, testset$x) RMSPE_test[l] &lt;- sqrt(mean((testset$y - fit) ^ 2)) } We can now see the average RMSPE and its variance: plot(RMSPE_test, col = &quot;red&quot;) abline(a = mean(RMSPE_test), b = 0, col = &quot;green&quot;, lwd = 3) mean(RMSPE_test) ## [1] 0.2595155 var(RMSPE_test) ## [1] 0.0001542721 9.5 Bootstrapped grid search Similar to cross-validation, the bootstrapping is another and a powerful resampling method that randomly samples from the original data with replacement to create multiple new datasets (also known as “bootstrap samples”). Due to the drawing with replacement, a bootstrap sample may contain multiple instances of the same original cases, and may completely omit other original cases. Although the primary use of bootstrapping is to obtain standard errors of an estimate, we can put these omitted original cases in a sample and form a hold-out set, which is also called as out-of-bag observations (OOB). On average, the split between training and OOB samples is around 65%-35%. Here is a simple application to show the split: x_train &lt;- unique(sample(1000, 1000, replace = TRUE)) length(x_train)/1000 # % of observation in training set ## [1] 0.619 1 - length(x_train)/1000 # % of observations in OOB ## [1] 0.381 We will have many grid-search applications with bootstrapping in the following chapters. Let’s have our first application with same example. #Using the same data n = 1000 set.seed(1) x &lt;- sort(runif(n) * 2 * pi) y &lt;- sin(x) + rnorm(n) / 4 data &lt;- data.frame(y, x) # Grid grid &lt;- expand.grid(seq(from = 0.01, to = 1, by = 0.02), c(1, 2)) t = 100 # number of times we loop RMSPE_test &lt;- c() # container for 100 RMSPE&#39;s for (l in 1:t) { # Training-test split set.seed(10 + l) sh_ind &lt;- sample(nrow(data), 0.20 * nrow(data), replace = FALSE) testset &lt;- data[sh_ind, ] #20% of data set a side trainset &lt;- data[-sh_ind,] # OOB loops OPT &lt;- c() for (i in 1:10) { ind &lt;- unique(sample(nrow(trainset), nrow(trainset), replace = TRUE)) data_val &lt;- trainset[-ind, ] # OOB data_train &lt;- trainset[ind, ] RMSPE &lt;- c() for (s in 1:nrow(grid)) { model &lt;- loess( y ~ x, control = loess.control(surface = &quot;direct&quot;), degree = grid[s, 2], span = grid[s, 1], data = data_train ) fit &lt;- predict(model, data_val$x) RMSPE[s] &lt;- sqrt(mean((data_val$y - fit) ^ 2)) } OPT[i] &lt;- which.min(RMSPE) } # Hyperparameters opgrid &lt;- grid[OPT, ] colnames(opgrid) &lt;- c(&quot;span&quot;, &quot;degree&quot;) rownames(opgrid) &lt;- c(1:10) opt_degree &lt;- raster::modal(opgrid[, 2]) opt_span &lt;- mean(opgrid[, 1]) #Test - RMSPE model &lt;- loess( y ~ x, control = loess.control(surface = &quot;direct&quot;), degree = opt_degree, span = opt_span, data = trainset ) fit &lt;- predict(model, testset$x) RMSPE_test[l] &lt;- sqrt(mean((testset$y - fit) ^ 2)) } We can now see the average RMSPE and its variance: plot(RMSPE_test, col = &quot;red&quot;) abline(a = mean(RMSPE_test), b = 0, col = &quot;green&quot;, lwd = 3) mean(RMSPE_test) ## [1] 0.259672 var(RMSPE_test) ## [1] 0.0001514393 Finally, we can change the order of loops: #Using the same data n = 1000 set.seed(1) x &lt;- sort(runif(n) * 2 * pi) y &lt;- sin(x) + rnorm(n) / 4 data &lt;- data.frame(y, x) # Grid grid &lt;- expand.grid(seq(from = 0.01, to = 1, by = 0.02), c(1, 2)) t = 100 # number of times we loop RMSPE_test &lt;- c() # container for 100 RMSPE&#39;s for (l in 1:t) { # Training-test split set.seed(10 + l) sh_ind &lt;- sample(nrow(data), 0.20 * nrow(data), replace = FALSE) testset &lt;- data[sh_ind,] #20% of data set a side trainset &lt;- data[-sh_ind, ] OPT &lt;- c() # Grid loop for (s in 1:nrow(grid)) { RMSPE &lt;- c() # OOB loops for (i in 1:10) { set.seed(i + 100) ind &lt;- unique(sample(nrow(trainset), nrow(trainset), replace = TRUE)) data_val &lt;- trainset[-ind,] # OOB data_train &lt;- trainset[ind,] model &lt;- loess( y ~ x, control = loess.control(surface = &quot;direct&quot;), degree = grid[s, 2], span = grid[s, 1], data = data_train ) fit &lt;- predict(model, data_val$x) RMSPE[i] &lt;- sqrt(mean((data_val$y - fit) ^ 2)) } OPT[s] &lt;- mean(RMSPE) } # Test RMSPE opgrid &lt;- grid[which.min(OPT),] model &lt;- loess( y ~ x, control = loess.control(surface = &quot;direct&quot;), degree = opgrid[, 2], span = opgrid[, 1], data = trainset ) fit &lt;- predict(model, testset$x) RMSPE_test[l] &lt;- sqrt(mean((testset$y - fit) ^ 2)) } We can now see the average RMSPE and its variance: plot(RMSPE_test, col = &quot;red&quot;) abline(a = mean(RMSPE_test), b = 0, col = &quot;green&quot;, lwd = 3) mean(RMSPE_test) ## [1] 0.2593541 var(RMSPE_test) ## [1] 0.0001504046 The results are similar but the code is cleaner. What happens when the data is time-series? 9.6 When the data is time-series While we have a dedicated section (Section VII) on forecasting with times series data, we will complete this chapter by looking at the fundamental differences between time-series and cross-sectional in terms of grid search. We will use the EuStockMarkets data set pre-loaded in R. The data contains the daily closing prices of major European stock indices: Germany DAX (Ibis), Switzerland SMI, France CAC, and UK FTSE. The data are sampled in business time, i.e., weekends and holidays are omitted. We will focus on the FTSE. Below, the data and its plot: #Data data &lt;- as.data.frame(EuStockMarkets) day_index &lt;- seq(1, nrow(data), by = 1) data &lt;- cbind(data, day_index) head(data) ## DAX SMI CAC FTSE day_index ## 1 1628.75 1678.1 1772.8 2443.6 1 ## 2 1613.63 1688.5 1750.5 2460.2 2 ## 3 1606.51 1678.6 1718.0 2448.2 3 ## 4 1621.04 1684.1 1708.1 2470.4 4 ## 5 1618.16 1686.6 1723.1 2484.7 5 ## 6 1610.61 1671.6 1714.3 2466.8 6 plot( data$day_index, data$FTSE, col = &quot;orange&quot;, cex.main = 0.80, cex.axis = 0.75, type = &quot;l&quot; ) We can use smoothing methods to detect trends in the presence of noisy data especially in cases where the shape of the trend is unknown. A decomposition would show the components of the data: trend, seasonal fluctuations, and the noise, which is unpredictable and remainder of after the trend (and seasonality) is removed Here is an illustration for the FTSE with additive decomposition: tsd &lt;- EuStockMarkets dctsd &lt;- decompose(tsd[, 4]) plot(dctsd, col = &quot;red&quot;) Separating the trend from the noise will enable us to predict the future values better. Having learnt how to model a learning algorithm, we can also train loess() to extract the trend in FTSE. Several smoothing lines are illustrated below to visualize the differences: plot( data$day_index, data$FTSE, type = &quot;l&quot;, col = &quot;red&quot;, cex.main = 0.80, cex.axis = 0.75, lwd = 2 ) lines(data$day_index, predict(lm(FTSE ~ day_index, data)), lwd = 1, col = &quot;green&quot;) lines(data$day_index, predict(loess( data$FTSE ~ data$day_index, degree = 1, span = 0.01 )), lwd = 2, col = &quot;grey&quot;) lines(data$day_index, predict(loess( data$FTSE ~ data$day_index, degree = 1, span = 0.1 )), lwd = 2, col = &quot;blue&quot;) lines(data$day_index, predict(loess( data$FTSE ~ data$day_index, degree = 1, span = 0.9 )), lwd = 2, col = &quot;yellow&quot;) It seems that a linear trend is not appropriate as it underfits to predict. Although a smoothing method like loess() would be a good choice, but which loess() would be a good fit? One way of validating time series data is to keep the time order in the data when we use k-fold cross validation so that in each fold the training data takes place before the test data. This type of cross validation is called as h-step-ahead rolling cross-validation. (There is also a method called as sliding-window-cross-validation). Below we can see an illustration of this kind of cross validation: We are going to split the data without a random shuffle: span &lt;- seq(from = 0.05, to = 1, by = 0.05) # *****h-step-rolling-CV******** h &lt;- 10 opt &lt;- c() #CV loop nvalid &lt;- round(nrow(data) / h) #This gives the 10 cutoff points in rows cut &lt;- c(1) for (j in 1:h) { cut &lt;- c(cut, nvalid * j) } for (i in 1:h) { if (i &lt; h) { train &lt;- data[(cut[1]:cut[i + 1]),] } else{ train &lt;- data[cut[1]:cut[i],] } if (i + 2 &lt; h) valid &lt;- data[(cut[i + 1]:cut[i + 2]),] RMSPE &lt;- c(rep(0), length(span)) #Matrix to store RMSPE for (s in 1:length(span)) { model &lt;- loess( FTSE ~ day_index, control = loess.control(surface = &quot;direct&quot;), degree = 2, span = span[s], data = train ) fit &lt;- predict(model, valid$day_index) RMSPE[s] &lt;- sqrt(mean((valid$FTSE - fit) ^ 2)) } opt[i] &lt;- which(RMSPE == min(RMSPE), arr.ind = TRUE) } #Hyperparameters opt_span &lt;- mean(span[opt]) opt_span ## [1] 0.43 plot( data$day_index, data$FTSE, type = &quot;l&quot;, col = &quot;gray&quot;, cex.main = 0.80, cex.axis = 0.75 ) lines(data$day_index, predict(loess( data$FTSE ~ data$day_index, degree = 2, span = opt_span )), lwd = 2, col = &quot;red&quot;) Note that we did not start this algorithm with the initial split for testing. For the full train-validate-test routine the initial split has to be added into this cross-validation script. Moreover, we started the validation after the first 10% split. We can also decide on this starting point. For example, we can change the code and decide to train the model after 30% training set. That flexibility is specially important if we apply Day Forward-Chaining Nested Cross-Validation, which is the same method but rolling windows are the days. The following figure helps demonstrate this method: Although it is designed for a day-chained cross validation, we can replace days with weeks, months or 21-day windows. In fact, our algorithm that uses 10% splits can be considered a 10% Split Forward-Chaining Nested Cross-Validation. We will see multiple applications with special methods unique to time series data in Section VII. 9.7 Speed Before concluding this section, notice that as the sample size rises, the learning algorithms take longer to complete, specially for cross sectional data. That’s why there are some other cross-validation methods that use only a subsample randomly selected from the original sample to speed up the validation and the test procedures. You can think how slow the conventional cross validation would be if the dataset has 1-2 million observations, for example. There are some methods to accelerate the training process. One method is to increase the delta (the increments) in our grid and identify the range of hyperparameters where the RMSPE becomes the lowest. Then we reshape our grid with finer increments targeting that specific range. Another method is called a random grid search. In this method, instead of the exhaustive enumeration of all combinations of hyperparameters, we select them randomly. This can be found in Random Search for Hyper-Parameter Optimization by James Bergstra and Yoshua Bengio (Berg_2012?). To accelerate the grid search, we can also use parallel processing so that each loop will be assigned to a separate core in capable computers. We will see several application using these options later in the book. Both methods are covered in Chapter 14. Finally, we did not use functions in our algorithms. We should create functions for each major process in algorithms and compile them in one clean “source” script. "],["tuning-in-classification.html", "Chapter 10 Tuning in Classification 10.1 Confusion matrix 10.2 Performance measures 10.3 ROC - Reciever Operating Curve", " Chapter 10 Tuning in Classification What metrics are we going to use when we train our classification models? In kNN, for example, our hyperparameter is \\(k\\), the number of observations in each bin. In our applications with mnist_27 and Adult datasets, \\(k\\) was determined by a metric called as accuracy. What is it? If the choice of \\(k\\) depends on what metrics we use in tuning, can we improve our prediction performance by using a different metric? Moreover, the accuracy is calculated from the confusion table. Yet, the confusion table will be different for a range of discriminating thresholds used for labeling predicted probabilities. These are important questions in classification problems. We will begin answering them in this chapter. 10.1 Confusion matrix In general, whether it is for training or not, measuring the performance of a classification model is an important issue and has to be well understood before fitting or training a model. To evaluate a model’s fit, we can look at its predictive accuracy. In classification problems, this requires predicting \\(Y\\), as either 0 or 1, from the predicted value of \\(p(x)\\), such as \\[ \\hat{Y}=\\left\\{\\begin{array}{ll}{1,} &amp; {\\hat{p}\\left(x_{1}, \\ldots, x_{k}\\right)&gt;\\frac{1}{2}} \\\\ {0,} &amp; {\\hat{p}\\left(x_{1}, \\ldots, x_{k}\\right)&lt;\\frac{1}{2}}\\end{array}\\right. \\] From this transformation of \\(\\hat{p}(x)\\) to \\(\\hat{Y}\\), the overall predictive accuracy can be summarized with a matrix, \\[ \\begin{array}{ccc}{\\text { Predicted vs. Reality}} &amp; {Y=1} &amp; {Y=0} \\\\ {\\hat{Y}=1} &amp; {\\text { TP }_{}} &amp; {\\text { FP }_{}} \\\\ {\\hat{Y}=0} &amp; {\\text { FN }_{}} &amp; {\\text { TN }_{}}\\end{array} \\] where, TP, FP, FN, TN are True positives, False Positives, False Negatives, and True Negatives, respectively. This table is also know as Confusion Table or confusion matrix. The name, confusion, is very intuitive because it is easy to see how the system is confusing two classes. There are many metrics that can be calculated from this table. Let’s use an example given in Wikipedia \\[ \\begin{array}{ccc}{\\text { Predicted vs. Reality}} &amp; {{Y}=Cat} &amp; {{Y}=Dog} \\\\ {\\hat{Y}=Cat} &amp; {\\text { 5 }_{}} &amp; {\\text { 2 }_{}} \\\\ {\\hat{Y}=Dog} &amp; {\\text { 3 }_{}} &amp; {\\text { 3 }_{}}\\end{array} \\] According to this confusion matrix, there are 8 actual cats and 5 actual dogs (column totals). The learning algorithm, however, predicts only 5 cats and 3 dogs correctly. The model predicts 3 cats as dogs and 2 dogs as cats. All correct predictions are located in the diagonal of the table, so it is easy to visually inspect the table for prediction errors, as they will be represented by values outside the diagonal. In predictive analytics, this table (matrix) allows more detailed analysis than mere proportion of correct classifications (accuracy). Accuracy (\\((TP+TN)/n\\)) is not a reliable metric for the real performance of a classifier, when the dataset is unbalanced in terms of numbers of observations in each class. It can be seen how misleading the use of \\((TP+TN)/n\\) could be, if there were 95 cats and only 5 dogs in our example. If we choose accuracy as the performance measure in our training, our learning algorithm might classify all the observations as cats, because the overall accuracy would be 95%. In that case, however, all the dog would be misclassified as cats. 10.2 Performance measures Which metrics should we be using in training our classification models? These questions are more important when the classes are not in balance. Moreover, in some situation, false predictions would be more important then true predictions. In a situation that you try to predict, for example, cancer, minimizing false negatives (the model misses cancer patients) would be more important than minimizing false positives (the model wrongly predicts cancer). When we have an algorithm to predict spam emails, however, false positives would be the target to minimize rather than false negatives. Here is the full picture of various metrics using the same confusion table from Wikipedia: Let’s summarize some of the metrics and their use with examples for detecting cancer: Accuracy: the number of correct predictions (with and without cancer) relative to the number of observations (patients). This can be used when the classes are balanced with not less than a 60-40% split. \\((TP+TN)/n\\). Balanced Accuracy: when the class balance is worse than 60-40% split, \\((TP/P + TN/N)/2\\). Precision: the percentage positive predictions that are correct. That is, the proportion of patients that we predict as having cancer, actually have cancer, \\(TP/(TP+FP)\\). Sensitivity: the percentage of positives that are predicted correctly. That is, the proportion of patients that actually have cancer was correctly predicted by the algorithm as having cancer, \\(TP/(TP+FN)\\). This measure is also called as True Positive Rate or as Recall. Specificity: the percentage of negatives that are predicted correctly. Proportion of patients that do not have cancer, are predicted by the model as non-cancerous, This measure is also called as True Positive Rate = \\(TN/(TN+FP)\\). Here is the summary: \\[ \\begin{array}{ccc}{\\text { Predicted vs. Reality}} &amp; {{Y}=Cat} &amp; {{Y}=Dog} \\\\ {\\hat{Y}=Cat} &amp; {\\text {TPR or Sensitivity }_{}} &amp; {\\text { FNR or Fall-out }_{}} \\\\ {\\hat{Y}=Dog} &amp; {\\text { FNR or Miss Rate }_{}} &amp; {\\text { TNR or Specificity }_{}}\\end{array} \\] Kappa is also calculated in most cases. It is an interesting measure because it compares the actual performance of prediction with what it would be if a random prediction was carried out. For example, suppose that your model predicts \\(Y\\) with 95% accuracy. How good your prediction power would be if a random choice would also predict 70% of \\(Y\\)s correctly? Let’s use an example: \\[ \\begin{array}{ccc}{\\text { Predicted vs. Reality}} &amp; {{Y}=Cat} &amp; {{Y}=Dog} \\\\ {\\hat{Y}=Cat} &amp; {\\text { 22 }_{}} &amp; {\\text { 9 }_{}} \\\\ {\\hat{Y}=Dog} &amp; {\\text { 7 }_{}} &amp; {\\text { 13 }_{}}\\end{array} \\] In this case the accuracy is \\((22+13)/51 = 0.69\\) But how much of it is due the model’s performance itself? In other words, the distribution of cats and dogs can also give a predictive clue such that a certain level of prediction accuracy can be achieved by chance without any learning algorithm. For the TP cell in the table, this can be calculated as the difference between observed accuracy (OA) and expected accuracy (EA), \\[ \\mathrm{(OA-EA)_{TP}}=\\mathrm{Pr}(\\hat{Y}=Cat)[\\mathrm{Pr}(Y=Cat |\\hat{Y}= Cat)-\\mathrm{P}(Y=Cat)], \\] Remember from your statistics class, if the two variables are independent, the conditional probability of \\(X\\) given \\(Y\\) has to be equal to the marginal probability of \\(X\\). Therefore, inside the brackets, the difference between the conditional probability, which reflects the probability of predicting cats due to the model, and the marginal probability of observing actual cats reflects the true level of predictive power of the model by removing the randomness in prediction. \\[ \\mathrm{(OA-EA)_{TN}}=\\mathrm{Pr}(\\hat{Y}=Dog)[\\mathrm{Pr}(Y=Dog |\\hat{Y}= Dog)-\\mathrm{P}(Y=Dog)], \\] If we use the joint and marginal probability definitions, these can be written as: \\[ OA-EA=\\frac{m_{i j}}{n}-\\frac{m_{i} m_{j}}{n^{2}} \\] Here is the calculation of Kappa for our example: Total, \\(n = 51\\), \\(OA-EA\\) for \\(TP\\) = \\(22/51-31 \\times (29/51^2) = 0.0857\\) \\(OA-EA\\) for \\(TN\\) = \\(13/51-20 \\times (21/51^2) = 0.0934\\) And we normalize it by \\(1-EA = 1- 31 \\times (29/51^2) + 20 \\times (21/51^2) = 0.51\\), which is the value if the prediction was 100% successful. Hence, Kappa: \\((0.0857+0.0934) / (1 - 0.51) = 0.3655\\) Finally, Jouden’s J statistics also as known as Youden’s index or Informedness, is a single statistics that captures the performance of prediction. It’s simply \\(J=TPR+TNR-1\\) and ranges between 0 and 1 indicating useless and perfect prediction performance, respectively. This metric is also related to Receiver Operating Curve (ROC) analysis, which is the subject of next section. 10.3 ROC - Reciever Operating Curve Our outcome variable is categorical (\\(Y = 1\\) or \\(0\\)). Most classification algorithms calculate the predicted probability of success (\\(Y = 1\\)). If the probability is larger than a fixed cut-off threshold (discriminating threshold), then we assume that the model predicts success (Y = 1); otherwise, we assume that it predicts failure. As a result of such a procedure, the comparison of the observed and predicted values summarized in a confusion table depends on the threshold. The predictive accuracy of a model as a function of threshold can be summarized by Area Under Curve (AUC) of Receiver Operating Characteristics (ROC). The ROC curve, which is is a graphical plot that illustrates the diagnostic ability of a binary classifier, indicates a trade-off between True Positive Rate (TPR) and False Positive Rate (FPR). Hence, the success of a model comes with its predictions that increases TPR without raising FPR. The ROC curve was first used during World War II for the analysis of radar signals before it was employed in signal detection theory. Here is a visualization: Let’s start with an example, where we have 100 individuals, 50 with \\(y_i=1\\) and 50 with \\(y_i=0\\), which is well-balanced. If we use a discriminating threshold (0%) that puts everybody into Category 1 or a threshold (100%) that puts everybody into Category 2, that is, \\[ \\hat{Y}=\\left\\{\\begin{array}{ll}{1,} &amp; {\\hat{p}\\left(x_{1}, \\ldots, x_{k}\\right)&gt;0 \\%} \\\\ {0,} &amp; {\\hat{p}\\left(x_{1}, \\ldots, x_{k}\\right)\\leq0 \\%}\\end{array}\\right. \\] and, \\[ \\hat{Y}=\\left\\{\\begin{array}{ll}{1,} &amp; {\\hat{p}\\left(x_{1}, \\ldots, x_{k}\\right)&gt;100 \\%} \\\\ {0,} &amp; {\\hat{p}\\left(x_{1}, \\ldots, x_{k}\\right)\\leq100 \\%}\\end{array}\\right. \\] this would have led to the following confusing tables, respectively: \\[ \\begin{array}{ccc}{\\text { Predicted vs. Reality}} &amp; {{Y}=1} &amp; {{Y}=0} \\\\ {\\hat{Y}=1} &amp; {\\text { 50 }_{}} &amp; {\\text { 50 }_{}} \\\\ {\\hat{Y}=0} &amp; {\\text { 0 }_{}} &amp; {\\text { 0 }_{}}\\end{array} \\] \\[ \\begin{array}{ccc}{\\text { Predicted vs. Reality}} &amp; {{Y}=1} &amp; {{Y}=0} \\\\ {\\hat{Y}=1} &amp; {\\text { 0 }_{}} &amp; {\\text { 0 }_{}} \\\\ {\\hat{Y}=0} &amp; {\\text { 50 }_{}} &amp; {\\text { 50 }_{}}\\end{array} \\] In the first case, \\(TPR = 1\\) and \\(FPR = 1\\); and in the second case, \\(TPR = 0\\) and \\(FPR = 0\\). So when we calculate all possible confusion tables with different values of thresholds ranging from 0% to 100%, we will have the same number of (\\(TPR, FPR\\)) points each corresponding to one threshold. The ROC curve is the curve that connects these points. Let’s use an example with the Boston Housing Market dataset to illustrate ROC: library(MASS) data(Boston) # Create our binary outcome data &lt;- Boston[, -14] #Dropping &quot;medv&quot; data$dummy &lt;- c(ifelse(Boston$medv &gt; 25, 1, 0)) # Use logistic regression for classification model &lt;- glm(dummy ~ ., data = data, family = &quot;binomial&quot;) summary(model) ## ## Call: ## glm(formula = dummy ~ ., family = &quot;binomial&quot;, data = data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.3498 -0.2806 -0.0932 -0.0006 3.3781 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 5.312511 4.876070 1.090 0.275930 ## crim -0.011101 0.045322 -0.245 0.806503 ## zn 0.010917 0.010834 1.008 0.313626 ## indus -0.110452 0.058740 -1.880 0.060060 . ## chas 0.966337 0.808960 1.195 0.232266 ## nox -6.844521 4.483514 -1.527 0.126861 ## rm 1.886872 0.452692 4.168 3.07e-05 *** ## age 0.003491 0.011133 0.314 0.753853 ## dis -0.589016 0.164013 -3.591 0.000329 *** ## rad 0.318042 0.082623 3.849 0.000118 *** ## tax -0.010826 0.004036 -2.682 0.007314 ** ## ptratio -0.353017 0.122259 -2.887 0.003884 ** ## black -0.002264 0.003826 -0.592 0.554105 ## lstat -0.367355 0.073020 -5.031 4.88e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 563.52 on 505 degrees of freedom ## Residual deviance: 209.11 on 492 degrees of freedom ## AIC: 237.11 ## ## Number of Fisher Scoring iterations: 7 And our prediction (in-sample): # Classified Y&#39;s by TRUE and FALSE yHat &lt;- model$fitted.values &gt; 0.5 conf_table &lt;- table(yHat, data$dummy) #let&#39;s change the order of cells ctt &lt;- as.matrix(conf_table) ct &lt;- matrix(0, 2, 2) ct[1,1] &lt;- ctt[2,2] ct[2,2] &lt;- ctt[1,1] ct[1,2] &lt;- ctt[2,1] ct[2,1] &lt;- ctt[1,2] rownames(ct) &lt;- c(&quot;Yhat = 1&quot;, &quot;Yhat = 0&quot;) colnames(ct) &lt;- c(&quot;Y = 1&quot;, &quot;Y = 0&quot;) ct ## Y = 1 Y = 0 ## Yhat = 1 100 16 ## Yhat = 0 24 366 It would be much easier if we create our own function to rotate a matrix/table: rot &lt;- function(x){ t &lt;- apply(x, 2, rev) tt &lt;- apply(t, 1, rev) return(t(tt)) } ct &lt;- rot(conf_table) rownames(ct) &lt;- c(&quot;Yhat = 1&quot;, &quot;Yhat = 0&quot;) colnames(ct) &lt;- c(&quot;Y = 1&quot;, &quot;Y = 0&quot;) ct ## ## yHat Y = 1 Y = 0 ## Yhat = 1 100 16 ## Yhat = 0 24 366 Now we calculate our TPR, FPR, and J-Index: #TPR TPR &lt;- ct[1,1]/(ct[1,1]+ct[2,1]) TPR ## [1] 0.8064516 #FPR FPR &lt;- ct[1,2]/(ct[1,2]+ct[2,2]) FPR ## [1] 0.04188482 #J-Index TPR-FPR ## [1] 0.7645668 These rates are calculated for the threshold of 0.5. We can have all pairs of \\(TPR\\) and \\(FPR\\) for all possible discrimination thresholds. What’s the possible set? We will use our \\(\\hat{P}\\) values for this. #We create an ordered grid from our fitted values summary(model$fitted.values) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000000 0.004205 0.035602 0.245059 0.371758 0.999549 phat &lt;- model$fitted.values[order(model$fitted.values)] length(phat) ## [1] 506 #We need to have containers for the pairs of TPR and FPR TPR &lt;- c() FPR &lt;- c() #Now the loop for (i in 1:length(phat)) { yHat &lt;- model$fitted.values &gt; phat[i] conf_table &lt;- table(yHat, data$dummy) ct &lt;- as.matrix(conf_table) if(sum(dim(ct))&gt;3){ #here we ignore the thresholds 0 and 1 TPR[i] &lt;- ct[2,2]/(ct[2,2]+ct[1,2]) FPR[i] &lt;- ct[2,1]/(ct[1,1]+ct[2,1]) } } plot(FPR, TPR, col= &quot;blue&quot;, type = &quot;l&quot;, main = &quot;ROC&quot;, lwd = 3) abline(a = 0, b = 1, col=&quot;red&quot;) Several things we observe on this curve. First, there is a trade-off between TPF and FPR. Approximately, after 70% of TPR, an increase in TPF can be achieved by increasing FPR, which means that if we care more about the possible lowest FPR, we can fix the discriminating rate at that point. Second, we can identify the best discriminating threshold that makes the distance between TPR and FPR largest. In other words, we can identify the threshold where the marginal gain on TPR would be equal to the marginal cost of FPR. This can be achieved by the Jouden’s J statistics, \\(J=TPR+TNR-1\\), which identifies the best discriminating threshold. Note that \\(TNR= 1-FPR\\). Hence \\(J = TPR-FPR\\). # Youden&#39;s J Statistics J &lt;- TPR - FPR # The best discriminating threshold phat[which.max(J)] ## 231 ## 0.1786863 #TPR and FPR at this threshold TPR[which.max(J)] ## [1] 0.9354839 FPR[which.max(J)] ## [1] 0.1361257 J[which.max(J)] ## [1] 0.7993582 This simple example shows that the best (in-sample) fit can be achieved by \\[ \\hat{Y}=\\left\\{\\begin{array}{ll}{1,} &amp; {\\hat{p}\\left(x_{1}, \\ldots, x_{k}\\right)&gt;17.86863 \\%} \\\\ {0,} &amp; {\\hat{p}\\left(x_{1}, \\ldots, x_{k}\\right)\\leq17.86863 \\%}\\end{array}\\right. \\] ## AUC - Area Under the Curve Finally, we measure the predictive accuracy by the area under the ROC curve. An area of 1 represents a perfect performance; an area of 0.5 represents a worthless prediction. This is because an area of 0.5 suggests its performance is no better than random chance. For example, an accepted rough guide for classifying the accuracy of a diagnostic test in medical procedures is 0.90-1.00 = Excellent (A) 0.80-0.90 = Good (B) 0.70-0.80 = Fair (C) 0.60-0.70 = Poor (D) 0.50-0.60 = Fail (F) Since the formula and its derivation is beyond the scope of this chapter, we will use the package ROCR to calculate it. library(ROCR) data$dummy &lt;- c(ifelse(Boston$medv &gt; 25, 1, 0)) model &lt;- glm(dummy ~ ., data = data, family = &quot;binomial&quot;) phat &lt;- model$fitted.values phat_df &lt;- data.frame(phat, &quot;Y&quot; = data$dummy) pred_rocr &lt;- prediction(phat_df[,1], phat_df[,2]) perf &lt;- performance(pred_rocr,&quot;tpr&quot;,&quot;fpr&quot;) plot(perf, colorize=TRUE) abline(a = 0, b = 1) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) AUC &lt;- auc_ROCR@y.values[[1]] AUC ## [1] 0.9600363 This ROC curve is the same as the one that we developed earlier. When we train a model, in each run (different train and test sets) we will obtain a different AUC. Differences in AUC across train and validation sets creates an uncertainty about AUC. Consequently, the asymptotic properties of AUC for comparing alternative models has become a subject of discussions in the literature. Another important point is that, while AUC represents the entire area under the curve, our interest would be on a specific location of TPR or FPR. Hence it’s possible that, for any given two competing algorithms, while one prediction algorithm has a higher overall AUC, the other one could have a better AUC in that specific location. This issue can be seen in the following figure taken from Bad practices in evaluation methodology relevant to class-imbalanced problems by Jan Brabec and Lukas Machlica (Brab_2018?). For example, in the domain of network traffic intrusion-detection, the imbalance ratio is often higher than 1:1000, and the cost of a false alarm for an applied system is very high. This is due to increased analysis and remediation costs of infected devices. In such systems, the region of interest on the ROC curve is for false positive rate at most 0.0001. If AUC was computed in the usual way over the complete ROC curve then 99.99% of the area would be irrelevant and would represent only noise in the final outcome. We demonstrate this phenomenon in Figure 1. If AUC has to be used, we suggest to discuss the region of interest, and eventually compute the area only at this region. This is even more important if ROC curves are not presented, but only AUCs of the compared algorithms are reported. Most of the challenges in classification problems are related to class imbalances in the data. We look at this issue in Cahpter 39. "],["classification-example.html", "Chapter 11 Classification Example 11.1 LPM 11.2 Logistic Regression 11.3 kNN", " Chapter 11 Classification Example We can conclude this section with a classification example. We will use Adult dataset. The information on the dataset is given at the Machine Learning Repository at UCI (Kohavi_1996?): The prediction task is to determine whether a person makes over $50K a year. This question would be similar to the question of whether the person makes less than 50K. However, we need to be careful in defining which class will be positive or negative. Suppose we have \\(Y\\), 0 and 1, and we define 1 as a positive class: \\[ \\begin{array}{ccc}{\\text { Predicted vs. Reality}} &amp; {{Y}=1+} &amp; {{Y}=0-} \\\\ {\\hat{Y}=1+} &amp; {\\text { TP }_{}} &amp; {\\text { FP }_{}} \\\\ {\\hat{Y}=0-} &amp; {\\text { FN }_{}} &amp; {\\text { TN }_{}}\\end{array} \\] Now suppose we define 1 as a negative class: \\[ \\begin{array}{ccc}{\\text { Predicted vs. Reality}} &amp; {{Y}=0+} &amp; {{Y}=1-} \\\\ {\\hat{Y}=0+} &amp; {\\text { TP }_{}} &amp; {\\text { FP }_{}} \\\\ {\\hat{Y}=1-} &amp; {\\text { FN }_{}} &amp; {\\text { TN }_{}}\\end{array} \\] Of course this is just a notational difference and nothing changes in calculations. But some performance measures, especially, sensitivity (TPR) and fall-out (FPR) will be different. We are going to use the original train set again to avoid some data cleaning jobs that we mentioned in Chapter 5. # Download adult income data # url.train &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data&quot; # url.names &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names&quot; # download.file(url.train, destfile = &quot;adult_train.csv&quot;) # download.file(url.names, destfile = &quot;adult_names.txt&quot;) # Read the training set into memory df &lt;- read.csv(&quot;adult_train.csv&quot;, header = FALSE) varNames &lt;- c(&quot;Age&quot;, &quot;WorkClass&quot;, &quot;fnlwgt&quot;, &quot;Education&quot;, &quot;EducationNum&quot;, &quot;MaritalStatus&quot;, &quot;Occupation&quot;, &quot;Relationship&quot;, &quot;Race&quot;, &quot;Sex&quot;, &quot;CapitalGain&quot;, &quot;CapitalLoss&quot;, &quot;HoursPerWeek&quot;, &quot;NativeCountry&quot;, &quot;IncomeLevel&quot;) names(df) &lt;- varNames data &lt;- df In each machine learning application, the data preparation stage (i.e. cleaning the data, organizing the columns and rows, checking out the columns’ names, checking the types of each feature, identifying and handling the missing observations, etc) is a very important step and should be dealt with a good care. First, let’s see if the data balanced or not: tbl &lt;- table(data$IncomeLevel) tbl ## ## &lt;=50K &gt;50K ## 24720 7841 tbl[2] / tbl[1] ## &gt;50K ## 0.3171926 There are multiple variables that are chr in the data. str(data) ## &#39;data.frame&#39;: 32561 obs. of 15 variables: ## $ Age : int 39 50 38 53 28 37 49 52 31 42 ... ## $ WorkClass : chr &quot; State-gov&quot; &quot; Self-emp-not-inc&quot; &quot; Private&quot; &quot; Private&quot; ... ## $ fnlwgt : int 77516 83311 215646 234721 338409 284582 160187 209642 45781 159449 ... ## $ Education : chr &quot; Bachelors&quot; &quot; Bachelors&quot; &quot; HS-grad&quot; &quot; 11th&quot; ... ## $ EducationNum : int 13 13 9 7 13 14 5 9 14 13 ... ## $ MaritalStatus: chr &quot; Never-married&quot; &quot; Married-civ-spouse&quot; &quot; Divorced&quot; &quot; Married-civ-spouse&quot; ... ## $ Occupation : chr &quot; Adm-clerical&quot; &quot; Exec-managerial&quot; &quot; Handlers-cleaners&quot; &quot; Handlers-cleaners&quot; ... ## $ Relationship : chr &quot; Not-in-family&quot; &quot; Husband&quot; &quot; Not-in-family&quot; &quot; Husband&quot; ... ## $ Race : chr &quot; White&quot; &quot; White&quot; &quot; White&quot; &quot; Black&quot; ... ## $ Sex : chr &quot; Male&quot; &quot; Male&quot; &quot; Male&quot; &quot; Male&quot; ... ## $ CapitalGain : int 2174 0 0 0 0 0 0 0 14084 5178 ... ## $ CapitalLoss : int 0 0 0 0 0 0 0 0 0 0 ... ## $ HoursPerWeek : int 40 13 40 40 40 40 16 45 50 40 ... ## $ NativeCountry: chr &quot; United-States&quot; &quot; United-States&quot; &quot; United-States&quot; &quot; United-States&quot; ... ## $ IncomeLevel : chr &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; ... table(data$WorkClass) ## ## ? Federal-gov Local-gov Never-worked ## 1836 960 2093 7 ## Private Self-emp-inc Self-emp-not-inc State-gov ## 22696 1116 2541 1298 ## Without-pay ## 14 table(data$NativeCountry) ## ## ? Cambodia ## 583 19 ## Canada China ## 121 75 ## Columbia Cuba ## 59 95 ## Dominican-Republic Ecuador ## 70 28 ## El-Salvador England ## 106 90 ## France Germany ## 29 137 ## Greece Guatemala ## 29 64 ## Haiti Holand-Netherlands ## 44 1 ## Honduras Hong ## 13 20 ## Hungary India ## 13 100 ## Iran Ireland ## 43 24 ## Italy Jamaica ## 73 81 ## Japan Laos ## 62 18 ## Mexico Nicaragua ## 643 34 ## Outlying-US(Guam-USVI-etc) Peru ## 14 31 ## Philippines Poland ## 198 60 ## Portugal Puerto-Rico ## 37 114 ## Scotland South ## 12 80 ## Taiwan Thailand ## 51 18 ## Trinadad&amp;Tobago United-States ## 19 29170 ## Vietnam Yugoslavia ## 67 16 We can see that there is only one observation in Holand-Netherlands. This is a problem because it will be either in the training set or the test set. Therefore, when you estimate without taking care of it, it will give this error: Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : factor NativeCountry has new levels Holand-Netherlands We will see later how to take care of these issues in a loop with several error handling options. But now, let’s drop this observation: ind &lt;- which(data$NativeCountry ==&quot; Holand-Netherlands&quot;) data &lt;- data[-ind, ] Although some packages like lm() and glm() can use character variables, we should take care of them properly before any type of data analysis. Here is an example: df &lt;- data #converting by a loop for (i in 1:ncol(df)) { if (is.character(df[, i])) df[, i] &lt;- as.factor(df[, i]) } df &lt;- data #Converting with `apply()` family df[sapply(df, is.character)] &lt;- lapply(df[sapply(df, is.character)], as.factor) The job is to use LPM, Logistic, and kNN models to see which one could be a better predictive model for the data. In LPM and Logistic, we do not (yet) have any parameter to tune for a better prediction. Although we could use a degree of polynomials for selected features, we will set aside that option for now. We will later see regularization methods for parametric models, which will make LPM and logistic models “trainable”. In kNN, \\(k\\) is the hyperparameter to train the model. There are several key points to keep in mind in this classification practice: What performance metric(s) are we going to use for comparing the alternative models? How are we going to transform the predicted probabilities to classes (0’s and 1’s) so that we can have the confusion matrix? Let’s start with LPM first. 11.1 LPM anyNA(data) ## [1] FALSE # Our LPM requires data$Y &lt;- ifelse(data$IncomeLevel==&quot; &lt;=50K&quot;, 0, 1) data &lt;- data[, -15] Now, we are ready. We will use ROC and AUC for comparing the models. library(ROCR) AUC &lt;- c() t = 100 # number of times we loop for (i in 1:t) { set.seed(i) shuffle &lt;- sample(nrow(data), nrow(data), replace = FALSE) k &lt;- 5 testind &lt;- shuffle[1:(nrow(data) / k)] trainind &lt;- shuffle[-testind] trdf &lt;- data[trainind, ] #80% of the data tsdf &lt;- data[testind, ] #20% of data set a side #LPM model1 &lt;- glm(Y ~ ., data = trdf, family = &quot;gaussian&quot;) phat &lt;- predict(model1, tsdf) phat[phat &lt; 0] &lt;- 0 phat[phat &gt; 1] &lt;- 1 # ROC &amp; AUC (from ROCR) phat_df &lt;- data.frame(phat, &quot;Y&quot; = tsdf$Y) pred_rocr &lt;- prediction(phat_df[, 1], phat_df[, 2]) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) AUC[i] &lt;- auc_ROCR@y.values[[1]] } plot(AUC, col = &quot;grey&quot;) abline(a = mean(AUC), b = 0, col = &quot;red&quot;) mean(AUC) ## [1] 0.8936181 sqrt(var(AUC)) ## [1] 0.003810335 Let’s see the ROC curve from the last run. # ROC from the last run by `ROCR` perf &lt;- performance(pred_rocr, &quot;tpr&quot;, &quot;fpr&quot;) plot(perf, colorize = TRUE) abline(a = 0, b = 1) # And our &quot;own&quot; ROC (we will use ROCR in this book, though) phator &lt;- phat[order(phat)] phator[phator &lt; 0] &lt;- 0 phator[phator &gt; 1] &lt;- 1 phator &lt;- unique(phator) TPR &lt;- c() FPR &lt;- c() for (i in 1:length(phator)) { yHat &lt;- phat &gt; phator[i] conf_table &lt;- table(yHat, tsdf$Y) ct &lt;- as.matrix(conf_table) if (sum(dim(ct)) &gt; 3) { #here we ignore the min and max thresholds TPR[i] &lt;- ct[2, 2] / (ct[2, 2] + ct[1, 2]) FPR[i] &lt;- ct[2, 1] / (ct[1, 1] + ct[2, 1]) } } # Flat and vertical sections are omitted plot(FPR, TPR, col = &quot;blue&quot;, type = &quot;l&quot;, main = &quot;ROC&quot;) abline(a = 0, b = 1, col = &quot;red&quot;) What’s the confusion table at the “best” discriminating threshold? The answer is the one where the difference between TPR and FPR is maximized: Youden’s J Statistics. Note that this answers would be different if we have different weights in TPR and FPR. We may also have different targets, maximum FPR, for example. # Youden&#39;s J Statistics J &lt;- TPR - FPR # The best discriminating threshold opt_th &lt;- phator[which.max(J)] opt_th ## [1] 0.318723 #TPR and FPR at this threshold TPR[which.max(J)] ## [1] 0.8494898 FPR[which.max(J)] ## [1] 0.2024676 J[which.max(J)] ## [1] 0.6470222 And the confusion table (from the last run): yHat &lt;- phat &gt; opt_th conf_table &lt;- table(yHat, tsdf$Y) # Function to rotate the table (we did before) rot &lt;- function(x){ t &lt;- apply(x, 2, rev) tt &lt;- apply(t, 1, rev) return(t(tt)) } # Better looking table ct &lt;- rot(conf_table) rownames(ct) &lt;- c(&quot;Yhat = 1&quot;, &quot;Yhat = 0&quot;) colnames(ct) &lt;- c(&quot;Y = 1&quot;, &quot;Y = 0&quot;) ct ## ## yHat Y = 1 Y = 0 ## Yhat = 1 1332 1001 ## Yhat = 0 236 3943 Note that the optimal threshold is almost the ratio of cases in the data around 31%. We will come back to this issue later. 11.2 Logistic Regression library(ROCR) AUC &lt;- c() t = 100 for (i in 1:t) { set.seed(i) shuffle &lt;- sample(nrow(data), nrow(data), replace = FALSE) k &lt;- 5 testind &lt;- shuffle[1:(nrow(data) / k)] trainind &lt;- shuffle[-testind] trdf &lt;- data[trainind,] #80% of the data tsdf &lt;- data[testind,] #20% of data set a side #Logistic model2 &lt;- glm(Y ~ ., data = trdf, family = &quot;binomial&quot;) #Note &quot;response&quot;. It predicts phat. Another option is &quot;class&quot; #which predicts yhat by using 0.5 phat &lt;- predict(model2, tsdf, type = &quot;response&quot;) phat[phat &lt; 0] &lt;- 0 phat[phat &gt; 1] &lt;- 1 # ROC &amp; AUC (from ROCR) phat_df &lt;- data.frame(phat, &quot;Y&quot; = tsdf$Y) pred_rocr &lt;- prediction(phat_df[, 1], phat_df[, 2]) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) AUC[i] &lt;- auc_ROCR@y.values[[1]] } plot(AUC, col = &quot;grey&quot;) abline(a = mean(AUC), b = 0, col = &quot;red&quot;) mean(AUC) ## [1] 0.908179 sqrt(var(AUC)) ## [1] 0.003593404 Both LPM and Logistic methods are linear classifiers. We can add polynomials and interactions manually to capture possible nonlinearities in the data but that would be an impossible job as the number of features would grow exponentially. This brings us to a nonparametric classifier, kNN. 11.3 kNN We will train kNN with the choice of \\(k\\) and use AUC as our performance criteria in choosing \\(k\\). 11.3.1 kNN 10-fold CV There are several packages in R for kNN applications: knn() from the class package and knn3() in the caret package. We will use knn3() in the caret package. Since kNN use distances, we should scale the numerical variables first to make their magnitudes on the same scale. rm(list = ls()) data &lt;- read.csv(&quot;adult_train.csv&quot;, header = FALSE) varNames &lt;- c(&quot;Age&quot;, &quot;WorkClass&quot;, &quot;fnlwgt&quot;, &quot;Education&quot;, &quot;EducationNum&quot;, &quot;MaritalStatus&quot;, &quot;Occupation&quot;, &quot;Relationship&quot;, &quot;Race&quot;, &quot;Sex&quot;, &quot;CapitalGain&quot;, &quot;CapitalLoss&quot;, &quot;HoursPerWeek&quot;, &quot;NativeCountry&quot;, &quot;IncomeLevel&quot;) names(data) &lt;- varNames df &lt;- data # Dropping single observation ind &lt;- which(df$NativeCountry==&quot; Holand-Netherlands&quot;) df &lt;- df[-ind, ] #Scaling the numerical variables for(i in 1:ncol(df)) if(is.integer(df[,i])) df[,i] &lt;- scale(df[,i]) #Converting the character variables to factor df[sapply(df, is.character)] &lt;- lapply(df[sapply(df, is.character)], as.factor) str(df) ## &#39;data.frame&#39;: 32560 obs. of 15 variables: ## $ Age : num [1:32560, 1] 0.0307 0.8371 -0.0427 1.057 -0.7758 ... ## ..- attr(*, &quot;scaled:center&quot;)= num 38.6 ## ..- attr(*, &quot;scaled:scale&quot;)= num 13.6 ## $ WorkClass : Factor w/ 9 levels &quot; ?&quot;,&quot; Federal-gov&quot;,..: 8 7 5 5 5 5 5 7 5 5 ... ## $ fnlwgt : num [1:32560, 1] -1.064 -1.009 0.245 0.426 1.408 ... ## ..- attr(*, &quot;scaled:center&quot;)= num 189783 ## ..- attr(*, &quot;scaled:scale&quot;)= num 105548 ## $ Education : Factor w/ 16 levels &quot; 10th&quot;,&quot; 11th&quot;,..: 10 10 12 2 10 13 7 12 13 10 ... ## $ EducationNum : num [1:32560, 1] 1.13 1.13 -0.42 -1.2 1.13 ... ## ..- attr(*, &quot;scaled:center&quot;)= num 10.1 ## ..- attr(*, &quot;scaled:scale&quot;)= num 2.57 ## $ MaritalStatus: Factor w/ 7 levels &quot; Divorced&quot;,&quot; Married-AF-spouse&quot;,..: 5 3 1 3 3 3 4 3 5 3 ... ## $ Occupation : Factor w/ 15 levels &quot; ?&quot;,&quot; Adm-clerical&quot;,..: 2 5 7 7 11 5 9 5 11 5 ... ## $ Relationship : Factor w/ 6 levels &quot; Husband&quot;,&quot; Not-in-family&quot;,..: 2 1 2 1 6 6 2 1 2 1 ... ## $ Race : Factor w/ 5 levels &quot; Amer-Indian-Eskimo&quot;,..: 5 5 5 3 3 5 3 5 5 5 ... ## $ Sex : Factor w/ 2 levels &quot; Female&quot;,&quot; Male&quot;: 2 2 2 2 1 1 1 2 1 2 ... ## $ CapitalGain : num [1:32560, 1] 0.148 -0.146 -0.146 -0.146 -0.146 ... ## ..- attr(*, &quot;scaled:center&quot;)= num 1078 ## ..- attr(*, &quot;scaled:scale&quot;)= num 7385 ## $ CapitalLoss : num [1:32560, 1] -0.217 -0.217 -0.217 -0.217 -0.217 ... ## ..- attr(*, &quot;scaled:center&quot;)= num 87.2 ## ..- attr(*, &quot;scaled:scale&quot;)= num 403 ## $ HoursPerWeek : num [1:32560, 1] -0.0354 -2.2221 -0.0354 -0.0354 -0.0354 ... ## ..- attr(*, &quot;scaled:center&quot;)= num 40.4 ## ..- attr(*, &quot;scaled:scale&quot;)= num 12.3 ## $ NativeCountry: Factor w/ 41 levels &quot; ?&quot;,&quot; Cambodia&quot;,..: 39 39 39 39 6 39 23 39 39 39 ... ## $ IncomeLevel : Factor w/ 2 levels &quot; &lt;=50K&quot;,&quot; &gt;50K&quot;: 1 1 1 1 1 1 1 2 2 2 ... Now we are ready. Here is our kNN training: library(caret) library(ROCR) set.seed(123) #for the same results, no need otherwise sh &lt;- sample(nrow(df), nrow(df), replace = FALSE) h &lt;- 10 ind_test &lt;- sh[1:(nrow(df) / h)] ind_train &lt;- sh[-ind_test] # Put 10% a side as a test set trdf &lt;- df[ind_train, ] tsdf &lt;- df[ind_test, ] # h - fold CV nval &lt;- floor(nrow(trdf) / h) k &lt;- seq(from = 3, to = 50, by = 2) AUC &lt;- c() MAUC2 &lt;- c() k_opt &lt;- c() for (i in 1:h) { if (i &lt; h) { ind_val &lt;- c(((i - 1) * nval + 1):(i * nval)) } else{ ind_val &lt;- c(((i - 1) * nval + 1):length(ind)) } ind_train &lt;- c(1:nrow(trdf))[-ind_val] df_train &lt;- trdf[ind_train, ] df_val &lt;- trdf[ind_val, ] for (s in 1:length(k)) { model &lt;- knn3(IncomeLevel ~ ., data = df_train, k = k[s]) phat &lt;- predict(model, df_val, type = &quot;prob&quot;) #AUC pred_rocr &lt;- prediction(phat[, 2], df_val$IncomeLevel) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) AUC[s] &lt;- auc_ROCR@y.values[[1]] } MAUC2[i] &lt;- AUC[which.max(AUC)] k_opt[i] &lt;- k[which.max(AUC)] } Note that kNN would best fit on data sets with true numeric variables. Now we can find the tuned kNN (i.e.the best “k”) and apply the trained kNN for prediction using the test data we split at the beginning cbind(k_opt, MAUC2) ## k_opt MAUC2 ## [1,] 49 0.9020390 ## [2,] 37 0.9015282 ## [3,] 27 0.8911303 ## [4,] 45 0.8967005 ## [5,] 47 0.9035859 ## [6,] 21 0.9004941 ## [7,] 33 0.8937860 ## [8,] 37 0.8985006 ## [9,] 43 0.8918030 ## [10,] 39 0.8862083 mean(k_opt) ## [1] 37.8 mean(MAUC2) ## [1] 0.8965776 We can compare kNN with LPM (and Logistic) by AUC (not the one given above!) but “k” is not stable. Although, we can go with the mean of “k” or the mode of “k”, we can address this problem by changing the order of loops and using bootstrapping in our training instead of 10-fold CV, which would also increase the number or loops hence the running time. Before jumping into this possible solution, we need to think about what we have done so far. We trained our kNN. That is, we got the value of our hyperparameter. We should use our tuned kNN to test it on the test data that we put aside at the beginning. The proper way to that, however, is to have several loops, instead of one like what we did here, and calculate the test AUC for comparison, which is similar to what we did in LPM and Logistic before. We will not do it here as the running time would be very long, which, by the way, shows the importance of having fast “machines” as well as efficient algorithms. A more stable, but much longer, suggestion for tuning our kNN application is using a bootstrapping method. It runs multiple loops and takes the average of AUC with the same “k”. The example below is restricted to 20 runs for each “k”. Note that bootstrapping (See Chapter 37.5) is a process of resampling with replacement (all values in the sample have an equal probability of being selected, including multiple times, so a value could have duplicates). #### Test/Train split - as before!######## # however, this is done only once here. # Should be done in a loop multiple times set.seed(123) sh &lt;- sample(nrow(df), nrow(df), replace = FALSE) h &lt;- 10 ind_test &lt;- sh[1:(nrow(df)/h)] ind_train &lt;- sh[-ind_test] # Put 10% a side as a test set trdf &lt;- df[ind_train, ] tsdf &lt;- df[ind_test, ] ########## Bootstrapping ############ # Note that we use `by=2` to reduce the running time # With a faster machine, that could be set to 1. k &lt;- seq(from = 3, to = 50, by = 2) m &lt;- 20 # number of bootstrap loops (could be higher to, like 50) MAUC &lt;- c() k_opt &lt;- c() for(i in 1:length(k)){ AUC &lt;- c() for(l in 1:m){ #Here is the heart of bootstrapped tuning set.seed(l) bind &lt;- sample(nrow(trdf), nrow(trdf), replace = TRUE) uind &lt;- unique(bind) df_train &lt;- df[uind, ] df_val &lt;- df[-uind, ] model &lt;- knn3(IncomeLevel ~., data = df_train, k = k[i]) phat &lt;- predict(model, df_val, type = &quot;prob&quot;) #AUC pred_rocr &lt;- prediction(phat[,2], df_val$IncomeLevel) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) AUC[l] &lt;- auc_ROCR@y.values[[1]] } MAUC[i] &lt;- mean(AUC) } OK … now finding the optimal “k” plot(k, MAUC, col = &quot;red&quot;, type = &quot;o&quot;) MAUC[which.max(MAUC)] ## [1] 0.895667 k[which.max(MAUC)] ## [1] 49 This algorithm can be more efficient with parallel processing using multicore loop applications, which we will see In Chapter 14 (14.4.2). The other way to reduce the running time is to make the increments in the grid (for “k”) larger, like 10, and then find the region where AUC is highest. Then, we can have a finer grid for that specific region to identify the best “k”. Before concluding this chapter, note that knn3() handles factor variables itself. This is an internal process, a good one. Remember, knn() could not do that and requires all features to be numeric. How could we do that? One way to handle it is to convert all factor variables to dummy (binary numerical) codes as shown below. This is also called as “one-hot encoding” in practice. This type of knowledge, what type of data handling is required by a package and how we can achieve it, is very important in data analytics. dftmp &lt;- df[,-15] ind &lt;- which(sapply(dftmp, is.factor)==TRUE) fctdf &lt;- dftmp[,ind] numdf &lt;- dftmp[, -ind] #dummy coding fctdum &lt;- model.matrix(~. - 1, data = fctdf) #Binding df_dum &lt;- cbind(Y = df$IncomeLevel, numdf, fctdum) Now, it can also be used with knn() from the class package. Note that kNN gets unstable as the number of variables increases. We can see it by calculating test AUC multiple times by adding an outer loop to our algorithm. 11.3.2 kNN with caret # kNN needs a proper levels with caret! levels(df$IncomeLevel)[levels(df$IncomeLevel)==&quot; &lt;=50K&quot;] &lt;- &quot;Less&quot; levels(df$IncomeLevel)[levels(df$IncomeLevel)==&quot; &gt;50K&quot;] &lt;- &quot;More&quot; levels(df$IncomeLevel) ## [1] &quot;Less&quot; &quot;More&quot; #### Test/Train split ######## set.seed(123) sh &lt;- sample(nrow(df), nrow(df), replace = FALSE) h &lt;- 10 ind_test &lt;- sh[1:(nrow(df)/h)] ind_train &lt;- sh[-ind_test] trdf &lt;- df[ind_train, ] tsdf &lt;- df[ind_test, ] ########## CARET SET-UP ################## # Here we use class probabilities, which is required for ROC training #`twoClassSummary` will compute the sensitivity, specificity, AUC, ROC cv &lt;- trainControl(method = &quot;cv&quot;, number = 10, p = 0.9, classProbs = TRUE, summaryFunction = twoClassSummary) #The main training process set.seed(5) # for the same results, no need otherwise model_knn3 &lt;- train(IncomeLevel ~ ., method = &quot;knn&quot;, data = trdf, tuneGrid = data.frame(k=seq(3, 50, 2)), trControl = cv, metric = &quot;ROC&quot;) #Here is the key difference. #we are asking caret to use ROC #as our main performance criteria #Optimal k ggplot(model_knn3, highlight = TRUE) model_knn3 ## k-Nearest Neighbors ## ## 29304 samples ## 14 predictor ## 2 classes: &#39;Less&#39;, &#39;More&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 26374, 26373, 26374, 26374, 26373, 26374, ... ## Resampling results across tuning parameters: ## ## k ROC Sens Spec ## 3 0.8262230 0.8941795 0.5961648 ## 5 0.8586528 0.9031179 0.6005682 ## 7 0.8724161 0.9090915 0.6086648 ## 9 0.8800527 0.9111126 0.6088068 ## 11 0.8848148 0.9129091 0.6079545 ## 13 0.8884125 0.9150201 0.6035511 ## 15 0.8904958 0.9174006 0.6041193 ## 17 0.8915694 0.9167720 0.6063920 ## 19 0.8923858 0.9171763 0.6036932 ## 21 0.8936219 0.9179848 0.6035511 ## 23 0.8940702 0.9159186 0.6042614 ## 25 0.8947602 0.9174457 0.6039773 ## 27 0.8952041 0.9176254 0.6026989 ## 29 0.8955018 0.9179398 0.6019886 ## 31 0.8956911 0.9180746 0.6017045 ## 33 0.8959661 0.9187034 0.6029830 ## 35 0.8960988 0.9179398 0.5998580 ## 37 0.8963903 0.9182991 0.5994318 ## 39 0.8968082 0.9191523 0.5977273 ## 41 0.8967777 0.9192421 0.5977273 ## 43 0.8968486 0.9204999 0.5977273 ## 45 0.8970198 0.9202755 0.5944602 ## 47 0.8972242 0.9205450 0.5944602 ## 49 0.8971898 0.9208593 0.5923295 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was k = 47. model_knn3$results ## k ROC Sens Spec ROCSD SensSD SpecSD ## 1 3 0.8262230 0.8941795 0.5961648 0.004815409 0.008727063 0.012670476 ## 2 5 0.8586528 0.9031179 0.6005682 0.005448823 0.007349595 0.011200694 ## 3 7 0.8724161 0.9090915 0.6086648 0.005166892 0.006160801 0.015110770 ## 4 9 0.8800527 0.9111126 0.6088068 0.004782100 0.006779982 0.015820347 ## 5 11 0.8848148 0.9129091 0.6079545 0.005121326 0.006691569 0.009772617 ## 6 13 0.8884125 0.9150201 0.6035511 0.004814653 0.007291597 0.006261826 ## 7 15 0.8904958 0.9174006 0.6041193 0.004443550 0.006821105 0.011003812 ## 8 17 0.8915694 0.9167720 0.6063920 0.004336396 0.006641748 0.009964578 ## 9 19 0.8923858 0.9171763 0.6036932 0.004357410 0.007690924 0.009156761 ## 10 21 0.8936219 0.9179848 0.6035511 0.004689076 0.007526214 0.009644457 ## 11 23 0.8940702 0.9159186 0.6042614 0.004753603 0.007840512 0.008710062 ## 12 25 0.8947602 0.9174457 0.6039773 0.004637773 0.007644920 0.009151863 ## 13 27 0.8952041 0.9176254 0.6026989 0.004438855 0.007110203 0.009279578 ## 14 29 0.8955018 0.9179398 0.6019886 0.004414619 0.006857247 0.007080142 ## 15 31 0.8956911 0.9180746 0.6017045 0.004228545 0.007160469 0.006567629 ## 16 33 0.8959661 0.9187034 0.6029830 0.004194696 0.007855452 0.007342833 ## 17 35 0.8960988 0.9179398 0.5998580 0.004149906 0.007520967 0.008654547 ## 18 37 0.8963903 0.9182991 0.5994318 0.004319967 0.007271261 0.007426320 ## 19 39 0.8968082 0.9191523 0.5977273 0.004422126 0.007898694 0.007080142 ## 20 41 0.8967777 0.9192421 0.5977273 0.004740533 0.007711601 0.007745494 ## 21 43 0.8968486 0.9204999 0.5977273 0.004691945 0.007227390 0.007420280 ## 22 45 0.8970198 0.9202755 0.5944602 0.004919464 0.007125413 0.008207829 ## 23 47 0.8972242 0.9205450 0.5944602 0.004863486 0.007306936 0.008180470 ## 24 49 0.8971898 0.9208593 0.5923295 0.004929357 0.007144172 0.007335196 Confusion matrix: # Performance metrics confusionMatrix(predict(model_knn3, tsdf, type = &quot;raw&quot;), tsdf$IncomeLevel) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Less More ## Less 2303 300 ## More 179 474 ## ## Accuracy : 0.8529 ## 95% CI : (0.8403, 0.8649) ## No Information Rate : 0.7623 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.571 ## ## Mcnemar&#39;s Test P-Value : 4.183e-08 ## ## Sensitivity : 0.9279 ## Specificity : 0.6124 ## Pos Pred Value : 0.8847 ## Neg Pred Value : 0.7259 ## Prevalence : 0.7623 ## Detection Rate : 0.7073 ## Detection Prevalence : 0.7994 ## Balanced Accuracy : 0.7701 ## ## &#39;Positive&#39; Class : Less ## # If we don&#39;t specify &quot;More&quot; as our positive results, the first level # &quot;Less&quot; will be used as the &quot;positive&quot; result. confusionMatrix(predict(model_knn3, tsdf, type = &quot;raw&quot;), tsdf$IncomeLevel, positive = &quot;More&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Less More ## Less 2303 300 ## More 179 474 ## ## Accuracy : 0.8529 ## 95% CI : (0.8403, 0.8649) ## No Information Rate : 0.7623 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.571 ## ## Mcnemar&#39;s Test P-Value : 4.183e-08 ## ## Sensitivity : 0.6124 ## Specificity : 0.9279 ## Pos Pred Value : 0.7259 ## Neg Pred Value : 0.8847 ## Prevalence : 0.2377 ## Detection Rate : 0.1456 ## Detection Prevalence : 0.2006 ## Balanced Accuracy : 0.7701 ## ## &#39;Positive&#39; Class : More ## We now know two things: (1) how good the prediction is with kNN; (2) how good it is relative to other “base” or “benchmark” models. These two questions must be answered every time to evaluate the prediction performance of a machine learning algorithm. Although we didn’t calculate the test AUC in our own kNN algorithm, we can accept that kNN performance is good with AUC that is close to 90%. However, it is not significantly better than LPM and Logistic "],["cart.html", "Chapter 12 CART 12.1 CART - Classification Tree 12.2 rpart() - Recursive Partitioning 12.3 Pruning 12.4 Classification with Titanic 12.5 Regression Tree", " Chapter 12 CART Tree-based learning algorithms are considered to be one of the best and most used supervised learning methods. Unlike linear models, they handle non-linear relationships quite well. They are adaptable at solving classification or regression problems, which gives its name: Classification And Regression Trees. Decision tree learning algorithms are based on a decision tree, which is a flowchart where each internal node represents a decision point (goes left or right), each branch represents those decisions, and each leaf at the end of a branch represents the outcome of the decision. Here is a simple decision tree about a gamble: How can we use a decision tree in a learning algorithm? Let’s start with a classification problem: 12.1 CART - Classification Tree Let’s start with a very simple example: suppose we have the following data: y &lt;- c(1,1,1,0,0,0,1,1,0,1) x1 &lt;- c(0.09, 0.11, 0.17, 0.23, 0.33, 0.5, 0.54, 0.62, 0.83, 0.88) x2 &lt;- c(0.5, 0.82, 0.2, 0.09, 0.58, 0.5, 0.93, 0.8, 0.3, 0.83) data &lt;- data.frame(y = y, x1 = x1, x2 = x2) plot(data$x1, data$x2, col = (data$y+1), lwd = 4, ylab = &quot;x2&quot;, xlab = &quot;x1&quot;) What’s the best rule on \\(x_2\\) to classify black (\\(0\\)) and red balls (\\(1\\))? Find a cutoff point on \\(x_2\\) such that the maximum number of observations is correctly classified To minimize the misclassification, we find that the cutoff point should be between \\((0.6; 0.79)\\). Hence the rule is \\(x_2 &lt; k\\), where \\(k \\in(0.6,0.79)\\) plot(data$x1, data$x2, col = (data$y+1), lwd = 4) abline(h = 0.62, col = &quot;blue&quot;, lty = 5, lwd = 2) From this simple rule, we have two misclassified balls. We can add a new rule in the area below the horizontal blue line: plot(data$x1, data$x2, col = (data$y+1), lwd = 4) abline(h = 0.62, v = 0.2, col = c(&quot;blue&quot;, &quot;darkgreen&quot;), lty = 5, lwd = 2) Using these two rules, we correctly classified all balls (\\(Y\\)). We did the classification manually by looking at the graph. How can we do it by an algorithm? First, we need to create an index that is going to measure the impurity in each node. Instead of counting misclassified \\(y\\)’s, the impurity index will give us a continuous metric. The first index is the Gini Index, which can be defined at some node \\(\\mathcal{N}\\): \\[ G(\\mathcal{N}) = \\sum_{k=1}^{K} p_{k}\\left(1-p_{k}\\right) = 1-\\sum_{k=1}^{K} p_{k}^{2} \\] where, with \\(p_k\\) is the fraction of items labeled with class \\(k\\) in the node. If we have a binary outcome \\((k=2)\\), when \\(p_k = 1\\), \\(G(\\mathcal{N})=0\\) and when \\(p_k = 0.5,\\) \\(G(\\mathcal{N})=0.5\\). The former implies the minimal impurity (diversity), the latter shows the maximal impurity. A small \\(G\\) means that a node contains predominantly observations from a single class. As in the previous example, when we have a binary outcome with two classes, \\(y_i \\in (0,1)\\), this index can be written as: \\[ G(\\mathcal{N})=\\sum_{k=1}^{2} p_{k}\\left(1-p_{k}\\right)=2p\\left(1-p\\right) \\] If we split the node into two leaves, \\(\\mathcal{N}_L\\) (left) and \\(\\mathcal{N}_R\\) (right), the \\(G\\) will be: \\[ G\\left(\\mathcal{N}_{L}, \\mathcal{N}_{R}\\right)=p_{L} G\\left(\\mathcal{N}_{L}\\right)+p_{R} G\\left(\\mathcal{N}_{R}\\right) \\] Where \\(p_L\\), \\(p_R\\) are the proportion of observations in \\(\\mathcal{N}_L\\) and \\(\\mathcal{N}_R\\). Remember, we are trying to find the rule that gives us the best cutoff point. Now we can write the rule: \\[ \\Delta=G(\\mathcal{N})-G\\left(\\mathcal{N}_{L}, \\mathcal{N}_{R}\\right)&gt;\\epsilon \\] When the impurity is reduced substantially, the difference will be some positive number (\\(\\epsilon\\)). Hence, we find the cutoff point on a single variable that minimizes the impurity. Let’s use a dataset4, which reports about heart attacks and fatality (our binary variable). library(readr) #Data #myocarde = read.table(&quot;http://freakonometrics.free.fr/myocarde.csv&quot;,head=TRUE, sep=&quot;;&quot;) myocarde &lt;- read_delim(&quot;myocarde.csv&quot;, delim = &quot;;&quot; , escape_double = FALSE, trim_ws = TRUE, show_col_types = FALSE) myocarde &lt;- data.frame(myocarde) str(myocarde) ## &#39;data.frame&#39;: 71 obs. of 8 variables: ## $ FRCAR: num 90 90 120 82 80 80 94 80 78 100 ... ## $ INCAR: num 1.71 1.68 1.4 1.79 1.58 1.13 2.04 1.19 2.16 2.28 ... ## $ INSYS: num 19 18.7 11.7 21.8 19.7 14.1 21.7 14.9 27.7 22.8 ... ## $ PRDIA: num 16 24 23 14 21 18 23 16 15 16 ... ## $ PAPUL: num 19.5 31 29 17.5 28 23.5 27 21 20.5 23 ... ## $ PVENT: num 16 14 8 10 18.5 9 10 16.5 11.5 4 ... ## $ REPUL: num 912 1476 1657 782 1418 ... ## $ PRONO: chr &quot;SURVIE&quot; &quot;DECES&quot; &quot;DECES&quot; &quot;SURVIE&quot; ... The variable definitions are as follows: FRCAR (heart rate), INCAR (heart index), INSYS (stroke index), PRDIA (diastolic pressure), PAPUL (pulmonary arterial pressure), PVENT (ventricular pressure), REPUL (lung resistance), PRONO, which is our outcome variable (death “DECES”, survival “SURVIE”). We are ready to calculate \\(G\\)-index: # Recode PRONO y &lt;- ifelse(myocarde$PRONO==&quot;SURVIE&quot;, 1, 0) # Find G(N) without L and R G &lt;- 2*mean(y)*(1-mean(y)) G ## [1] 0.4832375 This is the level of “impurity” in our data. Now, we need to pick one variable and find a cutoff point in the variable. Then, we will calculate the same \\(G\\) for both left and right of that point. The goal is the find the best cutoff point that reduces the “impurity”. Let’s pick FRCAR arbitrarily for now. Later we will see how to find the variable that the first split (left and right) should start from so that the reduction in “impurity” will be maximized. # Let&#39;s pick FRCAR to start x_1 &lt;- myocarde$FRCAR # Put x and y in table tab = table(y,x_1) tab ## x_1 ## y 60 61 65 67 70 75 78 79 80 81 82 84 85 86 87 90 92 94 95 96 99 100 102 103 ## 0 1 0 1 0 1 1 0 1 4 0 0 0 1 0 2 2 2 1 3 0 0 1 1 1 ## 1 0 2 1 1 0 3 1 0 7 1 3 1 0 4 0 4 2 1 1 1 1 3 0 0 ## x_1 ## y 105 108 110 116 118 120 122 125 ## 0 1 0 2 1 1 1 0 0 ## 1 0 1 1 0 1 0 1 1 Let’s see how we can calculate \\[ G\\left(\\mathcal{N}_{L}, \\mathcal{N}_{R}\\right)=p_{L} G\\left(\\mathcal{N}_{L}\\right)+p_{R} G\\left(\\mathcal{N}_{R}\\right), \\] when \\(x = 60\\), for example. # Let&#39;s pick an arbitrary x value, x = 60 to see if (GL + GR &gt; GN) GL &lt;- 2*mean(y[x_1 &lt;= 60])*(1-mean(y[x_1 &lt;= 60])) GR &lt;- 2*mean(y[x_1 &gt; 60])*(1-mean(y[x_1 &gt; 60])) pL &lt;- length(x_1[x_1 &lt;= 60])/length(x_1) #Proportion of obs. on Left pR &lt;- length(x_1[x_1 &gt; 60])/length(x_1) #Proportion of obs. on Right How much did we improve \\(G\\)? # How much did we improve G? delta = G - pL*GL - pR*GR delta ## [1] 0.009998016 We need go trough each number on \\(x_1\\) and identify the point that maximizes delta. A function can do that: GI &lt;- function(x){ GL &lt;- 2*mean(y[x_1 &lt;= x])*(1-mean(y[x_1 &lt;= x])) GR &lt;- 2*mean(y[x_1 &gt; x])*(1-mean(y[x_1 &gt; x])) pL &lt;- length(x_1[x_1 &lt;= x])/length(x_1) pR &lt;- length(x_1[x_1 &gt; x])/length(x_1) del = G - pL*GL - pR*GR return(del) } # Let&#39;s test it GI(60) ## [1] 0.009998016 It works! Now, we can use this function in a loop that goes over each unique \\(x\\) and calculate their delta. xm &lt;- sort(unique(x_1)) delta &lt;- c() # Since we don&#39;t split at the last number for (i in 1:length(xm)-1) { delta[i] &lt;- GI(xm[i]) } delta ## [1] 9.998016e-03 4.978782e-04 1.082036e-05 1.041714e-03 8.855953e-05 ## [6] 7.363859e-04 2.295303e-03 2.546756e-04 1.142757e-03 2.551599e-03 ## [11] 9.862318e-03 1.329134e-02 8.257492e-03 2.402430e-02 1.160767e-02 ## [16] 1.634414e-02 1.352527e-02 1.229951e-02 3.109723e-03 5.692941e-03 ## [21] 9.212475e-03 1.919591e-02 1.244092e-02 6.882353e-03 2.747959e-03 ## [26] 6.282533e-03 1.547312e-03 1.082036e-05 4.978782e-04 9.671419e-03 ## [31] 4.766628e-03 Let’s see the cutoff point that gives us the highest delta. max(delta) ## [1] 0.0240243 xm[which.max(delta)] ## [1] 86 Although this is a simple and an imperfect algorithm, it can show us how we can build a learning system based on a decision tree. On one variable, FRCAR and with only one split we improved the Gini index by 2.5%. Obviously this is not good enough. Can we do more splitting? Since we now have two nodes (Left and Right at \\(x_1 = 86\\)), we can think of each of them as one node and apply the same formula to both left and right nodes. As you can guess, this may give us a zero-\\(G\\), as we end up with splitting at every \\(x_{1i}\\). How can we prevent this overfitting? We will see this mechanism later, which is called pruning. Let’s continue our example. Wouldn’t it be a good idea if we check all seven variables and start with the one that has a significant improvements in delta when we split? We can do it easily with a loop: # Adjust our function a little: add &quot;tr&quot;, the cutoff GI &lt;- function(x, tr){ G &lt;- 2*mean(y)*(1-mean(y)) GL &lt;- 2*mean(y[x &lt;= tr])*(1-mean(y[x &lt;= tr])) GR &lt;- 2*mean(y[x &gt; tr])*(1-mean(y[x &gt; tr])) pL &lt;- length(x[x &lt;= tr])/length(x) pR &lt;- length(x[x &gt; tr])/length(x) del = G - pL*GL - pR*GR return(del) } # The loop that applies GI on every x d &lt;- myocarde[, 1:7] split &lt;- c() maxdelta &lt;- c() for (j in 1:ncol(d)) { xm &lt;- sort(unique(d[,j])) delta &lt;- c() for (i in 1:length(xm)-1) { delta[i] &lt;- GI(d[,j], xm[i]) } maxdelta[j] &lt;- max(delta) split[j] &lt;- xm[which.max(delta)] } data.frame(variables = colnames(d), delta = maxdelta) ## variables delta ## 1 FRCAR 0.02402430 ## 2 INCAR 0.26219024 ## 3 INSYS 0.28328013 ## 4 PRDIA 0.13184706 ## 5 PAPUL 0.09890283 ## 6 PVENT 0.04612125 ## 7 REPUL 0.26790701 This is good. We can identify that INSYS should be our first variable to split, as it has the highest delta. round(split[which.max(maxdelta)],0) # round it b/c the cutoff is x=18.7 ## [1] 19 We now know where to split on INSYS, which is 19. Next, we can split on INSYS, Left and Right and move on to the next variable to split, which would be the second best: REBUL. For a better interpretabilty, we can rank the importance of each variable by their gain in Gini. Without using rpart(), we can approximately order them by looking at our delta: # Variable importance dm &lt;- matrix(maxdelta, 7, 1) rownames(dm) &lt;- c(names(myocarde[1:7])) dm &lt;- dm[order(dm[,1]),] barplot(dm, horiz = TRUE, col = &quot;darkgreen&quot;, xlim = c(0, 0.3), cex.names = 0.5, cex.axis = 0.8, main = &quot;Variable Importance at the 1st Split&quot;) 12.2 rpart() - Recursive Partitioning The R package rpart implements Recursive PARTitioning. It is easy to use. As in our case, when the response variable is categorical, the resulting tree is called classification tree. The default criterion, which is maximized in each split is the Gini coefficient. The method-argument can be switched according to the type of the response variable. It is class for categorical, anova for numerical, poisson for count data and exp for survival data. If the outcome variable is a factor variable, as in our case, we do not have to specify the method. The tree is built by the following process in rpart: first the single variable is found that best splits the data into two groups. After the data is separated, this process is applied separately to each sub-group. This goes on recursively until the subgroups either reach a minimum size or until no improvement can be made. Details can be found in this vignette (Atkinson_2022?). Here, we apply rpart to our data without any modification to its default arguments: library(rpart) tree = rpart(PRONO ~., data = myocarde, method = &quot;class&quot;) # Plot it library(rpart.plot) # You can use plot() but prp() is much better prp(tree, type = 2, extra = 1, split.col = &quot;red&quot;, split.border.col = &quot;blue&quot;, box.col = &quot;pink&quot;) This shows that the left node (DECES) cannot be significantly improved by a further split on REPUL. But the right node (SURVIE) can be improved. Note that we haven’t trained our model explicitly. There are two ways to control the growth of a tree: We can limit the growth of our tree by using its control parameters and by checking if the split is worth it, which is, as a default, what rpart() is doing with 10-fold cross-validation; We can grow the tree without any limitation and then prune it. Since we use the default control parameters with 10-fold CV, our first tree was grown by the first strategy. Before going further, let’s spend some time on the main arguments of rpart(): rpart(formula, data, weights, subset, na.action = na.rpart, method, model = FALSE, x = FALSE, y = TRUE, parms, control, cost, ...) We briefly describe some of its arguments based on An Introduction to Recursive Partitioning Using the RPART Routines by Atkinson et.al. (Atkinson_2000?): formula: the model formula, as in lm(). If the outcome \\(y\\) has more than two levels, then categorical predictors must be fit by exhaustive enumeration, which can take a very long time. data, weights, subset: as in other models. parms: There are three parameters: prior (the vector of prior probabilities), loss (the loss matrix - for different weights for misclassification, split (could be “Gini” or “information Entropy”). na.action: default is na.part, which removes only those rows for which either the response or ALL of the predictors are missing. Hence rpart() retains partially missing observations. This is the single most useful feature of rpart models. control: a list of control parameters, usually the result of the rpart.control function: rpart.control(minsplit = 20, minbucket = round(minsplit/3), cp = 0.01, maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, xval = 10, surrogatestyle = 0, maxdepth = 30, ...) minsplit: The minimum number of observations in a node for which the routine will even try to compute a split. The default is 20. This parameter can save computation time since smaller nodes are almost always pruned away by cross-validation. minbucket: The minimum number of observations in a terminal node: This defaults to minsplit/3. maxcompete: This parameter controls the number that will be printed. The default is 5. xval: The number of cross-validations to be done. Default is 10. maxsurrogate: The maximum number of surrogate variables to retain at each node. Surrogates give different information than competitor splits. The competitor list asks which other splits would have as many correct classifications surrogates ask which other splits would classify the same subjects in the same way which is a harsher criteria. usesurrogate: If the value is 0, then a subject (observation) who is missing the primary split variable does not progress further down the tree. cp: The threshold complexity parameter. Default is 0.01. What are the surrogates? They have two primary functions: first, to split the data when the primary splitter is missing. Remember, rpart() does not drop the subject if it has a missing observation on a variable. When the observation missing on the primary split on that variable, rpart() find a surrogate for the variable so that it can carry out the split. As in our case, the primary splitter (\\(x\\) variable) may never have been missing in the training data. However, when it comes time to make predictions on future data, we have no idea whether that particular splitter will always be available for each observations. When it is missing, then the surrogates will be able to take over and take on the work that the primary splitter accomplished during the initial building of the tree. We can see the the growth of the tree by looking at its CV table: printcp(tree) ## ## Classification tree: ## rpart(formula = PRONO ~ ., data = myocarde, method = &quot;class&quot;) ## ## Variables actually used in tree construction: ## [1] INSYS REPUL ## ## Root node error: 29/71 = 0.40845 ## ## n= 71 ## ## CP nsplit rel error xerror xstd ## 1 0.724138 0 1.00000 1.00000 0.14282 ## 2 0.034483 1 0.27586 0.51724 0.11861 ## 3 0.010000 2 0.24138 0.55172 0.12140 The rel error of each iteration of the tree is the fraction of mislabeled elements in the iteration relative to the fraction of mislabeled elements in the root. Hence it’s 100% (1.000000 in the table) in the root node. The relative improvement, or gain, due to a split is given by CP (cost complexity pruning), which is 0.724138 in the first split on INSYS. Therefore, the first split on INSYS reduces (improves) this error by 72.4138% to 27.5862% (1.000000 rel error - 0.724138 CP). This relative gain (CP) can be calculated as follows: \\[ \\frac{\\Delta}{G(\\mathcal{N})}=\\frac{G(\\mathcal{N})-G\\left(\\mathcal{N}_{L}, \\mathcal{N}_{R}\\right)}{G(\\mathcal{N})}. \\] If this gain exceeds 1% - the default value - rpart() splits in two on a variable. As you can see from the table above, since there is no significant relative gain at the \\(3^{rd}\\) split exceeding the default parameter 0.01, rpart() decides to stop growing the tree after the \\(2^{nd}\\) split. Note that, we also calculated both the nominator and the denominator in our own algorithm: \\(\\Delta = 0.2832801\\) and \\(G(\\mathcal{N}) = 0.4832375\\). Hence the relative gain was \\(\\frac{\\Delta}{G(\\mathcal{N})}=0.586213\\) in our case. We can replicate the same results if we change our outcome from factor to numeric: myocarde_v2 &lt;- myocarde myocarde_v2$PRONO = (myocarde_v2$PRONO==&quot;SURVIE&quot;)*1 cart = rpart(PRONO~.,data=myocarde_v2) printcp(cart) ## ## Regression tree: ## rpart(formula = PRONO ~ ., data = myocarde_v2) ## ## Variables actually used in tree construction: ## [1] INSYS REPUL ## ## Root node error: 17.155/71 = 0.24162 ## ## n= 71 ## ## CP nsplit rel error xerror xstd ## 1 0.586213 0 1.00000 1.02337 0.045985 ## 2 0.101694 1 0.41379 0.82264 0.162392 ## 3 0.028263 2 0.31209 0.75870 0.161450 ## 4 0.010000 3 0.28383 0.71284 0.149930 It is not so easy to follow the rpart calculations for classification. Although the explanations in the vignette (Atkinson_2022?) suggests that Gini is used for classification, it seems that cost complexity pruning (cp) is reported based on accuracy (misclassification error) rather than Gini (Alan_2016?). As you see, when the outcome is not a factor variable, rpart applies a regression tree method, which minimizes the sum of squares, \\(\\sum_{i=1}^{n}\\left(y_i-f(x_i)\\right)^2\\). However, when \\(y_i\\) is a binary number with two values 0 and 1, the sum of squares becomes \\(np(1-p)\\), which gives the same relative gain as Gini. This is clear as both relative gains (our calculation and the calculation by rapart()above) are the same. What’s the variable importance of rpart()? # Variable Importance vi &lt;- tree$variable.importance vi &lt;- vi[order(vi)] barplot(vi/100, horiz = TRUE, col = &quot;lightgreen&quot;, cex.names = 0.5, cex.axis = 0.8, main = &quot;Variable Importance - rpart()&quot;) It seems that the order of variables are similar, but magnitudes are slightly different due to the differences in calculating methods. In rpart(), the value is calculated: (…) as the sum of the decrease in impurity both when the variable appear as a primary split and when it appears as a surrogate. 12.3 Pruning We can now apply the second method to our case by removing the default limits in growing our tree. We can do it by changing the parameters of the rpart fit. Let’s see what happens if we override these parameters: # let&#39;s change the minsplit and minbucket tree2 = rpart(PRONO ~., data = myocarde, control = rpart.control(minsplit = 2, minbucket = 1, cp = 0), method = &quot;class&quot;) # Plot it with a different package now library(rattle) # You can use plot() but prp() is much better fancyRpartPlot(tree2, caption = NULL) This is our fully grown tree with a “perfect” fit, because it identifies every outcome (DECES and SURVIE) correctly at the terminal nodes (%’s give proportion of observations). Obviously, this is not a good idea as it overfits. Let’s summarize what we have seen so far: we can either go with the first strategy and limit the growth of the tree or we can have a fully developed tree then we can prune it. The general idea in pruning is to reduce the tree’s complexity by keeping only the most important splits. When we grow a tree, rpart() performs 10-fold cross-validation on the data. We can see the cross-validation result by printcp(). printcp(tree2) ## ## Classification tree: ## rpart(formula = PRONO ~ ., data = myocarde, method = &quot;class&quot;, ## control = rpart.control(minsplit = 2, minbucket = 1, cp = 0)) ## ## Variables actually used in tree construction: ## [1] FRCAR INCAR INSYS PVENT REPUL ## ## Root node error: 29/71 = 0.40845 ## ## n= 71 ## ## CP nsplit rel error xerror xstd ## 1 0.724138 0 1.000000 1.00000 0.14282 ## 2 0.103448 1 0.275862 0.55172 0.12140 ## 3 0.034483 2 0.172414 0.55172 0.12140 ## 4 0.017241 6 0.034483 0.58621 0.12399 ## 5 0.000000 8 0.000000 0.62069 0.12640 plotcp(tree2) min_cp = tree2$cptable[which.min(tree2$cptable[,&quot;xerror&quot;]),&quot;CP&quot;] min_cp ## [1] 0.1034483 Remember rpart has a built-in process for cross-validation. The xerror is the cross-validation error, the classification error that is calculated on the test data with a cross-validation process. In general, more levels (each row represents a different height of the tree) in the tree mean that it has a lower classification error on the training. However, you run the risk of overfitting. Often, the cross-validation error will actually grow as the tree gets more levels. There are two common ways to prune a tree by rpart: Use the first level (i.e. least nsplit) with minimum xerror. The first level only kicks in when there are multiple levels having the same, minimum xerror. This is the most common used method. Use the first level where xerror &lt; min(xerror) + xstd, the level whose xerror is at or below horizontal line. This method takes into account the variability of xerror resulting from cross-validation. Therefore, it seems that we should prune our tree at the \\(4^{th}\\) split. We use cp to prune the tree in rpart as follows: ptree2 &lt;- prune(tree2, cp = min_cp) printcp(ptree2) ## ## Classification tree: ## rpart(formula = PRONO ~ ., data = myocarde, method = &quot;class&quot;, ## control = rpart.control(minsplit = 2, minbucket = 1, cp = 0)) ## ## Variables actually used in tree construction: ## [1] INSYS ## ## Root node error: 29/71 = 0.40845 ## ## n= 71 ## ## CP nsplit rel error xerror xstd ## 1 0.72414 0 1.00000 1.00000 0.14282 ## 2 0.10345 1 0.27586 0.55172 0.12140 fancyRpartPlot(ptree2) Now we have applied two approaches, limiting tree growth and pruning a fully grown tree. We also have two different trees: “tree” and “ptree2”. How can we test their performances? We know that we cannot test it with the training data. When applying this in practice, we should have a test dataset to check their performance. 12.4 Classification with Titanic Let’s end this sections with a more realistic example: we will predict survival on the Titanic. # load the data library(PASWR) data(titanic3) str(titanic3) ## &#39;data.frame&#39;: 1309 obs. of 14 variables: ## $ pclass : Factor w/ 3 levels &quot;1st&quot;,&quot;2nd&quot;,&quot;3rd&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ survived : int 1 1 0 0 0 1 1 0 1 0 ... ## $ name : Factor w/ 1307 levels &quot;Abbing, Mr. Anthony&quot;,..: 22 24 25 26 27 31 46 47 51 55 ... ## $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 2 1 2 1 2 1 2 1 2 ... ## $ age : num 29 0.917 2 30 25 ... ## $ sibsp : int 0 1 1 1 1 0 1 0 2 0 ... ## $ parch : int 0 2 2 2 2 0 0 0 0 0 ... ## $ ticket : Factor w/ 929 levels &quot;110152&quot;,&quot;110413&quot;,..: 188 50 50 50 50 125 93 16 77 826 ... ## $ fare : num 211 152 152 152 152 ... ## $ cabin : Factor w/ 187 levels &quot;&quot;,&quot;A10&quot;,&quot;A11&quot;,..: 45 81 81 81 81 151 147 17 63 1 ... ## $ embarked : Factor w/ 4 levels &quot;&quot;,&quot;Cherbourg&quot;,..: 4 4 4 4 4 4 4 4 4 2 ... ## $ boat : Factor w/ 28 levels &quot;&quot;,&quot;1&quot;,&quot;10&quot;,&quot;11&quot;,..: 13 4 1 1 1 14 3 1 28 1 ... ## $ body : int NA NA NA 135 NA NA NA NA NA 22 ... ## $ home.dest: Factor w/ 369 levels &quot;&quot;,&quot;?Havana, Cuba&quot;,..: 309 231 231 231 231 237 163 25 23 229 ... We will use the following variables: survived - 1 if true, 0 otherwise; sex - the gender of the passenger; age - age of the passenger in years; pclass - the passengers class of passage; sibsp - the number of siblings/spouses aboard; parch - the number of parents/children aboard. What predictors are associated with those who perished compared to those who survived? titan &lt;- rpart(survived~sex+age+pclass+sibsp+parch, data=titanic3, method=&quot;class&quot;) prp(titan, extra=1, faclen=5, box.col=c(&quot;indianred1&quot;,&quot;aquamarine&quot;)[tree$frame$yval]) barplot(titan$variable.importance, horiz=TRUE, col=&quot;yellow3&quot;, cex.axis = 0.7, cex.names = 0.7) If we want to see the cross-validation error and the cp table: printcp(titan) ## ## Classification tree: ## rpart(formula = survived ~ sex + age + pclass + sibsp + parch, ## data = titanic3, method = &quot;class&quot;) ## ## Variables actually used in tree construction: ## [1] age parch pclass sex sibsp ## ## Root node error: 500/1309 = 0.38197 ## ## n= 1309 ## ## CP nsplit rel error xerror xstd ## 1 0.424000 0 1.000 1.000 0.035158 ## 2 0.021000 1 0.576 0.576 0.029976 ## 3 0.015000 3 0.534 0.576 0.029976 ## 4 0.011333 5 0.504 0.552 0.029517 ## 5 0.010000 9 0.458 0.540 0.029279 plotcp(titan) Of course, we would like to see the tree’s prediction accuracy by using a test dataset and the confusion table metrics. library(ROCR) #test/train split set.seed(1) ind &lt;- sample(nrow(titanic3), nrow(titanic3)*0.7) train &lt;- titanic3[ind, ] test &lt;- titanic3[-ind, ] #Tree on train titan2 &lt;- rpart(survived~sex+age+pclass+sibsp+parch, data=train, method=&quot;class&quot;) phat &lt;- predict(titan2, test, type = &quot;prob&quot;) #AUC pred_rocr &lt;- prediction(phat[,2], test$survived) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc_ROCR@y.values[[1]] ## [1] 0.814118 Here, we report only AUC in this simple example. Moreover, we can reweigh variables so that the loss or the cost of a wrong split would be more or less important (see cost argument in rpart). Finally, as in every classification, we can put a different weight on the correct classifications than the wrong classifications (or vise verse). This can easily be done in rpart by the loss matrix. Before commenting on the strengths and weaknesses of CART, let’s see a regression tree. 12.5 Regression Tree The same partitioning procedure can be applied when the outcome variable is not qualitative. A splitting criterion, which is used to decide which variable gives the best split, was either the Gini or log-likelihood function for a classification problem. Now we can can use the anova method as a splitting criteria: \\[ S S_{T}-\\left(S S_{L}+S S_{R}\\right), \\] where \\[ SS_{T}=\\sum\\left(y_{i}-\\bar{y}\\right)^{2}, \\] which is the sum of squares for the node and \\(SS_R\\) and \\(SS_L\\) are the sums of squares for the right and left splits, respectively. Similar to our delta method, if \\(SS_{T}-\\left(SS_{L}+SS_{R}\\right)\\) is positive and significant, we make the split on the node (the variable). After the split, the fitted value of the node is the mean of \\(y\\) of that node. The anova method is used for regression trees, which is the default method if \\(y\\) a simple numeric vector. However, when \\(y_i \\in (0,1)\\), \\[ SS_{T}=\\sum\\left(y_{i}-\\bar{y}\\right)^{2}=\\sum y_{i}^2 -n\\bar{y}^2=\\sum y_{i} -n\\bar{y}^2=n\\bar y -n\\bar{y}^2=np(1-p) \\] Hence, we can show that the relative gain would be the same in regression trees using \\(SS_T\\) or Gini when \\(y_i \\in (0,1)\\). It is not hard to write a simple loop similar to our earlier algorithm, but it would be redundant. We will use rpart() in an example: # simulated data set.seed(1) x &lt;- runif(100, -2, 2) y &lt;- 1 + 1*x + 4*I(x^2) - 4*I(x^3) + rnorm(100, 0, 6) d &lt;- data.frame(&quot;y&quot; = y, &quot;x&quot; = x) plot(x, y, col = &quot;gray&quot;) # Tree fit1 &lt;- rpart(y ~ x, minsplit=83, d) # we want to have 1 split fancyRpartPlot(fit1) When we have split at \\(x=-0.65\\), rpart calculates two constant \\(\\hat{f}(x_i)\\)’s both for the “left” and “right” splits: mean(y[x &lt;= -0.65]) ## [1] 15.33681 mean(y[x &gt; -0.65]) ## [1] 0.9205211 Here we see them on the plot: z &lt;- seq(min(x), max(x), length.out=1000) plot(x, y, col = &quot;gray&quot;) lines(z, predict(fit1, data.frame(x=z)), col=&quot;blue&quot;, lwd=3) abline(v = -0.65, col=&quot;red&quot;) If we reduce the minsplit, # Tree fit2 &lt;- rpart(y ~ x, minsplit=6, d) fancyRpartPlot(fit2) # On the plot plot(x, y, col = &quot;gray&quot;) lines(z, predict(fit2, data.frame(x=z)), col=&quot;green&quot;, lwd=3) We will use an example of predicting Baseball players’ salaries, which is one of the most common example online (ISLR_2021?). This data set is deduced from the Baseball fielding data set: fielding performance includes the numbers of Errors, Putouts and Assists made by each player. # Hitters data library(ISLR) data(&quot;Hitters&quot;) str(Hitters) ## &#39;data.frame&#39;: 322 obs. of 20 variables: ## $ AtBat : int 293 315 479 496 321 594 185 298 323 401 ... ## $ Hits : int 66 81 130 141 87 169 37 73 81 92 ... ## $ HmRun : int 1 7 18 20 10 4 1 0 6 17 ... ## $ Runs : int 30 24 66 65 39 74 23 24 26 49 ... ## $ RBI : int 29 38 72 78 42 51 8 24 32 66 ... ## $ Walks : int 14 39 76 37 30 35 21 7 8 65 ... ## $ Years : int 1 14 3 11 2 11 2 3 2 13 ... ## $ CAtBat : int 293 3449 1624 5628 396 4408 214 509 341 5206 ... ## $ CHits : int 66 835 457 1575 101 1133 42 108 86 1332 ... ## $ CHmRun : int 1 69 63 225 12 19 1 0 6 253 ... ## $ CRuns : int 30 321 224 828 48 501 30 41 32 784 ... ## $ CRBI : int 29 414 266 838 46 336 9 37 34 890 ... ## $ CWalks : int 14 375 263 354 33 194 24 12 8 866 ... ## $ League : Factor w/ 2 levels &quot;A&quot;,&quot;N&quot;: 1 2 1 2 2 1 2 1 2 1 ... ## $ Division : Factor w/ 2 levels &quot;E&quot;,&quot;W&quot;: 1 2 2 1 1 2 1 2 2 1 ... ## $ PutOuts : int 446 632 880 200 805 282 76 121 143 0 ... ## $ Assists : int 33 43 82 11 40 421 127 283 290 0 ... ## $ Errors : int 20 10 14 3 4 25 7 9 19 0 ... ## $ Salary : num NA 475 480 500 91.5 750 70 100 75 1100 ... ## $ NewLeague: Factor w/ 2 levels &quot;A&quot;,&quot;N&quot;: 1 2 1 2 2 1 1 1 2 1 ... What predictors are associated with baseball player’s Salary (1987 annual salary on opening day in thousands of dollars)? Let’s consider 3 covariates for the sake of simplicity: Years (Number of years in the major leagues); Hits (Number of hits in 1986); Atbat (Number of times at bat in 1986). # Remove NA&#39;s df=Hitters[complete.cases(Hitters$Salary),] dfshort &lt;- df[, c(19, 7, 2, 1)] #Build the tree tree &lt;- rpart(log(Salary) ~ Years + Hits + AtBat, data=dfshort, cp=0) #cp=0 so fully grown prp(tree, extra=1, faclen=5) It works on the same principle as we described before: find terminal nodes that minimize the sum of squares. This process may give us a good prediction on the training set but not on the test set, as it overfits the data. Hence, we use a pruned tree found by rpart by cross-validation: ptree &lt;- rpart(log(Salary) ~ Years + Hits + AtBat, data=dfshort) prp(ptree, extra=1, faclen=5) We can see its prediction power similar to what we did in the Titanic data example. Since this is a regression, we can ask which one is better, a tree or a linear model? If the relationship between \\(y\\) and \\(X\\) is linear, a linear model should perform better. We can test this: #test/train split set.seed(123) ind &lt;- sample(nrow(dfshort), nrow(dfshort)*0.7) train &lt;- dfshort[ind, ] test &lt;- dfshort[-ind, ] #Tree and lm() on train ptree &lt;- rpart(log(Salary) ~ Years + Hits + AtBat, data=dfshort) predtree &lt;- predict(ptree, test) lin &lt;- lm(log(Salary) ~ ., data=dfshort) predlin &lt;- predict(lin, test) #RMSPE rmspe_tree &lt;- sqrt(mean((log(test$Salary) - predtree)^2)) rmspe_tree ## [1] 0.4601892 rmspe_lin &lt;- sqrt(mean((log(test$Salary) - predlin)^2)) rmspe_lin ## [1] 0.6026888 In this simple example, our the tree would do a better job. Trees tend to work well for problems where there are important nonlinearities and interactions. The results are really intuitive and interpretable. However, trees are known to be quite sensitive to the original sample. Therefore, the models trained in one sample may have poor predictive accuracy on another sample. These problems motivate Random Forest and Boosting methods, as we will describe in following chapters. freakonometrics (Charpentier_scratch?)↩︎ "],["ensemble-learning.html", "Chapter 13 Ensemble learning 13.1 Bagging 13.2 Random Forest 13.3 Boosting", " Chapter 13 Ensemble learning Bagging, random forests and, boosting methods are the main methods of ensemble learning - a machine learning method where multiple models are trained to solve the same problem. The main idea is that, instead of using all features (predictors) in one complex base model running on the whole data, we combine multiple models each using selected number of features and subsections of the data. With this, we can have a more robust learning system. Often, these single models are called a “weak” models. This is because, individually, they are imperfect due to their lack of complexity and/or their use of only a subsection of the data. The “weakness” can be thought of their insignificant contribution to the prediction problem. A weak classifier, for example, is that its error rate is only slightly better than a random guessing. When we use a single robust model, poor predictors would be eliminated in the training procedure. However, although each poor predictor has a very small contribution in training, their combination represented in weak models would be huge. Ensemble learning systems help these poor predictors have their “voice” in the training process by keeping them in the system rather than eliminating them. That’s the main reason why ensemble methods are robust and the main tools of machine learning. 13.1 Bagging Bagging gets its name from Bootstrap aggregating of trees. It works with a few simple arguments: Select number of trees B, and the tree depth D, Create a loop B times, In each loop, (a) generate a bootstrap sample from the original data; (b) estimate a tree of depth D on that sample. Let’s see an example with the titanic dataset: library(PASWR) library(rpart) library(rpart.plot) data(titanic3) # This is for a set of colors in each tree clr = c(&quot;pink&quot;,&quot;red&quot;,&quot;blue&quot;,&quot;yellow&quot;,&quot;darkgreen&quot;, &quot;orange&quot;,&quot;brown&quot;,&quot;purple&quot;,&quot;darkblue&quot;) n = nrow(titanic3) par(mfrow=c(3,3)) # this is for putting all plots together for(i in 1:9){ # Here B = 9 set.seed(i) idx = sample(n, n, replace = TRUE) #Bootstrap sampling with replacement TRUE tr &lt;- titanic3[idx,] cart = rpart(survived~sex+age+pclass+sibsp+parch, data = tr, cp=0, method=&quot;class&quot;) # Unpruned prp(cart, type=1, extra=1, box.col=clr[i]) } What are we going to do with these 9 trees? In regression trees, the prediction will be the average of the resulting predictions. In classification trees, we take a majority vote. Since averaging a set of observations by bootstrapping reduces the variance, the prediction accuracy increases. More importantly, compared to CART, the results would be much less sensitive to the original sample. As a result, they show impressive improvement in accuracy. Below, we have an algorithm that follows the steps for bagging in classification. Let’s start with a single tree and see how we can improve it with bagging: library(ROCR) #test/train split set.seed(1) ind &lt;- sample(nrow(titanic3), nrow(titanic3)*0.8) train &lt;- titanic3[ind, ] test &lt;- titanic3[-ind, ] #Single tree cart &lt;- rpart(survived ~ sex + age + pclass + sibsp + parch, data = train, method=&quot;class&quot;) #Pruned phat1 &lt;- predict(cart, test, type = &quot;prob&quot;) #AUC pred_rocr &lt;- prediction(phat1[,2], test$survived) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc_ROCR@y.values[[1]] ## [1] 0.8352739 Let’s do the bagging: B = 100 # number of trees phat2 &lt;- matrix(0, B, nrow(test)) #Container # Loop for(i in 1:B){ set.seed(i) # to make it reproducible idx &lt;- sample(nrow(train), nrow(train), replace=TRUE) dt &lt;- train[idx,] cart_B &lt;- rpart(survived ~ sex + age + pclass + sibsp + parch, data = dt, method=&quot;class&quot;) #Pruned phat2[i,] &lt;- predict(cart_B, test, type = &quot;prob&quot;)[, 2] } dim(phat2) ## [1] 100 262 You can see in that phat2 matrix is \\(100 \\times 262\\). Each column is representing the predicted probability that survived = 1. We have 100 trees (rows in phat2) and 100 predicted probabilities for each the observation in the test data. The only job we will have now to take the average of 100 predicted probabilities for each column. # Take the average phat_f &lt;- colMeans(phat2) #AUC pred_rocr &lt;- prediction(phat_f, test$survived) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc_ROCR@y.values[[1]] ## [1] 0.8577337 Hence, we have a slight improvement over a single tree. But the main idea behind bagging is to reduce the variance in prediction. The reason for this reduction is simple: we take the mean prediction of all bootstrapped samples. Remember, when we use a simple tree and make 500 bootstrapping validations, each one of them gives a different MSPE (in regressions) or AUC (in classification). We did that many times before. The difference now is that we average yhat in regressions or phat in classifications (or yhat with majority voting). This reduces the prediction uncertainty drastically. To test this, we have to run the same script multiple times and see how AUC with bagging would vary in each run relative to a single tree. Final point is that, B the number of trees could be a hyperparameter. That is, if we set less or more trees, what would happen to AUC? All these issues bring us to another model, random forest: 13.2 Random Forest Random Forest = Bagging + subsample of covariates at each node. We have done the first part above, and to some degree, it’s understandable. But, “subsampling covariates at each node” is new. We will use the Breiman and Cutler’s Random Forests for Classification and Regression: randomForest()(Brei_2004?). Here are the steps and the loop structure: Select number of trees ntree, subsampling parameter mtry, and the tree depth maxnodes, Create a loop ntree times, In each loop, (a) generate a bootstrap sample from the original data; (b) estimate a tree of depth maxnodes on that sample, But, for split in the tree (this is our second loop), randomly select mtry original covariates and do the split among those. Hence, bagging is a special case of random forest, with mtry = number of features (\\(P\\)). Let’s talk about this “subsampling covariates at each node”. When we think on this idea little bit more, we can see the rationale: suppose there is one very strong covariate in the sample. Almost all trees will use this covariate in the top split. All of the trees will look quite similar to each other. Hence the predictions will be highly correlated. Averaging many highly correlated quantities does not lead to a large reduction in variance. Random forests decorrelate the trees. Hence Random Forest further reduces the sensitivity of trees to the data points that are not in the original dataset. How are we going to pick mtry? In practice, default values are mtry = \\(P/3\\) in regression and mtry = \\(\\sqrt{P}\\) classification. (See mtry in ?randomForest). Note that, with this parameter (mtry), we can run a pure bagging model with randomForest(), instead of rpart(), if we set mtry = \\(P\\). You can think of a random forest model as a robust version of CART models. There are some default parameters that can be tuned in randomForest(), which lead to the same (two ways) way to grow a tree as in CART. Hastie et al. argues that (Elements of Statistical Learning, 2009, p.596) the problem of overfitting is not that large in random forests. As the depth of a tree increases (maxnodes, see below), overfitting is argued to be minor. Segal (2004) demonstrates small gains in performance by controlling the depths of the individual trees grown in random forests. Our experience is that using full-grown trees seldom costs much, and results in one less tuning parameter. Figure 15.8 shows the modest effect of depth control in a simple regression example. (Hastie et al., 2009, p.596) Let’s start with a simulation to show random forest and CART models: library(randomForest) # Note that this is actually Bagging since we have only 1 variable # Our simulated data set.seed(1) n = 500 x &lt;- runif(n) y &lt;- sin(12*(x+.2))/(x+.2) + rnorm(n)/2 # Fitting the models fit.tree &lt;- rpart(y~x) #CART fit.rf1 &lt;- randomForest(y~x) #No depth control fit.rf2 &lt;- randomForest(y~x, maxnodes=20) # Control it # Plot observations and predicted values z &lt;- seq(min(x), max(x), length.out=1000) par(mfrow=c(1,1)) plot(x, y, col=&quot;gray&quot;, ylim=c(-4,4)) lines(z, predict(fit.rf1, data.frame(x=z)), col=&quot;green&quot;, lwd=2) lines(z, predict(fit.rf2, data.frame(x=z)), col=&quot;red&quot;, lwd=2) lines(z, predict(fit.tree, data.frame(x=z)), col=&quot;blue&quot;, lwd=1.5) legend(&quot;bottomright&quot;, c(&quot;Random Forest: maxnodes=max&quot;,&quot;Random Forest: maxnodes=20&quot;, &quot;CART: single regression tree&quot;), col=c(&quot;green&quot;,&quot;red&quot;, &quot;blue&quot;), lty=c(1,1,1), bty = &quot;n&quot; ) The random forest models are definitely improvements over CART, but which one is better? Although random forest models should not overfit when increasing the number of trees (ntree) in the forest, it seems that there is no consensus on the depth of tree, maxnodes in the field. Hence, the question for practitioners becomes whether it would be possible to improve the model’s performance if we tune Random Forest parameters? Let’s use out-of-bag (OOB) MSE to tune Random Forest parameters in our case to see if there is any improvement. # OOB-MSE a &lt;- randomForest(y~x) a ## ## Call: ## randomForest(formula = y ~ x) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 1 ## ## Mean of squared residuals: 0.3891797 ## % Var explained: 80.43 a$mse[500] ## [1] 0.3891797 Let’s control our tree depth: maxnode &lt;- c(10,50,100,500) for (i in 1:length(maxnode)) { a &lt;- randomForest(y~x, maxnodes = maxnode[i]) print(c(maxnode[i], a$mse[500])) } ## [1] 10.0000000 0.3894515 ## [1] 50.0000000 0.3353509 ## [1] 100.0000000 0.3592304 ## [1] 500.000000 0.392889 We can see that OOB-MSE is smaller with maxnodes = 50. Of course we can have a more refined sequence of maxnodes series to test. Similarly, we can select parameter mtry with a grid search. We need to understand that there is no need to perform cross-validation in Random Forest. By bootstrapping, each tree uses around 2/3 of the observations. The remaining 1/3 of observations are referred to as the out-of-bag (OOB) observations, which are used for out-sample predictions. All performance measures for the model are based on these OOB predictions including MSE in case of regressions. We can manually calculate mse_oob[500] # OOB-MSE a &lt;- randomForest(y~x) a ## ## Call: ## randomForest(formula = y ~ x) ## Type of random forest: regression ## Number of trees: 500 ## No. of variables tried at each split: 1 ## ## Mean of squared residuals: 0.3945894 ## % Var explained: 80.16 a$mse[500] ## [1] 0.3945894 # Manually mean((a$predicted - y)^2) ## [1] 0.3945894 In a bagged model we set mtry = \\(P\\). If we want, we can tune the maximum number of terminal nodes that trees in the forest can have, maxnodes. If it is not specified, trees are grown to the maximum possible value, which is the number of observations. In practice, although there is no 100% consensus, the general approach is to grow the trees with the maximum number of terminal nodes. We have shown that, however, with a cross-validation, this parameter can be set so that OOB-MSE would be smaller than the one with the maximum maxnodes. In a random forest model, in addition to maxnodes (which is again set to \\(n\\)), we can tune the number of variables randomly sampled as candidates at each split, mtry. If we don’t set it, the default values for mtry are square-root of \\(p\\) for classification and \\(p/3\\) in regression, where \\(p\\) is number of variables/features. If we want, we can tune both parameters with cross-validation. The effectiveness of tuning random forest models in improving their prediction accuracy is an open question in practice. Bagging and random forest models tend to work well for problems where there are important nonlinearities and interactions. More importantly, they are robust to the original sample and more efficient than single trees. There are several tables in the Breiman’s original paper (Breiman_2001?) comparing the performance of random forest and single tree (CART) models. However, the results would be less intuitive and difficult to interpret. Nevertheless, we can obtain an overall summary of the importance of each covariates using SSR (for regression) or Gini index (for classification). The index records the total amount that the SSR or Gini is decreased due to splits over a given covariate, averaged over all ntree trees. rf &lt;- randomForest(as.factor(survived) ~ sex + age + pclass + sibsp + parch, data=titanic3, na.action = na.omit) importance(rf) ## MeanDecreaseGini ## sex 129.76641 ## age 63.36484 ## pclass 53.64906 ## sibsp 19.17399 ## parch 17.53605 We will see a several applications on CART, bagging and Random Forest in the lab section covering this chapter. 13.3 Boosting Boosting is an ensemble method that combines a set of “weak learners” into a strong learner to improve the prediction accuracy in self-learning algorithms. In boosting, the constructed iteration selects a random sample of data, fits a model, and then train sequentially. In each sequential model, the algorithm learns from the weaknesses of its predecessor (predictions errors) and tries to compensate for the weaknesses by “boosting” the weak rules from each individual classifier. The first original boosted application was offered in 1990 by Robert Schapire (1990) (Schapire?).5 Today, there are many boosting algorithms that are mainly grouped in the following three types: Gradient descent algorithm, AdaBoost algorithm, Xtreme gradient descent algorithm, We will start with a general application to show the idea behind the algorithm using the package gbm, Generalized Boosted Regression Models 13.3.1 Sequential ensemble with gbm Similar to bagging, boosting also combines a large number of decision trees. However, the trees are grown sequentially without bootstrap sampling. Instead each tree is fit on a modified version of the original dataset, the error. In regression trees, for example, each tree is fit to the residuals from the previous tree model so that each iteration is focused on improving previous errors. This process would be very weird for an econometrician. The accepted model building practice in econometrics is that you should have a model that the errors (residuals) should be orthogonal (independent from) to covariates. Here, what we suggest is the opposite of this practice: start with a very low-depth (shallow) model that omits many relevant variables, run a linear regression, get the residuals (prediction errors), and run another regression that explains the residuals with covariates. This is called learning from mistakes. Since there is no bootstrapping, this process is open to overfitting problem as it aims to minimize the in-sample prediction error. Hence, we introduce a hyperparameter that we can tune the learning process with cross-validation to stop the overfitting and get the best predictive model. This hyperparameter (shrinkage parameter, also known as the learning rate or step-size reduction) limits the size of the errors. Let’s see the whole process in a simple example inspired by Freakonometrics (Hyp_2018?): # First we will simulate our data n &lt;- 300 set.seed(1) x &lt;- sort(runif(n) * 2 * pi) y &lt;- sin(x) + rnorm(n) / 4 df &lt;- data.frame(&quot;x&quot; = x, &quot;y&quot; = y) plot(df$x, df$y, ylab = &quot;y&quot;, xlab = &quot;x&quot;, col = &quot;grey&quot;) We will “boost” a single regression tree: Step 1: Fit the model by using in-sample data # Regression tree with rpart() fit &lt;- rpart(y ~ x, data = df) # First fit: y~x yp &lt;- predict(fit) # using in-sample data # Plot for single regression tree plot(df$x, df$y, ylab = &quot;y&quot;, xlab = &quot;x&quot;, col = &quot;grey&quot;) lines(df$x, yp, type = &quot;s&quot;, col = &quot;blue&quot;, lwd = 3) Now, we will have a loop that will “boost” the model. What we mean by boosting is that we seek to improve yhat, i.e. \\(\\hat{f}(x_i)\\), in areas where it does not perform well by fitting trees to the residuals. Step 2: Find the “error” and introduce a hyperparameter h. h &lt;- 0.1 # shrinkage parameter # Add this adjusted prediction error, `yr` to our main data frame # which will be our target variable to predict later df$yr &lt;- df$y - h * yp # Store the &quot;first&quot; predictions in YP YP &lt;- h * yp Note that if h=1, it would give us usual “residuals”. Hence, h controls for “how much error” we would like to reduce. Step 3: Now, predict the “error” in a loop that repeats itself many times. # Boosting loop for t times (trees) for (t in 1:100) { fit &lt;- rpart(yr ~ x, data = df) # here it&#39;s yr~x. # We try to understand the prediction error by x&#39;s yp &lt;- predict(fit, newdata = df) # This is your main prediction added to YP YP &lt;- cbind(YP, h * yp) df$yr &lt;- df$yr - h * yp # errors for the next iteration # i.e. the next target to predict! } str(YP) ## num [1:300, 1:101] 0.00966 0.00966 0.00966 0.00966 0.00966 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : chr [1:300] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## ..$ : chr [1:101] &quot;YP&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... Look at YP now. We have a matrix 300 by 101. This is a matrix of predicted errors, except for the first column. So what? # Function to plot a single tree and boosted trees for different t viz &lt;- function(M) { # Boosting yhat &lt;- apply(YP[, 1:M], 1, sum) # This is predicted y for depth M plot(df$x, df$y, ylab = &quot;&quot;, xlab = &quot;&quot;) # Data points lines(df$x, yhat, type = &quot;s&quot;, col = &quot;red&quot;, lwd = 3) # line for boosting # Single Tree fit &lt;- rpart(y ~ x, data = df) # Single regression tree yp &lt;- predict(fit, newdata = df) # prediction for the single tree lines(df$x, yp, type = &quot;s&quot;, col = &quot;blue&quot;, lwd = 3) # line for single tree lines(df$x, sin(df$x), lty = 1, col = &quot;black&quot;) # Line for DGM } # Run each viz(5) viz(101) Each of 100 trees is given in the YP matrix. Boosting combines the outputs of many “weak” learners (each tree) to produce a powerful “committee”. What if we change the shrinkage parameter? Let’s increase it to 1.8. h &lt;- 1.8 # shrinkage parameter df$yr &lt;- df$y - h*yp # Prediction errors with &quot;h&quot; after rpart YP &lt;- h*yp #Store the &quot;first&quot; prediction errors in YP # Boosting Loop for t (number of trees) times for(t in 1:100){ fit &lt;- rpart(yr~x, data=df) # here it&#39;s yr~x. yhat &lt;- predict(fit, newdata=df) df$yr &lt;- df$yr - h*yhat # errors for the next iteration YP &lt;- cbind(YP, h*yhat) # This is your main prediction added to YP } viz(101) It overfits. Unlike random forests, boosting can overfit if the number of trees (B) and depth of each tree (D) are too large. By averaging over a large number of trees, bagging and random forests reduces variability. Boosting does not average over the trees. This shows that \\(h\\) should be tuned by a cross-validation process. The generalized boosted regression modeling (GBM) (Ridgeway_2020?) can be used for boosted regressions. Note that there are many arguments with their specific default values in the function. For example, n.tree (\\(B\\)) is 100 and shrinkage (\\(h\\)) is 0.1. interaction.depth, \\(D\\), specifying the maximum depth of each tree, is 1, which implies an additive model (2 implies a model with up to 2-way interactions). A smaller \\(h\\) typically requires more trees \\(B\\). It allows more and different shaped trees to attack the residuals. Here is the application of gbm() to our simulated data: library(gbm) # Note bag.fraction = 1 (no CV). The default is 0.5 bo1 &lt;- gbm(y ~ x, distribution=&quot;gaussian&quot;, n.tree = 100, data = df, shrinkage = 0.1, bag.fraction = 1) bo2 &lt;- gbm(y ~ x, distribution=&quot;gaussian&quot;, data = df) # All default plot(df$x, df$y, ylab=&quot;&quot;, xlab=&quot;&quot;) #Data points lines(df$x, predict(bo1, data=df, n.trees=t), type=&quot;s&quot;, col=&quot;red&quot;, lwd=3) #line for without CV lines(df$x, predict(bo2, n.trees=t, data=df), type=&quot;s&quot;, col=&quot;green&quot;, lwd=3) #line with default parameters with CV Let’s see boosting for classification 13.3.2 AdaBoost One of the most popular boosting algorithm is AdaBost.M1 due to Freund and Schpire (1997). We consider two-class problem where \\(y \\in\\{-1,1\\}\\), which is a qualitative variable. With a set of predictor variables \\(X\\), a classifier \\(\\hat{m}_{b}(x)\\) at tree b among B trees, produces a prediction taking the two values \\(\\{-1,1\\}\\). To understand how AdaBoost works, let’s look at the algorithm step by step: Select the number of trees B, and the tree depth D; Set initial weights, \\(w_i=1/n\\), for each observation. Fit a classification tree \\(\\hat{m}_{b}(x)\\) at \\(b=1\\), the first tree. Calculate the following misclassification error for \\(b=1\\): \\[ \\mathbf{err}_{b=1}=\\frac{\\sum_{i=1}^{n} \\mathbf{I}\\left(y_{i} \\neq \\hat{m}_{b}\\left(x_{i}\\right)\\right)}{n} \\] By using this error, calculate \\[ \\alpha_{b}=0.5\\log \\left(\\frac{1-e r r_{b}}{e r r_{b}}\\right) \\] This gives us log odds. For example, suppose \\(err_b = 0.3\\), then \\(\\alpha_{b}=\\text{log}(0.7/0.3)\\), which is a log odds or log(success/failure). If the observation i is misclassified, update its weights, if not, use \\(w_i\\) which is \\(1/n\\): \\[ w_{i} \\leftarrow w_{i} e^{\\alpha_b} \\] Let’s try some numbers: #Suppose err = 0.2, n=100 n = 100 err = 0.2 alpha &lt;- 0.5*log((1-err)/err) alpha ## [1] 0.6931472 exp(alpha) ## [1] 2 So, the new weight for the misclassified \\(i\\) in the second tree (i.e., \\(b=2\\) stump) will be # For misclassified obervations weight_miss &lt;- (1/n)*(exp(alpha)) weight_miss ## [1] 0.02 # For correctly classified observation weight_corr &lt;- (1/n)*(exp(-alpha)) weight_corr ## [1] 0.005 This shows that as the misclassification error goes up, it increases the weights (0.04) of each misclassified observation relative to correctly classified observations and reduces the weights for correctly classified observations (0.0025). With this procedure, in each loop from b to B, it reapply \\(\\hat{m}_{b}(x)\\) to the data using updated weights \\(w_i\\) in each b by updating in weights by: \\[ \\mathbf{err}_{b}=\\frac{\\sum_{i=1}^{n} w_{i} \\mathbf{I}\\left(y_{i} \\neq \\hat{m}_{b}\\left(x_{i}\\right)\\right)}{\\sum_{i=1}^{n} w_{i}} \\] We know that the sum of the weights are supposed to be 1. So a normalization for all weights between 0 and 1 would handle this issue in each iteration. The algorithm works in a way that it randomly replicates the observations as new data points by using the weights as their probabilities. This process also resembles to under- and oversampling at the same time so that the number of observations stays the same. The new dataset now is used again for the next tree (\\(b=2\\)) and this iteration continues until \\(B\\). We can use rpart() in each tree with weights option as we will shown momentarily. Here is an example with the myocarde data that we used in CARD: library(readr) myocarde &lt;- read_delim(&quot;myocarde.csv&quot;, delim = &quot;;&quot; , escape_double = FALSE, trim_ws = TRUE, show_col_types = FALSE) myocarde &lt;- data.frame(myocarde) df &lt;- head(myocarde) df$W = 1/nrow(df) df ## FRCAR INCAR INSYS PRDIA PAPUL PVENT REPUL PRONO W ## 1 90 1.71 19.0 16 19.5 16.0 912 SURVIE 0.1666667 ## 2 90 1.68 18.7 24 31.0 14.0 1476 DECES 0.1666667 ## 3 120 1.40 11.7 23 29.0 8.0 1657 DECES 0.1666667 ## 4 82 1.79 21.8 14 17.5 10.0 782 SURVIE 0.1666667 ## 5 80 1.58 19.7 21 28.0 18.5 1418 DECES 0.1666667 ## 6 80 1.13 14.1 18 23.5 9.0 1664 DECES 0.1666667 Suppose that our first stump misclassifies the first observation. So the error rate # Alpha n = nrow(df) err = 1/n alpha &lt;- 0.5*log((1-err)/err) alpha ## [1] 0.804719 exp(alpha) ## [1] 2.236068 # ****** Weights ******* # For misclassified obervations weight_miss &lt;- (1/n)*(exp(alpha)) weight_miss ## [1] 0.372678 # For correctly classified observation weight_corr &lt;- (1/n)*(exp(-alpha)) weight_corr ## [1] 0.0745356 Hence, our new sample weights df$NW &lt;- c(weight_miss, rep(weight_corr, 5)) df$Norm &lt;- df$NW/sum(df$NW) # nromalizing #Ignoring X&#39;s df[,8:11] ## PRONO W NW Norm ## 1 SURVIE 0.1666667 0.3726780 0.5 ## 2 DECES 0.1666667 0.0745356 0.1 ## 3 DECES 0.1666667 0.0745356 0.1 ## 4 SURVIE 0.1666667 0.0745356 0.1 ## 5 DECES 0.1666667 0.0745356 0.1 ## 6 DECES 0.1666667 0.0745356 0.1 Now, the new dataset: df$bins &lt;- cumsum(df$Norm) df[,8:12] ## PRONO W NW Norm bins ## 1 SURVIE 0.1666667 0.3726780 0.5 0.5 ## 2 DECES 0.1666667 0.0745356 0.1 0.6 ## 3 DECES 0.1666667 0.0745356 0.1 0.7 ## 4 SURVIE 0.1666667 0.0745356 0.1 0.8 ## 5 DECES 0.1666667 0.0745356 0.1 0.9 ## 6 DECES 0.1666667 0.0745356 0.1 1.0 Now supposed we run runif(n=6, min = 0, max = 1) and get \\(0.2201686, 0.9465279, 0.5118751, 0.8167266, 0.2208179, 0.1183170.\\) Since incorrectly classified records have higher sample weights, the probability to select those records is very high. We select the observation that these random numbers in their bin: df[c(1,6,2,4,1,1), -c(9:12)] # after ## FRCAR INCAR INSYS PRDIA PAPUL PVENT REPUL PRONO ## 1 90 1.71 19.0 16 19.5 16 912 SURVIE ## 6 80 1.13 14.1 18 23.5 9 1664 DECES ## 2 90 1.68 18.7 24 31.0 14 1476 DECES ## 4 82 1.79 21.8 14 17.5 10 782 SURVIE ## 1.1 90 1.71 19.0 16 19.5 16 912 SURVIE ## 1.2 90 1.71 19.0 16 19.5 16 912 SURVIE df[,1:8] # before ## FRCAR INCAR INSYS PRDIA PAPUL PVENT REPUL PRONO ## 1 90 1.71 19.0 16 19.5 16.0 912 SURVIE ## 2 90 1.68 18.7 24 31.0 14.0 1476 DECES ## 3 120 1.40 11.7 23 29.0 8.0 1657 DECES ## 4 82 1.79 21.8 14 17.5 10.0 782 SURVIE ## 5 80 1.58 19.7 21 28.0 18.5 1418 DECES ## 6 80 1.13 14.1 18 23.5 9.0 1664 DECES Hence, observations that are misclassified will have more influence in the next classifier. This is an incredible boost that forces the classification tree to adjust its prediction to do better job for misclassified observations. Finally, in the output, the contributions from classifiers that fit the data better are given more weight (a larger \\(\\alpha_b\\) means a better fit). Unlike a random forest algorithm where each tree gets an equal weight in final decision, here some stumps get more say in final classification. Moreover, “forest of stumps” the order of trees is important. Hence, the final prediction on \\(y_i\\) will be combined from all trees, b to B, through a weighted majority vote: \\[ \\hat{y}_{i}=\\operatorname{sign}\\left(\\sum_{b=1}^{B} \\alpha_{b} \\hat{m}_{b}(x)\\right), \\] which is a signum function defined as follows: \\[ \\operatorname{sign}(x):=\\left\\{\\begin{array}{ll} {-1} &amp; {\\text { if } x&lt;0} \\\\ {0} &amp; {\\text { if } x=0} \\\\ {1} &amp; {\\text { if } x&gt;0} \\end{array}\\right. \\] Here is a simple simulation to show how alpha will make the importance of each tree (\\(\\hat{m}_{b}(x)\\)) different: n = 1000 set.seed(1) err &lt;- sample(seq(0, 1, 0.01), n, replace = TRUE) alpha = 0.5*log((1-err)/err) ind = order(err) plot(err[ind], alpha[ind], xlab = &quot;error (err)&quot;, ylab = &quot;alpha&quot;, type = &quot;o&quot;, col = &quot;red&quot;, lwd = 2) We can see that when there is no misclassification error (err = 0), “alpha” will be a large positive number. When the classifier very weak and predicts as good as a random guess (err = 0.5), the importance of the classifier will be 0. If all the observations are incorrectly classified (err = 1), our alpha value will be a negative integer. The AdaBoost.M1 is known as a “discrete classifier” because it directly calculates discrete class labels \\(\\hat{y}_i\\), rather than predicted probabilities, \\(\\hat{p}_i\\). What type of classifier, \\(\\hat{m}_{b}(x)\\), would we choose? Usually a “weak classifier” like a “stump” (a two terminal-node classification tree, i.e 1 split) would be enough. The \\(\\hat{m}_{b}(x)\\) choose one variable to form a stump that gives the lowest Gini index. Here is our simple example with the myocarde data to show how we can boost a simple weak learner (stump) by using AdaBoost algorithm: library(rpart) # Data myocarde &lt;- read_delim(&quot;myocarde.csv&quot;, delim = &quot;;&quot; , escape_double = FALSE, trim_ws = TRUE, show_col_types = FALSE) myocarde &lt;- data.frame(myocarde) y &lt;- (myocarde[ , &quot;PRONO&quot;] == &quot;SURVIE&quot;) * 2 - 1 x &lt;- myocarde[ , 1:7] df &lt;- data.frame(x, y) # Setting rnd = 100 # number of rounds m = nrow(x) W &lt;- rep(1/m, m) # initial weights S &lt;- list() # container to save all stumps alpha &lt;- vector(mode = &quot;numeric&quot;, rnd) # container for alpha y_hat &lt;- vector(mode = &quot;numeric&quot;, m) # container for final predictions set.seed(123) for(i in 1:rnd) { S[[i]] &lt;- rpart(y ~., data = df, weights = W, maxdepth = 1, method = &quot;class&quot;) yhat &lt;- predict(S[[i]], x, type = &quot;class&quot;) yhat &lt;- as.numeric(as.character(yhat)) e &lt;- sum((yhat!=y) * W) # alpha alpha[i] &lt;- 0.5 * log((1-e) / e) # Updating weights W &lt;- W * exp(-alpha[i] * y * yhat) # Normalizing weights W &lt;- W / sum(W) } # Using each stump for S[i] for final predictions for (i in 1:rnd) { pred = predict(S[[i]], df, type = &quot;class&quot;) pred = as.numeric(as.character(pred)) y_hat = y_hat + (alpha[i] * pred) } # Let&#39;s see what y_hat is y_hat ## [1] 3.132649 -4.135656 -4.290437 7.547707 -3.118702 -6.946686 ## [7] 2.551433 1.960603 9.363346 6.221990 3.012195 6.982287 ## [13] 9.765139 8.053999 8.494254 7.454104 4.112493 5.838279 ## [19] 4.918513 9.514860 9.765139 -3.519537 -3.172093 -7.134057 ## [25] -3.066699 -4.539863 -2.532759 -2.490742 5.412605 2.903552 ## [31] 2.263095 -6.718090 -2.790474 6.813963 -5.131830 3.680202 ## [37] 3.495350 3.014052 -7.435835 6.594157 -7.435835 -6.838387 ## [43] 3.951168 5.091548 -3.594420 8.237515 -6.718090 -9.582674 ## [49] 2.658501 -10.282682 4.490239 9.765139 -5.891116 -5.593352 ## [55] 6.802687 -2.059754 2.832103 7.655197 10.635851 9.312842 ## [61] -5.804151 2.464149 -5.634676 1.938855 9.765139 7.023157 ## [67] -6.078756 -7.031840 5.651634 -1.867942 9.472835 # Now use sign() function pred &lt;- sign(y_hat) # Confusion matrix table(pred, y) ## y ## pred -1 1 ## -1 29 0 ## 1 0 42 This is our in-sample confusion table. We can also see several stumps: library(rpart.plot) plt &lt;- c(1,5,10,30, 60, 90) p = par(mfrow=c(2,3)) for(i in 1:length(plt)){ prp(S[[i]], type = 2, extra = 1, split.col = &quot;red&quot;, split.border.col = &quot;blue&quot;, box.col = &quot;pink&quot;) } par(p) Let’s see it with the JOUSBoost package: library(JOUSBoost) ada &lt;- adaboost(as.matrix(x), y, tree_depth = 1, n_rounds = rnd) summary(ada) ## Length Class Mode ## alphas 100 -none- numeric ## trees 100 -none- list ## tree_depth 1 -none- numeric ## terms 3 terms call ## confusion_matrix 4 table numeric pred &lt;- predict(ada, x) table(pred, y) ## y ## pred -1 1 ## -1 29 0 ## 1 0 42 These results provide in-sample predictions. When we use it in a real example, we can train AdaBoost.M1 by the tree depths (1 in our example) and the number of iterations (100 trees in our example). Although, there are simulations that AdaBoost is stubbornly resistant to overfitting, you can try with different tree depth and number of trees. An application is provided in the next chapter. 13.3.3 Extreme Gradient Boosting (XGBoost) Extreme Gradient Boosting (XGBoost) the most efficient version of the gradient boosting framework by its capacity to implement parallel computation on a single machine. It can be used for regression and classification problems with two modes: linear models and tree learning algorithm. This is important because, as we will see in the next section (and Section 6), XGBoost can be used for regularization in linear models. As always, however, decision trees are much better to catch a nonlinear link between predictors and outcome. Thus, comparison between two methods can provide quick information to the practitioner, specially in causal analyses, about the structure of alternative models. The XGBoost has several unique advantages: its speed is measured as “10 times faster than the gbm” (see its vignette) and it accepts very efficient input data structures, such as a sparse matrix6. This special input structure in xgboost requires some additional data preparation: a matrix input for the features and a vector for the response. Therefore, a matrix input of the features requires to encode our categorical variables, as we showed before. The matrix can also be selected 3 possible choices: a regular R matrix, a sparse matrix from the Matrix package, We will use a regression example here and leave the classification example to the next chapter in boosting applications. We will use the Ames housing data to see the best “predictors” of the sale price. library(xgboost) library(mltools) library(data.table) library(modeldata) # This can also be loaded by the tidymodels package data(ames) #str(ames) dim(ames) ## [1] 2930 74 The xgboost algorithm accepts its input data as a matrix. Therefore all the categorical variables have be one-hot coded, which creates a large matrix with even with a small size data. That’s why using more memory efficient matrix types (sparse matrix etc.) speeds up the process. We ignore it here and use a regular R matrix, for now. ames_new &lt;- one_hot(as.data.table(ames)) df &lt;- as.data.frame(ames_new) ind &lt;- sample(nrow(df), nrow(df), replace = TRUE) train &lt;- df[ind, ] test &lt;- df[-ind, ] X &lt;- as.matrix(train[, -which(names(train) == &quot;Sale_Price&quot;)]) Y &lt;- train$Sale_Price Now we are ready for finding the optimal tuning parameters. One strategy in tuning is to see if there is a substantial difference between train and CV errors. As we have seen in gbm, we first start with the number trees and the learning rate. If the difference still persists, we introduce regularization parameters. There are three regularization parameters: gamma, lambda, and alpha. The last two are similar to what we will see in a linear regularization next chapter. Let’s start with the data preparation as xgboost requires a matrix input for the features and a vector for the response. We will do it without a grid search. This should be done with sweeping a grid of hyperparamaters (including gamma, lambda, and alpha): set.seed(123) boost &lt;- xgb.cv( data = X, label = Y, nrounds = 3000, objective = &quot;reg:squarederror&quot;, early_stopping_rounds = 50, nfold = 10, params = list( eta = 0.1, max_depth = 3, min_child_weight = 3, subsample = 0.8, colsample_bytree = 1.0), verbose = 0 ) Let’s see the RMSE and the best iteration: boost$best_iteration ## [1] 1781 min(boost$evaluation_log$test_rmse_mean) ## [1] 16807.16 # One possible grid would be: # param_grid &lt;- expand.grid( # eta = 0.01, # max_depth = 3, # min_child_weight = 3, # subsample = 0.5, # colsample_bytree = 0.5, # gamma = c(0, 1, 10, 100, 1000), # lambda = seq(0, 0.01, 0.1, 1, 100, 1000), # alpha = c(0, 0.01, 0.1, 1, 100, 1000) # ) # After going through the grid in a loop with `xgb.cv` # we save multiple `test_rmse_mean` and `best_iteration` # and find the parameters that gives the minimum rmse Now after identifying the best model, we can use it on the test data: params &lt;- list( eta = 0.01, max_depth = 3, min_child_weight = 3, subsample = 0.8, colsample_bytree = 1.0 ) tr_model &lt;- xgboost( params = params, data = X, label = Y, nrounds = 2515, objective = &quot;reg:squarederror&quot;, verbose = 0 ) This trained (best fitting model) is enough to obtain the top 10 influential features in our final model using the impurity (gain) metric: library(vip) vip(tr_model, aesthetics = list(color = &quot;green&quot;, fill = &quot;darkgreen&quot;)) Now, we can use our trained model for predictions using our test set. Note that, again, xgboost would only accept matrix inputs. yhat &lt;- predict(tr_model, as.matrix(test[,-which(names(test) == &quot;Sale_Price&quot;)])) rmse_test &lt;- sqrt(mean((test[,which(names(train) == &quot;Sale_Price&quot;)]-yhat)^2)) rmse_test ## [1] 22895.51 Note the big difference between training and test RMSPE’s. This is an indication that our “example” grid is not doing a good job. We should include regularization tuning parameters and run a full scale grid search. We will look at a classification example in the next chapter (Ch.13). But, a curious reader would ask: would it be better had we run random forest? Or, could it have been better with a full scale grid search? https://web.archive.org/web/20121010030839/http://www.cs.princeton.edu/~schapire/papers/strengthofweak.pdf↩︎ In a sparse matrix, cells containing 0 are not stored in memory. Therefore, in a dataset mainly made of 0, the memory size is reduced.↩︎ "],["ensemble-applications.html", "Chapter 14 Ensemble Applications 14.1 Classification 14.2 Regression 14.3 Dataset-level explainers 14.4 Boosting Applications", " Chapter 14 Ensemble Applications To conclude this section we will cover classification and regression applications using bagging, random forest, boosting and SVM. First we will start with a classification problem. In comparing different ensemble methods, we must look not only at their accuracy, but evaluate their stability as well. 14.1 Classification We will again predict survival on the Titanic, using CART, bagging and random forest. We will use the following variables: survived - 1 if true, 0 otherwise; sex - the gender of the passenger; age - age of the passenger in years; pclass - the passengers class of passage; sibsp - the number of siblings/spouses aboard; parch - the number of parents/children aboard. library(PASWR) library(ROCR) library(rpart) library(randomForest) # Data data(titanic3) nam &lt;- c(&quot;survived&quot;, &quot;sex&quot;, &quot;age&quot;, &quot;pclass&quot;, &quot;sibsp&quot;, &quot;parch&quot;) df &lt;- titanic3[, nam] dfc &lt;- df[complete.cases(df), ] dfc$survived &lt;- as.factor(dfc$survived) AUC1 &lt;- c() AUC2 &lt;- c() AUC3 &lt;- c() n = 100 B = 100 for (i in 1:n) { set.seed(i+i*100) ind &lt;- sample(nrow(dfc), nrow(dfc), replace = TRUE) train &lt;- dfc[ind, ] test &lt;- dfc[-ind, ] p = ncol(train)-1 #3 Methods model1 &lt;- rpart(survived~sex+age+pclass+sibsp+parch, data=train, method=&quot;class&quot;) #Single tree, pruned model2 &lt;- randomForest(survived~sex+age+pclass+sibsp+parch, ntree = B, mtry = p, data = train) #Bagged model3 &lt;- randomForest(survived~sex+age+pclass+sibsp+parch, ntree = B, data = train) # RF phat1 &lt;- predict(model1, test, type = &quot;prob&quot;) phat2 &lt;- predict(model2, test, type = &quot;prob&quot;) phat3 &lt;- predict(model3, test, type = &quot;prob&quot;) #AUC1 pred_rocr1 &lt;- prediction(phat1[,2], test$survived) auc_ROCR1 &lt;- performance(pred_rocr1, measure = &quot;auc&quot;) AUC1[i] &lt;- auc_ROCR1@y.values[[1]] #AUC2 pred_rocr2 &lt;- prediction(phat2[,2], test$survived) auc_ROCR2 &lt;- performance(pred_rocr2, measure = &quot;auc&quot;) AUC2[i] &lt;- auc_ROCR2@y.values[[1]] #AUC3 pred_rocr3 &lt;- prediction(phat3[,2], test$survived) auc_ROCR3 &lt;- performance(pred_rocr3, measure = &quot;auc&quot;) AUC3[i] &lt;- auc_ROCR3@y.values[[1]] } model &lt;- c(&quot;Single-Tree&quot;, &quot;Bagging&quot;, &quot;RF&quot;) AUCs &lt;- c(mean(AUC1), mean(AUC2), mean(AUC3)) sd &lt;- c(sqrt(var(AUC1)), sqrt(var(AUC2)), sqrt(var(AUC3))) data.frame(model, AUCs, sd) ## model AUCs sd ## 1 Single-Tree 0.8129740 0.02585391 ## 2 Bagging 0.8128962 0.01709652 ## 3 RF 0.8409750 0.01659263 There is a consensus that we can determine a bagged model’s test error without using cross-validation. The reason for this is usually stated that each bootstrapped sample contains about two-thirds of the original dataset’s observations. Out-of-Bag (OOB) observations are the remaining 1/3 of the observations that were not used to fit the bagged tree. We did the bagging by using randomForest in the previous application. Let’s see if we can obtain a similar result with our manual bagging using rpart() pruned and unpruned: n &lt;- 100 B &lt;- 500 AUCp &lt;- c() AUCup &lt;- c() for (i in 1:n) { set.seed(i+i*100) ind &lt;- sample(nrow(dfc), nrow(dfc), replace = TRUE) train &lt;- dfc[ind, ] test &lt;- dfc[-ind, ] phatp &lt;- matrix(0, B, nrow(test)) phatup &lt;- matrix(0, B, nrow(test)) for (j in 1:B) { set.seed(j+j*2) ind &lt;- sample(nrow(train), nrow(train), replace = TRUE) tr &lt;- train[ind, ] modelp &lt;- rpart(survived ~ sex + age + pclass + sibsp + parch, data = tr, method = &quot;class&quot;) # Pruned modelup &lt;- rpart(survived ~ sex + age + pclass + sibsp + parch, data = tr, control = rpart.control(minsplit = 2, minbucket = 1 , cp = 0), method = &quot;class&quot;) # unpruned phatp[j, ] &lt;- predict(modelp, test, type = &quot;prob&quot;)[, 2] phatup[j, ] &lt;- predict(modelup, test, type = &quot;prob&quot;)[, 2] } # Averaging for B Trees phatpr &lt;- apply(phatp, 2, mean) phatupr &lt;- apply(phatup, 2, mean) # AUC pruned pred_rocr &lt;- prediction(phatpr, test$survived) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) AUCp[i] &lt;- auc_ROCR@y.values[[1]] # AUC unpruned pred_rocr &lt;- prediction(phatupr, test$survived) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) AUCup[i] &lt;- auc_ROCR@y.values[[1]] } mean(AUCp) ## [1] 0.8523158 sqrt(var(AUCp)) ## [1] 0.01626892 mean(AUCup) ## [1] 0.8180802 sqrt(var(AUCup)) ## [1] 0.01693003 We can see a significant reduction in uncertainty and improvement in accuracy relative to a single tree. Moreover, our manual bagging with the cross-validated (pruned) single tree using rpart() doing a better job than the bagging using randomForest(). When we use “unpruned” single-tree using rpart() for bagging, the result becomes very similar to one that we obtain with random forest. Further, the number of bootstrapped trees (B) would be a hyperparameter to tune in bagging. You can try the same script multiple times with different B vales such as 50, 100, 150. In our experiment (not shown here), it seems that results are not sensitive to B as long as B is large enough like 50 and more. This would also be the case in regression trees, where we would be averaging yhat’s and calculating RMSPE and its standard deviations instead of AUC. 14.2 Regression Consider the same data set we used earlier chapters to predict baseball player’s salary: library(ISLR) remove(list = ls()) data(Hitters) df &lt;- Hitters[complete.cases(Hitters$Salary), ] Let’s use only a single tree with bagging: library(rpart) # Data df$logsal &lt;- log(df$Salary) df &lt;- df[, -19] n = 100 B = 500 RMSPEp &lt;- c() RMSPEup &lt;- c() for (i in 1:n) { set.seed(i+i*8) ind &lt;- sample(nrow(df), nrow(df), replace = TRUE) train &lt;- df[ind, ] test &lt;- df[-ind, ] yhatp &lt;- matrix(0, B, nrow(test)) yhatup &lt;- matrix(0, B, nrow(test)) for (j in 1:B) { set.seed(j+j*2) ind &lt;- sample(nrow(train), nrow(train), replace = TRUE) tr &lt;- train[ind, ] modelp &lt;- rpart(logsal ~ ., data = tr, method = &quot;anova&quot;) # Pruned modelup &lt;- rpart(logsal ~ ., data = tr, control = rpart.control(minsplit = 2, minbucket = 1 ,cp = 0), method = &quot;anova&quot;) # unpruned yhatp[j,] &lt;- predict(modelp, test) yhatup[j,] &lt;- predict(modelup, test) } # Averaging for B Trees yhatpr &lt;- apply(yhatp, 2, mean) yhatupr &lt;- apply(yhatup, 2, mean) RMSPEp[i] &lt;- sqrt(mean((test$logsal - yhatpr)^2)) RMSPEup[i] &lt;- sqrt(mean((test$logsal - yhatupr)^2)) } mean(RMSPEp) ## [1] 0.501984 sqrt(var(RMSPEp)) ## [1] 0.05817388 mean(RMSPEup) ## [1] 0.4808079 sqrt(var(RMSPEup)) ## [1] 0.06223845 With and without pruning, the results are very similar. Let’s put all these together and do it with Random Forest: library(randomForest) library(rpart) # Data remove(list = ls()) data(Hitters) df &lt;- Hitters[complete.cases(Hitters$Salary), ] df$logsal &lt;- log(df$Salary) df &lt;- df[, -19] n &lt;- 100 B &lt;- 500 RMSPE1 &lt;- c() RMSPE2 &lt;- c() RMSPE3 &lt;- c() for (i in 1:n) { set.seed(i+i*8) ind &lt;- sample(nrow(df), nrow(df), replace = TRUE) train &lt;- df[ind, ] test &lt;- df[-ind, ] p = ncol(train)-1 model1 &lt;- rpart(logsal~., data =train) # Single Tree model2 &lt;- randomForest(logsal~., ntree = B, mtry = p, data = train) #Bagged model3 &lt;- randomForest(logsal~., ntree = B, localImp = TRUE, data = train) # RF yhat1 &lt;- predict(model1, test) yhat2 &lt;- predict(model2, test) yhat3 &lt;- predict(model3, test) RMSPE1[i] &lt;- sqrt(mean((test$logsal - yhat1)^2)) RMSPE2[i] &lt;- sqrt(mean((test$logsal - yhat2)^2)) RMSPE3[i] &lt;- sqrt(mean((test$logsal - yhat3)^2)) } model &lt;- c(&quot;Single-Tree&quot;, &quot;Bagging&quot;, &quot;RF&quot;) RMSPEs &lt;- c(mean(RMSPE1), mean(RMSPE2), mean(RMSPE3)) sd &lt;- c(sqrt(var(RMSPE1)), sqrt(var(RMSPE2)), sqrt(var(RMSPE3))) data.frame(model, RMSPEs, sd) ## model RMSPEs sd ## 1 Single-Tree 0.5739631 0.05360920 ## 2 Bagging 0.4807763 0.06119187 ## 3 RF 0.4631194 0.06045187 As you can see, random forest has the lowest RMSPE and thus performs the best, whereas the CART model has the highest RMSPE. 14.3 Dataset-level explainers library(randomForest) varImpPlot(model3) We will now dig deeper with randomForestExplainer (see its vignette) (Palu_2012?) and DALEX packages. Assuming that the observations form a representative sample from a general population, dataset-level explainers can provide information about the quality of predictions for the population. library(randomForestExplainer) library(DT) min_depth_frame &lt;- min_depth_distribution(model3) importance_frame &lt;- measure_importance(model3) tabl &lt;- cbind(importance_frame[,1], round(importance_frame[,2:7],4)) datatable(tabl, rownames = FALSE, filter=&quot;top&quot;, options = list(pageLength = 10, scrollX=T) ) plot_multi_way_importance(importance_frame, x_measure = &quot;mean_min_depth&quot;, y_measure = &quot;mse_increase&quot;, size_measure = &quot;p_value&quot;, no_of_labels = 6) plot_min_depth_distribution(min_depth_frame, mean_sample = &quot;all_trees&quot;, k =20, main = &quot;Distribution of minimal depth and its mean&quot;) partialPlot(model3, test, CRuns, xlab=&quot;CRuns&quot;, main=&quot;Effects of CRuns&quot;, col = &quot;red&quot;, lwd = 3) 14.4 Boosting Applications We need to tune the boosting applications with gbm(). There are three tuning parameters: h, B, and D. We will do the tuning with grid search and apply parallel processing. We will have both regression and classification problems. Finally we will compare OLS, CART, Bagging, RF and boosting. 14.4.1 Regression library(ISLR) data(Hitters) df &lt;- Hitters[complete.cases(Hitters$Salary), ] df$Salary &lt;- log(df$Salary) # Test/Train Split set.seed(1) ind &lt;- sample(nrow(df), nrow(df), replace = TRUE) train &lt;- df[ind, ] test &lt;- df[-ind, ] This will give you an idea how tuning the boosting by using h would be done: library(gbm) h &lt;- seq(0.01, 1.8, 0.01) test_mse &lt;- c() # D = 1 and B = 1000 for(i in 1:length(h)){ boos &lt;- gbm(Salary~., distribution =&quot;gaussian&quot;, n.trees=1000, interaction.depth = 1, shrinkage = h[i], data = train) prboos &lt;- predict(boos, test, n.trees = 100) test_mse[i] &lt;- mean((test$Salary - prboos)^2) } plot(h, test_mse, type = &quot;l&quot;, col=&quot;blue&quot;, main = &quot;MSE - Prediction&quot;) h[which.min(test_mse)] ## [1] 0.08 min(test_mse) ## [1] 0.181286 test_mse[10] ## [1] 0.1895487 A complete but limited grid search is here: library(gbm) h &lt;- seq(0.01, 0.3, 0.01) B &lt;- c(100, 300, 500, 750, 900) D &lt;- 1:2 grid &lt;- as.matrix(expand.grid(D, B, h)) mse &lt;-c() sdmse &lt;-c() for(i in 1:nrow(grid)){ test_mse &lt;- c() for (j in 1:20) { try({ set.seed(j) ind &lt;- sample(nrow(df), nrow(df), replace = TRUE) train &lt;- df[ind, ] test &lt;- df[-ind, ] boos &lt;- gbm(Salary~., distribution =&quot;gaussian&quot;, n.trees=1000, interaction.depth = grid[i,1], shrinkage = grid[i,3], data = train) prboos &lt;- predict(boos, test, n.trees = grid[i,2]) test_mse[j] &lt;- mean((test$Salary - prboos)^2) }, silent = TRUE) } mse[i] &lt;- mean(test_mse) sdmse[i] &lt;- sd(test_mse) } min(mse) ## [1] 0.2108654 grid[as.numeric(which.min(mse)), ] ## Var1 Var2 Var3 ## 2e+00 9e+02 1e-02 14.4.2 Random search with parallel processing Now, we will apply a random grid search (see Random Search for Hyper-Parameter Optimization) (Bergs_2012?). We also apply a parallel multicore processing using doParallel and foreach() to accelerate the grid search. library(gbm) library(doParallel) library(foreach) h &lt;- seq(0.001, 0.25, 0.001) B &lt;- seq(100, 800, 20) D &lt;- 1:4 grid &lt;- as.matrix(expand.grid(D, B, h)) #Random grid-search conf_lev &lt;- 0.95 num_max &lt;- 5 n &lt;- log(1-conf_lev)/log(1-num_max/nrow(grid)) set.seed(123) ind &lt;- sample(nrow(grid), nrow(grid)*(n/nrow(grid)), replace = FALSE) comb &lt;- grid[ind, ] # Set-up for multicore loops trials &lt;- 1:nrow(comb) numCores &lt;- detectCores() registerDoParallel(numCores) # Bootstrapping with parallel process lst &lt;- foreach(k=trials, .combine=c, .errorhandling = &#39;remove&#39;) %dopar% { test_mse &lt;- c() for (j in 1:10) { try({ set.seed(j) ind &lt;- sample(nrow(df), nrow(df), replace = TRUE) train &lt;- df[ind, ] test &lt;- df[-ind, ] boos &lt;- gbm(Salary~., distribution =&quot;gaussian&quot;, n.trees=1000, interaction.depth =comb[k,1], shrinkage = comb[k,3], data = train) prboos &lt;- predict(boos, test, n.trees = comb[k,2]) test_mse[j] &lt;- mean((test$Salary - prboos)^2) }, silent = TRUE) } list(c(k, mean(test_mse), sd(test_mse))) } stopImplicitCluster() unlst &lt;- do.call(rbind, lst) result &lt;- cbind(comb[unlst[,1],], unlst) sorted &lt;- result[order(result[,5]), -4] colnames(sorted) &lt;- c(&quot;D&quot;, &quot;B&quot;, &quot;h&quot;, &quot;MSPE&quot;, &quot;sd&quot;) head(sorted) ## D B h MSPE sd ## [1,] 2 360 0.024 0.2057671 0.05657079 ## [2,] 2 300 0.024 0.2060013 0.05807494 ## [3,] 2 340 0.022 0.2061847 0.05827857 ## [4,] 2 340 0.023 0.2061895 0.05823719 ## [5,] 2 320 0.023 0.2062056 0.05874694 ## [6,] 2 360 0.021 0.2062124 0.05785775 You can increase for (j in 1:10) to for (j in 1:50) depending on your computer’s capacity. 14.4.3 Boosting vs. Others Let’s add OLS to this competition just for curiosity. Here is a one possible script: library(ISLR) library(randomForest) library(rpart) df &lt;- Hitters[complete.cases(Hitters$Salary), ] df$Salary &lt;- log(df$Salary) # Containers mse_cart &lt;- c(0) mse_bag &lt;- c(0) mse_rf &lt;- c(0) mse_boost &lt;- c(0) mse_ols &lt;- c(0) for(i in 1:200){ set.seed(i) ind &lt;- sample(nrow(df), nrow(df), replace = TRUE) train &lt;- df[ind, ] test &lt;- df[-ind, ] OLS &lt;- lm(Salary~., data = train) pols &lt;- predict(OLS, test) cart &lt;- rpart(Salary~., data = train) pcart &lt;- predict(cart, test) bags &lt;- randomForest(Salary ~., mtry = 19, data = train) pbag &lt;- predict(bags, test) rf &lt;- randomForest(Salary ~., data = train) prf &lt;- predict(rf, test) boost &lt;- gbm(Salary~., distribution =&quot;gaussian&quot;, n.trees = 1000, data = train) # without a grid search pboost &lt;- predict(boost, test, n.trees = 100) mse_ols[i] &lt;- mean((test$Salary - pols)^2) mse_cart[i] &lt;- mean((test$Salary - pcart)^2) mse_bag[i] &lt;- mean((test$Salary - pbag)^2) mse_rf[i] &lt;- mean((test$Salary - prf)^2) mse_boost[i] &lt;- mean((test$Salary - pboost)^2) } # Bootstrapping Results a &lt;- matrix(c(mean(mse_cart), mean(mse_bag), mean(mse_rf), mean(mse_boost), mean(mse_ols)), 5, 1) row.names(a) &lt;- c(&quot;mse_cart&quot;, &quot;mse_bag&quot;, &quot;mse_rf&quot;, &quot;mse_boost&quot;, &quot;mse_ols&quot;) a ## [,1] ## mse_cart 0.3172687 ## mse_bag 0.2205504 ## mse_rf 0.2057802 ## mse_boost 0.2454886 ## mse_ols 0.4584240 b &lt;- matrix(c(sqrt(var(mse_cart)), sqrt(var(mse_bag)), sqrt(var(mse_rf)), sqrt(var(mse_boost)), sqrt(var(mse_ols))), 5, 1) row.names(b) &lt;- c(&quot;mse_cart&quot;, &quot;mse_bag&quot;, &quot;mse_rf&quot;, &quot;mse_boost&quot;, &quot;mse_ols&quot;) b ## [,1] ## mse_cart 0.07308726 ## mse_bag 0.06272648 ## mse_rf 0.05976196 ## mse_boost 0.05923404 ## mse_ols 0.06907506 Interesting! The random forest is the winner. However, boosting is not tuned in the algorithm. With the full grid search in the previous algorithm, boosting and RF are close contenders. Let’s have a classification example. 14.4.4 Classification A simulated data set containing sales of child car seats at 400 different stores from. We will predict the sale, a binary variable that will be 1 if the sale is higher than 8. See ISLR (ISLR_Car?) for the details. library(ISLR) df &lt;- Carseats str(df) ## &#39;data.frame&#39;: 400 obs. of 11 variables: ## $ Sales : num 9.5 11.22 10.06 7.4 4.15 ... ## $ CompPrice : num 138 111 113 117 141 124 115 136 132 132 ... ## $ Income : num 73 48 35 100 64 113 105 81 110 113 ... ## $ Advertising: num 11 16 10 4 3 13 0 15 0 0 ... ## $ Population : num 276 260 269 466 340 501 45 425 108 131 ... ## $ Price : num 120 83 80 97 128 72 108 120 124 124 ... ## $ ShelveLoc : Factor w/ 3 levels &quot;Bad&quot;,&quot;Good&quot;,&quot;Medium&quot;: 1 2 3 3 1 1 3 2 3 3 ... ## $ Age : num 42 65 59 55 38 78 71 67 76 76 ... ## $ Education : num 17 10 12 14 13 16 15 10 10 17 ... ## $ Urban : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 1 2 2 1 1 ... ## $ US : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 1 2 1 2 1 2 ... #Change SALES to a factor variable df$Sales &lt;- ifelse(Carseats$Sales&lt;=8, 0, 1) str(df$Sales) ## num [1:400] 1 1 1 0 0 1 0 1 0 0 ... library(PASWR) library(ROCR) library(rpart) library(randomForest) df &lt;- df[complete.cases(df),] df$d &lt;- as.factor(df$Sales) n &lt;- 50 B &lt;- 1000 AUC1 &lt;- c() AUC2 &lt;- c() AUC3 &lt;- c() AUC4 &lt;- c() for (i in 1:n) { set.seed(i) ind &lt;- sample(nrow(df), nrow(df), replace = TRUE) train &lt;- df[ind, ] test &lt;- df[-ind, ] p = ncol(train)-1 # We used two different outcome structure: &quot;Sales&quot; and &quot;d&quot; # &quot;d&quot; is a factor and &quot;Sales&quot; is numeric # Factor variable is necessary for RF but GBM needs a numeric variable # That&#39;s sometimes annoying but wee need to be careful about the data model1 &lt;- rpart(Sales~., data=train[,-12], method = &quot;class&quot;) model2 &lt;- randomForest(d~., ntree = B, mtry = p, data = train[, -1]) #Bagged model3 &lt;- randomForest(d~., ntree = B, data = train[, -1]) # RF model4 &lt;- gbm(Sales~., data=train[,-12], n.trees = B, distribution = &quot;bernoulli&quot;) # Boosting without grid search phat1 &lt;- predict(model1, test[,-12], type = &quot;prob&quot;) phat2 &lt;- predict(model2, test[,-1], type = &quot;prob&quot;) phat3 &lt;- predict(model3, test[,-1], type = &quot;prob&quot;) phat4 &lt;- predict(model4, n.trees = B, test[,-12], type = &quot;response&quot;) #AUC1 pred_rocr1 &lt;- prediction(phat1[,2], test$Sales) auc_ROCR1 &lt;- performance(pred_rocr1, measure = &quot;auc&quot;) AUC1[i] &lt;- auc_ROCR1@y.values[[1]] #AUC2 pred_rocr2 &lt;- prediction(phat2[,2], test$d) auc_ROCR2 &lt;- performance(pred_rocr2, measure = &quot;auc&quot;) AUC2[i] &lt;- auc_ROCR2@y.values[[1]] #AUC3 pred_rocr3 &lt;- prediction(phat3[,2], test$d) auc_ROCR3 &lt;- performance(pred_rocr3, measure = &quot;auc&quot;) AUC3[i] &lt;- auc_ROCR3@y.values[[1]] #AUC4 pred_rocr4 &lt;- prediction(phat4, test$Sales) auc_ROCR4 &lt;- performance(pred_rocr4, measure = &quot;auc&quot;) AUC4[i] &lt;- auc_ROCR4@y.values[[1]] } model &lt;- c(&quot;Single-Tree&quot;, &quot;Bagging&quot;, &quot;RF&quot;, &quot;Boosting&quot;) AUCs &lt;- c(mean(AUC1), mean(AUC2), mean(AUC3), mean(AUC4)) sd &lt;- c(sqrt(var(AUC1)), sqrt(var(AUC2)), sqrt(var(AUC3)), sqrt(var(AUC4))) data.frame(model, AUCs, sd) ## model AUCs sd ## 1 Single-Tree 0.7607756 0.03203628 ## 2 Bagging 0.8642944 0.02670766 ## 3 RF 0.8778809 0.02356684 ## 4 Boosting 0.9176274 0.01791244 The results are very telling: booster is a clear winner for prediction accuracy and stability. When we have these machine learning applications, one should always show the “baseline” prediction that we can judge the winner performance: A simple LPM would be a good baseline model: AUC5 &lt;- c() for (i in 1:100) { set.seed(i) ind &lt;- sample(nrow(df), nrow(df), replace = TRUE) train &lt;- df[ind, ] test &lt;- df[-ind, ] model &lt;- lm(Sales ~ ., data= train[,-12]) phat5 &lt;- predict(model, test[, -12]) pred_rocr5 &lt;- prediction(phat5, test$Sales) auc_ROCR5 &lt;- performance(pred_rocr5, measure = &quot;auc&quot;) AUC5[i] &lt;- auc_ROCR5@y.values[[1]] } mean(AUC5) ## [1] 0.9546986 sqrt(var(AUC5)) ## [1] 0.0117673 I choose this example, not because I want to give you a pessimistic impressions about machine learning applications. But, when we do predictions, we cannot assume that our complex algorithms will always be better than a simple OLS. We judge the success of prediction not only its own AUC and stability, but also how much it improves over a benchmark. 14.4.5 AdaBoost.M1 Let’s apply AdaBoost to our example to see if we can have any improvements library(JOUSBoost) library(ISLR) df &lt;- Carseats #Change SALES to a factor variable df$Sales &lt;- ifelse(Carseats$Sales&lt;=8, -1, 1) #JOUSBoost requires -1,1 coding str(df$Sales) ## num [1:400] 1 1 1 -1 -1 1 -1 1 -1 -1 ... # JOUSBoost requires X as a matrix # so factor variables must be coded as numerical # With `one-hot()` library(mltools) library(data.table) df_new &lt;- one_hot(as.data.table(df)) Now, we are ready: rnd = 100 AUC &lt;- c() for (i in 1:100) { set.seed(i) ind &lt;- sample(nrow(df_new), nrow(df_new), replace = TRUE) train &lt;- df_new[ind, ] test &lt;- df_new[-ind, ] ada &lt;- adaboost(as.matrix(train[,-&quot;Sales&quot;]), train$Sales, tree_depth = 1, n_rounds = rnd) phat &lt;- predict(ada, test, type=&quot;prob&quot;) pred_rocr &lt;- prediction(phat, test$Sales) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) AUC[i] &lt;- auc_ROCR@y.values[[1]] } mean(AUC) ## [1] 0.9258234 sqrt(var(AUC)) ## [1] 0.0183194 It’s slightly better than the gradient boosting (gbm) but not much from LPM. 14.4.6 Classification with XGBoost Before jumping into an example, let’s first understand about the most frequently used hyperparameters in xgboost. You can refer to its official documentation for more details. We will classify them in three groups: Booster type: Booster = gbtree is the default. It could be set to gblinear or dart. The first one uses a linear model and the second one refers to Dropout Additive Regression Trees. When constructing a gradient boosting machine, the first few trees at the beginning dominate the model performance relative to trees added later. Thus, the idea of “dropout” is to build an ensemble by randomly dropping trees in the boosting sequence. Tuning parameters (note that when gblinear is used, only nround, lambda, and alpha are used): nrounds = 100 (default). It controls the maximum number of iterations (or trees for classification). eta = 0.3. It controls the learning rate. Typically, it lies between 0.01 - 0.3. gamma = 0. It controls regularization (or prevents overfitting - a higher difference between the train and test prediction performance). It can be used as it the brings improvements when shallow (low max_depth) trees are employed. max_depth = 6. It controls the depth of the tree. min_child_weight = 1. It blocks the potential feature interactions to prevent overfitting. (The minimum number of instances required in a child node.) subsample = 1. It controls the number of observations supplied to a tree. Generally, it lies between 0.01 - 0.3. (remember bagging). subsample = 1. It control the number of features (variables) supplied to a tree. Both subsample and subsample can be use to build a “random forest” type learner. lambda = 0, equivalent to Ridge regression alpha = 1, equivalent to Lasso regression (more useful on high dimensional data sets). When both are set different than zero, it becomes an “Elastic Net”, which we will see later. Evaluation parameters: objective = “reg:squarederror” for linear regression, “binary:logistic” binary classification (it returns class probabilities). See the official guide for more options. eval_metric = no default. Depending on objective selected, it could be one of those: mae, Logloss, AUC, RMSE, error - (#wrong cases/#all cases), mlogloss - multiclass. Before executing a full-scale grid search, see what default parameters provide you. That’s your “base” model’s prediction accuracy, which can improve from. If the result is not giving you a desired accuracy, as we did in Chapter 12, set eta = 0.1 and the other parameters at their default values. Using xgb.cv function get best n_rounds and build a model with these parameters. See how much improvement you will get in its accuracy. Then apply the full-scale grid search. We will use the same data (“Adult”) as we used in Chapter 11. library(xgboost) library(mltools) library(data.table) train &lt;- read.csv(&quot;adult_train.csv&quot;, header = FALSE) varNames &lt;- c(&quot;Age&quot;, &quot;WorkClass&quot;, &quot;fnlwgt&quot;, &quot;Education&quot;, &quot;EducationNum&quot;, &quot;MaritalStatus&quot;, &quot;Occupation&quot;, &quot;Relationship&quot;, &quot;Race&quot;, &quot;Sex&quot;, &quot;CapitalGain&quot;, &quot;CapitalLoss&quot;, &quot;HoursPerWeek&quot;, &quot;NativeCountry&quot;, &quot;IncomeLevel&quot;) names(train) &lt;- varNames data &lt;- train tbl &lt;- table(data$IncomeLevel) tbl ## ## &lt;=50K &gt;50K ## 24720 7841 # we remove some outliers - See Ch.11 ind &lt;- which(data$NativeCountry==&quot; Holand-Netherlands&quot;) data &lt;- data[-ind, ] #Converting chr to factor with `apply()` family df &lt;- data df[sapply(df, is.character)] &lt;- lapply(df[sapply(df, is.character)], as.factor) str(df) ## &#39;data.frame&#39;: 32560 obs. of 15 variables: ## $ Age : int 39 50 38 53 28 37 49 52 31 42 ... ## $ WorkClass : Factor w/ 9 levels &quot; ?&quot;,&quot; Federal-gov&quot;,..: 8 7 5 5 5 5 5 7 5 5 ... ## $ fnlwgt : int 77516 83311 215646 234721 338409 284582 160187 209642 45781 159449 ... ## $ Education : Factor w/ 16 levels &quot; 10th&quot;,&quot; 11th&quot;,..: 10 10 12 2 10 13 7 12 13 10 ... ## $ EducationNum : int 13 13 9 7 13 14 5 9 14 13 ... ## $ MaritalStatus: Factor w/ 7 levels &quot; Divorced&quot;,&quot; Married-AF-spouse&quot;,..: 5 3 1 3 3 3 4 3 5 3 ... ## $ Occupation : Factor w/ 15 levels &quot; ?&quot;,&quot; Adm-clerical&quot;,..: 2 5 7 7 11 5 9 5 11 5 ... ## $ Relationship : Factor w/ 6 levels &quot; Husband&quot;,&quot; Not-in-family&quot;,..: 2 1 2 1 6 6 2 1 2 1 ... ## $ Race : Factor w/ 5 levels &quot; Amer-Indian-Eskimo&quot;,..: 5 5 5 3 3 5 3 5 5 5 ... ## $ Sex : Factor w/ 2 levels &quot; Female&quot;,&quot; Male&quot;: 2 2 2 2 1 1 1 2 1 2 ... ## $ CapitalGain : int 2174 0 0 0 0 0 0 0 14084 5178 ... ## $ CapitalLoss : int 0 0 0 0 0 0 0 0 0 0 ... ## $ HoursPerWeek : int 40 13 40 40 40 40 16 45 50 40 ... ## $ NativeCountry: Factor w/ 41 levels &quot; ?&quot;,&quot; Cambodia&quot;,..: 39 39 39 39 6 39 23 39 39 39 ... ## $ IncomeLevel : Factor w/ 2 levels &quot; &lt;=50K&quot;,&quot; &gt;50K&quot;: 1 1 1 1 1 1 1 2 2 2 ... As required by the xgboost package, we need a numeric \\(Y\\) and all the factor variables have to be one-hot coded df$Y &lt;- ifelse(data$IncomeLevel==&quot; &lt;=50K&quot;, 0, 1) #Remove `IncomeLevel` df &lt;- df[, -15] anyNA(df) # no NA&#39;s ## [1] FALSE # Initial Split 90-10% split set.seed(321) ind &lt;- sample(nrow(df), nrow(df)*0.90, replace = FALSE) train &lt;- df[ind, ] test &lt;- df[-ind, ] # One-hot coding using R&#39;s `model.matrix` ty &lt;- train$Y tsy &lt;- test$Y hot_tr &lt;- model.matrix(~.+0, data = train[,-which(names(train) == &quot;Y&quot;)]) hot_ts &lt;- model.matrix(~.+0, data = test[,-which(names(train) == &quot;Y&quot;)]) # Preparing efficient matrix ttrain &lt;- xgb.DMatrix(data = hot_tr, label = ty) ttest &lt;- xgb.DMatrix(data = hot_ts, label = tsy) Now we are ready to set our first xgb.sv with default parameters params &lt;- list(booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot; ) set.seed(112) cvb &lt;- xgb.cv( params = params, nrounds = 100, data = ttrain, nfold = 5, showsd = T, stratified = T, print.every.n = 10, early.stop.round = 20, maximize = F ) ## [1] train-logloss:0.541285+0.000640 test-logloss:0.542411+0.001768 ## Multiple eval metrics are present. Will use test_logloss for early stopping. ## Will train until test_logloss hasn&#39;t improved in 20 rounds. ## ## [11] train-logloss:0.290701+0.000486 test-logloss:0.302696+0.003658 ## [21] train-logloss:0.264326+0.000814 test-logloss:0.285655+0.004132 ## [31] train-logloss:0.251203+0.001082 test-logloss:0.280880+0.004269 ## [41] train-logloss:0.243382+0.001291 test-logloss:0.279297+0.004772 ## [51] train-logloss:0.237065+0.001390 test-logloss:0.278460+0.004780 ## [61] train-logloss:0.230541+0.001288 test-logloss:0.278528+0.004913 ## [71] train-logloss:0.225721+0.001117 test-logloss:0.279118+0.005197 ## Stopping. Best iteration: ## [59] train-logloss:0.231852+0.000732 test-logloss:0.278273+0.004699 We have the best iteration at 55. cvb$best_iteration ## [1] 59 model_default &lt;- xgb.train (params = params, data = ttrain, nrounds = 79, watchlist = list(val=ttest,train=ttrain), print_every_n = 10, maximize = F , eval_metric = &quot;auc&quot;) ## [1] val-auc:0.898067 train-auc:0.895080 ## [11] val-auc:0.922919 train-auc:0.925884 ## [21] val-auc:0.927905 train-auc:0.936823 ## [31] val-auc:0.928464 train-auc:0.942277 ## [41] val-auc:0.929252 train-auc:0.946379 ## [51] val-auc:0.928459 train-auc:0.949633 ## [61] val-auc:0.928208 train-auc:0.952297 ## [71] val-auc:0.927833 train-auc:0.954337 ## [79] val-auc:0.927487 train-auc:0.956330 Which is the same, if we had used xgboost() instead of xgb.train(): phat &lt;- predict (model_default, ttest) # AUC library(ROCR) pred_rocr &lt;- prediction(phat, tsy) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc_ROCR@y.values[[1]] ## [1] 0.9274875 # ROCR perf &lt;- performance(pred_rocr,&quot;tpr&quot;,&quot;fpr&quot;) plot(perf, colorize=TRUE) abline(a = 0, b = 1) You can go back to 11.3.2 and see that XGBoost is better than kNN in this example without even a proper grid search. To get the confusion matrix, we need to find the optimal discriminating threshold, where the J-index at the ROC curve (with the maximum AUC is achived) is maximized (See Chapter 10). Both machine learning algorithms embed non-linearity. This is done, in the case of SVMs, through the usage of a kernel method. Neural networks, instead, embed non-linearity by using non-linear activation functions. Both classes of algorithms can, therefore, approximate non-linear decision functions, though with different approaches. Both SVMs and NNs can tackle the same problem of classification against the same dataset. This means that there’s no reason that derives from the characteristics of the problem for preferring one over the other. What’s more important, though, is that they both perform with comparable accuracy against the same dataset, if given comparable training. If given as much training and computational power as possible, however, NNs tend to outperform SVMs. As we’ll see in the next section, though, the time required to train the two algorithms is vastly different for the same dataset. "],["support-vector-machines.html", "Chapter 15 Support Vector Machines 15.1 Optimal Separating Classifier 15.2 Nonlinear Boundary with Kernels 15.3 Application with SVM", " Chapter 15 Support Vector Machines Up to this point we have seen “probabilistic” binary classifiers, such as kNN, CART, Ensemble models, and classification regressions (logistic , LPM), where probabilistic predictions are made on observations and then converted to binary predictions based a tuned discriminating threshold. Support-vector machines do not use probabilistic predictions to classify the outcomes, which is inspired from one of the oldest algorithms in machine learning introduced by Rosenblatt in 1958, the perceptron algorithm, for learning a linear classifier. Support Vector Machine (SVM) is a modern approach to linear separation. Here is the history and little introduction to SVM by Wikipedia: In machine learning, support-vector machines are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&amp;T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995, Vapnik et al., 1997) SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall. We will develop our discussion in this chapter on two cases: Separable by a linear class boundary (Optimal Separating Classifier) and separable by a non-linear class boundary (Support-Vector Machines). First section has no practical importance as (when) we usually face non-linear class boundary problems in real life. However, it will help us build SVM step by step. We will use a simplifying assumption here to start with: the classes are perfectly linearly separable at the data points by using a single straight line. Thus we have two predictors: \\(X_1\\) and \\(X_2\\). Let’s look at an example: y &lt;- c(1,1,0,0,1,0,1,1,0,0) x1 &lt;- c(0.09,0.11, 0.17, 0.23, 0.33,0.5, 0.54,0.65,0.83,0.78) x2 &lt;- c(0.5,0.82, 0.24, 0.09,0.56, 0.40, 0.93, 0.82, 0.3, 0.72) data &lt;- data.frame(&quot;y&quot; = y, &quot;x1&quot; = x1, &quot;x2&quot; = x2) plot(data$x1, data$x2, col = (data$y+1), lwd = 4, xlab = &quot;x1&quot;, ylab = &quot;x2&quot;) Can we come up with a boundary (line) that separates blacks from reds? plot(data$x1, data$x2, col = (data$y+1), lwd = 4, xlab = &quot;x1&quot;, ylab = &quot;x2&quot;) abline(a = 0.29, b = 0.6, col = &quot;orange&quot;,lwd = 2) We call this line a hyperplane (in 2-dimensional case it’s a line) that separates blacks from reds. Let’s mathematically define it: \\[ \\beta_{0}+X_{1} \\beta_{1}+X_{2} \\beta_{2} = 0 \\] hence the “line”: \\[ X_{2}=-\\hat{\\beta}_{0} / \\hat{\\beta}_{2}-\\hat{\\beta}_{1} / \\hat{\\beta}_{2} X_{1} . \\] And the classiciation rule after getting the “line” is simple \\[ \\beta_{0}+X_{1} \\beta_{1}+X_{2} \\beta_{2}&gt;0 \\text { (red) } \\text { or }&lt;0 \\text { (black) } \\] As soon as we come up with the line, the classification is simple. But, we have two questions to answer: (1) how are we going to get the line from the data? (2) How can we decide which line among many alternatives giving the same classification score on the training data is the best in terms of generalization (a better prediction accuracy on different observations). There are many possible hyperplanes with the same classification score: plot(data$x1, data$x2, col = (data$y+1), lwd = 4, xlab = &quot;x1&quot;, ylab = &quot;x2&quot;) abline(a = 0.29, b = 0.6, col = &quot;blue&quot;, lwd = 2) abline(a = 0.20, b = 0.8, col = &quot;orange&quot;, lwd = 2) abline(a = 0.10, b = 1.05, col = &quot;green&quot;, lwd = 2) abline(a = 0.38, b = 0.47, col = &quot;brown&quot;, lwd = 2) The orange line in our example is \\[ -0.87-1.8X_{1}+3X_{2} = 0, \\\\ X_{2}=0.29 - 0.60 X_{1} \\] 15.1 Optimal Separating Classifier Given a decision boundary separating the dataset and satisfying: \\[ \\mathbf{w} \\cdot \\mathbf{x}+b=0, \\] where \\(\\mathbf{w}\\) is the vector of weights (coefficients) and \\(b\\) is the intercept. We use \\(\\mathbf{w} \\cdot \\mathbf{x}\\) with a dot product, instead of \\(\\mathbf{w}^{T} \\mathbf{x}\\). We can select two others hyperplanes \\(\\mathcal{H}_{1}\\) and \\(\\mathcal{H}_{0}\\)$ which also separate the data and have the following equations : \\[ \\mathbf{w} \\cdot \\mathbf{x}+b=\\delta \\\\ \\mathbf{w} \\cdot \\mathbf{x}+b=-\\delta \\] so that decision boundary is equidistant from \\(\\mathcal{H}_{1}\\) and \\(\\mathcal{H}_{0}\\). For now, we can arbitrarily set \\(\\delta=1\\) to simplify the problem. \\[ \\mathbf{w} \\cdot \\mathbf{x}+b=1 \\\\ \\mathbf{w} \\cdot \\mathbf{x}+b=-1 \\] Here is our illustration: Plot shows the following lines: \\(\\mathcal{H}_{1}\\): \\(\\mathbf{w} \\cdot \\mathbf{x}+b=1\\); \\(\\mathcal{H}_{0}\\): \\(\\mathbf{w} \\cdot \\mathbf{x}+b=-1\\); Decision Boundary: \\(\\mathbf{w} \\cdot \\mathbf{x}+b=0.\\) The data points lying on \\(\\mathcal{H}_{1}\\) (2 reds) or \\(\\mathcal{H}_{0}\\) (2 blacks) are called support vectors and only these points influence the decision boundary! The margin (\\(m\\)), which is a perpendicular line (arrow), is defined as the perpendicular distance from the points on the dash lines (\\(\\mathcal{H}_{1}\\) and \\(\\mathcal{H}_{0}\\)) to the boundary (gray line). Since all these margins would be equidistant, both definitions of \\(m\\) would measure the same magnitude. Our job is to find the maximum margin. The model is invariant with respect to the training set changes, except the changes of support vectors. If we make a small error in estimating the boundary, the classification will likely stay correct. Moreover, the distance of an observation from the hyperplane can be seen as a measure of our confidence that the observation was correctly classified. 15.1.1 The Margin We will use a little vector algebra and start with the vector normal Let \\(\\mathbf{u}=\\left\\langle u_{1}, u_{2}, u_{3}\\right\\rangle\\) and \\(\\mathbf{v}=\\left\\langle v_{1}, v_{2}, v_{3}\\right\\rangle\\) be two vectors with a common initial point. Then \\(\\mathbf{u}, \\mathbf{v}\\) and \\(\\mathbf{u}-\\mathbf{v}\\) form a triangle, as shown. By the Law of Cosines, \\[ |\\mathbf{u}-\\mathbf{v}|^{2}=|\\mathbf{u}|^{2}+|\\mathbf{v}|^{2}-2|\\mathbf{u}||\\mathbf{v}| \\cos \\theta \\] where \\(\\theta\\) is the angle between \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\). Note that \\(|\\mathbf{u}|\\) is representing the vector norm not the absolute value. Using the formula for the magnitude of a vector, we obtain \\[ \\left(u_{1}-v_{1}\\right)^{2}+\\left(u_{2}-v_{2}\\right)^{2}+\\left(u_{3}-v_{3}\\right)^{2}=\\left(u_{1}^{2}+u_{2}^{2}+u_{3}^{2}\\right)+\\left(v_{1}^{2}+v_{2}^{2}+v_{3}^{2}\\right)-2|\\mathbf{u}||\\mathbf{v}| \\cos \\theta \\\\ u_{1} v_{1}+u_{2} v_{2}+u_{3} v_{3}=|\\mathbf{u}||\\mathbf{v}| \\cos \\theta \\\\ \\mathbf{u} \\cdot \\mathbf{v}=|\\mathbf{u}||\\mathbf{v}| \\cos \\theta\\text {. } \\] Suppose that two nonzero vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) have an angle between them that is \\(\\theta=\\pi / 2\\). That is, \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are perpendicular, or orthogonal. Then, we have \\[ \\mathbf{u} \\cdot \\mathbf{v}=|\\mathbf{u}||\\mathbf{v}| \\cos \\frac{\\pi}{2}=0 \\] In other words, if \\(\\mathbf{u} \\cdot \\mathbf{v}=0\\), then we must have \\(\\cos \\theta=0\\), where \\(\\theta\\) is the angle between them, which implies that \\(\\theta=\\pi / 2\\), and therefore \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are orthogonal. In summary, \\(\\mathbf{u} \\cdot \\mathbf{v}=0\\) if and only if \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are orthogonal. Using this fact, we can see that the vector \\(\\mathbf{w}\\) is perpendicular (a.k.a “normal”) to \\(\\mathcal{H}_{1}\\), \\(\\mathbf{w} \\cdot \\mathbf{x}+b=0.\\) Consider the points \\(x_{a}\\) and \\(x_{b}\\), which lie on \\(\\mathcal{H}_{1}\\). This gives us two equations: \\[ \\begin{aligned} &amp;\\mathbf{w} \\cdot \\mathbf{x}_a+b=1 \\\\ &amp;\\mathbf{w} \\cdot \\mathbf{x}_b+b=1 \\end{aligned} \\] Subtracting these two equations results in \\(\\mathbf{w} .\\left(\\mathbf{x}_{a}-\\mathbf{x}_{b}\\right)=0\\). Note that the vector \\(\\mathbf{x}_{a}-\\mathbf{x}_{b}\\) lies on \\(\\mathcal{H}_{1}\\). Since the dot product \\(\\mathbf{w} .\\left(\\mathbf{x}_{a}-\\mathbf{x}_{b}\\right)\\) is zero, \\(\\mathbf{w}\\) must be orthogonal to \\(\\mathbf{x}_{a}-\\mathbf{x}_{b}\\), and in turn, to \\(\\mathcal{H}_{1}\\). Note that this can be repeated for the decision boundary or \\(\\mathcal{H}_{0}\\) too. Let’s define a unit vector of \\(\\mathbf{w}\\) \\[ \\mathbf{u}=\\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|}, \\] where \\(\\|\\mathbf{w}\\| = \\sqrt{w_{1}^{2}+w_{2}^{2}} \\dots=\\sqrt{w_{1} w_{1}+w_{2} w_{2} \\dots} = \\mathbf{w}.\\mathbf{w}\\), which is called the magnitude (or length) of the vector7. As it is a unit vector \\(\\|\\mathbf{u}\\|=1\\) and it has the same direction as \\(\\mathbf{w}\\) it is also perpendicular to the hyperplane. If we multiply \\(\\mathbf{u}\\) by \\(m\\), which is the distance from either hyperplanes to the boundary, we get the vector \\(\\mathbf{k}=m \\mathbf{u}\\). We observed that \\(\\|\\mathbf{k}\\|=m\\) and \\(\\mathbf{k}\\) is perpendicular to \\(\\mathcal{H}_{1}\\) (since it has the same direction as \\(\\mathbf{u}\\)). Hence, \\(\\mathbf{k}\\) is the vector with same magnitude and direction of \\(m\\) we were looking for. The rest will be relatively a simple algebra: \\[ \\mathbf{k}=m \\mathbf{u}=m \\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|} \\] We start from a point, \\(\\mathbf{x}_{0}\\) on \\(\\mathcal{H}_{0}\\) and add \\(k\\) to find the point \\(\\mathbf{x^\\prime}=\\mathbf{x}_{0}+\\mathbf{k}\\) is on the decision boundary, which means that \\(\\mathbf{w} \\cdot \\mathbf{x^\\prime}+b=0\\). \\[ \\begin{gathered} \\mathbf{w} \\cdot\\left(\\mathbf{x}_{0}+\\mathbf{k}\\right)+b=0, \\\\ \\mathbf{w} \\cdot\\left(\\mathbf{x}_{0}+m \\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|}\\right)+b=0, \\\\ \\mathbf{w} \\cdot \\mathbf{x}_{0}+m \\frac{\\mathbf{w} \\cdot \\mathbf{w}}{\\|\\mathbf{w}\\|}+b=0, \\\\ \\mathbf{w} \\cdot \\mathbf{x}_{0}+m \\frac{\\|\\mathbf{w}\\|^{2}}{\\|\\mathbf{w}\\|}+b=0, \\\\ \\mathbf{w} \\cdot \\mathbf{x}_{0}+m\\|\\mathbf{w}\\|+b=0, \\\\ \\mathbf{w} \\cdot \\mathbf{x}_{0}+b= -m\\|\\mathbf{w}\\|, \\\\ -1=-m\\|\\mathbf{w}\\|, \\\\ m\\|\\mathbf{w}\\|=1,\\\\ m=\\frac{1}{\\|\\mathbf{w}\\|}. \\end{gathered} \\] One can easily see that the bigger the norm is, the smaller the margin become. Thus, maximizing the margin is the same thing as minimizing the norm of \\(\\mathbf{w}\\). Among all possible hyperplanes meeting the constraints, if we choose the hyperplane with the smallest \\(\\|\\mathbf{w}\\|\\), it would be the one which will have the biggest margin8. Finally, the above derivation can be written to find the distance between the decision boundary and any point (\\(\\mathbf{x}\\)). Supposed that \\(\\mathbf{x^\\prime}\\) on the decision boundary: \\[ \\begin{gathered} \\mathbf{x^\\prime}=\\mathbf{x}-\\mathbf{k}, \\\\ \\mathbf{w} \\cdot \\mathbf{x^\\prime}+b=0, \\\\ \\mathbf{w} \\cdot\\left(\\mathbf{x}-m \\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|}\\right)+b=0, \\\\ \\mathbf{w} \\cdot \\mathbf{x}-m \\frac{\\mathbf{w} \\cdot \\mathbf{w}}{\\|\\mathbf{w}\\|}+b=0, \\\\ \\mathbf{w} \\cdot \\mathbf{x}-m \\frac{\\|\\mathbf{w}\\|^{2}}{\\|\\mathbf{w}\\|}+b=0, \\\\ \\mathbf{w} \\cdot\\mathbf{x}-m\\|\\mathbf{w}\\|+b=0, \\\\ m=\\frac{\\mathbf{w} \\cdot \\mathbf{x^\\prime}+b}{\\|\\mathbf{w}\\|}, \\\\ m=\\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|} \\cdot \\mathbf{x}+\\frac{b}{\\|\\mathbf{w}\\|}, \\end{gathered} \\] which shows the distance between boundary and \\(\\mathcal{H}_{1}\\) is 1 as the result (\\(\\mathbf{w} \\cdot \\mathbf{x}+b=1\\)) reveals9. Given the following hyperplanes, \\[ \\mathbf{w} \\cdot \\mathbf{x}+b=1, \\\\ \\mathbf{w} \\cdot \\mathbf{x}+b=-1, \\] we can write our decision rules as \\[ \\mathbf{w} \\cdot \\mathbf{x}_i+b \\geq 1 ~~\\Longrightarrow ~~~ y_i = 1,\\\\ \\mathbf{w} \\cdot \\mathbf{x}_i+b \\leq-1 ~~\\Longrightarrow ~~~ y_i = -1. \\] And, when we combine them, we can get a unique constraint: \\[ y_{i}\\left(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b\\right) \\geq 1 ~~~~~\\text { for all} ~~~ i \\] Usually, it is confusing to have a fixed threshold, “1”, in the constraint. To see the origin of this, we define our optimization problem as \\[ \\operatorname{argmax}\\left(\\mathbf{w}^{*}, b^{*}\\right)~~ m ~~~~~\\text {such that } ~~~~~ y_{i}(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b) \\geq m. \\] Since the hyperplane can be scaled any way we want: \\[ \\mathbf{w} \\cdot \\mathbf{x}_{i}+b = 0 ~~~ \\Rightarrow~~~ s\\left(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b\\right) = 0\\\\\\text{where}~~~ s \\neq 0. \\] Hence, we can write \\[ \\frac{1}{\\|\\mathbf{w}\\|}(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b) =0. \\] Therefore, \\[ \\begin{gathered} \\frac{1}{\\|\\mathbf{w}\\|}\\mathbf{y}_{i}(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b) \\geq m,\\\\ y_{i}(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b) \\geq m\\|\\mathbf{w}\\|\\\\ y_{i}\\left(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b\\right) \\geq 1 \\end{gathered} \\] Finally, we can write our optimization problem as \\[ \\operatorname{argmin}\\left(\\mathbf{w}^{*}, b^{*}\\right) ~~ \\|\\mathbf{w}\\| ~~~~\\text {such that } ~~~~ \\mathbf{y}_{i}(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b) \\geq 1, \\] which can be re-written as you will see it in the literature: \\[ \\begin{array}{ll} \\underset{\\mathbf{w}, b}{\\operatorname{minimize}} &amp; \\frac{1}{2}\\|\\mathbf{w}\\|^{2} \\\\ \\text { subject to } &amp; y_{i}\\left(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b\\right) \\geq 1, \\quad i=1, \\ldots, n \\end{array} \\] where squaring the norm has the advantage of removing the square root and \\(1/2\\) helps solving the quadratic problem. All gives the same solution: \\[ \\hat{f}(x) = \\hat{\\mathbf{w}} \\cdot \\mathbf{x}_{i}+\\hat{b}, \\] which can be used for classifying new observation by \\(\\text{sign}\\hat{f}(x)\\). A Lagrangian function can be used to solve this optimization problem. We will not show the details of the solution process, but we will continue on with the non-separable case10. Let’s use svm() command from the e1071 package for an example: library(e1071) # Sample data - Perfectly separated set.seed(1) x &lt;- matrix(rnorm(20*2), ncol = 2) y &lt;- c(rep(-1,10), rep(1,10)) x[y==1,] &lt;- x[y==1,] + 2 dat &lt;- data.frame(x=x, y=as.factor(y)) # Support Vector Machine model mfit &lt;- svm(y~., data = dat, kernel = &quot;linear&quot;, scale = FALSE) summary(mfit) ## ## Call: ## svm(formula = y ~ ., data = dat, kernel = &quot;linear&quot;, scale = FALSE) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: linear ## cost: 1 ## ## Number of Support Vectors: 4 ## ## ( 2 2 ) ## ## ## Number of Classes: 2 ## ## Levels: ## -1 1 plot(mfit, dat, grid = 200, col = c(&quot;lightgray&quot;, &quot;lightpink&quot;)) Points indicated by an “x” are the support vectors. They directly affect the classification line. The points shown with an “o” don’t affect the calculation of the line. This principle distinguishes support vector method from other classification methods that use the entire data to fit the classification boundary. 15.1.2 The Non-Separable Case What if we have cases like, In the first plot, although the orange boundary would perfectly separates the classes, it would be less “generalizable” (i.e., more specific to the train data means more prediction errors) than the blue boundary. In the second plot, there doesn’t exist a linear boundary without an error. If we can tolerate a mistake, however, the blue line can be used as a separating boundary. In both cases the blue lines could be the solution with some kind of “tolerance” level. It turn out that, if we are able to introduce this “error tolerance” to our optimization problem described in the perfectly separable case, we can make the “Optimal Separating Classifier” as a trainable model by tuning the “error tolerance”, which can be our hyperparameter. This is exactly what we will do: \\[ \\operatorname{argmin}\\left(\\mathbf{w}^{*}, b^{*}\\right) \\|\\mathbf{w}\\| ~~~~~~~\\text {such that } ~~~~~~~ y_{i}(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b) \\geq 1 \\] \\[ \\operatorname{argmin}\\left(\\mathbf{w}^{*}, b^{*}\\right) \\|\\mathbf{w}\\| \\quad \\text { such that }\\left\\{\\begin{array}{l} y_{i}(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b) \\geq 1-\\epsilon_{i} ~~~~\\forall i \\\\ \\epsilon_{i} \\geq 0,~~~~ \\sum \\epsilon_{i} \\leq C \\end{array}\\right. \\] where \\(\\epsilon\\) is the “tolarence” for an error and \\(C\\) is a nonnegative hyperparameter. The first constraint can be written as \\(y_{i}(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b) + \\epsilon_{i} \\geq 1\\). Remember that, by the nature of this constraint, the points well inside their class boundary will not play a roll in shaping the tolerance level. This could be written another way: \\[ \\min _{\\mathbf{w} \\in \\mathbb{R}^{d}, b \\in \\mathbb{R}, \\epsilon \\in \\mathbb{R}^{n}}\\left\\{\\frac{1}{2}\\|\\mathbf{w}\\|^{2}+C \\sum_{i=1}^{n} \\epsilon_{i}\\right\\}\\\\ \\text {subject to} \\\\ y_{i} \\cdot\\left(\\mathbf{w}^{T} \\mathbf{x}_{i}+b\\right) \\geq 1-\\epsilon_{i} ~~ \\text{and} ~~ \\epsilon_{i} \\geq 0, ~~\\forall i=1, \\cdots, n. \\] And as a maximization problem, \\[ \\operatorname{argmax}\\left(\\mathbf{w}^{*}, b^{*}\\right) ~m \\quad \\text { such that }\\left\\{\\begin{array}{l} y_{i}(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b) \\geq m(1-\\epsilon_{i}) ~~~~\\forall i \\\\ \\epsilon_{i} \\geq 0,~~~~ \\sum \\epsilon_{i} \\leq C \\end{array}\\right. \\] This approach is also called soft margin classification or support vector classifier in practice. Although this setting will relax the requirement of a perfect separation, it still requires a linear separation. set.seed(1) x &lt;- matrix(rnorm(20*2), ncol = 2) y &lt;- c(rep(-1,10), rep(1,10)) x[y==1,] &lt;- x[y==1,] + 1 dt &lt;- data.frame(x=x, y=as.factor(y)) # C = 10 mfit10 &lt;- svm(y~., data = dt, kernel = &quot;linear&quot;, scale = FALSE, cost = 10) plot(mfit10, dat, grid = 200, col = c(&quot;lightgray&quot;, &quot;lightpink&quot;), main = &quot;C = 10&quot;) # Tuning C tuned &lt;- tune(svm, y~., data = dat, kernel = &quot;linear&quot;, ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100))) (best &lt;- tuned$best.model) ## ## Call: ## best.tune(METHOD = svm, train.x = y ~ ., data = dat, ranges = list(cost = c(0.001, ## 0.01, 0.1, 1, 5, 10, 100)), kernel = &quot;linear&quot;) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: linear ## cost: 0.1 ## ## Number of Support Vectors: 13 # Using tuned model on the whole data yhat &lt;- predict(best, dat) (misclass &lt;- table(predict = yhat, truth = dt$y)) ## truth ## predict -1 1 ## -1 10 1 ## 1 0 9 We will now look at how we can introduce non-linearity to the class boundaries. 15.2 Nonlinear Boundary with Kernels Many data sets are not linearly separable. Although, adding polynomial features and interactions can be used, a low polynomial degree cannot deal with very complex data sets. The support vector machine (SVM) is an extension of the “support vector classifier” that results from enlarging the feature space in a specific way, using kernels. SVM works well for complex but small- or medium-sized data sets. To demonstrate a nonlinear classification boundary, we will construct a new data set: set.seed (1) x &lt;- matrix(rnorm(200*2), ncol = 2) x[1:100, ] &lt;- x[1:100, ] + 2 x[101:150, ] &lt;- x[101:150, ] - 2 y &lt;- c(rep(1,150), rep(2,50)) dt &lt;- data.frame(x=x,y=as.factor(y)) plot(x[ ,1], x[ ,2], pch=16, col = y*2) Notice that the data is not linearly separable and isn’t all clustered together in a single group either. We can of course make our decision boundary nonlinear by adding the polynomials and interaction terms. Adding more terms, however, may expand the feature space to the point that leads to inefficient computations. We haven’t shown the explicit solution to the optimization problem we stated for a separable case \\[ \\begin{array}{ll} \\underset{\\mathbf{w}, b}{\\operatorname{minimize}} &amp; \\frac{1}{2}\\|\\mathbf{w}\\|^{2} \\\\ \\text { subject to } &amp; y_{i}\\left(\\mathbf{w} \\cdot \\mathbf{x}_{i}+b\\right) \\geq 1, \\quad i=1, \\ldots, n \\end{array} \\] Which can be set in Lagrangian: \\[ \\min L=1 / 2\\|\\mathbf{w}\\|^{2}-\\sum \\alpha_i \\left[y_i \\left(\\mathbf{w} \\cdot \\mathbf{x}_i + b\\right)-1\\right],\\\\ \\min L=1 / 2\\|\\mathbf{w}\\|^{2}-\\sum \\alpha_i y_i \\left(\\mathbf{w} \\cdot \\mathbf{x}_i\\right) + b\\sum \\alpha_i y_i+\\sum \\alpha_i, \\] with respect to \\(\\mathbf{w},b\\). These are also called as “primal form”. Hence the first order conditions are \\[ \\begin{aligned} &amp;\\frac{\\partial L}{\\partial \\mathbf{w}}=\\mathbf{w}-\\sum_{i=1}^{n} \\alpha_{i} y_{i} \\mathbf{x}_{i}=0 \\\\ &amp;\\frac{\\partial L}{\\partial b}=\\sum_{i=1}^{n} \\alpha_{i} y_{i}=0 \\end{aligned} \\] We solve the optimization problem by now solving for the dual of this original problem (substituting for \\(\\mathbf{w} = \\sum_{i=1}^{n} \\alpha_{i} y_{i} \\mathbf{x}_{i}\\) and \\(\\sum_{i=1}^{n} \\alpha_{i} y_{i}=0\\) back into the original equation). Hence the “dual problem: \\[ \\max L\\left(\\alpha_{i}\\right)=\\sum_{i=1}^{n}\\alpha_{i}-\\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n}\\alpha_{i} \\alpha_{j} y_{i} y_{j}\\left(\\mathbf{x}_{i} \\cdot \\mathbf{x}_{j}\\right) \\] The solution to this involves computing the just the inner products of \\(x_{i}, x_{j}\\), which the key point SVM problems. \\[ \\alpha_{i}\\left[y_{i}\\left(\\mathbf{w} \\cdot \\mathbf{x}_i + b\\right)-1\\right]=0 ~~~~~\\forall i \\] From these we can see that, if \\(\\left(\\mathbf{w} \\cdot \\mathbf{x}_i + b\\right)&gt;1\\) (since \\(x_{i}\\) is not on the boundary of the slab), \\(\\alpha_{i}\\) will be \\(0\\). Therefore, the most of the \\(\\alpha_{i}\\) ’s will be zero as we have a few support vectors (on the gutters or margin). This reduces the dimensionality of the solution! Notice that inner products provide some measure of “similarity”. The inner product between 2 vectors of unit length returns the cosine of the angle between them, which reveals how “far apart” they are. We have seen that if they are perpendicular (completely unlike) their inner product is 0; or, if they are parallel their inner product is 1 (completely similar). Now consider the function for only non zero \\(\\alpha\\) ’s. \\[ \\max L\\left(\\alpha_{i}\\right)=\\sum_{i=1}^{n}\\alpha_{i}-\\frac{1}{2} \\sum_{i=1}^{n}\\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j}\\left(\\mathbf{x}_{i} \\cdot \\mathbf{x}_{j}\\right) \\] If two features \\(\\mathbf{x}_{i}, \\mathbf{x}_{j}\\) are completely dissimilar (their dot product will be 0), they don’t contribute to \\(L\\). Or, if they are completely alike, their dot product will be 1. In this case, suppose that both \\(\\mathbf{x}_{i}\\) and \\(\\mathbf{x}_{j}\\) predict the same output value \\(y_{i}\\) (either \\(+1\\) or \\(-1\\) ). Then \\(y_{i} y_{j}\\) is always 1, and the value of \\(\\alpha_{i} \\alpha_{j} y_{i} y_{j} \\mathbf{x}_{i} \\mathbf{x}_{j}\\) will be positive. But this would decrease the value of \\(L\\) (since it would subtract from the first term sum). So, the algorithm downgrades similar feature vectors that make the same prediction. On the other hand, when \\(x_{i}\\), and \\(x_{j}\\) make opposite predictions (i.e., predicting different classes, one is \\(+1\\), the other \\(-1\\)) about the output value \\(y_{i}\\), but are otherwise very closely similar (i.e., their dot product is \\(1\\)), then the product \\(a_{i} a_{j} y_{i} y_{j} x_{i} x\\) will be negative. Since we are subtracting it, it adds to the sum maximizing \\(L\\). This is precisely the examples that algorithm is looking for: the critical ones that tell the two classes apart. What if the decision function is not linear as we have in the figure above? What transform would separate these? The idea in SVM is to obtain a nonlinear separation by mapping the data to a higher dimensional space. Remember the function we want to optimize: \\(L=\\sum \\alpha_{i}-1 / 2 \\sum \\alpha_{i} \\alpha_{j} y_{i} y_{j}\\left(\\mathbf{x}_{i} \\cdot \\mathbf{x}_{j}\\right)\\) where \\(\\left(\\mathbf{x}_{i} \\cdot \\mathbf{x}_{j}\\right)\\) is the dot product of the two feature vectors. We can transform them, for example, by \\(\\phi\\) that is a quadratic polynomial. As we discussed earlier, however, we don’t know the function explicitly. And worse, as the we increase the degree of polynomial, the optimization becomes computational impossible. If there is a “kernel function” \\(K\\) such that \\(K\\left(\\mathbf{x}_{i} \\cdot \\mathbf{x}_{j}\\right)=\\phi\\left(\\mathbf{x}_{i}\\right) \\cdot \\phi\\left(\\mathbf{x}_{j}\\right)\\), then we do not need to know or compute \\(\\phi\\) at all. That is, the kernel function defines inner products in the transformed space. Or, it defines similarity in the transformed space. The function we want to optimize becomes: \\[ \\max L\\left(\\alpha_{i}\\right)=\\sum_{i=1}^{n}\\alpha_{i}-\\frac{1}{2} \\sum_{i=1}^{n}\\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} K\\left(\\mathbf{x}_{i} \\cdot \\mathbf{x}_{j}\\right) \\] The polynomial kernel \\(K\\left(\\mathbf{x}_{i} \\cdot \\mathbf{x}_{j}\\right)=\\left(\\mathbf{x}_i \\cdot \\mathbf{x}_{j}+1\\right)^{p}\\), where \\(p\\) is a hyperparmater Examples for Non Linear SVMs \\[ \\begin{gathered} K(\\mathbf{x}, \\mathbf{y})=(\\mathbf{x} \\cdot \\mathbf{y}+1)^{p} \\\\ K(\\mathbf{x}, \\mathbf{y})=\\exp \\left\\{-\\|\\mathbf{x}-\\mathbf{y}\\|^{2} / 2 \\sigma^{2}\\right\\} \\\\ K(\\mathbf{x}, \\mathbf{y})=\\tanh (\\kappa \\mathbf{x} \\cdot \\mathbf{y}-\\delta) \\end{gathered} \\] \\(1^{\\text {st }}\\) is polynomial (includes \\(\\mathrm{x} \\cdot \\mathrm{x}\\) as special case) \\(2^{\\text {nd }}\\) is radial basis function (Gaussian) \\(3^{\\text {rd }}\\) is sigmoid (Neural Net activation function) Here is the SVM application to our data: library (e1071) svmfit &lt;- svm(y~., data=dt, kernel = &quot;polynomial&quot;, cost = 1, degree = 2) plot(svmfit, dt, grid=200, col= c(&quot;pink&quot;, &quot;lightblue&quot;)) Or select the cost parameter by 10-fold CV among several values with radial kernel: tune.out &lt;- tune(svm, y~., data=dt, kernel=&quot;radial&quot;, ranges = list(cost = c(0.1, 1, 10, 100), gamma = c(0.5, 1, 2, 3, 4))) plot(tune.out$best.model,dt, grid=200, col= c(&quot;pink&quot;, &quot;lightblue&quot;)) With more than two features, we can’t plot decision boundary. We can, however, produce a ROC curve to analyze the results. As we know, SVM doesn’t give probabilities to belong to classes. We compute scores of the form \\(\\hat{f}(X)=\\varphi\\left(X_{i}\\right) \\hat{\\beta}\\) for each observation. Then use the scores as predicted values. Here is the application: library(ROCR) # Let&#39;s fit a SVM with radial kernel and plot a ROC curve: set.seed(1) train &lt;- sample(200, 100) train &lt;- sort(train, decreasing=TRUE) model &lt;- svm(y~., data = dt[train,], kernel = &quot;radial&quot;, cost = 1, gamma=0.5) fit &lt;- attributes(predict(model, dt[-train, ], decision.values=TRUE))$decision.values # AUC pred_rocr &lt;- prediction(fit, dt[-train,&quot;y&quot;]) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc_ROCR@y.values[[1]] ## [1] 0.9614225 # ROCR perf &lt;- performance(pred_rocr,&quot;tpr&quot;,&quot;fpr&quot;, main = &quot;SVM&quot;) plot(perf, colorize=TRUE) abline(a = 0, b = 1) # Let&#39;s also fit a Logistic model: logit &lt;- glm(y ~., data = dt[train, ], family = binomial(link = &#39;logit&#39;)) fit2 &lt;- predict(logit, dt[-train, ], type = &quot;response&quot;) pred_rocr &lt;- prediction(fit2, dt[-train,&quot;y&quot;]) perf &lt;- performance(pred_rocr,&quot;tpr&quot;,&quot;fpr&quot;, main = &quot;SVM&quot;) pred_rocr &lt;- prediction(fit2, dt[-train,&quot;y&quot;]) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc_ROCR@y.values[[1]] ## [1] 0.6274864 par(new = TRUE) plot(perf, colorize=TRUE) abline(a = 0, b = 1) 15.3 Application with SVM train &lt;- read.csv(&quot;adult_train.csv&quot;, header = FALSE) varNames &lt;- c(&quot;Age&quot;, &quot;WorkClass&quot;, &quot;fnlwgt&quot;, &quot;Education&quot;, &quot;EducationNum&quot;, &quot;MaritalStatus&quot;, &quot;Occupation&quot;, &quot;Relationship&quot;, &quot;Race&quot;, &quot;Sex&quot;, &quot;CapitalGain&quot;, &quot;CapitalLoss&quot;, &quot;HoursPerWeek&quot;, &quot;NativeCountry&quot;, &quot;IncomeLevel&quot;) names(train) &lt;- varNames data &lt;- train tbl &lt;- table(data$IncomeLevel) tbl ## ## &lt;=50K &gt;50K ## 24720 7841 # we remove some outliers - See Ch.11 ind &lt;- which(data$NativeCountry==&quot; Holand-Netherlands&quot;) data &lt;- data[-ind, ] #Converting chr to factor with `apply()` family df &lt;- data df[sapply(df, is.character)] &lt;- lapply(df[sapply(df, is.character)], as.factor) str(df) ## &#39;data.frame&#39;: 32560 obs. of 15 variables: ## $ Age : int 39 50 38 53 28 37 49 52 31 42 ... ## $ WorkClass : Factor w/ 9 levels &quot; ?&quot;,&quot; Federal-gov&quot;,..: 8 7 5 5 5 5 5 7 5 5 ... ## $ fnlwgt : int 77516 83311 215646 234721 338409 284582 160187 209642 45781 159449 ... ## $ Education : Factor w/ 16 levels &quot; 10th&quot;,&quot; 11th&quot;,..: 10 10 12 2 10 13 7 12 13 10 ... ## $ EducationNum : int 13 13 9 7 13 14 5 9 14 13 ... ## $ MaritalStatus: Factor w/ 7 levels &quot; Divorced&quot;,&quot; Married-AF-spouse&quot;,..: 5 3 1 3 3 3 4 3 5 3 ... ## $ Occupation : Factor w/ 15 levels &quot; ?&quot;,&quot; Adm-clerical&quot;,..: 2 5 7 7 11 5 9 5 11 5 ... ## $ Relationship : Factor w/ 6 levels &quot; Husband&quot;,&quot; Not-in-family&quot;,..: 2 1 2 1 6 6 2 1 2 1 ... ## $ Race : Factor w/ 5 levels &quot; Amer-Indian-Eskimo&quot;,..: 5 5 5 3 3 5 3 5 5 5 ... ## $ Sex : Factor w/ 2 levels &quot; Female&quot;,&quot; Male&quot;: 2 2 2 2 1 1 1 2 1 2 ... ## $ CapitalGain : int 2174 0 0 0 0 0 0 0 14084 5178 ... ## $ CapitalLoss : int 0 0 0 0 0 0 0 0 0 0 ... ## $ HoursPerWeek : int 40 13 40 40 40 40 16 45 50 40 ... ## $ NativeCountry: Factor w/ 41 levels &quot; ?&quot;,&quot; Cambodia&quot;,..: 39 39 39 39 6 39 23 39 39 39 ... ## $ IncomeLevel : Factor w/ 2 levels &quot; &lt;=50K&quot;,&quot; &gt;50K&quot;: 1 1 1 1 1 1 1 2 2 2 ... When we use the whole data it takes very long time and memory. A much better way to deal with this issue is to not use all of the data. This is because, most data pints will be redundant from the SVM’s perspective. Remember, SVM only benefits from having more data near the decision boundaries. Therefore, we can randomly select, say, 10% of the training data (it should be done multiple times to see its consistency), and understand what its performance looks like: # Initial Split 90-10% split set.seed(123) ind &lt;- sample(nrow(df), nrow(df)*0.90, replace = FALSE) train &lt;- df[ind, ] test &lt;- df[-ind, ] # Using 10% of the train set.seed(321) ind &lt;- sample(nrow(train), nrow(train)*0.10, replace = FALSE) dft &lt;- train[ind, ] # You should check different kernels with a finer grid tuning &lt;- tune(svm, IncomeLevel~., data=dft, kernel=&quot;radial&quot;, ranges = list(cost = c(0.1, 1, 10, 100), gamma = c(0.05, 0.5, 1, 2, 3, 4))) tuning$best.model ## ## Call: ## best.tune(METHOD = svm, train.x = IncomeLevel ~ ., data = dft, ranges = list(cost = c(0.1, ## 1, 10, 100), gamma = c(0.05, 0.5, 1, 2, 3, 4)), kernel = &quot;radial&quot;) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: radial ## cost: 1 ## ## Number of Support Vectors: 1131 Now, let’s have our the tuned model tuned &lt;- svm(IncomeLevel~., data= dft, kernel=&quot;radial&quot;, cost =1) caret::confusionMatrix(reference = test$IncomeLevel, predict(tuned, newdata = test, type = &quot;class&quot;)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction &lt;=50K &gt;50K ## &lt;=50K 2328 392 ## &gt;50K 115 421 ## ## Accuracy : 0.8443 ## 95% CI : (0.8314, 0.8566) ## No Information Rate : 0.7503 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.5311 ## ## Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 ## ## Sensitivity : 0.9529 ## Specificity : 0.5178 ## Pos Pred Value : 0.8559 ## Neg Pred Value : 0.7854 ## Prevalence : 0.7503 ## Detection Rate : 0.7150 ## Detection Prevalence : 0.8354 ## Balanced Accuracy : 0.7354 ## ## &#39;Positive&#39; Class : &lt;=50K ## Another (simpler) way to get AUC and ROC: # Getting phats tuned2 &lt;- svm(IncomeLevel~., data= dft, kernel=&quot;radial&quot;, cost =1, probability = TRUE) svm.prob &lt;- predict(tuned2, type=&quot;prob&quot;, newdata=test, probability = TRUE) phat &lt;- attr(svm.prob, &quot;probabilities&quot;)[,2] # AUC pred.rocr &lt;- prediction(phat, test$IncomeLevel) auc_ROCR &lt;- performance(pred.rocr, measure = &quot;auc&quot;) auc_ROCR@y.values[[1]] ## [1] 0.9022007 # ROC perf &lt;- performance(pred.rocr, &quot;tpr&quot;,&quot;fpr&quot;) plot(perf, colorize = TRUE) abline(a = 0, b = 1) Unfortunately, there is no direct way to get information on predictors with SVM, in contrast to, for example, random forest or GBM. The package rminer provides some sort of information in variable importance, but the details are beyond the scope of this chapter. It’s application is given below. # library(rminer) # M &lt;- fit(IncomeLevel~., data=dft, model=&quot;svm&quot;, kpar=list(sigma=0), C=1) # (svm.imp &lt;- Importance(M, data=train)) Note that we switch the norm notation from a single bar to double bars↩︎ Note that this result could have been different had we chosen \\(\\delta\\) different than 1.↩︎ The distance calculation can be generalized for any points such as \\(x_{1,0}\\) and \\(x_{2,0}\\): \\(d=\\frac{\\left|a x_{1,0}+b x_{2,0}+c\\right|}{\\sqrt{a^{2}+b^{2}}}.\\) See the multiple proofs at Wikipedia: https://en.wikipedia.org/wiki/Distance_from_a_point_to_a_line↩︎ see Elements of Statistical Learning, Page 133 (Hastie et al., 2009).↩︎ "],["artificial-neural-networks.html", "Chapter 16 Artificial Neural Networks 16.1 Neural Network - the idea 16.2 Backpropagation 16.3 Neural Network - More inputs 16.4 Deep Learning", " Chapter 16 Artificial Neural Networks Artificial neural networks (ANNs) are a type of machine learning model that are inspired by the structure and function of the human brain. They consist of interconnected units called artificial neurons or nodes, which are organized into layers. The concept of artificial neural networks dates back to the 1940s, when Warren McCulloch and Walter Pitts (1944) proposed a model of the neuron as a simple threshold logic unit. In the 1950s and 1960s, researchers began developing more complex models of the neuron and exploring the use of neural networks for tasks such as pattern recognition and machine translation. However, these early efforts were largely unsuccessful due to the limited computational power of the time. It wasn’t until the 1980s and 1990s that significant progress was made in the development of artificial neural networks, thanks to advances in computer technology and the availability of larger and more diverse datasets. In 1986, Geoffrey Hinton and his team developed the backpropagation algorithm, which revolutionized the field by allowing neural networks to be trained more efficiently and accurately. Since then, artificial neural networks have been applied to a wide range of tasks, including image and speech recognition, natural language processing, and even playing games like chess and Go. They have also been used in a variety of fields, including finance, healthcare, and transportation. Today, artificial neural networks are an important tool in the field of machine learning, and continue to be an active area of research and development. There have been many influential works published in the field of artificial neural networks (ANNs) over the years. Here are a few examples of some of the most important and influential papers in the history of ANNs: Perceptrons by Frank Rosenblatt (1957): This paper introduced the concept of the perceptron, which is a type of ANN that can be trained to recognize patterns in data. The perceptron became a foundational concept in the field of machine learning and was a key catalyst for the development of more advanced ANNs. Backpropagation by Rumelhart, Hinton, and Williams (1986): This paper introduced the backpropagation algorithm, which is a method for training ANNs that allows them to learn and adapt over time. The backpropagation algorithm is still widely used today and has been a key factor in the success of ANNs in many applications. LeNet-5 by Yann LeCun et al. (1998): This paper described the development of LeNet-5, an ANN designed for recognizing handwritten digits. LeNet-5 was one of the first successful applications of ANNs in the field of image recognition and set the stage for many subsequent advances in this area. Deep Learning by Yann LeCun, Yoshua Bengio, and Geoffrey Hinton (2015): This paper provided a comprehensive review of the field of deep learning, which is a type of ANN that uses many layers of interconnected neurons to process data. It has had a major impact on the development of deep learning and has helped to drive many of the recent advances in the field. Deep Learning by Ian Goodfellow et al. (2016): This book provided a comprehensive overview of the state of the art in deep learning, which is a type of ANN with multiple layers of interconnected nodes. The book helped to popularize deep learning and contributed to its widespread adoption in a variety of fields. These are just a few examples of the many influential papers that have been published in the field of ANNs. There have been many other important contributions to the field as well, and the field continues to evolve and grow as new technologies and techniques are developed. Both Support Vector Machines and Neural Networks employ some kind of data transformation that moves them into a higher dimensional space. What the kernel function does for the SVM, the hidden layers do for neural networks. Are Neural Networks is better than Support Vector Machines? The post by Igor F. gives a nice starting example. 16.1 Neural Network - the idea Let’s start with a predictive model with a single input (covariate). The simplest model could be a linear model: \\[ y \\approx \\alpha+\\beta x \\] Since this model could be a quite restrictive, we can have a more flexible one by a polynomial regression: \\[ y \\approx \\alpha+\\beta_1 x+\\beta_2 x^2+\\beta_3 x^3+\\ldots = \\alpha+\\sum_{m=1}^M \\beta_m x^m \\] We can see the first simple ANN as nonlinear functions of linear combinations: \\[ y \\approx \\alpha+\\beta_1 f\\left(\\alpha_1+\\delta_1 x\\right)+\\beta_2 f\\left(\\alpha_2+\\delta_2 x\\right)+\\beta_3 f\\left(\\alpha_3+\\delta_3 x\\right)+\\ldots\\\\ = \\alpha+\\sum_{m=1}^M \\beta_m f\\left(\\alpha_m+\\delta_m x\\right) \\] where \\(f(.)\\) is an activation function – a fixed nonlinear function. Common examples of activation functions are The logistic (or sigmoid) function: \\(f(x)=\\frac{1}{1+e^{-x}}\\); The hyperbolic tangent function: \\(f(x)=\\tanh (x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}\\); The Rectified Linear Unit (ReLU): \\(f(x)=\\max (0, x)\\); The full list of activation functions can be found at Wikipedia. Let us consider a realistic (simulated) sample: n &lt;- 200 set.seed(1) x &lt;- sort(runif(n)) y &lt;- sin(12*(x + 0.2))/(x + 0.2) + rnorm(n)/2 df &lt;- data.frame(y, x) plot(x, y, main=&quot;Simulated data&quot;, col= &quot;grey&quot;) We can fit a polynomial regression with \\(M = 3\\): ols &lt;- lm(y ~ x + I(x^2) + I(x^3)) plot(x, y, main=&quot;Polynomial: M = 3&quot;, col= &quot;grey&quot;) lines(x, predict(ols), col=&quot;blue&quot;, lwd = 3) Now, we can think of the line as weighted sum of fixed components: \\(\\alpha_1+\\beta_1 x+\\beta_2 x^2+\\beta_3 x^3\\). # Parts first &lt;- ols$coefficients[2]*x second &lt;- ols$coefficients[3]*x^2 third &lt;- ols$coefficients[4]*x^3 yhat &lt;- ols$coefficients[1] + first + second + third # Plots par(mfrow=c(1,4), oma = c(0,0,2,0)) plot(x, first, ylab = &quot;y&quot;, col = &quot;pink&quot;, main = &quot;x&quot;) plot(x, second, ylab = &quot;y&quot;, col = &quot;orange&quot;, main = expression(x^2)) plot(x, third, ylab = &quot;y&quot;, col = &quot;green&quot;, main = expression(x^3)) plot(x, y, ylab=&quot;y&quot;, col = &quot;grey&quot;, main = expression(alpha + bx + bx^2 + bx^3)) lines(x, yhat, col = &quot;red&quot;, lwd = 3) mtext(&quot;Fixed Components&quot;, outer=TRUE, cex = 1.5, col=&quot;olivedrab&quot;) The polynomial regression is based on fixed components, or bases: \\(x, x^2, x^3, \\ldots, x^M.\\) The artificial neural net replaces these fixed components with adjustable ones or bases: \\(f\\left(\\alpha_1+\\delta_1 x\\right)\\), \\(f\\left(\\alpha_2+\\delta_2 x\\right)\\), \\(\\ldots, f\\left(\\alpha_M+\\delta_M x\\right).\\) Each component is more flexible than a fixed component. They are adjustable with tunable internal parameters. They can express several shapes, not just one (fixed) shape. Hence, adjustable components enable to capture complex models with fewer components (smaller M). Let’s replace those fixed components \\(x, x^2, x^3\\) in our polynomial regression with \\(f\\left(\\alpha_1+\\delta_1 x\\right)\\), \\(f\\left(\\alpha_2+\\delta_2 x\\right)\\), \\(f\\left(\\alpha_3+\\delta_3 x\\right).\\) library(neuralnet) set.seed(2) nn &lt;- neuralnet(y ~ x, data = df, hidden = 3, threshold = 0.05) yhat &lt;- compute(nn, data.frame(x))$net.result plot(x, y, main=&quot;Neural Networks: M=3&quot;) lines(x, yhat, col=&quot;red&quot;, lwd = 3) Why did neural networks perform better than polynomial regression in the previous example? Again, adjustable components enable to capture complex models. Let’s delve little deeper. Here is the weight structure of \\[ y \\approx \\alpha+\\sum_{m=1}^3 \\beta_m f\\left(\\alpha_m+\\delta_m x\\right)\\\\ = \\alpha+\\beta_1 f\\left(\\alpha_1+\\delta_1 x\\right)+\\beta_2 f\\left(\\alpha_2+\\delta_2 x\\right)+\\beta_3 f\\left(\\alpha_3+\\delta_3 x\\right) \\] nn$weights ## [[1]] ## [[1]][[1]] ## [,1] [,2] [,3] ## [1,] 1.26253 6.59977 2.504890 ## [2,] -18.95937 -12.24665 -5.700564 ## ## [[1]][[2]] ## [,1] ## [1,] 2.407654 ## [2,] 13.032092 ## [3,] 19.923742 ## [4,] -32.173264 plot(nn, rep = &quot;best&quot;) We used sigmoid (logistic) activation functions \\[ \\text{Node 1:} ~~~f(x)=\\frac{1}{1+e^{-x}}=\\frac{1}{1+e^{-(1.26253-18.95937x)}}\\\\ \\text{Node 2:} ~~~f(x)=\\frac{1}{1+e^{-x}}=\\frac{1}{1+e^{-(6.599773-12.24665x)}}\\\\ \\text{Node 3:} ~~~f(x)=\\frac{1}{1+e^{-x}}=\\frac{1}{1+e^{-(2.504890-5.700564x)}} \\] We can calculate the value of each activation function by using our data, \\(x\\): X &lt;- cbind(1, x) # to 1st Node n1 &lt;- nn$weights[[1]][[1]][,1] f1 &lt;- nn$act.fct(X%*%n1) # to 2nd Node n2 &lt;- nn$weights[[1]][[1]][,2] f2 &lt;- nn$act.fct(X%*%n2) # to 3rd Node n3 &lt;- nn$weights[[1]][[1]][,3] f3 &lt;- nn$act.fct(X%*%n3) par(mfrow=c(1,3), oma = c(0,0,2,0)) plot(x, f1, col = &quot;pink&quot;, main = expression(f(a1 + b1x))) plot(x, f2, col = &quot;orange&quot;, main = expression(f(a2 + b2x))) plot(x, f3, col = &quot;green&quot;, main = expression(f(a3 + b3x))) mtext(&quot;Flexible Components&quot;, outer=TRUE, cex = 1.5, col=&quot;olivedrab&quot;) Now we will go from these nodes to the “sink”: \\[ \\frac{1}{1+e^{-(1.26253-18.95937x)}} \\times 13.032092\\\\ \\frac{1}{1+e^{-(6.599773-12.24665x)}}\\times 19.923742\\\\ \\frac{1}{1+e^{-(2.504890-5.700564x)}}\\times -32.173264 \\] Fianlly we will add these with a “bias”, the intercept: \\[ 2.407654 + \\\\ \\frac{1}{1+e^{-(1.26253-18.95937x)}} \\times 13.032092+\\\\ \\frac{1}{1+e^{-(6.599773-12.24665x)}}\\times 19.923742+\\\\ \\frac{1}{1+e^{-(2.504890-5.700564x)}}\\times -32.173264 \\] Here are the results: # From Nodes to sink (Y) ## f12 &lt;- f1*nn$weights[[1]][[2]][2] f22 &lt;- f2*nn$weights[[1]][[2]][3] f23 &lt;- f3*nn$weights[[1]][[2]][4] ## Results yhat &lt;- nn$weights[[1]][[2]][1] + f12 + f22 + f23 plot(x, y, main=&quot;ANN: M = 3&quot;) lines(x, yhat, col=&quot;red&quot;, lwd = 3) 16.2 Backpropagation In 1986, Rumelhart et al. found a way to train neural networks, with the backpropagation algorithm. Today, we would call it a Gradient Descent using reverse-mode autodiff. Backpropagation is an algorithm used to train neural networks by adjusting the weights and biases of the network to minimize the cost function. Suppose we have a simple neural network as follows: The first layer is the source layer (with \\(X\\)). The second layer is called as hidden layer with three “neurons” each of which has an activation function (\\(A\\)). The last layer is the “sink” or output layer. First, let’s define a loss function, MSPE: \\[ \\text{MSPE}=\\frac{1}{n} \\sum_{i=1}^n\\left(y_i-p\\left(\\mathbf{x}_i\\right)\\right)^2 \\] And we want to solve: \\[ \\omega^{\\star}=\\operatorname{argmin}\\left\\{\\frac{1}{n} \\sum_{i=1}^n\\left(y_i-p\\left(\\mathbf{x}_i\\right)\\right)^2\\right\\} \\] To compute the gradient of the error with respect to a weight \\(w\\) or a bias \\(b\\), we use the chain rule: \\[ \\frac{\\partial \\text{MSPE}}{\\partial w} =\\frac{\\partial \\text{MSPE}}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial z}\\frac{\\partial z}{\\partial w} \\] Remember, \\(\\hat{y} = f(x)=\\frac{1}{1+e^{-z}}=\\frac{1}{1+e^{-(\\alpha+wx)}}\\), where \\(w\\) is the weight or bias in the network. By repeating this process for each weight and bias in the network, we can calculate the error gradient and use it to adjust the weights and biases in order to minimize the error of the neural network. This can be done by using gradient descent (See Appendix 1), which is an iterative method for optimizing a differentiable objective function, typically by minimizing it. However, with a multilayer ANN, we use stochastic gradient descent (SGD), which is a faster method iteratively minimizing the loss function by taking small steps in the opposite direction of the gradient of the function at the current position. The gradient is calculated using a randomly selected subset of the data, rather than the entire data set, which is why it is called “stochastic.” One of the main advantages of SGD is that it can be implemented very efficiently and can handle large data sets very well. In a regular network, the number of parameters is very large and it gets larger even in deep neural networks. Therefore, finding the most efficient (fast) backpropagation method is an active research area in the artificial intelligence engineering. One of the recent developments in this area is Tensors and Tensorflow that we will talk in Section VII. 16.3 Neural Network - More inputs With a set of covariates \\(X=\\left(1, x_1, x_2, \\ldots, x_k\\right)\\), we have \\[ y \\approx \\alpha+\\sum_{m=1}^M \\beta_m f\\left(\\alpha_m+\\textbf{X} \\delta_m\\right)=\\\\ = \\alpha+\\beta_1 f\\left(\\alpha_1+\\delta_{11} x_{1i}+\\delta_{12} x_{2i} \\dots +\\delta_{1k} x_{ki}\\right)+\\dots\\\\ +\\beta_M f\\left(\\alpha_{M1}+\\delta_{M1} x_{1i}+\\delta_{M2} x_{2i} \\dots +\\delta_{Mk} x_{ki}\\right) \\] By adding nonlinear functions of linear combinations with \\(M&gt;1\\), we have seen that we can capture nonlinearity. With multiple features, we can now capture interaction effects and, hence, obtain a more flexible model. This can be seen in blue and orange arrows in the following picture: Let’s have an application using a Mincer equation and the data (SPS 1985 - cross-section data originating from the May 1985 Current Population Survey by the US Census Bureau) from AER package. Before we start, there are few important pre-processing steps to complete. First, ANN are inefficient when the data are not scaled. The reason is backpropagation. Since ANN use gradient descent, the different scales in features will cause different step sizes. Scaling the data before feeding it to the model enables the steps in gradient descent updated at the same rate for all the features. Second, indicator predictors should be included in the input matrix by dummy-coding. Note that, the standardization should avoid those indicator variables. Finally, the formula for the model needs to be constructed to initialize the algorithm. Let’s see all of these pre-processing steps below: library(AER) data(&quot;CPS1985&quot;) df &lt;- CPS1985 # Scaling and Dummy coding df[,sapply(df, is.numeric)] &lt;- scale((df[, sapply(df, is.numeric)])) ddf &lt;- model.matrix(~.-1, data= df, contrasts.arg = lapply(df[,sapply(df, is.factor)], contrasts, contrasts = FALSE)) ddf &lt;- as.data.frame(ddf) # formula to pass on ANN and lm() w.ind &lt;- which(colnames(ddf)==&quot;wage&quot;) frmnn &lt;- as.formula(paste(&quot;wage~&quot;, paste(colnames(ddf[-w.ind]), collapse=&#39;+&#39;))) frmln &lt;- as.formula(paste(&quot;wage~&quot;, &quot;I(experience^2)&quot;, &quot;+&quot;, paste(colnames(ddf[-w.ind]), collapse = &quot;+&quot;))) # Bootstrapping loops instead of CV mse.test &lt;- matrix(0, 10, 5) for(i in 1:10){ set.seed(i+1) trainid &lt;- unique(sample(nrow(ddf), nrow(ddf), replace = TRUE)) train &lt;- ddf[trainid,] test &lt;- ddf[-trainid,] # Models fit.lm &lt;- lm(frmln, data = train) fit.nn &lt;- neuralnet(frmnn, data = train, hidden = 1, threshold = 0.05, linear.output = FALSE) fit.nn2 &lt;- neuralnet(frmnn, data = train, hidden = 2, threshold = 0.05) fit.nn3 &lt;- neuralnet(frmnn, data = train, hidden = 3, threshold = 0.05) fit.nn4 &lt;- neuralnet(frmnn, data = train, hidden = 3, threshold = 0.05, act.fct = &quot;tanh&quot;, linear.output = FALSE) # Prediction errors mse.test[i,1] &lt;- mean((test$wage - predict(fit.lm, test))^2) mse.test[i,2] &lt;- mean((test$wage - predict(fit.nn, test))^2) mse.test[i,3] &lt;- mean((test$wage - predict(fit.nn2, test))^2) mse.test[i,4] &lt;- mean((test$wage - predict(fit.nn3, test))^2) mse.test[i,5] &lt;- mean((test$wage - predict(fit.nn4, test))^2) } colMeans(mse.test) ## [1] 0.7296417 0.8919442 0.9038211 1.0403616 0.8926576 This experiment alone shows that a linear Mincer equation (with I(expreince^2)) is a much better predictor than ANN. As the complexity of ANN rises with more neurons, the likelihood that ANN overfits goes up, which is the case in our experiment. In general, linear regression may be a good choice for simple, low-dimensional datasets with a strong linear relationship between the variables, while ANNs may be better suited for more complex, high-dimensional datasets with nonlinear relationships between variables. Overfitting can be a concern when using ANNs for prediction tasks. Overfitting occurs when a model is overly complex and has too many parameters relative to the size of the training data, leading it to fit the noise in the training data rather than the underlying pattern. As a result, the model may perform well on the training data but poorly on new, unseen data. One way to mitigate overfitting in ANNs is to use techniques such as regularization, which imposes constraints on the model to prevent it from becoming too complex. Another approach is to use techniques such as early stopping, which involves interrupting the training process when the model starts to overfit the training data. 16.4 Deep Learning Simply, a Deep Neural Network (DNN), or Deep Learning, is an artificial neural network that has two or more hidden layers. Even greater flexibility is achieved via composition of activation functions: \\[ y \\approx \\alpha+\\sum_{m=1}^M \\beta_m f\\left(\\alpha_m^{(1)}+\\underbrace{\\sum_{p=1}^P f\\left(\\alpha_p^{(2)}+\\textbf{X} \\delta_p^{(2)}\\right)}_{\\text {it replaces } \\textbf{X}} \\delta_m^{(1)}\\right) \\] Before having an application, note that there are many packages that offer ANN implementations in R and even more with Python. For example, CRAN hosts more than 80 packages related to neural network modeling. Above, we just saw one example with neuralnet. The work by Mahdi et al, 2021 surveys and ranks these packages for their accuracy, reliability, and ease-of-use. mse.test &lt;- c() for(i in 1:10){ set.seed(i+1) trainid &lt;- unique(sample(nrow(ddf), nrow(ddf), replace = TRUE)) train &lt;- ddf[trainid,] test &lt;- ddf[-trainid,] # Models fit.nn22 &lt;- neuralnet(frmnn, data = train, hidden = c(3,3), threshold = 0.05) mse.test[i] &lt;- mean((test$wage - predict(fit.nn22, test))^2) } mean(mse.test) ## [1] 1.211114 Increasing overfitting! Here is the plot for our DNN: plot(fit.nn22, rep = &quot;best&quot;) A better plot could be obtained by using the NeuralNetTools package: library(NeuralNetTools) plotnet(fit.nn22) Training DNN is an important concept and we leave it to the end. As we see, deep neural networks can model complex non-linear relationships. With very complex problems, such as detecting hundreds of types of objects in high-resolution images, we need to train deeper NN, perhaps with 10 layers or more each with hundreds of neurons. Therefore, training a fully-connected DNN is a very slow process facing a severe risk of overfitting with millions of parameters. Moreover, gradients problems make lower layers very hard to train. Solutions: Convolutional Neural Networks (CNN or ConvNets) and Recurrent Neural Networks (RNN). Before moving on with these new DNN solutions, we should ask if we can obtain any information about the relationship between the prediction and predictors. The interpretability of an artificial neural network (ANN), which is known to be a “blackbox” method, can be an issue regardless of the complexity of the network. However, it is generally easier to understand the decisions made by a simple ANN than by a more complex one. A simple ANN might have only a few layers and a relatively small number of neurons, making it easier to understand how the input data is processed and how the final output is produced. However, even a simple ANN can still be a black box in the sense that the specific calculations and decisions made by the individual neurons within the network are not fully visible or understood. On the other hand, a more complex ANN with many layers and a large number of neurons can be more difficult to interpret, as the internal workings of the network are more complex and harder to understand. In these cases, it can be more challenging to understand how the ANN is making its decisions or to identify any biases or errors in its output. Overall, the interpretability of an ANN depends on the complexity of the network and the specific task it is being used for. Simple ANNs may be more interpretable, but even they can be considered black boxes to some extent. Here are a few resources that provide information about the interpretability of artificial neural networks (ANNs): Interpretable Machine Learning by Christoph Molnar is a online book that provides an overview of interpretability in machine learning, including techniques for interpreting ANNs. Interpretability of Deep Neural Networks by Chakraborty is a survey paper that discusses the interpretability of deep neural networks and presents an overview of the various techniques and approaches that have been developed to improve their interpretability. Before concluding this section we apply DNN to a classification problem using the same data that we have in Section 14.4.4: library(ISLR) df &lt;- Carseats str(df) ## &#39;data.frame&#39;: 400 obs. of 11 variables: ## $ Sales : num 9.5 11.22 10.06 7.4 4.15 ... ## $ CompPrice : num 138 111 113 117 141 124 115 136 132 132 ... ## $ Income : num 73 48 35 100 64 113 105 81 110 113 ... ## $ Advertising: num 11 16 10 4 3 13 0 15 0 0 ... ## $ Population : num 276 260 269 466 340 501 45 425 108 131 ... ## $ Price : num 120 83 80 97 128 72 108 120 124 124 ... ## $ ShelveLoc : Factor w/ 3 levels &quot;Bad&quot;,&quot;Good&quot;,&quot;Medium&quot;: 1 2 3 3 1 1 3 2 3 3 ... ## $ Age : num 42 65 59 55 38 78 71 67 76 76 ... ## $ Education : num 17 10 12 14 13 16 15 10 10 17 ... ## $ Urban : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 1 2 2 1 1 ... ## $ US : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 1 2 1 2 1 2 ... #Change SALES to a factor variable df$Sales &lt;- as.factor(ifelse(Carseats$Sales &lt;= 8, 0, 1)) dff &lt;- df[, -1] # Scaling and Dummy coding dff[,sapply(dff, is.numeric)] &lt;- scale((dff[, sapply(dff, is.numeric)])) ddf &lt;- model.matrix(~.-1, data= dff, contrasts.arg = lapply(dff[, sapply(dff, is.factor)], contrasts, contrasts = FALSE)) ddf &lt;- data.frame(Sales = df$Sales, ddf) # Formula w.ind &lt;- which(colnames(ddf)==&quot;Sales&quot;) frm &lt;- as.formula(paste(&quot;Sales~&quot;, paste(colnames(ddf[-w.ind]), collapse=&#39;+&#39;))) library(ROCR) n &lt;- 10 AUC1 &lt;- c() AUC2 &lt;- c() for (i in 1:n) { set.seed(i) ind &lt;- unique(sample(nrow(ddf), nrow(ddf), replace = TRUE)) train &lt;- ddf[ind, ] test &lt;- ddf[-ind, ] # Models fit.ln &lt;- glm(frm, data = train, family = binomial(link = &quot;logit&quot;)) fit.dnn &lt;- neuralnet(frm, data = train, hidden = 2, threshold = 0.05, linear.output = FALSE, err.fct = &quot;ce&quot;) #Predictions phat.ln &lt;- predict(fit.ln, test, type = &quot;response&quot;) phat.dnn &lt;- predict(fit.dnn, test, type = &quot;repsonse&quot;)[,2] #AUC for predicting Y = 1 pred_rocr1 &lt;- ROCR::prediction(phat.ln, test$Sales) auc_ROCR1 &lt;- ROCR::performance(pred_rocr1, measure = &quot;auc&quot;) AUC1[i] &lt;- auc_ROCR1@y.values[[1]] pred_rocr2 &lt;- ROCR::prediction(phat.dnn, test$Sales) auc_ROCR2 &lt;- ROCR::performance(pred_rocr2, measure = &quot;auc&quot;) AUC2[i] &lt;- auc_ROCR2@y.values[[1]] } (c(mean(AUC1), mean(AUC2))) ## [1] 0.9471081 0.9186785 Again the results are not very convincing to use DNN in this example. Let’s have a more complex task with Red Wine dataset from Kaggle (Cortez et.al, 2009). Our job us to use 11 attributes to classify each wine dfr &lt;- read.csv(&quot;wineQualityReds.csv&quot;, header = TRUE) dfr &lt;- dfr[,-1] # removing the index table(dfr$quality) ## ## 3 4 5 6 7 8 ## 10 53 681 638 199 18 # Let&#39;s remove the outlier qualities: indo &lt;- which(dfr$quality==&quot;3&quot;|dfr$quality==&quot;8&quot;) dfr &lt;- dfr[-indo, ] dfr$quality &lt;- as.factor(dfr$quality) table(dfr$quality) ## ## 4 5 6 7 ## 53 681 638 199 Let’s scale it and get the formula # Scaling and Dummy coding dfr[,sapply(dfr, is.numeric)] &lt;- scale((dfr[, sapply(dfr, is.numeric)])) ddf &lt;- model.matrix(~quality -1, data = dfr) w.ind &lt;- which(colnames(dfr)==&quot;quality&quot;) dfr &lt;- dfr[,-w.ind] # removing &#39;quality` df &lt;- cbind(ddf, dfr) frm &lt;- as.formula(paste(paste(colnames(ddf), collapse=&#39;+&#39;), &quot;~&quot;, paste(colnames(dfr), collapse=&#39;+&#39;))) frm ## quality4 + quality5 + quality6 + quality7 ~ fixed.acidity + volatile.acidity + ## citric.acid + residual.sugar + chlorides + free.sulfur.dioxide + ## total.sulfur.dioxide + density + pH + sulphates + alcohol And, our simple DNN application ind &lt;- sample(nrow(df), nrow(df)*.7) train &lt;- df[ind,] test &lt;- df[-ind,] fit.nn &lt;- neuralnet(frm, data = train, hidden = c(3,2), threshold = 0.05, linear.output = FALSE, err.fct = &quot;ce&quot;) plot(fit.nn, rep = &quot;best&quot;) And our prediction: library(utiml) phat &lt;- predict(fit.nn, test) head(phat) ## [,1] [,2] [,3] [,4] ## 11 0.04931942 0.6859165 0.2825146 0.007453298 ## 14 0.02015208 0.2300976 0.5716617 0.050629158 ## 16 0.08538068 0.8882124 0.0550048 0.009824072 ## 17 0.03592572 0.5136539 0.5273030 0.006685037 ## 22 0.03616818 0.5173671 0.5263112 0.006541895 ## 27 0.03092853 0.4318134 0.5389403 0.011412135 label.hat &lt;- t(apply(phat, 1, function(x) as.numeric(x == max(x)))) head(label.hat) ## [,1] [,2] [,3] [,4] ## 11 0 1 0 0 ## 14 0 0 1 0 ## 16 0 1 0 0 ## 17 0 0 1 0 ## 22 0 0 1 0 ## 27 0 0 1 0 # Confusion Table pred &lt;- apply(phat, 1, which.max) fck &lt;- colnames(test)[1:4] predicted &lt;- fck[pred] act&lt;- apply(test[,1:4], 1, which.max) actual &lt;- fck[act] table(predicted, actual) ## actual ## predicted quality4 quality5 quality6 quality7 ## quality5 4 107 32 3 ## quality6 7 78 160 51 ## quality7 1 0 11 18 This is just an example and the results are not reflecting a trained model. Advance DNN applications with a proper training requires a longer time and more capable machines. We can do a grid search on different number of hidden layers and neurons. However, a large datasets and more complex DNNs need better applications, like, Keras that uses GPU on Linux systems allowing a much better efficiency in training. So far, we have used the neuralnet package. There are several packages in R that are capable of implementing and training artificial neural networks (ANNs), and the most suitable one for your needs will depend on your specific requirements and preferences. Some popular packages for implementing ANNs in R include: nnet: This package is specifically designed for training and using feedforward neural networks, which are a type of ANN that consists of layers of interconnected “neurons” that process and transmit information. It provides functions for building and training feedforward networks, and for making predictions with them. caret: As we’ve seen before, this is a general-purpose package for building and evaluating machine learning models, including ANNs. It provides functions for building and training ANNs, and for tuning their hyperparameters using techniques like cross-validation. It also offers a wide range of functions for preprocessing and analyzing data, and for evaluating the performance of trained models. deepnet: This package is designed for building and training deep learning models, which are a type of ANN with many layers that are capable of learning complex patterns in data. It provides functions for building and training deep learning models using a variety of techniques, including backpropagation, autoencoders, and convolutional neural networks (CNNs). Ultimately, the best package for ANNs in R will depend on your specific needs and requirements. If you are looking for a powerful and flexible package for building and training ANNs, neuralnet or deepnet may be good options. If you just need a simple and easy-to-use package for training feedforward networks and making predictions, nnet may be a good choice. If you want a general-purpose package that can handle a wide range of machine learning tasks, including ANNs, caret may be a good option. Deep neural networks (DNNs) are neural networks with many layers, which can be difficult to train because of the large number of parameters that need to be optimized. This can make the training process computationally intensive and prone to overfitting. Convolutional neural networks (CNNs), on the other hand, are specifically designed to process data that has a grid-like structure, such as an image. One key aspect of CNNs is that they use convolutional layers, which apply a set of filters to the input data and produce a set of transformed feature maps. These filters are able to detect specific features in the input data, such as edges, corners, or textures, and are able to share these features across the input data. This means that the number of parameters in a CNN is typically much smaller than in a DNN, which makes the model easier to train and less prone to overfitting. Overall, CNNs are well-suited for tasks such as image classification, object detection and, speech recognition. We will not cover the details of CNN here. There are several packages available in R for working with CNNs. Keras running on TensowFlow is one of the most efficient engines in building artificial neural networks. Finally, in a deep neural network, “dropout” and “regularization” are techniques used to prevent overfitting. Dropout is a regularization technique that randomly drops out, or removes, a certain percentage of neurons from the network during training. This has the effect of reducing the complexity of the model, as it can’t rely on any one neuron or group of neurons to make predictions. Regularization is a general term that refers to any method used to prevent overfitting in a machine learning model. There are many types of regularization techniques, which add a penalty term to the parameters of the the activation functions. We will be back to ANN later in Section VII - Time Series. "],["parametric-models-in-prediction.html", "Parametric models in prediction", " Parametric models in prediction In simple regression or classification problems, we cannot train a parametric model in a way that the fitted model minimizes the out-of-sample prediction error. We could (and did) fit the parametric models manually by adding or removing predictors and their interactions and polynomials. As we have seen in earlier chapters, by dropping a variable in a regression, for example, it is possible to reduce the variance at the cost of a negligible increase in bias. In fitting the predictive model, some of the variables used in a multiple regression model may not be well associated with the response. Keeping those “irrelevant” variables often leads to unnecessary complexity in the resulting model. Regularization or penalization is an alternative and automated fitting procedure that refers to a process that removes irrelevant variables or shrinks the magnitude of their parameters, which can yield better prediction accuracy and model interpretability by preventing overfitting. There are several types of regularization techniques that can be used in parametric models. Each of these techniques adds a different type of penalty term to the objective function and can be used in different situations depending on the characteristics of the data and the desired properties of the model. Two methods, Ridge and Lasso, are two of well-known benchmark techniques that reduce the model complexity and prevent overfitting resulting from simple linear regression. The general principle in penalization can be shown as \\[ \\widehat{m}_\\lambda(\\boldsymbol{x})=\\operatorname{argmin}\\left\\{\\sum_{i=1}^n \\underbrace{\\mathcal{L}\\left(y_i, m(\\boldsymbol{x})\\right)}_{\\text {loss function }}+\\underbrace{\\lambda\\|m\\|_{\\ell_q}}_{\\text {penalization }}\\right\\} \\] where \\(\\mathcal{L}\\) could be conditional mean, quantiles, expectiles, \\(m\\) could be linear, logit, splines, tree-based models, neural networks. The penalization, \\(\\ell_q\\), could be lasso (\\(\\ell_1\\)) or ridge (\\(\\ell_2\\)). And, \\(\\lambda\\) regulates overfitting that can be determined by cross-validation or other methods. It puts a price to pay for a having more flexible model: \\(\\lambda\\rightarrow0\\): it interpolates data, low bias, high variance \\(\\lambda\\rightarrow\\infty\\): linear model high bias, low variance There are two fundamental goals in statistical learning: achieving a high prediction accuracy and identifying relevant predictors. The second objective, variable selection, is particularly important when there is a true sparsity in the underlying model. By their nature, penalized parametric models are not well-performing tools for prediction. But, they provide important tools for model selection specially when \\(p&gt;N\\) and the true model is sparse. This section starts with two major models in regularized regressions, Ridge and Lasso, and develops an idea on sparse statistical modelling with Adaptive Lasso. Although there are many sources on the subject, perhaps the most fundamental one is Statistical Learning with Sparsity by Hastie et al. (2015). "],["ridge.html", "Chapter 17 Ridge", " Chapter 17 Ridge We know that the least squares fitting procedure is that one estimates \\(\\beta_{0}, \\beta_{1}, \\ldots, \\beta_{p}\\) that minimize the residual sum of squares: \\[ \\mathrm{RSS}=\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{p} \\beta_{j} x_{i j}\\right)^{2} \\] Ridge regression is very similar to least squares, except that the coefficients are estimated by minimizing a slightly different quantity. \\[ \\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{p} \\beta_{j} x_{i j}\\right)^{2}+\\lambda \\sum_{j=1}^{p} \\beta_{j}^{2} =\\mathrm{RSS}+\\lambda \\sum_{j=1}^{p} \\beta_{j}^{2}, \\] where \\(\\lambda\\) is the hyperparameter that can be tuned by cross-validation and grid search. The last term, \\(\\lambda \\sum_{j} \\beta_{j}^{2}\\), is a constraint, which is also called shrinkage penalty. This type of penalty is called as \\(\\ell_{2}\\). As with Ordinary Least Squares (OLS), this cost function tries to minimize RSS but also penalizes the size of the coefficients. More specifically, \\[ \\hat{\\beta}_\\lambda^{\\text {ridge }}=\\operatorname{argmin}\\left\\{\\left\\|\\mathbf{y}-\\left(\\beta_0+\\mathbf{X} \\beta\\right)\\right\\|_{\\ell_2}^2+\\lambda\\|\\beta\\|_{\\ell_2}^2\\right\\} \\] Explicit solution: \\[ \\hat{\\beta}_\\lambda=\\left(\\mathbf{X}^{\\top} \\mathbf{X}+\\lambda \\mathbf{I}\\right)^{-1} \\mathbf{X}^{\\top} \\mathbf{y} \\] where, If \\(\\lambda \\rightarrow 0, \\quad \\hat{\\beta}_0^{\\text {ridge }}=\\hat{\\beta}^{\\text {ols }}\\), If \\(\\lambda \\rightarrow \\infty, \\quad \\hat{\\beta}_{\\infty}^{\\text {ridge }}=\\mathbf{0}\\). The hyperparameter \\(\\lambda\\) controls the relative impact of the penalization on the regression coefficient estimates. When \\(\\lambda = 0\\), the cost function becomes RSS, that is the cost function of OLS and the estimations produce the least squares estimates. However, as \\(\\lambda\\) gets higher, the impact of the shrinkage penalty grows, and the coefficients of the ridge regression will approach zero. Note that, the shrinkage penalty is applied to slope coefficients not to the intercept, which is simply a measure of the mean value of the response, when all features are zero. Lets apply this to the same data we used earlier, Hitters from the ISLR (ISLR_2021?) package: library(ISLR) remove(list = ls()) data(Hitters) df &lt;- Hitters[complete.cases(Hitters$Salary), ] We will use glmnet to fit a ridge regression. The generic function in glmnet is defined by \\[ \\min _{\\beta_0, \\beta} \\frac{1}{N} \\sum_{i=1}^N w_i l\\left(y_i, \\beta_0+\\beta^T x_i\\right)+\\lambda\\left[(1-\\alpha)\\|\\beta\\|_2^2 / 2+\\alpha\\|\\beta\\|_1\\right] \\text {, } \\] where \\(l\\left(y_i, \\eta_i\\right)\\) is the negative log-likelihood contribution for observation \\(i\\) and \\(\\alpha\\) is the elastic net penalty: lasso regression ( \\(\\alpha=1\\), the default) and ridge regression \\((\\alpha=0)\\). As before, the tuning parameter \\(\\lambda\\) controls the overall strength of the penalty. Since the penalty shrinks (ridge) the coefficients of correlated variables or pick one of them and discard the others (lasso), the variables are supposed to be standardized, which is done by glmnet. This function has slightly different syntax from other model-fitting functions that we have used so far in this book, such as the y ~ X syntax. Therefore, before we execute the syntax, we have the prepare the model so that X will be a matrix and y will be vector. The matrix X has to be prepared before we proceed, which must be free of NAs. X &lt;- model.matrix(Salary~., df)[,-1] y &lt;- df$Salary The glmnet package is maintained by Trevor Hastie who provides a friendly vignette (Hastie_glmnet?). They describe the importance of model.matrix() in glmnet as follows: (…)particularly useful for creating \\(x\\); not only does it produce a matrix corresponding to the 19 predictors but it also automatically transforms any qualitative variables into dummy variables. The latter property is important because glmnet() can only take numerical, quantitative inputs. Here is the example for a ridge regression: library(glmnet) grid = 10^seq(10, -2, length = 100) model &lt;- glmnet(X, y, alpha = 0, lambda = grid) Although we defined the grid, we did not do a grid search explicitly by cross validation. By default, the glmnet() function performs ridge regression for an automatically selected range of \\(\\lambda\\) values. It ranges from the null model - only intercept when \\(\\lambda\\) is at the upper bound and the least squares fit when the \\(\\lambda\\) is at lower bound. The application above is to show that we can also choose to implement the function over a grid of values. Moreover, the glmnet() function standardizes the variables so that they are on the same scale. To turn off this default setting, use the argument standardize=FALSE. The methods here, ridge and lasso, are parametric models. Unlike non-parametric methods, each model is defined by a set of parameters or, as in our case, coefficients. Therefore, when we do a grid search, each value of the hyperparameter (\\(\\lambda\\)) is associated with one model defined by a set of coefficients. In order to see the coefficients we need to apply another function, coef(). Remember, we have 100 \\(\\lambda&#39;s\\). Hence, coef() produces a 20 x 100 matrix, with 20 rows (one for each predictor, plus an intercept) and 100 columns (one for each value of \\(\\lambda\\)). dim(coef(model)) ## [1] 20 100 model$lambda[c(20,80)] ## [1] 4.977024e+07 2.656088e+00 coef(model)[, c(20,80)] ## 20 x 2 sparse Matrix of class &quot;dgCMatrix&quot; ## s19 s79 ## (Intercept) 5.358880e+02 156.6073700 ## AtBat 1.093664e-05 -1.7526436 ## Hits 3.967221e-05 6.1739859 ## HmRun 1.598556e-04 1.3285278 ## Runs 6.708833e-05 -0.7689372 ## RBI 7.086606e-05 -0.1297830 ## Walks 8.340541e-05 5.5357165 ## Years 3.410894e-04 -9.2923000 ## CAtBat 9.390097e-07 -0.0792321 ## CHits 3.455823e-06 0.2132942 ## CHmRun 2.606160e-05 0.6557328 ## CRuns 6.933126e-06 0.8349167 ## CRBI 7.155123e-06 0.4090719 ## CWalks 7.570013e-06 -0.6623253 ## LeagueN -1.164983e-04 62.0427219 ## DivisionW -1.568625e-03 -121.5286522 ## PutOuts 4.380543e-06 0.2809457 ## Assists 7.154972e-07 0.3124435 ## Errors -3.336588e-06 -3.6852362 ## NewLeagueN -2.312257e-05 -27.9849755 Due to the penalty, we see that the coefficient estimates are much smaller, when a large value of \\(\\lambda\\) is used. We generally use the predict() function as before. But, here we can also use it to estimate the ridge regression coefficients for a new value of \\(\\lambda\\). Hence, if we don’t want to rely on the internal grid search provided by glmnet(), we can do our own grid search by predict(). This is an example when \\(\\lambda = 50\\), which wasn’t in the grid. predict(model, s = 50, type = &quot;coefficients&quot;) ## 20 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 ## (Intercept) 4.876610e+01 ## AtBat -3.580999e-01 ## Hits 1.969359e+00 ## HmRun -1.278248e+00 ## Runs 1.145892e+00 ## RBI 8.038292e-01 ## Walks 2.716186e+00 ## Years -6.218319e+00 ## CAtBat 5.447837e-03 ## CHits 1.064895e-01 ## CHmRun 6.244860e-01 ## CRuns 2.214985e-01 ## CRBI 2.186914e-01 ## CWalks -1.500245e-01 ## LeagueN 4.592589e+01 ## DivisionW -1.182011e+02 ## PutOuts 2.502322e-01 ## Assists 1.215665e-01 ## Errors -3.278600e+00 ## NewLeagueN -9.496680e+00 There are two ways that we can train ridge (and Lasso): Use our own training algorithm; Rely on 'glmnet internal cross-validation process. Here is an example for our own training algorithm for training ridge regression: grid = 10^seq(10, -2, length = 100) MSPE &lt;- c() MMSPE &lt;- c() for(i in 1:length(grid)){ for(j in 1:100){ set.seed(j) ind &lt;- unique(sample(nrow(df), nrow(df), replace = TRUE)) train &lt;- df[ind, ] xtrain &lt;- model.matrix(Salary~., train)[,-1] ytrain &lt;- df[ind, &quot;Salary&quot;] test &lt;- df[-ind, ] xtest &lt;- model.matrix(Salary~., test)[,-1] ytest &lt;- df[-ind, &quot;Salary&quot;] model &lt;- glmnet(xtrain, ytrain, alpha = 0, lambda = grid[i], thresh = 1e-12) yhat &lt;- predict(model, s = grid[i], newx = xtest) MSPE[j] &lt;- mean((yhat - ytest)^2) } MMSPE[i] &lt;- mean(MSPE) } min(MMSPE) ## [1] 119058.3 grid[which.min(MMSPE)] ## [1] 14.17474 plot(log(grid), MMSPE, type=&quot;o&quot;, col = &quot;red&quot;, lwd = 3) What is the tuned model using the last training test with this \\(\\lambda\\)? lambda &lt;- grid[which.min(MMSPE)] coeff &lt;- predict(model, s = lambda , type = &quot;coefficients&quot;, newx = xtrain) coeff ## 20 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 ## (Intercept) 285.78834247 ## AtBat -1.27240085 ## Hits 2.06931134 ## HmRun 0.04319066 ## Runs 2.75588969 ## RBI 0.45631590 ## Walks 3.46189297 ## Years -8.82528502 ## CAtBat -0.26127780 ## CHits 1.28540111 ## CHmRun 1.31904979 ## CRuns 0.05880843 ## CRBI -0.05103190 ## CWalks -0.34003983 ## LeagueN 131.98795986 ## DivisionW -119.25402540 ## PutOuts 0.19785230 ## Assists 0.64820842 ## Errors -6.97397640 ## NewLeagueN -54.55149894 We may want to compare the ridge with a simple OLS: MSPE &lt;- c() for(j in 1:100){ set.seed(j) ind &lt;- unique(sample(nrow(df), nrow(df), replace = TRUE)) train &lt;- df[ind, ] test &lt;- df[-ind,] model &lt;- lm(Salary~., data = train) yhat &lt;- predict(model, newdata = test) MSPE[j] &lt;- mean((yhat - test$Salary)^2) } mean(MSPE) ## [1] 124217.3 summary(model) ## ## Call: ## lm(formula = Salary ~ ., data = train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -715.51 -187.40 -32.85 148.29 1686.38 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 285.95478 126.06479 2.268 0.0248 * ## AtBat -1.26497 0.94674 -1.336 0.1837 ## Hits 2.02174 3.61275 0.560 0.5766 ## HmRun -0.01383 8.03787 -0.002 0.9986 ## Runs 2.79786 4.23051 0.661 0.5095 ## RBI 0.47768 3.56888 0.134 0.8937 ## Walks 3.44099 2.57671 1.335 0.1839 ## Years -8.76533 17.25334 -0.508 0.6122 ## CAtBat -0.26610 0.20435 -1.302 0.1950 ## CHits 1.31361 1.09982 1.194 0.2343 ## CHmRun 1.35851 2.30018 0.591 0.5557 ## CRuns 0.04142 1.02393 0.040 0.9678 ## CRBI -0.06982 1.08722 -0.064 0.9489 ## CWalks -0.33312 0.45479 -0.732 0.4651 ## LeagueN 132.36961 113.39037 1.167 0.2450 ## DivisionW -119.16837 56.96453 -2.092 0.0382 * ## PutOuts 0.19795 0.10911 1.814 0.0718 . ## Assists 0.64902 0.29986 2.164 0.0321 * ## Errors -6.97871 5.97011 -1.169 0.2444 ## NewLeagueN -54.96821 111.81338 -0.492 0.6238 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 335.6 on 140 degrees of freedom ## Multiple R-squared: 0.4428, Adjusted R-squared: 0.3672 ## F-statistic: 5.856 on 19 and 140 DF, p-value: 1.346e-10 The second way is to rely on the glmnet internal training process, cv.glmnet, which is the main function to do cross-validation along with various supporting methods such as plotting and prediction. A part of the following scripts follows the same algorithm as the one in the book (Introduction to Statistical Learning - ISLR p.254). This approach uses a specific grid on \\(\\lambda\\). # With a defined grid on lambda bestlam &lt;- c() mse &lt;- c() grid = 10^seq(10, -2, length = 100) for(i in 1:100){ set.seed(i) train &lt;- sample(1:nrow(X), nrow(X)*0.5) # 50% split test &lt;- c(-train) ytest &lt;- y[test] #finding lambda cv.out &lt;- cv.glmnet(X[train,], y[train], alpha = 0) bestlam[i] &lt;- cv.out$lambda.min #Predicting with that lambda ridge.mod &lt;- glmnet(X[train,], y[train], alpha = 0, lambda = grid, thresh = 1e-12) yhat &lt;- predict(ridge.mod, s = bestlam[i], newx = X[test,]) mse[i] &lt;- mean((yhat - ytest)^2) } mean(bestlam) ## [1] 290.227 mean(mse) ## [1] 127472.6 plot(bestlam, col = &quot;blue&quot;) plot(mse, col = &quot;pink&quot;) Now the same application without a specific grid: bestlam &lt;- c() mse &lt;- c() # Without a pre-defined grid on lambda for(i in 1:100){ set.seed(i) train &lt;- sample(1:nrow(X), nrow(X)*0.5) # arbitrary split test &lt;- c(-train) ytest &lt;- y[test] cv.out &lt;- cv.glmnet(X[train,], y[train], alpha = 0) yhat &lt;- predict(cv.out, s = &quot;lambda.min&quot;, newx = X[test,]) mse[i] &lt;- mean((yhat - ytest)^2) } mean(mse) ## [1] 127481.6 plot(mse, col = &quot;pink&quot;) Ridge regression adds a penalty term that is the sum of the squares of the coefficients of the features in the model. This results in a penalty that is continuous and differentiable, which makes Ridge regression easy to optimize using gradient descent. Ridge regression can be useful when you have a large number of features, and you want to shrink the coefficients of all of the features towards zero, but you still want to keep all of the features in the model. Ridge regression works best in situations where the least squares estimates have high variance. On the other hand, Lasso (Least Absolute Shrinkage and Selection Operator) adds a penalty term that is the sum of the absolute values of the coefficients of the features in the model. This results in a penalty that is non-differentiable, which makes it more difficult to optimize using gradient descent. However, Lasso has the advantage of being able to set the coefficients of some features to exactly zero, effectively eliminating those features from the model. This can be useful when you have a large number of features, and you want to select a subset of the most important features to include in the model. "],["lasso.html", "Chapter 18 Lasso", " Chapter 18 Lasso The penalty in ridge regression, \\(\\lambda \\sum_{j} \\beta_{j}^{2}\\), will shrink all of the coefficients towards zero, but it will not set any of them exactly to zero. This may present a problem in model interpretation when the number of variables is quite large. One of the key advantages of Lasso is that it can set the coefficients of some features to exactly zero, effectively eliminating those features from the model. By eliminating unnecessary or redundant features from the model, Lasso can help to improve the interpretability and simplicity of the model. This can be particularly useful when you have a large number of features and you want to identify the most important ones for predicting the target variable. The lasso (least absolute shrinkage and selection operator) is a relatively recent alternative to ridge regression that overcomes this disadvantage. The lasso coefficients minimize the following quantity: \\[\\begin{equation} \\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{p} \\beta_{j} x_{i j}\\right)^{2}+\\lambda \\sum_{j=1}^{p}\\left|\\beta_{j}\\right|=\\operatorname{RSS}+\\lambda \\sum_{j=1}^{p}\\left|\\beta_{j}\\right| \\tag{18.1} \\end{equation}\\] The lasso also shrinks the coefficient estimates towards zero. However, the \\(\\ell_{1}\\) penalty, the second term of equation 18.1, has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter \\(\\lambda\\) is sufficiently large. Hence, the lasso performs variable selection. As a result, models generated from the lasso are generally much easier to interpret than those produced by ridge regression. In general, one might expect lasso to perform better in a setting where a relatively small number of predictors have substantial coefficients and the remaining predictors have no significant effect on the outcome. This property is known as “sparsity”, because it results in a model with a relatively small number of non-zero coefficients. In some cases, Lasso can find a true sparsity pattern in the data, which means that it can identify a small subset of the most important features that are sufficient to accurately predict the target variable. This can be particularly useful when you have a large number of features, and you want to identify the most important ones for predicting the target variable. Now, we will apply lasso to the same data in the last chapter, Hitters. Again, we will follow a similar way to compare ridge and lasso as in ISLR : library(glmnet) library(ISLR) remove(list = ls()) data(Hitters) df &lt;- Hitters[complete.cases(Hitters$Salary), ] X &lt;- model.matrix(Salary~., df)[,-1] y &lt;- df$Salary # Without a specific grid on lambda set.seed(1) train &lt;- sample(1:nrow(X), nrow(X)*0.5) test &lt;- c(-train) ytest &lt;- y[test] # Ridge set.seed(1) ridge.out &lt;- cv.glmnet(X[train,], y[train], alpha = 0) yhatR &lt;- predict(ridge.out, s = &quot;lambda.min&quot;, newx = X[test,]) mse_r &lt;- mean((yhatR - ytest)^2) # Lasso set.seed(1) lasso.out &lt;- cv.glmnet(X[train,], y[train], alpha = 1) yhatL &lt;- predict(lasso.out, s = &quot;lambda.min&quot;, newx = X[test,]) mse_l &lt;- mean((yhatL - ytest)^2) mse_r ## [1] 139863.2 mse_l ## [1] 143668.8 Now, we will define our own grid search: # With a specific grid on lambda + lm() grid = 10^seq(10, -2, length = 100) set.seed(1) train &lt;- sample(1:nrow(X), nrow(X)*0.5) test &lt;- c(-train) ytest &lt;- y[test] #Ridge ridge.mod &lt;- glmnet(X[train,], y[train], alpha = 0, lambda = grid, thresh = 1e-12) set.seed(1) cv.outR &lt;- cv.glmnet(X[train,], y[train], alpha = 0) bestlamR &lt;- cv.outR$lambda.min yhatR &lt;- predict(ridge.mod, s = bestlamR, newx = X[test,]) mse_R &lt;- mean((yhatR - ytest)^2) # Lasso lasso.mod &lt;- glmnet(X[train,], y[train], alpha = 1, lambda = grid, thresh = 1e-12) set.seed(1) cv.outL &lt;- cv.glmnet(X[train,], y[train], alpha = 1) bestlamL &lt;- cv.outL$lambda.min yhatL &lt;- predict(lasso.mod, s = bestlamL, newx = X[test,]) mse_L &lt;- mean((yhatL - ytest)^2) mse_R ## [1] 139856.6 mse_L ## [1] 143572.1 Now we will apply our own algorithm grid = 10^seq(10, -2, length = 100) MSPE &lt;- c() MMSPE &lt;- c() for(i in 1:length(grid)){ for(j in 1:100){ set.seed(j) ind &lt;- unique(sample(nrow(df), nrow(df), replace = TRUE)) train &lt;- df[ind, ] xtrain &lt;- model.matrix(Salary~., train)[,-1] ytrain &lt;- df[ind, 19] test &lt;- df[-ind, ] xtest &lt;- model.matrix(Salary~., test)[,-1] ytest &lt;- df[-ind, 19] model &lt;- glmnet(xtrain, ytrain, alpha = 1, lambda = grid[i], thresh = 1e-12) yhat &lt;- predict(model, s = grid[i], newx = xtest) MSPE[j] &lt;- mean((yhat - ytest)^2) } MMSPE[i] &lt;- mean(MSPE) } min(MMSPE) ## [1] 119855.1 grid[which.min(MMSPE)] ## [1] 2.656088 plot(log(grid), MMSPE, type=&quot;o&quot;, col = &quot;red&quot;, lwd = 3) What are the coefficients? coef_lasso &lt;- coef(model, s=grid[which.min(MMSPE)], nonzero = T) coef_lasso ## NULL We can also try a classification problem with LPM or Logistic regression when the response is categorical. If there are two possible outcomes, we use the binomial distribution, else we use the multinomial. "],["adaptive-lasso.html", "Chapter 19 Adaptive Lasso", " Chapter 19 Adaptive Lasso Unlike lasso, which uses a simple \\(\\ell_{1}\\) penalty, adaptive lasso uses a weighted \\(\\ell_{1}\\) penalty. The weights are chosen to adapt to the correlation structure of the data, which can result in a more stable model with fewer coefficients being exactly zero. Adaptive lasso is a method for regularization and variable selection in regression analysis that was introduced by Zou (2006) in The Adaptive Lasso and Its Oracle Properties by Zou (2006). In this paper, the author proposed the use of a weighted \\(\\ell_{1}\\) penalty in the objective function, with the weights chosen to adapt to the correlation structure of the data. He showed that this method can result in a more stable model with fewer coefficients being exactly zero, compared to the standard lasso method which uses a simple \\(\\ell_{1}\\) penalty. The adaptive lasso enjoys the oracle properties; namely, it performs as well as if the true underlying model were given in advance. Since its introduction, adaptive lasso has been widely used in a variety of applications in statistical modeling and machine learning. It has been applied to problems such as feature selection in genomic data, high-dimensional regression, and model selection in generalized linear models. Adaptive lasso is useful in situations where the predictors are correlated and there is a need to select a small subset of important variables to include in the model. It is also useful in situations where the goal is to identify a representative model from the set of all possible models, rather than just selecting a single model. Consider the linear regression model: \\[ y_i=x_i^{\\prime} \\beta^0+\\epsilon_i, ~~~~i=1, \\ldots, n ~~~~\\text{and} ~~~~\\beta^0 \\text { is } (p \\times 1) \\] The adaptive Lasso estimates \\(\\beta^0\\) by minimizing \\[ L(\\beta)=\\sum_{i=1}^n\\left(y_i-x_i^{\\prime} \\beta\\right)^2+\\lambda_n \\sum_{j=1}^p \\frac{1}{w_j}\\left|\\beta_j\\right| \\] where, typically \\(w_j=(\\left|\\hat{\\beta}_{O L S_j}\\right|)^{\\gamma}\\) or \\(w_j=(\\left|\\hat{\\beta}_{Ridge_j}\\right|)^{\\gamma}\\), where \\(\\gamma\\) is a positive constant for adjustment of the Adaptive Weights vector, and suggested to be the possible values of 0.5, 1, and 2. The weights in Adaptive lasso (AL) are more “intelligent” than those for the plain Lasso. The plain Lasso penalizes all parameters equally, while the adaptive Lasso is likely to penalize non-zero coefficients less than the zero ones. This is due to the fact, that the weights are based on the consistent least squares estimator. If \\(\\beta_{AL, j}=0\\), then \\(\\hat{\\beta}_{O L S, j}\\) is likely to be close to zero and so \\(w_j\\) is small. Hence, truly zero coefficients are penalized a lot. However, it might require a two-step procedure as opposed to the one-step plain Lasso. Some studies (Zou, 2006) state that the plain lasso is not oracle efficient (consistency in variable selection and asymptotic normality in coefficient estimation) while adaptive lasso is. Here is an example: library(ISLR) library(glmnet) remove(list = ls()) data(Hitters) df &lt;- Hitters[complete.cases(Hitters$Salary), ] X &lt;- model.matrix(Salary~., df)[,-1] y &lt;- df$Salary # Ridge weights with gamma = 1 g = 1 set.seed(1) modelr &lt;- cv.glmnet(X, y, alpha = 0) coefr &lt;- as.matrix(coef(modelr, s = modelr$lambda.min)) w.r &lt;- 1/(abs(coefr[-1,]))^g ## Adaptive Lasso set.seed(1) alasso &lt;- cv.glmnet(X, y, alpha=1, penalty.factor = w.r) ## Lasso set.seed(1) lasso &lt;- cv.glmnet(X, y, alpha=1) # Sparsity cbind(LASSO = coef(lasso, s=&quot;lambda.1se&quot;), ALASSO = coef(alasso, s=&quot;lambda.1se&quot;)) ## 20 x 2 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 s1 ## (Intercept) 127.95694754 -7.109481 ## AtBat . . ## Hits 1.42342566 2.054867 ## HmRun . . ## Runs . . ## RBI . . ## Walks 1.58214111 3.573120 ## Years . 31.573334 ## CAtBat . . ## CHits . . ## CHmRun . . ## CRuns 0.16027975 . ## CRBI 0.33667715 . ## CWalks . . ## LeagueN . 29.811080 ## DivisionW -8.06171262 -138.088953 ## PutOuts 0.08393604 . ## Assists . . ## Errors . . ## NewLeagueN . . We can see the difference between lasso and adaptive lasso in this example: PutOuts, CRuns, and CRBI picked by lasso are not selected by adaptive lasso. There are only three common features in both methods: Hits, Walks, and DivisionW. To understand which model is better in terms of catching the true sparsity, we will have a simulation to illustrate some of the properties of the Lasso and the adaptive Lasso. "],["sparsity.html", "Chapter 20 Sparsity", " Chapter 20 Sparsity This is a simulation to illustrate some of the properties of Lasso-type estimations. There are two objectives in using these penalized regressions: model selection (identifying “correct” sparsity) and prediction accuracy. These two objectives require different optimization approaches and usually are not compatible. In model selection, the objective is to shrink the dimension of the model to the “true” sparsity. This is usually evaluated by checking whether the Oracle properties are satisfied. These asymptotic properties look at (1) if the model identified by the penalized regression converges to the “true” sparsity, (2) if the coefficients are consistent. The literature suggests that Lasso is not an “Oracle” estimator. Adaptive Lasso was developed (Zou 2006) to fill this gap. Let’s specify a data generating process with a linear regression model: \\[ y_i=x_i^{\\prime} \\beta_0+u_i, ~~~~~i=1, \\ldots, n \\] where \\(\\beta_0\\) is \\(p \\times 1\\). First, we consider the case where \\(p&lt;n\\) then move to the case where \\(p \\geq n\\). We define \\(\\beta_0=(1,1,0,0)^{\\prime}\\) and \\(n=100\\). #This function generates the data dgp &lt;- function(N, Beta){ p = length(Beta) X &lt;- matrix(rnorm(N*p), ncol = p) u &lt;- matrix(rnorm(N), ncol = 1) dgm &lt;- X%*%Beta y &lt;- X%*%Beta + u return &lt;- list(y, X) } N = 100 Beta = c(1,1,0,0) set.seed(148) Output &lt;- dgp(N, Beta) y &lt;-Output[[1]] X &lt;-Output[[2]] First, we apply lasso library(glmnet) set.seed(432) lasso &lt;- glmnet(x = X, y = y, family = &quot;gaussian&quot;) beta_hat &lt;- lasso$beta S_matrix &lt;- cbind(t(beta_hat), &quot;lambda&quot; = lasso$lambda) S_matrix[c(1:8, 25:30, 55:60), ] # selected rows ## 20 x 5 sparse Matrix of class &quot;dgCMatrix&quot; ## V1 V2 V3 V4 lambda ## s0 . . . . 1.083220708 ## s1 0.09439841 0.0283513 . . 0.986990366 ## s2 0.17344129 0.1097255 . . 0.899308862 ## s3 0.24546220 0.1838706 . . 0.819416741 ## s4 0.31108496 0.2514289 . . 0.746622016 ## s5 0.37087798 0.3129855 . . 0.680294174 ## s6 0.42535915 0.3690736 . . 0.619858715 ## s7 0.47500037 0.4201789 . . 0.564792175 ## s24 0.87944075 0.8365481 . . 0.116150206 ## s25 0.88874261 0.8461243 . . 0.105831742 ## s26 0.89685610 0.8542117 -0.00686322 . 0.096429941 ## s27 0.90418482 0.8614679 -0.01432988 . 0.087863371 ## s28 0.91086250 0.8680794 -0.02113323 . 0.080057832 ## s29 0.91694695 0.8741036 -0.02733218 . 0.072945714 ## s54 0.98352129 0.9289175 -0.09282009 0.05192379 0.007126869 ## s55 0.98423271 0.9294382 -0.09350608 0.05278151 0.006493738 ## s56 0.98488092 0.9299126 -0.09413113 0.05356303 0.005916852 ## s57 0.98547155 0.9303449 -0.09470066 0.05427512 0.005391215 ## s58 0.98600972 0.9307388 -0.09521958 0.05492395 0.004912274 ## s59 0.98650007 0.9310977 -0.09569241 0.05551515 0.004475881 Which beta_hat? To answer this question we need to find the lambda. We need \\(\\lambda_n \\rightarrow \\infty\\) in order to shrink the truly zero coefficients to zero. This requires \\(\\lambda_n\\) to be sufficiently large. On the other hand, \\(\\lambda_n / \\sqrt{n} \\rightarrow 0\\) is needed in order not to shrink too much. This would introduce asymptotic bias to the non-zero coefficients. Information-criterion based model selection is very fast, but it relies on a proper estimation of degrees of freedom, are derived for large samples (asymptotic results) and assume the model is correct, i.e. that the data are actually generated by this model. In practice, choosing \\(\\lambda_n\\) by \\(\\mathrm{BIC}\\) results in consistent model selection in the fixed \\(p\\) setting, i.e., Let \\(\\mathcal{A}=\\left\\{j: \\beta_{0, j} \\neq 0\\right\\}\\), active set or relevant variables \\[ P\\left(\\hat{\\mathcal{A}}_{\\lambda_{BIC}}=\\mathcal{A}\\right) \\rightarrow 1 \\] We find lambda by minimum Bayesian Information Criterion (BIC). Let \\(S S E_\\lambda\\) be the sum of squared error terms for a given value of \\(\\lambda\\) and \\(n z_\\lambda\\) be the number of non-zero coefficients. Then, \\[ B I C_\\lambda=\\log \\left(S S E_\\lambda\\right)+\\frac{\\log (n)}{n} n z_\\lambda \\] # Predict yhat for each of 61 lambda (s) y_hat = predict(lasso, newx = X) dim(y_hat) ## [1] 100 60 # SSE for each lambda (s) SSE &lt;- c() for (i in 1:ncol(y_hat)) { SSE_each &lt;- sum((y_hat[,i]-y[,1])^(2)) SSE &lt;- c(SSE, SSE_each) } # BIC nz &lt;- colSums(beta_hat!=0) # Number of non-zero coefficients for each lambda BIC &lt;- log(SSE) + (log(N)/N)*nz #Now BIC BIC ## s0 s1 s2 s3 s4 s5 s6 s7 ## 5.598919 5.595359 5.468287 5.348947 5.237755 5.135013 5.040883 4.955387 ## s8 s9 s10 s11 s12 s13 s14 s15 ## 4.878394 4.809638 4.748729 4.695181 4.648437 4.607898 4.572946 4.542971 ## s16 s17 s18 s19 s20 s21 s22 s23 ## 4.517383 4.495631 4.477205 4.461646 4.448541 4.437530 4.428295 4.420563 ## s24 s25 s26 s27 s28 s29 s30 s31 ## 4.414098 4.408698 4.448661 4.443309 4.438844 4.435121 4.432021 4.429439 ## s32 s33 s34 s35 s36 s37 s38 s39 ## 4.427290 4.425503 4.424017 4.468004 4.466218 4.464732 4.463498 4.462471 ## s40 s41 s42 s43 s44 s45 s46 s47 ## 4.461618 4.460910 4.460321 4.459832 4.459426 4.459088 4.458808 4.458575 ## s48 s49 s50 s51 s52 s53 s54 s55 ## 4.458382 4.458222 4.458088 4.457978 4.457886 4.457810 4.457746 4.457694 ## s56 s57 s58 s59 ## 4.457650 4.457614 4.457584 4.457559 Let’s select \\(\\lambda\\) that has the minimum BIC beta_lasso &lt;- beta_hat[ ,which(BIC==min(BIC))] beta_lasso ## V1 V2 V3 V4 ## 0.8887426 0.8461243 0.0000000 0.0000000 This is the beta_hat that identifies the true sparsity. And, the second Oracle property, the \\(\\ell_2\\) error: l_2 &lt;- sqrt(sum((beta_lasso - Beta)^2)) l_2 ## [1] 0.189884 Here we will create a simulation that will report two Oracle Properties Lasso and Adaptive Lasso: True sparsity, \\(\\ell_2\\) error. Lasso We first have a function, msc(), that executes a simulation with all the steps shown before: mcs &lt;- function(mc, N, Beta){ mcmat &lt;- matrix(0, nrow = mc, ncol = 3) beta_lasso_mat &lt;- matrix(0, nr = mc, nc = length(Beta)) for (i in 1:mc) { set.seed(i) data &lt;- dgp(N, Beta) y &lt;- data[[1]] X &lt;- data[[2]] set.seed(i) lasso &lt;- glmnet(x = X, y = y, family = &quot;gaussian&quot;) beta_hat &lt;- lasso$beta; # beta_hat is a matrix y_hat = predict(lasso, newx = X) SSE &lt;- c() for (j in 1:ncol(y_hat)) { SSE_each &lt;- sum((y_hat[,j]-y[,1])^(2)) SSE &lt;- c(SSE, SSE_each) } nz &lt;- colSums(beta_hat!=0) BIC &lt;- log(SSE) + (log(N)/N)*nz beta_lasso &lt;- beta_hat[ ,which(BIC==min(BIC))] nonz_beta = length(Beta[Beta==0]) nonz_beta_hat = length(beta_lasso[beta_lasso==0]) mcmat[i,1] &lt;- sqrt(sum((beta_lasso - Beta)^2)) mcmat[i,2] &lt;- ifelse(nonz_beta!= nonz_beta_hat, 0, 1) mcmat[i,3] &lt;- sum(beta_lasso!=0) beta_lasso_mat[i,] &lt;- beta_lasso } return(list(mcmat,beta_lasso_mat)) } We are ready for simulation: mc &lt;- 500 N &lt;- 1000 Beta &lt;- matrix(c(1,1,0,0), nc = 1) output &lt;- mcs(mc, N, Beta) #see the function MC_betas = output[[2]] MC_performance = output[[1]]; sum(MC_performance[,2]) #how many times lasso finds true sparsity ## [1] 400 This is the first property: lasso identifies the true sparsity \\(400/500 = 80\\%\\) of cases. And the second property, \\(\\ell_2\\) error, in the simulation is (in total): sum(MC_performance[,1]) ## [1] 29.41841 Adaptive Lasso This time we let our adaptive lasso use lasso coefficients as penalty weights in glmnet(). Let’s have the same function with Adaptive Lasso for the simulation: # Adaptive LASSO mcsA &lt;- function(mc, N, Beta){ mcmat &lt;- matrix(0, nr = mc, nc = 3) beta_lasso_mat &lt;- matrix(0, nr = mc, nc = length(Beta)) for (i in 1:mc) { data &lt;- dgp(N, Beta) y &lt;- data[[1]] X &lt;- data[[2]] lasso &lt;- glmnet(x = X, y = y, family = &quot;gaussian&quot;) beta_hat &lt;- lasso$beta; y_hat = predict(lasso, newx = X) SSE &lt;- c() for (j in 1:ncol(y_hat)) { SSE_each &lt;- sum((y_hat[,j]-y[,1])^(2)) SSE &lt;- c(SSE, SSE_each) } nz &lt;- colSums(beta_hat!=0) BIC &lt;- log(SSE) + (log(N)/N)*nz beta_lasso &lt;- beta_hat[ ,which(BIC==min(BIC))] weights = abs(beta_lasso)^(-1) weights[beta_lasso==0] = 10^10 # to handle inf&#39;s #Now Adaptive Lasso lasso &lt;- glmnet(x = X, y = y, family = &quot;gaussian&quot;, penalty.factor = weights) beta_hat &lt;- lasso$beta; y_hat = predict(lasso, newx = X) SSE &lt;- c() for (j in 1:ncol(y_hat)) { SSE_each &lt;- sum((y_hat[,j]-y[,1])^(2)) SSE &lt;- c(SSE, SSE_each) } nz &lt;- colSums(beta_hat!=0) BIC &lt;- log(SSE) + (log(N)/N)*nz beta_lasso &lt;- beta_hat[ ,which(BIC==min(BIC))] nonz_beta = length(Beta[Beta==0]) nonz_beta_hat = length(beta_lasso[beta_lasso==0]) mcmat[i,1] &lt;- sqrt(sum((beta_lasso - Beta)^2)) mcmat[i,2] &lt;- ifelse(nonz_beta!= nonz_beta_hat, 0, 1) mcmat[i,3] &lt;- sum(beta_lasso!=0) beta_lasso_mat[i,] &lt;- beta_lasso } return(list(mcmat,beta_lasso_mat)) } Here are the results for adaptive lasso mc &lt;- 500 N &lt;- 1000 beta &lt;- matrix(c(1,1,0,0), nc = 1) output &lt;- mcsA(mc, N, beta) #see the function MC_betas = output[[2]] MC_performance = output[[1]]; sum(MC_performance[,2]) ## [1] 492 And, sum(MC_performance[,1]) ## [1] 20.21311 The simulation results clearly show that Adaptive Lasso is an Oracle estimator while the regular Lasso is not. Therefore, in model selection, Adaptive Lasso is a better choice. We saw here a basic application of a plain adaptive lasso. It has many different variations in practice such as Thresholded Lasso and Rigorous Lasso. Application of lasso on model selection has been an active research area. One of the well-known applications is the double lasso method introduced by Victor Chernozhukov that can be used for variable selections. Moreover, lasso type applications are also used in time-series forecasting and graphical network analysis for dimension reductions. "],["forecasting.html", "Forecasting", " Forecasting Time series forecasting is a task that involves using a model to predict future values of a time series based on its past values. The data consists of sequences of values that are recorded at regular intervals over a period of time, such as daily stock prices or monthly weather data. Time series forecasting can be approached using a variety of machine learning techniques, including linear regression, decision trees, and neural networks. One key difference between time series forecasting and other types of machine learning tasks is the presence of temporal dependencies in the data. In time series data, the value at a particular time point is often influenced by the values that came before it, which means that the order in which the data points are presented is important. This can make time series forecasting more challenging, as the model must take into account the relationships between past and future values in order to make accurate predictions. One of the most accessible and comprehensive source on forecasting using R is Forecasting: Principles and Practice by Rob J Hyndman and George Athanasopoulos. The book now has the \\(3^{rd}\\) edition that uses the tsibble and fable packages rather than the forecast package. This brings a better integration to the tidyverse collection of packages. We will not summarize the book (FPP3) here nor use the same examples. However, we will use the tsibble and fable packages along with the fpp3 package. There is a paper by Wang et al. (2020) describing tsibble and the package in more details. A move from FPP2 to FPP3 brings a move from forecast to fable. The main difference is that fable is designed for tsibble objects and forecast is designed for ts objects. We cover five main topics: applications with ARIMA models, grid search for ARIMA, timr series embedding, random forest applications with time series data, and artificial neural network applications, RNN and LSTM. The time-series analysis and forecasting is a very deep and complex subject, which is beyond the scope of this book to cover in detail. FPP3 is free and very accessible even for those without any background on time-series forecasting. Therefore, this section assumes that some major concepts, like stationarity, time series decomposition, and exponential smoothing, are already checked and understood by further readings of FPP3. "],["arima-models.html", "Chapter 21 ARIMA models 21.1 Hyndman-Khandakar algorithm 21.2 TS Plots 21.3 Box-Cox transformation 21.4 Stationarity 21.5 Modeling ARIMA", " Chapter 21 ARIMA models ARIMA (Autoregressive Integrated Moving Average) is a statistical model for time series forecasting. It is a type of linear model that can be used to analyze and forecast data that exhibits temporal dependencies, such as seasonality and autocorrelation. The model is comprised of three components: Autoregressive (AR) component: This component models the dependencies between the current value of the time series and the past values. Integrated (I) component: This component component refers to the degree of differencing that is applied to the time series data. The degree of differencing is the number of times that the data is differenced in order to make it stationary meaning that the mean, variance, and covariance are constant over time. Moving average (MA) component: This component models the dependencies between the current value of the time series and the past forecast errors. The moving average component of an ARIMA model is used to capture the short-term fluctuations in the time series data that are not captured by the autoregressive component. For example, if the time series data exhibits random noise or sudden spikes, the moving average component can help to smooth out these fluctuations and improve the forecast accuracy. The ARIMA model can be written as ARIMA(p,d,q), where p is the order of the autoregressive component, d is the degree of differencing, and q is the order of the moving average component. The values of p, d, and q are chosen based on the characteristics of the time series data and the desired level of forecasting accuracy. To use the ARIMA model, the time series data must first be preprocessed to remove any trend and seasonality, and to ensure that the data is stationary (meaning that the mean and variance are constant over time). The model is then fit to the preprocessed data, and forecasts are generated based on the fitted model. ARIMA models can be used to make point forecasts (predictions for a specific time point) or interval forecasts (predictions with a range of possible values). The mathematical foundation of the ARIMA model is based on the concept of autoregressive (AR) and moving average (MA) processes. An autoregressive process is a type of stochastic process in which the current value of a time series depends on a linear combination of past values of the series. An autoregressive process can be represented mathematically as: \\[ X_{t} = c + \\sum_{i=1}^{p}(\\phi_{i} X_{t-i}) + \\epsilon_{t}, \\] where \\(X_{t}\\) is the value of the time series at time \\(t\\), \\(c\\) is a constant, \\(\\phi_{i}\\) is the autoregressive coefficient for lag \\(i\\), and \\(\\epsilon_{t}\\) is white noise (a sequence of random variables with a mean of zero and a constant variance). A moving average process is a type of stochastic process in which the current value of a time series depends on a linear combination of past errors or residuals (the difference between the actual value and the forecasted value). A moving average process can be represented mathematically as: \\[ X_{t} = c + \\sum_{i=1}^{q}(\\theta_{i} \\epsilon_{t-i}) + \\epsilon_{t}, \\] where \\(\\theta_{i}\\) is the moving average coefficient for lag \\(i\\), and $ _{t}$ is again white noise. The ARIMA model is a combination of an autoregressive process and a moving average process. It can be represented mathematically as: \\[ X_{t} = c + \\sum_{i=1}^{p}(\\phi_{i} X_{t-i}) + \\sum_{i=1}^{q}(\\theta_{i} \\epsilon_{t-i}) + \\epsilon_{t} \\] It is possible to write any stationary \\(\\operatorname{AR}(p)\\) model as an MA(\\(\\infty\\)) model by using repeated substitution. Here is the example for an \\(\\mathrm{AR}(1)\\) model without a constant: \\[ X_{t} = \\phi_{1} X_{t-1} + \\epsilon_{t} ~~~ \\text{and} ~~~ X_{t-1} = \\phi_{1} X_{t-2} + \\epsilon_{t-1}\\\\ X_{t}=\\phi_1\\left(\\phi_1 X_{t-2}+\\epsilon_{t-1}\\right)+\\epsilon_t\\\\ =\\phi_1^2 X_{t-2}+\\phi_1 \\epsilon_{t-1}+\\epsilon_t\\\\ =\\phi_1^3 X_{t-3}+\\phi_1^2 \\epsilon_{t-2}+\\phi_1 \\epsilon_{t-1}+\\epsilon_t\\\\ \\vdots \\] With \\(-1&lt;\\phi_1&lt;1\\), the value of \\(\\phi_1^k\\) will get smaller as \\(k\\) gets bigger. Therefore, \\(\\mathrm{AR}(1)\\) becomes an MA \\((\\infty)\\) process: \\[ X_t=\\epsilon_t+\\phi_1 \\epsilon_{t-1}+\\phi_1^2 \\epsilon_{t-2}+\\phi_1^3 \\epsilon_{t-3}+\\cdots, \\] The parameters of the ARIMA model (\\(c\\), \\(\\phi_{i}\\), \\(\\theta_{i}\\)) are estimated using maximum likelihood estimation (MLE), which involves finding the values of the parameters that maximize the likelihood of the observed data given the model. Once the model has been fit to the data, it can be used to make point forecasts (predictions for a specific time point) or interval forecasts (predictions with a range of possible values). In the ARIMA(p,d,q), the order of the autoregressive component (p) is the number of past values that are used to predict the current value of the time series, and the order of the moving average component (q) is the number of past errors or residuals that are used to predict the current value of the time series. Some common methods for selecting p and q include: Autocorrelation function (ACF) plot: This plot shows the correlations between the time series data and lagged versions of itself. A high positive autocorrelation at a lag of p suggests that p may be a good value for p. Partial autocorrelation function (PACF) plot: This plot shows the correlations between the time series data and lagged versions of itself, after accounting for the correlations at all lower lags. A high positive autocorrelation at a lag of q suggests that q may be a good value for q. Information criteria: There are several statistical measures that can be used to compare the goodness of fit of different ARIMA models, such as Akaike’s Information Criterion (AIC) and the Bayesian Information Criterion (BIC). These measures can be used to select the model with the lowest value, which is generally considered to be the best model. It’s important to note that determining the values of p and q is an iterative process, and you may need to try different values and evaluate the results in order to find the best fit for your data. 21.1 Hyndman-Khandakar algorithm The Hyndman-Khandakar algorithm (Hyndman &amp; Khandakar, 2008) combines several steps for modeling (and estimation) of the ARIMA model: unit root tests, minimization of the AICc, and MLE to obtain an ARIMA model. The arguments to ARIMA() in the fable package provide for many variations for modeling ARIMA. The modeling procedure to a set of (non-seasonal) time series data for ARIMA is defined in FPP3 as follows: Plot the data to identify any outliers. If the data shows variation that increases or decreases with the level of the series, transform the data (Box-Cox transformation) to stabilize the variance. Check if the data are non-stationary. And, make them stationary, if they are not. Start with an ARIMA \\((p, d, 0)\\) or ARIMA \\((0, d, q)\\) depending of what ACF/PACF indicates. Try your chosen model(s), and use the AICc to search for a better model. However, after step 5, the residuals from the chosen model are supposed to be white noise. Otherwise, the model has to be modified. Once the residuals look like white noise, the ARIMA model is ready for forecasting. We will show all these steps by using the epidemic curve of COVID-19 in Toronto covering 266 days between the March \\(1^{st}\\) and the November \\(21^{st}\\) of 2020. An epidemic curve (or epi curve) is a visual display of the onset of illness among cases associated with an outbreak. The data contain the first wave and the first part of the second wave. It is from Ontario Data Catalogue sorted by Episode Date, which is the date when the first symptoms were started. Our data set also contains the mobility data is from Facebook, all_day_bing_tiles_visited_relative_change, which is reflects positive or negative change in movement relative to baseline. 21.2 TS Plots Let’s first load the data and convert it to tsibble, which can be: library(tsibble) library(fpp3) load(&quot;~/Dropbox/ToolShed_draft/dftoronto.RData&quot;) day &lt;- seq.Date(from = as.Date(&quot;2020/03/01&quot;), to = as.Date(&quot;2020/11/21&quot;), by = 1) tdata &lt;- tibble(Day = day, mob = data$mob, cases = data$cases) toronto &lt;- tdata %&gt;% as_tsibble(index = Day) toronto ## # A tsibble: 266 x 3 [1D] ## Day mob cases ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2020-03-01 -0.0172 4 ## 2 2020-03-02 -0.0320 6 ## 3 2020-03-03 -0.0119 10 ## 4 2020-03-04 0.0186 7 ## 5 2020-03-05 0.0223 7 ## 6 2020-03-06 -0.00626 10 ## 7 2020-03-07 0.0261 8 ## 8 2020-03-08 0.0273 10 ## 9 2020-03-09 -0.0158 18 ## 10 2020-03-10 -0.0521 29 ## # … with 256 more rows Note the [1D] in the header indicating this is daily data. Dealing with daily and sub-daily data with ts class is not an easy process. The tsibble class handles such data with no problem. More details on tsibbles can be found at Tidy time series data using tsibbles. Although there are better plotting option cosmetically, we will stick to applications in what fpp3 simply offers: a &lt;- toronto %&gt;% autoplot(mob, col = &#39;blue&#39;) + labs( title = &quot;Mobility Index&quot;, subtitle = &quot;Toronto 2020&quot;, x = &quot;Days&quot;, y = &quot;Index&quot; ) b &lt;- toronto %&gt;% autoplot(cases, col = &#39;red&#39;) + labs( title = &quot;Covid-19 Cases&quot;, subtitle = &quot;Toronto 2020&quot;, x = &quot;Days&quot;, y = &quot;Cases&quot; ) require(gridExtra) grid.arrange(b, a, ncol=2) 21.3 Box-Cox transformation There are different ways to transform the data, which make the size of the variation about the same across the whole series. A proper variance-stabilizing transformation makes the forecasting model simpler and better. For example, Proietti and Lutkepohl (2012) find that the Box–Cox transformation produces forecasts which are significantly better than the untransformed data at the one-step-ahead horizon (See Does the Box–Cox transformation help in forecasting macroeconomic time series?). lmbd &lt;- toronto %&gt;% features(cases, features = guerrero) %&gt;% pull(lambda_guerrero) toronto %&gt;% autoplot(box_cox(cases, lambda = lmbd), col = &quot;red&quot;) + labs(y = &quot;&quot;, title = latex2exp::TeX(paste0( &quot;Cases - Transformed with $\\\\lambda$ = &quot;, round(lmbd,2)))) The option guerrero computes the optimal \\(\\lambda\\) value for a Box-Cox transformation using the Guerrero method. We should also have a transformation that converts the case numbers to a “positivity rate”, which is the percentage of positive results in all COVID-19 tests applied on a given day. We ignore this transformation for now, but the number of tests performed in a given day changes the numbers of cases. 21.4 Stationarity A time series is stationary if a shift in time does not cause a change in the shape of the distribution, like the mean, variance, and covariance. Stationarity is an important assumption in many time series forecasting methods, because the patterns and trends in the data can be modeled effectively. If a time series is not stationary, it can lead to inaccurate or unreliable forecasts. This is because non-stationary data has statistical properties that change over time making the current patterns and trends ungeneralizable for the future. There are several tests that can be used to determine whether a time series is stationary or not, including the Dickey-Fuller test and the KPSS (Kwiatkowski-Phillips-Schmidt-Shin) test. If a time series is found to be non-stationary, it may be necessary to transform the data in some way before applying a forecasting method in order to obtain reliable forecasts. The main method is differencing, which involves taking the difference between consecutive values in the series. Let’s first formally test all these series and see what we get: # number of first differences toronto %&gt;% features(cases, unitroot_ndiffs) ## # A tibble: 1 × 1 ## ndiffs ## &lt;int&gt; ## 1 1 # Formal KPSS test on level toronto %&gt;% features(cases, unitroot_kpss) ## # A tibble: 1 × 2 ## kpss_stat kpss_pvalue ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1.61 0.01 # Formal KPSS test on the first difference toronto %&gt;% mutate(diffcases = difference(cases)) %&gt;% features(diffcases, unitroot_kpss) ## # A tibble: 1 × 2 ## kpss_stat kpss_pvalue ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0970 0.1 It seems that the first difference can make the cases series stationary. The null in this test is taht the series are stationary. So, the p-value indicates that the null is rejected. So, it seems that the test after first differencing gives us a green light! However, ACF’s are telling us that seasonal differencing would be needed: level &lt;- toronto %&gt;% ACF(cases) %&gt;% autoplot() + labs(subtitle = &quot;Covid-19 Cases&quot;) fdiff &lt;- toronto %&gt;% ACF(difference(cases)) %&gt;% autoplot() + labs(subtitle = &quot;First-difference&quot;) diffbc &lt;- toronto %&gt;% ACF(difference(box_cox(cases, lmbd))) %&gt;% autoplot() + labs(subtitle = &quot;First-difference Box-Cox&quot;) ddiff &lt;- toronto %&gt;% ACF(difference(difference(box_cox(cases, lmbd)))) %&gt;% autoplot() + labs(subtitle = &quot;Double-difference Box-Cox&quot;) require(gridExtra) grid.arrange(level, fdiff, diffbc, ddiff, ncol = 2, nrow =2) From ACF’s, there seems to be a weekly seasonal pattern at 7, 14, and 21, which are Sundays: reported Covid-19 cases on Sundays tend to be lower than the rest of the week at least for during the first wave. We can also test if we need seasonal differencing: toronto %&gt;% features(cases, unitroot_nsdiffs) ## # A tibble: 1 × 1 ## nsdiffs ## &lt;int&gt; ## 1 0 # with Box-Cox toronto %&gt;% features(box_cox(cases, lmbd), unitroot_nsdiffs) ## # A tibble: 1 × 1 ## nsdiffs ## &lt;int&gt; ## 1 0 The feature unitroot_nsdiffs returns 0 for both original and transformed series indicating no seasonal difference is required. We will stick to this “advice” because of two reasons: first, an unnecessary differencing would create more problems than a solution;, second, we can also modify ARIMA to incorporate seasonalllty in the data, which we will see shortly . However, out of curiosity, let’s remove the “seemingly” weekly seasonality and see what happens to ACF’s. Since, the order of differencing is not important, we first applied the seasonal differencing then applied the first difference: toronto %&gt;% gg_tsdisplay(difference(box_cox(cases, lmbd), 7) %&gt;% difference(), plot_type=&#39;partial&#39;, lag=36) + labs(title = &quot;Seasonal &amp; first differenced&quot;, y=&quot;&quot;) We can calculate the strength of the trend (T) and seasonality (S) in the time series, \\(y_t=T_t+S_t+R_t\\), by \\[ F_{Trend}=\\max \\left(0,1-\\frac{\\operatorname{Var}\\left(R_t\\right)}{\\operatorname{Var}\\left(T_t+R_t\\right)}\\right),\\\\ F_{Seasonality}=\\max \\left(0,1-\\frac{\\operatorname{Var}\\left(R_t\\right)}{\\operatorname{Var}\\left(S_t+R_t\\right)}\\right), \\] where \\(R_t\\) is the remainder component: t &lt;- toronto %&gt;% features(cases, feat_stl) t(t[1:2]) ## [,1] ## trend_strength 0.9843102 ## seasonal_strength_week 0.5142436 Relative to \\(F_{Trend}\\), the seasonality is not robust in the data. So, our decision is to go with a simple first-differencing with Box-Cox transformation. However, we will look at the final predictive performance if the transformation provides any benefit. 21.5 Modeling ARIMA In his post, Forecasting COVID-19, Rob J Hyndman makes the following comment in March 2020: (…) the COVID-19 pandemic, it is easy to see why forecasting its effect is difficult. While we have a good understanding of how it works in terms of person-to-person infections, we have limited and misleading data. The current numbers of confirmed cases are known to be vastly underestimated due to the limited testing available. There are almost certainly many more cases of COVID-19 that have not been diagnosed than those that have. Also, the level of under-estimation varies enormously between countries. In a country like South Korea with a lot of testing, the numbers of confirmed cases are going to be closer to the numbers of actual cases than in the US where there has been much less testing. So we simply cannot easily model the spread of the pandemic using the data that is available. The second problem is that the forecasts of COVID-19 can affect the thing we are trying to forecast because governments are reacting, some better than others. A simple model using the available data will be misleading unless it can incorporate the various steps being taken to slow transmission. In summary, fitting simple models to the available data is pointless, misleading and dangerous. With our selection of the data, we do not intent to create another debate on forecasting COVID-19. There are hundreds of different forecasting models currently operational in a hub, The COVID-19 Forecast Hub, that can be used live. We will start with an automated algorithm ARIMA() that will allow a seasonal parameters: \\[ \\text { ARIMA }(p, d, q) \\times(P, D, Q) S \\] The first term is the non-seasonal part of ARIMA with \\(p=\\) AR order, \\(d=\\) non-seasonal differencing, \\(q=\\) MA order. The secon term is seasonal part of the model with \\(P=\\) seasonal AR order, \\(D=\\) seasonal differencing, \\(Q\\) = seasonal MA order, and \\(S=\\) seasonal pattern, which defines the number of time periods until the pattern repeats again. In our case, low values tend always to occur in some particular days, Sundays. Therefore, we may think that \\(\\mathrm{S}=7\\) is the span of the periodic seasonal behavior in our data. We can think of a seasonal first order autoregressive model, AR(1), would use \\(X_{t-7}\\) to predict \\(X_t\\). Likewise, a seasonal second order autoregressive model would use \\(X_{t-7}\\) and \\(X_{t-14}\\) to predict \\(X_t\\). A seasonal first order MA(1) model would use \\(\\epsilon_{t-7}\\) as a predictor. A seasonal second order MA(2) model would use \\(\\epsilon_{t-7}\\) and \\(\\epsilon_{t-14}\\). Let’s use our data first-differenced and transformed: toronto &lt;- toronto %&gt;% mutate(boxcases = box_cox(cases, lambda = lmbd)) toronto %&gt;% gg_tsdisplay(difference(boxcases), plot_type=&#39;partial&#39;) We look at the spikes and decays in ACF and PACF: a exponential decay in ACF is observed at seasonal spikes of 7, 14, and 21 as well as two spikes at 7 and 14 in PACF indicate seasonal AR(2) We will also add non-seasonal AR(2) due to 2 spikes in PACF at days 1 and 2. Here are our initial models: \\[ \\operatorname{ARIMA}(2,1,0)(2,1,0)_{7}\\\\ \\operatorname{ARIMA}(0,1,2)(0,1,3)_{7} \\] covfit &lt;- toronto %&gt;% model(AR2 = ARIMA(boxcases ~ pdq(2,1,0) + PDQ(3,1,0)), MA3 = ARIMA(boxcases ~ pdq(0,1,2) + PDQ(0,1,3)), auto = ARIMA(boxcases, stepwise=FALSE, approx=FALSE)) t(cbind(&quot;AR2&quot; = covfit$AR2, &quot;MA3&quot; = covfit$MA3, &quot;auto&quot; = covfit$auto)) ## [,1] ## AR2 ARIMA(2,1,0)(3,1,0)[7] ## MA3 ARIMA(0,1,2)(0,1,3)[7] ## auto ARIMA(2,1,1)(2,0,0)[7] glance(covfit) %&gt;% arrange(AICc) %&gt;% dplyr::select(.model:BIC) ## # A tibble: 3 × 6 ## .model sigma2 log_lik AIC AICc BIC ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MA3 0.468 -277. 567. 567. 588. ## 2 AR2 0.534 -285. 582. 583. 604. ## 3 auto 0.525 -289. 590. 591. 612. covfit %&gt;% dplyr::select(MA3) %&gt;% report() ## Series: boxcases ## Model: ARIMA(0,1,2)(0,1,3)[7] ## ## Coefficients: ## ma1 ma2 sma1 sma2 sma3 ## -0.4340 0.1330 -0.8617 -0.0573 -0.0809 ## s.e. 0.0648 0.0612 0.0827 0.0733 0.0600 ## ## sigma^2 estimated as 0.4684: log likelihood=-277.29 ## AIC=566.58 AICc=566.92 BIC=587.9 The ARIMA() function uses unitroot_nsdiffs() to determine \\(D\\) when it is not specified. Earlier, we run this function that suggested no seasonal differencing. All other parameters are determined by minimizing the AICc. AICc (Akaike’s Information Criterion with a correction for finite sample sizes) is similar to Akaike’s Information Criterion (AIC), but it includes a correction factor to account for the fact that the sample size may be small relative to the number of parameters in the model. This correction helps to reduce the bias in the AIC estimate and make it more accurate for small sample sizes. When the sample size is large, AIC and AICc are nearly equivalent and either one can be used. Although AICc values across the models are not comparable (for “auto”, as it has no seasonal differencing), it seems that our manually constructed ARIMA, \\(\\operatorname{ARIMA}(0,1,2)(0,1,3)_{7}\\) could also be an option. This brings the possibility of a grid search to our attention. Let’s check their residuals: rbind(augment(covfit) %&gt;% filter(.model == &quot;auto&quot;) %&gt;% features(.innov, ljung_box, lag=24, dof=5), augment(covfit) %&gt;% filter(.model == &quot;MA3&quot;) %&gt;% features(.innov, ljung_box, lag=24, dof=5), augment(covfit) %&gt;% filter(.model == &quot;AR2&quot;) %&gt;% features(.innov, ljung_box, lag=24, dof=5)) ## # A tibble: 3 × 3 ## .model lb_stat lb_pvalue ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 auto 19.0 0.459 ## 2 MA3 27.3 0.0971 ## 3 AR2 21.1 0.331 covfit %&gt;%dplyr::select(MA3) %&gt;% gg_tsresiduals(lag=36) There are several significant spikes in the ACF. But, the model passes the Ljung-Box test at the 5 percent significance level. Meanwhile, a model can still be used for forecasting, but the prediction intervals may not be accurate due to the correlated residuals. Sometimes it is possible that we cannot find a model that passes this test. In practice, we may have to look at the tradeoff between prediction accuracy and and reliable confidence intervals. If the difference is too high, we may chose the best model with the highest prediction accuracy. Before looking at a cross-validation approach for model selection in ARIMA modeling, let use our model to predict a week ahead (2020-11-22 to 2020-11-28): fc &lt;- covfit %&gt;% forecast(h = 7) fc ## # A fable: 21 x 4 [1D] ## # Key: .model [3] ## .model Day boxcases .mean ## &lt;chr&gt; &lt;date&gt; &lt;dist&gt; &lt;dbl&gt; ## 1 AR2 2020-11-22 N(12, 0.53) 12.1 ## 2 AR2 2020-11-23 N(13, 0.68) 13.3 ## 3 AR2 2020-11-24 N(13, 0.87) 12.8 ## 4 AR2 2020-11-25 N(13, 1.1) 12.8 ## 5 AR2 2020-11-26 N(12, 1.3) 12.2 ## 6 AR2 2020-11-27 N(12, 1.5) 12.3 ## 7 AR2 2020-11-28 N(12, 1.7) 11.5 ## 8 MA3 2020-11-22 N(12, 0.48) 12.4 ## 9 MA3 2020-11-23 N(13, 0.63) 13.2 ## 10 MA3 2020-11-24 N(13, 0.87) 13.1 ## # … with 11 more rows fc %&gt;% autoplot(toronto, level = NULL) + xlab(&quot;Days&quot;) + ylab(&quot;Transformed Cases with Box-Cox&quot;) a &lt;- forecast(covfit, h=7) %&gt;% filter(.model==&#39;auto&#39;) %&gt;% autoplot(toronto) + labs(title = &quot;COVID-19 Forecasting - Auto&quot;, y=&quot;Box-Cox Tranformed Cases&quot;) b &lt;- forecast(covfit, h=7) %&gt;% filter(.model==&#39;MA3&#39;) %&gt;% autoplot(toronto) + labs(title = &quot;COVID-19 Forecasting - MA3&quot;, y=&quot;Box-Cox Transformed Cases&quot;) require(gridExtra) grid.arrange(a, b, ncol=2) We have predicted values for coming 7 days but we do not have realized values. Hence, we cannot compare these models in terms of their accuracy. We now look at the forecast accuracy of these models by using a training set containing all data up to 2020-11-14. We then forecast the remaining seven days in the data set and compare the results with the actual values. train &lt;- toronto %&gt;% filter_index(~ &quot;2020-11-14&quot;) fit &lt;- train %&gt;% model( AR2 = ARIMA(boxcases ~ pdq(2,1,0) + PDQ(3,1,0)), MA3 = ARIMA(boxcases ~ pdq(0,1,2) + PDQ(0,1,3)), auto = ARIMA(boxcases, stepwise=FALSE, approx=FALSE) ) %&gt;% mutate(mixed = (auto + AR2 + MA3) / 3) And, now the accuracy measures: fc &lt;- fit %&gt;% forecast(h = 7) fc %&gt;% autoplot(toronto, level = NULL) accuracy(fc, toronto) ## # A tibble: 4 × 10 ## .model .type ME RMSE MAE MPE MAPE MASE RMSSE ACF1 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AR2 Test -1.57 1.88 1.57 -11.6 11.6 1.35 1.30 0.359 ## 2 auto Test -1.43 1.79 1.43 -10.7 10.7 1.23 1.24 0.428 ## 3 MA3 Test -1.61 1.91 1.61 -11.9 11.9 1.38 1.32 0.501 ## 4 mixed Test -1.54 1.86 1.54 -11.4 11.4 1.32 1.28 0.436 In all measures, the model “auto” (ARIMA with the Hyndman-Khandakar algorithm) is better than others. Although mixing several different ARIMA models does not make sense, we can have an ensemble forecast mixing several different time series models in addition ARIMA modeling. A nice discussion can be found in this post at Stackoverflow. Finally, it is always good to check ARIMA (or any time series forecasting) against the base benchmark. bfit &lt;- train %&gt;% model( ave = MEAN(boxcases), lm = TSLM(boxcases ~ trend()+season()) ) bfc &lt;- bfit %&gt;% forecast(h = 7) bfc %&gt;% autoplot(toronto, level = NULL) accuracy(bfc, toronto) ## # A tibble: 2 × 10 ## .model .type ME RMSE MAE MPE MAPE MASE RMSSE ACF1 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ave Test 4.59 4.72 4.59 31.8 31.8 3.94 3.26 0.507 ## 2 lm Test 2.07 2.32 2.07 14.1 14.1 1.77 1.60 0.516 The results shows our ARIMA model is doing much better job relative to a time-series linear model or a simple average As we discussed earlier in this book, there are basically two ways to select a best fitting predictive model: ex-post and ex-ante tools to penalize the overfitting. With AIC (Akaike Information Criterion) and BIC (Bayesian Information Criteria) measures, we can indirectly estimate the test (out-of-sample) error by making an adjustment to the training (in-sample) error to account for the bias due to overfitting. Therefore, these methods are ex-post tools to penalize the overfitting. The Hyndman-Khandakar algorithm uses this ex-post approach by selecting the best predictive ARIMA model with minimum AICc among alternatives. We can directly estimate the test error (out-sample) and choose the model that minimizes it. Instead of selecting a model with AICc, we can do it by tuning the parameters of ARIMA with a cross-validation approach so that the tuned model achieves the highest predictive accuracy. "],["grid-search-for-arima.html", "Chapter 22 Grid search for ARIMA", " Chapter 22 Grid search for ARIMA Before we apply a cross validation approach to choose the model that has the minimum test error (out-sample), we would like to do a grid search for a seasonal ARIMA with \\(d=1\\), \\(D=1\\), and \\(S=7\\). We will report two outcomes: AICc and RMSE (root mean squared error). #In-sample grid-search p &lt;- 0:3 q &lt;- 0:3 P &lt;- 0:3 Q &lt;- 0:2 comb &lt;- as.matrix(expand.grid(p,q,P,Q)) # We remove the unstable grids comb &lt;- as.data.frame(comb[-1, ]) ind &lt;- which(comb$Var1==0 &amp; comb$Var2==0, arr.ind = TRUE) comb &lt;- comb[-ind, ] row.names(comb) &lt;- NULL colnames(comb) &lt;- c(&quot;p&quot;, &quot;q&quot;, &quot;P&quot;, &quot;Q&quot;) aicc &lt;- c() RMSE &lt;- c() for(k in 1:nrow(comb)){ tryCatch({ fit &lt;- toronto %&gt;% model(ARIMA(boxcases ~ 0 + pdq(comb[k,1],1,comb[k,2]) + PDQ(comb[k,3],1,comb[k,4]))) wtf &lt;- fit %&gt;% glance res &lt;- fit %&gt;% residuals() aicc[k] &lt;- wtf$AICc RMSE[k] &lt;- sqrt(mean((res$.resid)^2)) }, error=function(e){}) } cbind(comb[which.min(aicc),], &quot;AICc&quot; = min(aicc, na.rm = TRUE)) ## p q P Q AICc ## 75 3 3 0 1 558.7747 cbind(comb[which.min(RMSE),], &quot;RMSE&quot; = min(RMSE, na.rm = TRUE)) ## p q P Q RMSE ## 165 3 3 2 2 0.6482865 Although we set the ARIMA without a constant, we could extend the grid with a constant. We can also add a line (ljung_box) that extracts and reports the Ljung-Box test for each model. We can then select the one that has a minimum AICc and passes the test. We do not need this grid search as the Hyndman-Khandakar algorithm for automatic ARIMA modelling (ARIMA()) in fable is able to do it for us very effectively (except for the Ljung-Box test for each model). The Hyndman-Khandakar algorithm, selecting an ARIMA model with the minimum AICc for forecasting, is an ex-post process. In practice, we can apply a similar grid search with cross validation for selecting the best model that has the minimum out-of-sample prediction error without checking if it passes the Ljung-Box test or not. Here is a simple example: #In-sample grid-search p &lt;- 0:3 q &lt;- 0:3 P &lt;- 0:3 Q &lt;- 0:2 comb &lt;- as.matrix(expand.grid(p,q,P,Q)) # We remove the unstable grids comb &lt;- as.data.frame(comb[-1, ]) ind &lt;- which(comb$Var1==0&amp;comb$Var2==0, arr.ind = TRUE) comb &lt;- comb[-ind,] row.names(comb) &lt;- NULL colnames(comb) &lt;- c(&quot;p&quot;, &quot;q&quot;, &quot;P&quot;, &quot;Q&quot;) train &lt;- toronto %&gt;% filter_index(~ &quot;2020-11-14&quot;) RMSE &lt;- c() for(k in 1:nrow(comb)){ tryCatch({ amk &lt;- train %&gt;% model(ARIMA(boxcases ~ 0 + pdq(comb[k,1],1,comb[k,2]) + PDQ(comb[k,3],1,comb[k,4]))) %&gt;% forecast(h = 7) %&gt;% accuracy(toronto) RMSE[k] &lt;- amk$RMSE }, error=function(e){}) } cbind(comb[which.min(RMSE),], &quot;RMSE&quot; = min(RMSE, na.rm = TRUE)) ## p q P Q RMSE ## 12 0 3 0 0 0.7937723 g &lt;- which.min(RMSE) toronto %&gt;% model(ARIMA(boxcases ~ 0 + pdq(comb[g,1],1,comb[g,2]) + PDQ(comb[g,3],1,comb[g,4]))) %&gt;% forecast(h = 7) %&gt;% autoplot(toronto, level = NULL) We will not apply h-step-ahead rolling-window cross-validations, which can be found in the post, Time series cross-validation using fable, by Hyndman (2021). However, when we have multiple competing models, we may not want to compare their predictive accuracy by looking at their error rates using only few out-of-sample observations. If we use, however, rolling windows or continuously expanding windows, we can effectively create a large number of days tested within the data. "],["time-series-embedding.html", "Chapter 23 Time Series Embedding 23.1 VAR for Recursive Forecasting 23.2 Embedding for Direct Forecast", " Chapter 23 Time Series Embedding In general, forecasting models use either direct or recursive forecasting, or their combinations (See Taieb and Hyndman, 2012). The difference between these two methods is related to discussion on prediction accuracy and forecasting variance. As we see below, recursive forecasting requires a parametric model and would face increasing forecasting error when the underlying model is not linear. Direct forecasting, however, can be achieved by a nonparametric predictive algorithm, while it may have a higher variance as the forecast horizon gets longer. Multi-period recursive forecasting use a single time series model, like AR(1). With iterative substitutions of the estimated model, any forecast period of \\(h\\) can be computed. Let’s start with a simple AR(1) to see recursive forecasting: \\[ x_{t+1}=\\alpha_0+\\phi_1 x_t+\\epsilon_{t} \\] If we use this AR(1) to have a 3-period forecast: \\[ \\hat{x}_{t+1}=\\hat{\\alpha}_0+\\hat{\\phi}_1 x_t, \\\\ \\hat{x}_{t+2}=\\hat{\\alpha}_0+\\hat{\\phi}_1 \\hat{x}_{t+1}, \\\\ \\hat{x}_{t+3}=\\hat{\\alpha}_0+\\hat{\\phi}_1 \\hat{x}_{t+2} \\] With iterative substitutions: \\[ \\hat{x}_{t+1}=\\hat{\\alpha}_0+\\hat{\\phi}_1 x_t ~~~~ 1^{st} ~ \\text{Period}\\\\ \\hat{x}_{t+2}=\\hat{\\alpha}_0+\\hat{\\alpha}_0\\hat{\\alpha}_1+\\hat{\\phi}^2_1 x_{t} ~~~~ 2^{nd} ~ \\text{Period}\\\\ \\hat{x}_{t+3}=\\hat{\\alpha}_0+\\hat{\\alpha}_0\\hat{\\alpha}_1+\\hat{\\alpha}_0\\hat{\\alpha}^2_1+\\hat{\\phi}^3_1 x_t~~~~ 3^{rd} ~ \\text{Period} \\] Of course, we can generalize it for \\(h\\) periods: \\[ \\hat{x}_{t+h}=\\hat{\\alpha}_0 \\sum_{i=1}^h \\hat{\\phi}_1^{i-1}+\\hat{\\phi}_1^h x_t \\] The estimated coefficients (\\(\\hat{\\alpha}_0\\), \\(\\hat{\\phi}_1\\)) are the same; hence, we need only one model for any period. Alternatively, we can apply the direct multi-period forecasting, where a separate predictive model for each forecasting horizon between \\(h\\) and \\(t\\) is estimated. Here is the example with AR(1): \\[ x_{t+1}=\\alpha_0+\\alpha_1 x_t+\\epsilon_{t}, \\\\ x_{t+2}=\\beta_0+\\beta_1 x_t+\\epsilon_{t}, \\\\ x_{t+3}=\\omega_0+\\omega_1 x_t+\\epsilon_{t}. \\\\ \\] And, the 3-period direct forecasts with three different models: \\[ \\hat{x}_{t+1}=\\hat{\\alpha}_0+\\hat{\\alpha}_1 x_t ~~~~ 1^{st} ~ \\text{Period}\\\\ \\hat{x}_{t+2}=\\hat{\\beta}_0+\\hat{\\beta}_1 x_{t} ~~~~ 2^{nd} ~ \\text{Period}\\\\ \\hat{x}_{t+3}=\\hat{\\omega}_0+\\hat{\\omega}_1x_t~~~~ 3^{rd} ~ \\text{Period} \\] 23.1 VAR for Recursive Forecasting The problem with a multi-period recursive forecasting becomes clear when we have multivariate model: \\[ y_{t+1}=\\beta_0+\\beta_1 y_t+\\beta_2x_t+\\epsilon_{t} \\] If we want a 2-period forecast, \\[ \\hat{y}_{t+2}=\\hat{\\beta}_0+\\hat{\\beta}_1 \\hat{y}_{t+1}+\\hat{\\beta}_2 \\hat{x}_{t+1}, \\] Hence, \\(\\hat{x}_{t+1}\\) has to be estimated. This can be done with a Vector Autorregressive (VAR) framework. A VAR model consists of multiple equations, one per variable. Each equation includes a constant and lags of all of the variables in the system. \\[ \\begin{aligned} &amp; y_{t}=c_1+\\beta_{1} y_{t-1}+\\beta_{2} x_{t-1}+\\varepsilon_{t} \\\\ &amp; x_{t}=c_2+\\phi_{1} x_{t-1}+\\phi_{2} y_{t-1}+e_{t} \\end{aligned} \\] Each model is estimated using the principle of ordinary least squares, given that series are stationary. Forecasts in VAR are calculated with recursive iterations. Therefore, the set of equations generates forecasts for each variable. To decide the number of lags in each equation, the BIC is used Let’s have our COVID-19 data and include the mobility to forecasting library(tsibble) library(fpp3) load(&quot;~/Dropbox/ToolShed_draft/dftoronto.RData&quot;) day &lt;- seq.Date(from = as.Date(&quot;2020/03/01&quot;), to = as.Date(&quot;2020/11/21&quot;), by = 1) tdata &lt;- tibble(Day = day, mob = data$mob, cases = data$cases) toronto &lt;- tdata %&gt;% as_tsibble(index = Day) toronto ## # A tsibble: 266 x 3 [1D] ## Day mob cases ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2020-03-01 -0.0172 4 ## 2 2020-03-02 -0.0320 6 ## 3 2020-03-03 -0.0119 10 ## 4 2020-03-04 0.0186 7 ## 5 2020-03-05 0.0223 7 ## 6 2020-03-06 -0.00626 10 ## 7 2020-03-07 0.0261 8 ## 8 2020-03-08 0.0273 10 ## 9 2020-03-09 -0.0158 18 ## 10 2020-03-10 -0.0521 29 ## # … with 256 more rows We will estimate the recursive forecasts for 1 to 14 days ahead. # We need make series stationary trdf &lt;- toronto %&gt;% mutate(diffcases = difference(cases), diffmob = difference(mob)) # VAR with BIC fit &lt;- trdf[-1,] %&gt;% model( VAR(vars(diffcases, diffmob), ic = &quot;bic&quot;) ) glance(fit) ## # A tibble: 1 × 6 ## .model sigma2 log_lik AIC AICc BIC ## &lt;chr&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 &quot;VAR(vars(diffcases, diffmob), ic = \\&quot;bic\\… &lt;dbl[…]&gt; -854. 1755. 1760. 1841. fit %&gt;% report() ## Series: diffcases, diffmob ## Model: VAR(5) ## ## Coefficients for diffcases: ## lag(diffcases,1) lag(diffmob,1) lag(diffcases,2) lag(diffmob,2) ## -0.4074 -105.6524 -0.0703 11.0374 ## s.e. 0.0639 28.3643 0.0695 29.9761 ## lag(diffcases,3) lag(diffmob,3) lag(diffcases,4) lag(diffmob,4) ## 0.0528 10.8093 -0.0123 -4.8989 ## s.e. 0.0701 31.8601 0.0713 30.0019 ## lag(diffcases,5) lag(diffmob,5) ## 0.0227 6.1099 ## s.e. 0.0640 29.2678 ## ## Coefficients for diffmob: ## lag(diffcases,1) lag(diffmob,1) lag(diffcases,2) lag(diffmob,2) ## 0e+00 -0.314 0e+00 -0.4688 ## s.e. 1e-04 0.057 1e-04 0.0603 ## lag(diffcases,3) lag(diffmob,3) lag(diffcases,4) lag(diffmob,4) ## 1e-04 -0.2931 -1e-04 -0.2664 ## s.e. 1e-04 0.0641 1e-04 0.0603 ## lag(diffcases,5) lag(diffmob,5) ## 3e-04 -0.4059 ## s.e. 1e-04 0.0588 ## ## Residual covariance matrix: ## diffcases diffmob ## diffcases 811.6771 -0.1648 ## diffmob -0.1648 0.0033 ## ## log likelihood = -853.64 ## AIC = 1755.28 AICc = 1760.38 BIC = 1840.73 fit %&gt;% forecast(h=14) %&gt;% autoplot(trdf[-c(1:200),]) We should have transformed both series by the Box-Cox transformation, but we ignored it above. 23.2 Embedding for Direct Forecast For direct forecasting, we need to rearrange the data in a way that we can estimate 7 models for forecasting ahead each day of 7 days. We will use embed() function to show what we mean with rearranging data for AR(3), for example: Y &lt;- 1:10 Y &lt;- embed(Y, 3) colnames(Y)=c(&quot;Y(t)&quot;,&quot;Y(t-1)&quot;,&quot;Y(t-2)&quot;) Y ## Y(t) Y(t-1) Y(t-2) ## [1,] 3 2 1 ## [2,] 4 3 2 ## [3,] 5 4 3 ## [4,] 6 5 4 ## [5,] 7 6 5 ## [6,] 8 7 6 ## [7,] 9 8 7 ## [8,] 10 9 8 Now the key point is there is no a temporal dependence between each row so that shuffling this data after re-structuring it admissible. Let’s have an AR(1) example on this simulated data # Stationary data rho &lt; 1 but = 0.85 n &lt;- 10000 rho &lt;- 0.85 y &lt;- c(0, n) set.seed(345) eps &lt;- rnorm(n, 0, 1) for(j in 1:(n-1)) { y[j+1] &lt;- y[j]*rho + eps[j] } ylagged &lt;- y[2:n] par(mfrow=c(1,2)) plot(ylagged, y[1:(n-1)], col = &quot;lightpink&quot;, ylab = &quot;y&quot;, xlab = &quot;y(t-1)&quot;) plot(y[1:500], type = &quot;l&quot;, col = &quot;red&quot;, ylab = &quot;y&quot;, xlab = &quot;t&quot;) We will use an AR(1) estimation with OLS after embedding: y_em &lt;- embed(y, 2) head(y) ## [1] 0.0000000 -0.7849082 -0.9466863 -0.9661413 -1.1118166 -1.0125757 colnames(y_em) &lt;- c(&quot;yt&quot;, &quot;yt_1&quot;) head(y_em) ## yt yt_1 ## [1,] -0.7849082 0.0000000 ## [2,] -0.9466863 -0.7849082 ## [3,] -0.9661413 -0.9466863 ## [4,] -1.1118166 -0.9661413 ## [5,] -1.0125757 -1.1118166 ## [6,] -1.4942098 -1.0125757 And estimation of AR(1) with OLS: y_em &lt;- as.data.frame(y_em) ar1 &lt;- lm(yt ~ yt_1 - 1, y_em) ar1 ## ## Call: ## lm(formula = yt ~ yt_1 - 1, data = y_em) ## ## Coefficients: ## yt_1 ## 0.8496 Now, let’s shuffle y_em: # Shuffle ind &lt;- sample(nrow(y_em), nrow(y_em), replace = FALSE) y_em_sh &lt;- y_em[ind, ] ar1 &lt;- lm(yt ~ yt_1 - 1, y_em_sh) ar1 ## ## Call: ## lm(formula = yt ~ yt_1 - 1, data = y_em_sh) ## ## Coefficients: ## yt_1 ## 0.8496 This proves the temporal independence across the observations in the rearranged data. This is important because the temporal order in the time series data would not affect the cross-validation process or bootstrapping applications in a grid search anymore. When we have this freedom, we can use all conventional machine learning applications on time series data, like random forests, which we see in the next chapter. This re-arrangement can also be applied to multivariate data sets: tsdf &lt;- matrix(c(1:10, 21:30), nrow = 10) colnames(tsdf) &lt;- c(&quot;Y&quot;, &quot;X&quot;) first &lt;- embed(tsdf, 3) colnames(first) &lt;- c(&quot;y(t)&quot;,&quot;x(t)&quot;,&quot;y(t-1)&quot;,&quot;x(t-1)&quot;, &quot;y(t-2)&quot;, &quot;x(t-2)&quot;) head(first) ## y(t) x(t) y(t-1) x(t-1) y(t-2) x(t-2) ## [1,] 3 23 2 22 1 21 ## [2,] 4 24 3 23 2 22 ## [3,] 5 25 4 24 3 23 ## [4,] 6 26 5 25 4 24 ## [5,] 7 27 6 26 5 25 ## [6,] 8 28 7 27 6 26 Now, we need to have three models for three forecasting horizons. Here are these models: \\[ \\hat{y}_{t+1}=\\hat{\\alpha}_0+\\hat{\\alpha}_1 y_t + \\hat{\\alpha}_2 y_{t-1}+ \\hat{\\alpha}_3 x_t + \\hat{\\alpha}_4 x_{t-1}+ \\hat{\\alpha}_5 x_{t-2} ~~~~ 1^{st} ~ \\text{Period}\\\\ \\hat{y}_{t+2}=\\hat{\\beta}_0+\\hat{\\beta}_1 y_t + \\hat{\\beta}_2 y_{t-1}+ \\hat{\\beta}_3 x_t + \\hat{\\beta}_4 x_{t-1}+ \\hat{\\beta}_5 x_{t-2} ~~~~ 2^{nd} ~ \\text{Period}\\\\ \\hat{y}_{t+3}=\\hat{\\omega}_0+\\hat{\\omega}_1 y_t + \\hat{\\omega}_2 y_{t-1}+ \\hat{\\omega}_3 x_t + \\hat{\\omega}_4 x_{t-1}+ \\hat{\\omega}_5 x_{t-2} ~~~~ 3^{rd} ~ \\text{Period} \\] Each one of these models requires a different rearrangement in the data. Here are the required arrangement for each model: ## y(t) x(t) y(t-1) x(t-1) y(t-2) x(t-2) ## [1,] 3 23 2 22 1 21 ## [2,] 4 24 3 23 2 22 ## [3,] 5 25 4 24 3 23 ## [4,] 6 26 5 25 4 24 ## [5,] 7 27 6 26 5 25 ## [6,] 8 28 7 27 6 26 ## y(t) x(t-1) y(t-2) x(t-2) y(t-3) x(t-3) ## [1,] 4 23 2 22 1 21 ## [2,] 5 24 3 23 2 22 ## [3,] 6 25 4 24 3 23 ## [4,] 7 26 5 25 4 24 ## [5,] 8 27 6 26 5 25 ## [6,] 9 28 7 27 6 26 ## y(t) x(t-2) y(t-3) x(t-3) y(t-4) x(t-4) ## [1,] 5 23 2 22 1 21 ## [2,] 6 24 3 23 2 22 ## [3,] 7 25 4 24 3 23 ## [4,] 8 26 5 25 4 24 ## [5,] 9 27 6 26 5 25 ## [6,] 10 28 7 27 6 26 We already rearranged the data for the first model. if we remove the first row in y(t) and the last row in the remaining set, we can get the the data for the second model: cbind(first[-1,1], first[-nrow(first),-1]) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 4 23 2 22 1 21 ## [2,] 5 24 3 23 2 22 ## [3,] 6 25 4 24 3 23 ## [4,] 7 26 5 25 4 24 ## [5,] 8 27 6 26 5 25 ## [6,] 9 28 7 27 6 26 ## [7,] 10 29 8 28 7 27 We will use our COVID-19 data and a simple linear regression as an example of direct forecasting: # Preparing data df &lt;- data.frame(dcases = trdf$diffcases, dmob = trdf$diffmob) df &lt;- df[complete.cases(df), ] rownames(df) &lt;- NULL df &lt;-as.matrix(df) head(df) ## dcases dmob ## [1,] 2 -0.01480 ## [2,] 4 0.02013 ## [3,] -3 0.03049 ## [4,] 0 0.00367 ## [5,] 3 -0.02854 ## [6,] -2 0.03232 Now we need to decide on two parameters: the window size, that is, how many lags will be included in each row; and how many days we will forecast. The next section will use more advance functions for re-arranging the data and apply the direct forecasting with random forests. For now, let’s use a 3-day window and a 3-day forecast: h = 3 w = 3 fh &lt;- c() # storage for forecast # Start with first dt &lt;- embed(df, w) y &lt;- dt[,1] X &lt;- dt[,-1] for (i in 1:h) { fit &lt;- lm(y ~ X) l &lt;- length(fit$fitted.values) fh[i] &lt;- fit$fitted.values[l] y &lt;- y[-1] X &lt;- X[-nrow(X),] } fh ## [1] 10.288416 -11.587090 0.302522 plot(1:266, trdf$diffcases, col = &quot;red&quot;, type = &quot;l&quot;) lines(267:269, fh, col=&quot;blue&quot;) We haven’t used training and test sets above. If we apply a proper splitting, we can even set the window size as our hyperparameter to minimize the forecast error: # We set the last 7 days as our test set train &lt;- df[1:258, ] test &lt;- df[-c(1:258), ] h = 7 w &lt;- 3:14 # a grid for window size fh &lt;- matrix(0, length(w), h) rownames(fh) &lt;- w colnames(fh) &lt;- 1:7 for(s in 1:length(w)){ dt &lt;- embed(train, w[s]) y &lt;- dt[,1] X &lt;- dt[,-1] for (i in 1:h) { fit &lt;- lm(y ~ X) fh[s,i] &lt;- last(fit$fitted.values) y &lt;- y[-1] X &lt;- X[-nrow(X),] } } fh ## 1 2 3 4 5 6 ## 3 -4.292862 -6.288479 5.2727764 10.692206 22.133103 -0.5252184 ## 4 -5.014668 -1.626752 8.2861736 23.982849 4.611554 -0.2773355 ## 5 -1.125996 1.634917 20.7212780 6.767507 5.115816 -0.5577792 ## 6 1.533541 14.584416 5.6832803 8.066816 4.937718 -6.8419291 ## 7 13.228621 1.612629 7.3973443 7.980486 -1.484987 -5.3696924 ## 8 2.812780 3.308271 7.6799879 1.589578 -1.265470 -9.6077196 ## 9 -5.430448 1.811491 0.7675925 1.698785 -7.123733 -16.9647249 ## 10 -5.488847 -4.382922 0.8842250 -4.199708 -14.615359 -13.8839491 ## 11 -11.104866 -4.133680 -5.3274242 -11.510596 -11.935885 -18.5728995 ## 12 -11.656935 -8.289153 -11.9044832 -9.515252 -16.534428 -16.8239307 ## 13 -18.314269 -13.292359 -9.2157517 -14.330746 -15.341226 -13.0680709 ## 14 -23.661938 -10.963027 -13.9621680 -12.855445 -11.683527 -12.3975126 ## 7 ## 3 -19.79742 ## 4 -19.62517 ## 5 -26.29534 ## 6 -23.77712 ## 7 -20.07199 ## 8 -27.04771 ## 9 -25.44710 ## 10 -30.22356 ## 11 -29.91304 ## 12 -25.62393 ## 13 -25.15019 ## 14 -27.72488 Rows in fh show the 7-day forecast for each window size. We can see which window size is the best: rmspe &lt;- c() for(i in 1: nrow(fh)){ rmspe[i] &lt;- sqrt(mean((fh[i,]-test)^2)) } rmspe ## [1] 33.45400 35.28827 31.67333 29.69115 31.57618 28.99568 28.53882 28.70796 ## [9] 27.16182 28.59872 28.77714 28.99870 which.min(rmspe) ## [1] 9 We used the last 7 days in our data as our test set and previous days as our training set. A natural question would be whether we could shuffle the data and use any 7 days as our test set? The answer is yes, because we do not need to follow a temporal order in the data after rearranging it with embedding. This is important because we can add a bootstrapping loop to our grid search above and get better tuning for finding the best window size. We incorporate all these ideas with our random forest application in the next chapter. "],["random-forest-1.html", "Chapter 24 Random Forest 24.1 Univariate 24.2 Multivariate 24.3 Rolling and expanding windows", " Chapter 24 Random Forest We will utilize embedding for direct forecasting with Random Forests. We choose the random forests algorithm because it does not need an explicit tuning by a grid search. In the practice, however, we can still search for the number of trees and the number of variables randomly sampled as candidates at each split. Let’s get our COVID-19 data: library(tsibble) library(fpp3) load(&quot;~/Dropbox/ToolShed_draft/toronto2.rds&quot;) day &lt;- seq.Date(from = as.Date(&quot;2020/03/01&quot;), to = as.Date(&quot;2020/11/21&quot;), by = 1) tdata &lt;- tibble(Day = day, data[,-1]) toronto2 &lt;- tdata %&gt;% as_tsibble(index = Day) toronto2 ## # A tsibble: 266 x 8 [1D] ## Day cases mob delay male age temp hum ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2020-03-01 4 -0.0172 36.8 0.75 55 -4.2 65.5 ## 2 2020-03-02 6 -0.0320 8.5 1 45 3.8 84 ## 3 2020-03-03 10 -0.0119 15 0.7 54 2.3 90 ## 4 2020-03-04 7 0.0186 25.7 0.286 50 3.35 71 ## 5 2020-03-05 7 0.0223 21 0.429 48.6 1.2 63.5 ## 6 2020-03-06 10 -0.00626 13.1 0.5 36 0.04 75 ## 7 2020-03-07 8 0.0261 10.4 0.5 46.2 -1.65 54 ## 8 2020-03-08 10 0.0273 11.6 0.9 50 6.3 56 ## 9 2020-03-09 18 -0.0158 8.89 0.611 35.6 12.5 55 ## 10 2020-03-10 29 -0.0521 9.69 0.448 41.7 5.15 79 ## # … with 256 more rows As before, the data contain the first wave and the initial part of the second wave in Toronto for 2020. It is from Ontario Data Catalogue sorted by episode dates (Day), which is the date when the first symptoms were started. The mobility data is from Facebook, all_day_bing_tiles_visited_relative_change, which is reflects positive or negative change in movement relative to baseline. The other variables related to tests are delay, which is the time between test results and the episode date, the gender distribution of people is given by male, age shows the average age among tested people any given day. The last two variables, temp and hum, show the daily maximum day temperature and the average outdoor humidity during the day, respectively. Except for age all other variables are non-stationary. We will take their first difference and make the series stationary before we proceed. df &lt;- toronto2 %&gt;% mutate(dcases = difference(cases), dmob = difference(mob), ddelay = difference(delay), dmale = difference(male), dtemp = difference(temp), dhum = difference(hum)) dft &lt;- df[ ,-c(2:5,7,8)] #removing levels dft &lt;- dft[-1, c(1,3:7,2)] # reordering the columns Let’s first use a univariate setting for a single-window forecasting, which is the last 7 days. 24.1 Univariate We will not have a grid search on the random forest algorithm, which could be added to the following script: library(randomForest) h = 7 w &lt;- 3:21 # a grid for window size fh &lt;- matrix(0, length(w), h) rownames(fh) &lt;- w colnames(fh) &lt;- 1:h for(s in 1:length(w)){ dt &lt;- as.data.frame(embed(as.matrix(dft[ ,2]), w[s])) test_ind = nrow(dt) - (h) train &lt;- dt[1:test_ind, ] test &lt;- dt[-c(1:test_ind), ] y &lt;- train[ ,1] X &lt;- train[ ,-1] for (i in 1:h) { fit &lt;- randomForest(X, y) fh[s, ] &lt;- predict(fit, test[ ,-1]) y &lt;- y[-1] X &lt;- X[-nrow(X), ] } } fh ## 1 2 3 4 5 6 7 ## 3 -15.9234333 8.674467 16.80702 -5.516478 -2.665583 15.1997667 1.656136 ## 4 -12.1391000 -5.546633 19.06582 -3.354100 -0.781700 0.7203905 3.027567 ## 5 -0.5163333 -7.000667 30.40157 -9.348467 -1.751900 -8.8061667 12.010333 ## 6 4.7688333 -8.129314 24.36007 -11.748200 17.193033 -8.4335000 18.690967 ## 7 1.0963667 -15.134067 29.55220 -14.437467 19.655000 -21.4036333 14.804667 ## 8 9.4242333 -23.274567 44.91027 -11.082867 15.167933 -17.3076667 28.031933 ## 9 -8.8280333 -34.704467 63.34503 -21.473367 12.320333 -27.7585667 16.435400 ## 10 -8.7552000 -35.269467 66.01573 -25.050200 15.561733 -23.4471333 10.459300 ## 11 0.0473000 -30.091833 65.44997 -25.430833 16.551233 -18.7945333 11.521900 ## 12 4.2920667 -30.847700 56.32973 -23.433933 16.762100 -16.7179000 14.347600 ## 13 -2.4865333 -30.588533 52.20083 -23.308600 10.000033 -17.6130667 8.027267 ## 14 -2.5725000 -29.501833 54.57207 -22.387400 13.898833 -10.7203667 9.691200 ## 15 -5.8711000 -33.123400 59.61483 -20.856933 14.605767 -7.3618000 13.209600 ## 16 -8.8194000 -36.204400 56.20417 -23.577100 13.916733 -10.8225667 11.797167 ## 17 -9.2253667 -34.702633 59.10270 -25.113500 15.856067 -15.2575333 13.861200 ## 18 -6.0064000 -31.865800 56.80097 -23.461433 12.593400 -8.8054000 13.980500 ## 19 -8.1841333 -30.077667 60.42817 -23.972900 13.934367 -12.5083000 10.801400 ## 20 -9.0513000 -29.037767 58.41837 -25.682367 15.712033 -15.0894667 13.966800 ## 21 -8.3895333 -28.543033 60.40127 -21.189900 9.592267 -13.4498333 14.317333 We can now see RMSPE for each row (window size): actual &lt;- test[, 1] rmspe &lt;- c() for (i in 1:nrow(fh)) { rmspe[i] &lt;- sqrt(mean((fh[i, ] - actual)^2)) } rmspe ## [1] 42.90267 43.74960 44.27032 45.50069 44.84153 51.95157 51.61115 50.98179 ## [9] 50.43206 50.16100 47.74427 48.95038 51.59196 50.63836 50.58698 50.30308 ## [17] 49.61772 49.12016 50.09223 which.min(rmspe) ## [1] 1 And, if we plot several series of our forecast with different window sizes: plot(actual, type = &quot;l&quot;, col = &quot;red&quot;, ylim = c(-80, 50), ylab = &quot;Actual (red) vs. Forecasts&quot;, xlab = &quot;Last 7 days&quot;, main = &quot;7-Day Foerecasts&quot;, lwd = 3) lines(fh[1, ], type = &quot;l&quot;, col = &quot;blue&quot;) lines(fh[2, ], type = &quot;l&quot;, col = &quot;green&quot;) lines(fh[5, ], type = &quot;l&quot;, col = &quot;orange&quot;) lines(fh[12, ], type = &quot;l&quot;, col = &quot;black&quot;) legend(&quot;bottomright&quot;, title = &quot;Lags&quot;, legend = c(&quot;3-day&quot;, &quot;4-day&quot;, &quot;7-day&quot;, &quot;14-day&quot;), col = c(&quot;blue&quot;, &quot;green&quot;, &quot;orange&quot;), lty = c(1, 1, 1, 1, 1), bty = &quot;o&quot;, cex = 0.75) It seems that, as the window size gets larger, the forecast becomes increasingly smooth, perhaps missing the short term dynamics. Another observation is that, although “blue” (3-day window) has the minimum RMSPE, it is not able to capture ups and downs relative to 7-day or 14-day windows. 24.2 Multivariate Can we increase the prediction accuracy with additional predictors? library(randomForest) h = 7 w &lt;- 3:14 # a grid for window size fh &lt;- matrix(0, length(w), h) rownames(fh) &lt;- w colnames(fh) &lt;- 1:h for(s in 1:length(w)){ dt &lt;- as.data.frame(embed(as.matrix(dft[ ,-1]), w[s])) test_ind = nrow(dt) - (h) train &lt;- dt[1:test_ind, ] test &lt;- dt[-c(1:test_ind), ] y &lt;- train[ ,1] X &lt;- train[ ,-1] for (i in 1:h) { fit &lt;- randomForest(X, y) fh[s, ] &lt;- predict(fit, test[ ,-1]) y &lt;- y[-1] X &lt;- X[-nrow(X), ] } } fh ## 1 2 3 4 5 6 7 ## 3 -19.616100 -2.8396333 15.76550 -8.219767 -16.4661333 7.949800 -3.080133 ## 4 -24.775700 -0.1640333 16.82507 -9.264933 -14.5296667 3.325567 -3.893867 ## 5 -15.648333 2.4859667 17.33100 -9.950767 -7.5546333 -10.245533 0.793400 ## 6 -11.680233 1.5806667 21.59943 -11.161267 -0.5577333 -12.481567 4.542567 ## 7 -14.020267 -4.3378333 20.33187 -19.613567 3.3353000 -16.100300 7.316267 ## 8 -7.641467 -16.5848000 27.67567 -12.794867 6.0683333 -13.786733 10.812467 ## 9 -10.453500 -22.4751667 48.13513 -23.511633 9.6593000 -19.240467 9.976533 ## 10 -7.424200 -18.4726000 51.79417 -22.836133 7.9836667 -18.590167 10.097967 ## 11 -9.056833 -17.6223000 46.04070 -23.502033 12.5686000 -21.852333 11.043567 ## 12 -10.829933 -17.5915000 43.96557 -23.946867 13.2478333 -19.329100 6.901733 ## 13 -8.425000 -18.0482000 47.88200 -24.631767 11.1274333 -18.260600 9.713367 ## 14 -8.772400 -18.7200333 41.89373 -23.411867 9.4375333 -19.075533 6.366667 actual &lt;- test[,1] rmspe &lt;- c() for (i in 1:nrow(fh)) { rmspe[i] &lt;- sqrt(mean((fh[i,]-actual)^2)) } rmspe ## [1] 42.33777 40.84328 38.90279 39.42948 38.40943 44.11712 45.14647 45.10846 ## [9] 43.96055 42.98153 44.06632 42.85867 which.min(rmspe) ## [1] 5 plot(actual, type = &quot;l&quot;, col = &quot;red&quot;, ylim = c(-80, +50), ylab = &quot;Actual (red) vs. Forecasts&quot;, xlab = &quot;Last 7 days&quot;, main = &quot;7-Day Foerecasts&quot;, lwd = 3) lines(fh[1, ], type = &quot;l&quot;, col = &quot;blue&quot;) lines(fh[3, ], type = &quot;l&quot;, col = &quot;green&quot;) lines(fh[5, ], type = &quot;l&quot;, col = &quot;orange&quot;) lines(fh[12, ], type = &quot;l&quot;, col = &quot;black&quot;) legend(&quot;bottomright&quot;, title = &quot;Lags&quot;, legend = c(&quot;3-day&quot;, &quot;5-day&quot;, &quot;7-day&quot;, &quot;14-day&quot;), col = c(&quot;blue&quot;, &quot;green&quot;, &quot;orange&quot;, &quot;black&quot;), lty = c(1, 1, 1, 1, 1), bty = &quot;o&quot;, cex = 0.75) It seems that additional predictors do increase the accuracy. Again, relative to the best model (5-day window) our 7-day window correctly captures most ups and downs in the forecast. Now, a visual inspection shows that all RMSPE’s are lower than the univariate forecasts. We would tend to conclude that this is because of the new predictors, specially mobility, temperature, and humidity. But, the COVID-19 data have many well-known issues related to measurement inaccuracies. As a side note, we need to test if those differences are statistical significant or not (i.e. Diebold-Mariano Test). 24.3 Rolling and expanding windows A seven-day window is not enough for a reliable judgment on the forecast accuracy. One way to deal with this issue is to use rolling or expanding windows to predict the next h days. The following example shows a 1-day-ahead forecast with varying lags for embedding. library(randomForest) l = 3:10 # lags for embedding ws = 150 # size of each rolling window rmspe &lt;- c() all_fh &lt;- vector(mode = &quot;list&quot;, length = length(l)) all_y &lt;- vector(mode = &quot;list&quot;, length = length(l)) for(s in 1:length(l)) { dt &lt;- as.data.frame(embed(as.matrix(dft[, -1]), l[s])) nwin &lt;- nrow(dt) - ws #number of windows fh &lt;- c() y &lt;- c() for(i in 1:nwin){ train &lt;- dt[i:(ws + i - 1), ] # each loop, window moves one day forward test &lt;- dt[(ws + i), ] set.seed(i+s) fit &lt;- randomForest(train[, -1], train[, 1]) fh[i] &lt;- predict(fit, test[, -1]) y[i] &lt;- test[, 1] # to use later for plotting } all_y[[s]] &lt;- y all_fh[[s]] &lt;- fh err &lt;- test[, 1] - fh rmspe[s] &lt;- sqrt(mean(err^2)) } rmspe ## [1] 45.17990 44.74564 45.36820 45.07520 45.89481 46.96887 46.98404 46.80637 bst &lt;- which.min(rmspe) l[bst] # Winning lag in embedding ## [1] 4 To change the application above to an expanding-window forecast, we just need to change dt[i:(ws + i - 1), ] to dt[1:(ws + i - 1), ] in the script. Now, we can plot the results: par(mfrow=c(1,2)) plot(all_y[[bst]], type = &quot;l&quot;, col=&quot;red&quot;, ylab = &quot;Actual (red) vs Predicted (Blue)&quot;, xlab = &quot;Days&quot;, main = &quot;1-Day-Ahead&quot;) lines(all_fh[[bst]], col=&quot;blue&quot;) plot(all_y[[bst]][60:110], type = &quot;o&quot;, col=&quot;red&quot;, ylab = &quot;Actual (red) vs Predicted (Blue)&quot;, xlab = &quot;Days&quot;, main = &quot;Last 50 Days&quot;) lines(all_fh[[bst]][60:110], col=&quot;blue&quot;) Getting the predicted values back to originals can be achieved by: \\[ \\begin{aligned} &amp; y_{t+1}=y_t+z_{t+1} \\\\ &amp; y_{t+2}=y_{t+1}+z_{t+2}=y_t+z_{t+1}+z_{t+2} \\end{aligned} \\] set.seed(321) y &lt;- rnorm(10) z &lt;- diff(y) # first differences back &lt;- cumsum(c(y[1], z)) cbind(y, back) ## y back ## [1,] 1.7049032 1.7049032 ## [2,] -0.7120386 -0.7120386 ## [3,] -0.2779849 -0.2779849 ## [4,] -0.1196490 -0.1196490 ## [5,] -0.1239606 -0.1239606 ## [6,] 0.2681838 0.2681838 ## [7,] 0.7268415 0.7268415 ## [8,] 0.2331354 0.2331354 ## [9,] 0.3391139 0.3391139 ## [10,] -0.5519147 -0.5519147 Since our algorithm predict the changes in observations, a simple sum would do the job for back transformation. For example, as a starting point, our algorithm predicts the change in \\(Y\\) from day 156 to 157 (window size 150 plus the best lag window, 6). When we add this predicted change to the actual \\(Y\\) at 156, it will give us the back-transformed forecast at day 157. y &lt;- df$cases # The first forecast is at ws (150) + l[best] (6) + 1, which is 157 # The first actual Y should start a day earlier # removing all Y&#39;s until ws+l[bst] y_a_day_before &lt;- y[-c(1:(ws+l[bst]-1))] # This adds predicted changes to observed values a day earlier back_forecast &lt;- head(y_a_day_before, -1) + all_fh[[bst]] # Actual Y&#39;s in the test set starting at ws (150) + l[best] (6) + 1, which is 157 ytest &lt;- y[-c(1:(ws+l[bst]))] plot(ytest, type = &quot;l&quot;, col = &quot;blue&quot;, ylab = &quot;Actual Y (Blue) vs Forecast (Red)&quot;, xlab = &quot;Days&quot;, main = &quot;Back-transformed Forecast&quot; ) lines(back_forecast, type = &quot;l&quot;, col = &quot;red&quot;) It seems that, for most days, our algorithm simply forecasts the next day by using the value from a day before. If we change our algorithm to a 7-day-ahead forecast, this would be different. This is also a common problem when the predictive model has a poor forecasting power. Again, this is not due to our algorithm, but forecasting an epi curve with imperfect test data is almost impossible job, as we highlighted earlier. In practice, however, there are several ways that we can improve the scripts above. For example, we can consider the (rolling or expanding) window size as a hyperparameter. We can also have an explicit training for the Random Forest algorithm. We can have an ensemble forecasting by adding other predictive algorithms to the script, like boosting. Further, we can develop a base forecast that would give us a benchmark to see how much our algorithm improves against that base. Moreover, we could apply a transformation to the data in order to stabilize the variance in all variables. "],["recurrent-neural-networks.html", "Chapter 25 Recurrent Neural Networks 25.1 Keras 25.2 Input Tensors 25.3 Plain RNN 25.4 LSTM", " Chapter 25 Recurrent Neural Networks Recurrent neural networks (RNNs) are a type of artificial neural network that are particularly well-suited for processing sequential data, such as time series, natural language, and speech. They are called “recurrent” because they perform the same task for every element in a sequence, with the output being dependent on the previous computations. The idea of using neural networks to process sequential data dates back to the 1980s, but it wasn’t until the late 1990s that RNNs began to see widespread use. One of the key developments in this period was the use of long short-term memory (LSTM) units, which are a type of “memory” cell that can store information for long periods of time and help prevent the vanishing gradients problem that can occur when training RNNs. The RNN processes the input data using a series of “neurons”. Each neuron receives input from other neurons and from the input data, processes them using an activation function, and sends the output to other neurons as an input or to the final output of the network. Hence, the output of a neuron at a given time step is used as the input to the same neuron at the next time step, allowing the network to incorporate information from previous time steps into its current computation. The RNN process can be illustrated as follows: The network above is designed such that it takes input \\(X_t\\) sequentially. Each \\(X_t\\) feeds into a hidden layer that has a vector of activation functions, \\(A_t\\). Except for the first starting point, each activation function also feeds into the next activation function, \\(A_{t+1}\\), sequentially. The connection between each activation function (\\(h\\) - hidden state) reflects the fact that RNN uses the last period’s prediction as an input in the next period. The weight vectors are denoted \\(a = \\{\\omega_0, \\omega_1\\}\\), \\(b = \\{\\theta_1\\}\\), and \\(c = \\{\\beta_0, \\beta_1\\}\\), as expressed below: \\[ \\begin{aligned} &amp; A_1=g\\left(\\omega_0+\\omega_1 X_1\\right)\\\\ &amp; A_2=g\\left(\\omega_0+\\omega_1 X_2+\\theta_1 A_1\\right) \\\\ &amp; =g\\left(\\omega_0+\\omega_1 X_2+\\theta_1 g\\left(\\omega_0+\\omega_1 X_1\\right)\\right) \\\\ &amp; A_3=g\\left(\\omega_0+\\omega_1 X_3+\\theta_1 A_2\\right) \\\\ &amp; =g\\left(\\omega_0+\\omega_1 X_3+\\theta_1 g\\left(\\omega_0+\\omega_1 X_2+\\theta_1 g\\left(\\omega_0+\\omega_1 X_1\\right)\\right)\\right. \\\\ &amp; ~~~~~~~~~~~~~~~~\\vdots\\\\ &amp; A_t=g\\left(\\omega_0+\\omega_1 X_t+\\theta_1 A_{t-1}\\right) \\end{aligned} \\] Note that weights are the same in each sequence. Although each output layer produces a prediction, the final output is the network’s prediction. \\[ Y_{t}=\\beta_0+ \\beta_k A_{t} \\] In case of multiple inputs at time \\(t\\), \\(X_{t}=\\left(X_{t1}, X_{t2}, \\ldots, X_{tp}\\right)\\), and multiple units (\\(k\\)) in the hidden layer, \\(A_t = \\left(A_{t1}, A_{t2}, \\ldots, A_{tk}\\right)\\), the network at time \\(t\\) becomes: \\[ A_{k, t}=g\\left(\\omega_{k 0}+\\sum_{j=1}^p \\omega_{k j} X_{t j}+\\sum_{v=1}^k \\theta_{k v} A_{v,t-1}\\right). \\] For example, for two units and two variables, \\(A_{k,t}\\) will be \\[ A_{1,t}=g\\left(\\omega_{10}+ \\omega_{1 1} X_{t,1}+\\omega_{1 2} X_{t,2}+ \\theta_{1 1} A_{1,t-1}+\\theta_{1 2} A_{2,t-1}\\right),\\\\ A_{2,t}=g\\left(\\omega_{20}+ \\omega_{2 1} X_{t, 1}+\\omega_{2 2} X_{t,2}+ \\theta_{2 1} A_{1,t-1}+\\theta_{2 2} A_{2,t-1}\\right) \\] and the output \\(O_{\\ell}\\) is computed as \\[ Y_{t}=\\beta_0+\\sum_{k=1}^2 \\beta_k A_{k,t} \\] 25.1 Keras We will use the Keras deep-learning framework (https://keras.rstudio.com) and the package keras, which provides high-level building blocks for developing deep-learning models. Keras operates on several tensor libraries to tensor manipulations and differentiation, one of which is TensorFlow. Tensors are simply multidimensional arrays, which are a generalization of vectors and matrices to an arbitrary number of dimensions. For example, vectors are 1D tensors, matrices are used for 2D tensors, and arrays (which support any number of dimensions) are used for multi-dimensional objects. Keras works on CPUs, but the most efficient implementations of Keras use NVIDIA GPUs and properly configured CUDA and cuDNN libraries. For CPU-based installation of Keras, which is what we use in this chapter, we suggest the following steps after installing the keras package. # Sys.unsetenv (&quot;RETICULATE_PYTHON&quot;) # remotes : sinstall_github(&quot;Istudio/reticulate&quot;) # reticulate::install_miniconda() # keras::install_keras () The best source using Keras for artificial neural network projects with R is “Deep Learning with R” by Chollet and Allaire. In this section, we will use the keras package (on CPU) for two main time series applications: RNN and LSTM. Let’s set up our COVID-19 data and standardize each of the variables. library(tsibble) library(fpp3) load(&quot;~/Dropbox/ToolShed_draft/toronto2.rds&quot;) toronto2 &lt;- data df &lt;- toronto2 %&gt;% mutate(dcases = difference(cases), dmob = difference(mob), ddelay = difference(delay), dmale = difference(male), dtemp = difference(temp), dhum = difference(hum)) dft &lt;- df[ ,-c(2:5,7,8)] #removing levels dft &lt;- dft[-1, c(3:7,2)] # reordering the columns sdtf &lt;- scale(dft) # head(sdtf) ## dcases dmob ddelay dmale dtemp age ## 2 0.04202890 -0.21389272 -7.6496254 2.16845790 2.4818892 0.5144024 ## 3 0.10622289 0.30023017 1.8050246 -2.58211378 -0.4756078 1.6374603 ## 4 -0.11845609 0.45271551 2.9516317 -3.56924556 0.3182466 1.1383235 ## 5 -0.02216510 0.05796098 -1.2461163 1.24302186 -0.6779629 0.9600603 ## 6 0.07412590 -0.41612714 -2.1128735 0.62606450 -0.3697605 -0.6086555 ## 7 -0.08635909 0.47965067 -0.7048789 0.00910714 -0.5347577 0.6703827 There are four stages in developing ANN models in Keras: Preparing the training set with input tensors and target tensors; Defining the model, that is a network of layers; Choosing the learning parameters: a loss function, an optimizer, and some metrics to monitor And finally fitting this model to the training set 25.2 Input Tensors We will define a three dimensional array that contains time series data. First, let’s see an array: # array x1 = c(1, 2, 3) x2 = c(4, 5, 6, 7, 8, 9) adata &lt;- array(c(x1, x2), dim = c(3,3,2)) dim(adata) ## [1] 3 3 2 adata ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 adata[1,,] ## [,1] [,2] ## [1,] 1 1 ## [2,] 4 4 ## [3,] 7 7 Now, we create our data matrix: # Data toydata &lt;- matrix(c(1:100, 101:200, 201:300), 100) colnames(toydata) &lt;- c(&quot;y&quot;, &quot;x1&quot;, &quot;x2&quot;) head(toydata) ## y x1 x2 ## [1,] 1 101 201 ## [2,] 2 102 202 ## [3,] 3 103 203 ## [4,] 4 104 204 ## [5,] 5 105 205 ## [6,] 6 106 206 Suppose that this is daily data and we try to make 1-day-ahead predictions. In preparing the input tensor, we need to decide how many earlier days we need to predict the next day’s value. Suppose that we decide on 5 days. As we seen before, we transform the data by embedding to a new structure: datam &lt;- embed(toydata, 6) datam &lt;- datam[, -c(2:3)] head(datam) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] ## [1,] 6 5 105 205 4 104 204 3 103 203 2 102 202 1 ## [2,] 7 6 106 206 5 105 205 4 104 204 3 103 203 2 ## [3,] 8 7 107 207 6 106 206 5 105 205 4 104 204 3 ## [4,] 9 8 108 208 7 107 207 6 106 206 5 105 205 4 ## [5,] 10 9 109 209 8 108 208 7 107 207 6 106 206 5 ## [6,] 11 10 110 210 9 109 209 8 108 208 7 107 207 6 ## [,15] [,16] ## [1,] 101 201 ## [2,] 102 202 ## [3,] 103 203 ## [4,] 104 204 ## [5,] 105 205 ## [6,] 106 206 The second line in the code above removes the contemporaneous features. We should have \\(100 - 5 = 95\\) samples, in each one we have 3 features and 5 timesteps. The first two samples, each is a matrix of \\(5 \\times 3\\), are shown below: ## [,1] [,2] [,3] ## [1,] 1 101 201 ## [2,] 2 102 202 ## [3,] 3 103 203 ## [4,] 4 104 204 ## [5,] 5 105 205 ## [,1] [,2] [,3] ## [1,] 2 102 202 ## [2,] 3 103 203 ## [3,] 4 104 204 ## [4,] 5 105 205 ## [5,] 6 106 206 The outcome variable \\(y\\) is 6 and 7 in the first and second samples. Let’s see how we can manipulate our embedded data datam to achieve it: n &lt;- nrow(datam) f1 &lt;- data.matrix(datam[, -1]) # Removing Y f2 &lt;- array(f1, c(n, 3, 5)) f2[1,,] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 5 4 3 2 1 ## [2,] 105 104 103 102 101 ## [3,] 205 204 203 202 201 We need reverse the order f3 &lt;- f2[,, 5:1] f3[1,,] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 2 3 4 5 ## [2,] 101 102 103 104 105 ## [3,] 201 202 203 204 205 And, taking the transposition, t(f3[1,,]) ## [,1] [,2] [,3] ## [1,] 1 101 201 ## [2,] 2 102 202 ## [3,] 3 103 203 ## [4,] 4 104 204 ## [5,] 5 105 205 For the whole array of datam, we use array transposition: f4 &lt;- aperm(f3, c(1, 3, 2)) f4[1,,] ## [,1] [,2] [,3] ## [1,] 1 101 201 ## [2,] 2 102 202 ## [3,] 3 103 203 ## [4,] 4 104 204 ## [5,] 5 105 205 Now, we are ready to apply all these steps to our toy data with a function: tensorin &lt;- function(l, x){ maxl = l+1 xm &lt;- embed(x, maxl) xm &lt;- xm[, -c(2:3)] n &lt;- nrow(xm) f1 &lt;- data.matrix(xm[, -1]) y &lt;- xm[, 1] f2 &lt;- array(f1, c(n, ncol(x), l)) f3 &lt;- f2[,, l:1] f4 &lt;- aperm(f3, c(1, 3, 2)) list(f4, y) } tensored &lt;- tensorin(5, toydata) X &lt;- tensored[1] y &lt;- tensored[2] X[[1]][1,,] ## [,1] [,2] [,3] ## [1,] 1 101 201 ## [2,] 2 102 202 ## [3,] 3 103 203 ## [4,] 4 104 204 ## [5,] 5 105 205 y[[1]][1] ## [1] 6 Note that this type of data transformation can be achieved several different ways. We can apply it to our COVID-19 data for 7-day windows: trnt &lt;- tensorin(7, sdtf) X &lt;- trnt[1] y &lt;- trnt[2] X[[1]][1,,] ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] -2.58211378 -0.4756078 1.6374603 0.04202890 -0.21389272 -7.6496254 ## [2,] -3.56924556 0.3182466 1.1383235 0.10622289 0.30023017 1.8050246 ## [3,] 1.24302186 -0.6779629 0.9600603 -0.11845609 0.45271551 2.9516317 ## [4,] 0.62606450 -0.3697605 -0.6086555 -0.02216510 0.05796098 -1.2461163 ## [5,] 0.00910714 -0.5347577 0.6703827 0.07412590 -0.41612714 -2.1128735 ## [6,] 3.46406836 2.4663234 1.1383235 -0.08635909 0.47965067 -0.7048789 ## [7,] -2.48614263 1.9215213 -0.6641151 0.04202890 0.02204745 0.3698224 y[[1]][1] ## [1] 0.2346109 Obviously, our choice of \\(l\\) (7) is arbitrary and should be decided with a proper validation. 25.3 Plain RNN As we have the input tensor stored as an array of (258, 7, 6), we are ready to design our network for an RNN with one layer with 24 hidden units (neurons): library(keras) model &lt;- keras_model_sequential() %&gt;% layer_simple_rnn(units = 24, input_shape = list(7, 6), dropout = 0.1, recurrent_dropout = 0.1) %&gt;% layer_dense(units = 1) %&gt;% compile(optimizer = optimizer_rmsprop(), loss = &quot;mse&quot;) As before, neural networks consist of layers and neurons in each layer. Since we use sequence data stored in 3D tensors of shape (samples, timesteps, features) we will use recurrent layers for our RNN. The term layer_dense is the output layer. We also (arbitrarily) specify two types of dropout for the units feeding into the hidden layer. The first one is set for the input feeding into a layer. The second one is for the previous hidden units feeding into the same layer. One of the tools to fight with overfitting is randomly removing inputs to a layer. Similar to Random Forest, this dropping out process has the effect of generating a large number of networks with different network structure and, in turn, breaking the possible correlation between the inputs that the layers are exposed to. These “dropped out” inputs may be variables in the data sample or activations from a previous layer. This is a conventional regularization method to in ANN but how this can be applied to sequential data is a complex issue. Every recurrent layer in Keras has two dropout-related arguments: dropout, a float specifying the dropout rate for input units of the layer, and recurrent_dropout, specifying the dropout rate of the recurrent units. These are again additions to our hyperparameter grid. It has the effect of simulating a large number of networks with very different network structure and, in turn, making nodes in the network generally more robust to the inputs. Before fitting the model, we need to split the data. We have 258 observations in total. We will take the last 50 observations as our test set: dim(X[[1]]) ## [1] 258 7 6 train &lt;- 1:208 test &lt;- 208:dim(X[[1]])[1] And, finally we fit our RNN. There are two hyperparameters that Keras use in fitting RNN: batch size and epoch. They are both related to how and how many times the weights in the network will be updated The batch size is the number of observations (“samples”) used in its gradient descent to update its internal parameters. For example, a conventional (batch) gradient descent uses the entire data in one batch so that the batch size would be the number of samples in the data. The stochastic gradient descent, on the other hand, uses randomly selected each observation. While the batch gradient descent is efficient (fast) it is not as robust as the stochastic gradient descent. Therefore, Keras uses a mini-batch gradient descent as a parameter that balance the between efficiency and robustness. The number of epochs is the number of times the algorithm works trough the complete training dataset. We need multiple passes through the entire data because updating the weights with gradient descent in a single pass (one epoch) is not enough. But, when the number of epochs goes up, the algorithm updates the weights more. As a result, the curve goes from underfitting (very few runs) to overfitting (too many runs). Hence, these two parameters, batch size and epoch, should be set as hyperparameters. Note that we pick arbitrary numbers below. model %&gt;% fit( X[[1]][train,, ], y[[1]][train], batch_size = 12, epochs = 75, validation_data = list(X[[1]][test,, ], y[[1]][test]), verbose = 0 ) %&gt;% plot() # prediction y_act &lt;- y[[1]][test] var_y &lt;- var(y_act) yhat &lt;- model %&gt;% predict(X[[1]][test,, ]) 1 - mean((yhat -y_act)^2) / var_y # R^2 ## [1] 0.2491995 sqrt(mean((yhat -y_act)^2)) # RMSPE ## [1] 1.495303 Although it could be done easily as we shown in the previous chapter, we will not back-transform the predictions to levels. Here is the plot for the last 50 days: plot(y[[1]][test], type =&quot;l&quot;, col = &quot;blue&quot;, ylab = &quot;Actual (Blue) vs. Prediction (Red)&quot;, xlab = &quot;Last 50 Days&quot;, main = &quot;RNN Forecasting for Covid-19 Cases - in differences&quot;) lines(yhat, col = &quot;red&quot;, lwd = 2) It looks like, our RNN without a proper training is capturing most ups and downs correctly. There are three groups of hyperparameters that we need to search by validation: How many days we need in the past to predict the next day’s value? (we picked 7), The number of units per layer (we picked 24), Regularization parameters, dropout and recurrent_dropout (we picked 0.1 for both), Stochastic gradient descent parameters, batch_size and epochs (we picked 12 and 75) All these parameters that we picked arbitrarily should be selected by a proper validation. Model tuning in ANN highly depends on the package we use in deep learning. Keras with TensorFlow is one the top AI engines available for all type of networks. The best source for learning more on deep learning using Keras is “Deep Learning with R” by Chollet and Allaire. 25.4 LSTM One issue with RNN is that, although it is able to retain information trough time by its recurrent network, it quickly forgets long-term dependencies. This problem is called the vanishing gradient problem and can be easily seen here: \\[ \\begin{aligned} &amp; A_3=g\\left(\\omega_0+\\omega_1 X_3+\\theta_1 A_2\\right) \\\\ &amp; =g\\left(\\omega_0+\\omega_1 X_3+\\theta_1 g\\left(\\omega_0+\\omega_1 X_2+ \\theta_1 g\\left(\\omega_0+\\omega_1 X_1\\right)\\right)\\right. \\\\ \\end{aligned} \\] Although this has only two iterations, the function has \\(\\theta_1^2\\) (if \\(g()\\) is ReLu). For example, if \\(\\theta_1 = 0.5\\), the effect of \\(A_5\\) on \\(A_1\\) will be regulated by \\(\\theta_1^4 = 0.0625\\). We can argue that \\(\\theta_1\\) could be one (random walk). But, the first differencing will usually remove the unit root in the data \\(\\theta_1\\) will be bounded between 0 and 1. If we only need to look at recent information to predict the present, RNN would be fine even with this problem. But, the gap between the relevant information to predict the present could be very large and RNN quickly forgets those long-term dependencies. Long Short-Term Memory network (LSTM) layers in RNN are designed to solve this problem. Similar to RNN, LSTMs also have a chain-like structure, but the repeating activation module has a different structure. Unlike RNN, which has only a single neural network layer in the repeating activation modules, the LSTM activation module has four interacting each other. The key difference between LSTM and RNN is the cell state \\(C_t\\) (the horizontal red line). The cell state functions like a conveyor belt and each LSTM repeating module is able to add to and remove from this belt through three gates, , as shown. This figure shows how LSTM works. We have three gates as numbered in the figure (G1, G2, and G3). Each gate is regulated by a sigmoid neural net layer (\\(\\frac{1}{1+e^{-x}}\\)), which outputs numbers between zero and one. Hence, it works like a regulator or “gate keeper”. The first gate (G1) is the Forget Gate, the first layer of the four layers, which takes \\(H_{t-1}\\) and \\(X_t\\) into a sigmoid function, \\[ f_t=\\sigma\\left(w_f \\cdot\\left[H_{t-1}, X_t\\right]+b_f\\right) \\] and produces a number between 0 and 1. This percentage reflects the degree of \\(C_{t-1}\\) that will be forgotten. For example, if it is zero, nothing in \\(C_{t-1}\\) will be let through on the belt (cell state). It is interesting to note that this degree, how much of the long-term information will be kept, is determined by the recent information (\\(H_{t-1}\\), \\(X_t\\)). That is, if the recent information is very relevant for the prediction, the network will tune this sigmoid function so that the output will be a percentage close to 0, which will reduce the effect of the long-term information in the past, \\(C_{t-1}\\), on prediction. The second gate (G2), the Input Gate, uses the same inputs, \\(H_{t-1}\\) and \\(X_t\\), but has two layers. The first later is again a sigmoid function that works as a gate keeper. The second layer is a tanh function (\\(\\tanh x=\\frac{e^x-e^{-x}}{e^x+e^{-x}}\\)) that produces a number between \\(-1\\) and \\(+1\\). The objective of this layer is to update cell state \\(C_{t-1}\\) by adding \\(\\tilde{C_{t}}\\), which contains the recent information hidden in \\(H_{t-1}\\) and \\(X_t\\). This process happens in two steps: \\[ \\begin{aligned} i_t &amp; =\\sigma\\left(w_i \\cdot\\left[H_{t-1}, X_t\\right]+b_i\\right) \\\\ \\tilde{C}_t &amp; =\\tanh \\left(w_\\tilde{C} \\cdot\\left[H_{t-1}, X_t\\right]+b_\\tilde{C}\\right) \\end{aligned} \\] The first step, \\(i_t\\), is a sigmoid function, hence a “gate keeper”. We already get it in the first layer with different weights: \\(f_t=\\sigma\\left(w_f \\cdot\\left[h_{t-1}, x_t\\right]+b_f\\right)\\). The second later, tanh function, produces the information (\\(h_{t-1}, x_t\\)) in a candidate value normalized between \\(-1\\) and \\(+1\\). When the network multiplies \\(\\tilde{C_{t}}\\) with \\(i_t\\) (\\(i_t \\times \\tilde{C_{t}}\\)), this new candidate value between \\(-1\\) and \\(+1\\) will be scaled by \\(i_t\\) that reflects how much the network would like to update \\(C_{t-1}\\): \\[ C_t=f_t \\times C_{t-1}+i_t \\times \\tilde{C}_t \\] While the first two gates are about regulating the cell state (\\(C_t\\)), the last one (G3) is the Output Gate. The prediction at time \\(t\\), \\(H_t\\), has two inputs: \\(C_t\\) and the recent information, \\(H_{t-1}\\) and \\(X_t\\). The output gate will decide how it will balance between these two sources and produce \\(H_t\\): \\[ \\begin{aligned} o_t &amp; =\\sigma\\left(w_o\\left[H_{t-1}, X_t\\right]+b_o\\right) \\\\ H_t &amp; =o_t \\times \\tanh \\left(C_t\\right) \\end{aligned} \\] Note that the tanh activation in the output function could be changed depending on the type of network we build. The LSTM network that we described so far is a conceptual one. In practice, however, there are many different variants of LSTM. One of them is called the Gated Recurrent Unit (GRU) introduced by Cho, et al. (2014). The details of GRU is beyond this book. But, after understanding the structure of LSTM networks, GRU should not be difficult to grasp. One of the accessible sources to learn different types of RNN is blog posts by Christopher Olah. Now, we return to the application of LSTM to our COVID-19 data. We use the “Adam” optimization algorithm, which is an extension to stochastic gradient descent and works with LSTM very well. Below, the code shows an arbitrary network with LSTM. model = keras_model_sequential() %&gt;% layer_lstm(units=128, input_shape = c(7, 6), activation=&quot;relu&quot;) %&gt;% layer_dense(units=64, activation = &quot;relu&quot;) %&gt;% layer_dense(units=32) %&gt;% layer_dense(units=16) %&gt;% layer_dense(units=1, activation = &quot;linear&quot;) model %&gt;% compile(loss = &#39;mse&#39;, optimizer = &#39;adam&#39;, metrics = list(&quot;mean_absolute_error&quot;) ) %&gt;% summary() ## Model: &quot;sequential_1&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## lstm (LSTM) (None, 128) 69120 ## dense_4 (Dense) (None, 64) 8256 ## dense_3 (Dense) (None, 32) 2080 ## dense_2 (Dense) (None, 16) 528 ## dense_1 (Dense) (None, 1) 17 ## ================================================================================ ## Total params: 80,001 ## Trainable params: 80,001 ## Non-trainable params: 0 ## ________________________________________________________________________________ model %&gt;% fit(X[[1]][train,, ], y[[1]][train], batch_size = 12, epochs = 75, validation_data = list(X[[1]][test,, ], y[[1]][test]), verbose = 0 ) %&gt;% plot() yhat &lt;- predict(model, X[[1]][test,, ]) y_act &lt;- y[[1]][test] var_y &lt;- var(y_act) 1 - mean((yhat -y_act)^2) / var_y # R^2 ## [1] -0.0434925 sqrt(mean((yhat -y_act)^2)) # RMSPE ## [1] 1.762835 plot(y[[1]][test], type =&quot;l&quot;, col = &quot;blue&quot;, ylab = &quot;Actual (Blue) vs. Prediction (Red)&quot;, xlab = &quot;Last 50 Days&quot;, main = &quot;LSTM Forecasting for Covid-19 Cases&quot;, lwd = 1) lines(yhat, col = &quot;red&quot;, lwd = 2) Although LSTM does a good job for the last 10 days, there are specific days that it is way off. That’s why it has a higher RMSPE than RNN we had earlier. Before concluding this chapter, let’s change our network setting slightly and see the results model &lt;- keras_model_sequential() %&gt;% layer_lstm(units=24, input_shape = c(7, 6), activation=&quot;tanh&quot;) %&gt;% layer_dense(units=1, activation = &quot;linear&quot;) %&gt;% compile(loss = &#39;mse&#39;, optimizer = &#39;adam&#39;, metrics = list(&quot;mean_absolute_error&quot;) ) model %&gt;% summary() ## Model: &quot;sequential_2&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## lstm_1 (LSTM) (None, 24) 2976 ## dense_5 (Dense) (None, 1) 25 ## ================================================================================ ## Total params: 3,001 ## Trainable params: 3,001 ## Non-trainable params: 0 ## ________________________________________________________________________________ model %&gt;% fit(X[[1]][train,, ], y[[1]][train], batch_size = 12, epochs = 75, validation_data = list(X[[1]][test,, ], y[[1]][test]), verbose = 0 ) %&gt;% plot() y_act &lt;- y[[1]][test] var_y &lt;- var(y_act) yhat &lt;- predict(model, X[[1]][test,, ]) 1 - mean((yhat -y_act)^2) / var_y # R^2 ## [1] 0.2535225 sqrt(mean((yhat -y_act)^2)) # RMSPE ## [1] 1.490992 plot(y[[1]][test], type =&quot;l&quot;, col = &quot;blue&quot;, ylab = &quot;Actual (Blue) vs. Prediction (Red)&quot;, xlab = &quot;Last 50 Days&quot;, main = &quot;LSTM Forecasting for Covid-19 Cases&quot;, lwd = 1) lines(yhat, col = &quot;red&quot;, lwd = 2) Building a network that does relatively a good job requires a well-designed validation process and a good network architecture that can be achieved after several trials. "],["matrix-decompositions.html", "Matrix Decompositions", " Matrix Decompositions Any matrix decomposition (and related topics) requires a solid understanding of eigenvalues and singular value decomposition (SVD). "],["eigenvectors-and-eigenvalues.html", "Chapter 26 Eigenvectors and eigenvalues", " Chapter 26 Eigenvectors and eigenvalues To explain eigenvalues, we first explain eigenvectors. Almost all vectors change direction, when they are multiplied by \\(\\mathbf{A}\\). Certain exceptional vectors \\(x\\) are in the same direction as \\(\\mathbf{A} x .\\) Those are the “eigenvectors”. Multiply an eigenvector by \\(\\mathbf{A}\\), and the vector \\(\\mathbf{A}x\\) is a number \\(\\lambda\\) times the original \\(x\\). The basic equation is \\(\\mathbf{A} x=\\lambda x\\). The number \\(\\lambda\\) is an eigenvalue of \\(\\mathbf{A}\\). The eigenvalue \\(\\lambda\\) tells whether the special vector \\(x\\) is stretched or shrunk or reversed or left unchanged-when it is multiplied by \\(\\mathbf{A}\\). We may find \\(\\lambda=2\\) or \\(\\frac{1}{2}\\) or \\(-1\\) or 1. The eigenvalue \\(\\lambda\\) could be zero! Then \\(\\mathbf{A} x=0 x\\) means that this eigenvector \\(x\\) is in the nullspace. A good example comes from the powers \\(\\mathbf{A, A^{2}, A^{3}}, \\ldots\\) of a matrix. Suppose you need the hundredth power \\(\\mathbf{A}^{100}\\). The starting matrix \\(\\mathbf{A}\\) becomes unrecognizable after a few steps, and \\(\\mathbf{A}^{100}\\) is very close to \\(\\left[\\begin{array}{llll}.6 &amp; .6 ; &amp; .4 &amp; .4\\end{array}\\right]:\\) \\[ \\begin{aligned} &amp;{\\left[\\begin{array}{cc} .8 &amp; .3 \\\\ .2 &amp; .7 \\end{array}\\right] \\quad\\left[\\begin{array}{cc} .70 &amp; .45 \\\\ .30 &amp; .55 \\end{array}\\right] \\quad\\left[\\begin{array}{cc} .650 &amp; .525 \\\\ .350 &amp; .475 \\end{array}\\right] \\ldots} &amp; {\\left[\\begin{array}{ll} .6000 &amp; .6000 \\\\ .4000 &amp; .4000 \\end{array}\\right]} \\\\ &amp;~~~~\\mathbf{A} ~~~~~~~~~~~~~~~~~~~~~~~~~\\mathbf{A}^{2}~~~~~~~~~~~~~~~~~~~~\\mathbf{A}^{3}&amp; \\mathbf{A}^{100} \\end{aligned} \\] \\(\\mathbf{A}^{100}\\) was found by using the eigenvalues of \\(\\mathbf{A}\\), not by multiplying 100 matrices. Those eigenvalues (here they are 1 and \\(1 / 2\\) ) are a new way to see into the heart of a matrix. See http://math.mit.edu/~gs/linearalgebra/ila0601.pdf (Strang_2016?) for more details. Most \\(2 \\times 2\\) matrices have two eigenvector directions and two eigenvalues and it can be shown that \\(\\operatorname{det}(\\mathbf{A}-\\lambda I)=0\\). #*****Eigenvalues and vectors******* #AX = lambdaX #(A − lambdaI)X = 0 A &lt;- matrix(c(2,1,8,5), 2, 2) ev &lt;- eigen(A)$values # Sum of eigenvalues = sum of diagonal terms of A (called the trace of A) sum(ev) ## [1] 7 sum(diag(A)) ## [1] 7 # Product of eigenvalues = determinant of A prod(ev) ## [1] 2 det(A) ## [1] 2 # Diagonal matrix D has eigenvalues = diagonal elements D &lt;- matrix(c(2,0,0,5), 2, 2) eigen(D) ## eigen() decomposition ## $values ## [1] 5 2 ## ## $vectors ## [,1] [,2] ## [1,] 0 -1 ## [2,] 1 0 \\(\\text{Rank}(\\mathbf{A})\\) is number of nonzero singular values of \\(\\mathbf{A}\\). Singular values are eigenvalues of \\(\\mathbf{X&#39;X}\\) which is an \\(n \\times n\\) matrix and \\(\\mathbf{X}\\) is \\(m \\times n\\) matrix. SVD starts with eigenvalue decomposition. See http://www.onmyphd.com/?p=eigen.decomposition. Eigendecomposition is the method to decompose a square matrix into its eigenvalues and eigenvectors. For a matrix \\(\\mathbf{A}\\), if \\[ \\mathbf{A} \\mathbf{v}=\\lambda \\mathbf{v} \\] then \\(\\mathbf{v}\\) is an eigenvector of matrix \\(\\mathbf{A}\\) and \\(\\lambda\\) is the corresponding eigenvalue. That is, if matrix \\(\\mathbf{A}\\) is multiplied by a vector and the result is a scaled version of the same vector, then it is an eigenvector of \\(\\mathbf{A}\\) and the scaling factor is its eigenvalue. So how do we find the eigenvectors of a matrix? \\[ \\begin{aligned} &amp;\\mathbf{A} \\mathbf{v}-\\lambda \\mathbf{I} \\mathbf{v}=0 \\\\ &amp;(\\mathbf{A}-\\lambda \\mathbf{I}) \\mathbf{v}=0, \\end{aligned} \\] where \\(\\mathbf{I}\\) is the identity matrix. It turns out that this equation is equivalent to: \\[ \\operatorname{det}(\\mathbf{A}-\\lambda \\mathbf{I})=0, \\] because \\(\\operatorname{det}(\\mathbf{A}-\\lambda \\mathbf{I}) \\equiv(\\mathbf{A}-\\lambda \\mathbf{I}) \\mathbf{v}=0\\). Why? Since you want non-trivial solutions to \\((\\mathbf{A}-\\lambda \\mathbf{I}) \\mathbf{v}=0\\), you want \\((\\mathbf{A}-\\lambda \\mathbf{I})\\) to be non-invertible. Otherwise, its invertible and you get \\(\\mathbf{v}=(\\mathbf{A}-\\lambda \\mathbf{I})^{-1} \\cdot 0=0\\) which is a trivial solution. But a linear transformation or a matrix is non-invertible if and only if its determinant is 0 . So \\(\\operatorname{det}(\\mathbf{A}-\\lambda \\mathbf{I})=0\\) for non-trivial solutions. It’s hard to understand the intuition or why eigenvectors and values are important. Here is the excerpt from How to intuitively understand eigenvalue and eigenvector (Use_eigen?): blockquote { padding: 10px 20px; margin: 0 0 20px; font-size: 14px; border-left: 5px solid #eee; } First let us think what a square matrix does to a vector. Consider a matrix \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\). Let us see what the matrix \\(\\mathbf{A}\\) acting on a vector \\(x\\) does to this vector. By action, we mean multiplication i.e. we get a new vector \\(y=\\mathbf{A} x\\). The matrix acting on a vector \\(x\\) does two things to the vector \\(x\\). (1) It scales the vector; (2) It rotates the vector. It is important to understand what the matrix \\(\\mathbf{A}\\) in a set of equations \\(\\mathbf{A x}=\\mathbf{b}\\) does. Matrix \\(\\mathbf{A}\\) simply “transforms” a vector \\(\\mathbf{x}\\) into another vector \\(\\mathbf{b}\\) by applying linear combination. The transformation is done within the same space or subspace. Sometimes we only want to know what would be the vector \\(\\mathbf{b}\\) if linear combination is applied, that is when we execute the equation \\(\\mathbf{A x}=\\mathbf{b}\\). Other times we are interested in a reverse problem and we want to solve the equation \\(\\mathbf{x}=\\mathbf{A}^{-1} \\mathbf{b}\\). However, for any matrix \\(\\mathbf{A}\\), there are some favored vectors/directions. When the matrix acts on these favored vectors, the action essentially results in just scaling the vector. There is no rotation. These favored vectors are precisely the eigenvectors and the amount by which each of these favored vectors stretches or compresses is the eigenvalue. So why are these eigenvectors and eigenvalues important? Consider the eigenvector corresponding to the maximum (absolute) eigenvalue. If we take a vector along this eigenvector, then the action of the matrix is maximum. No other vector when acted by this matrix will get stretched as much as this eigenvector. Hence, if a vector were to lie “close” to this eigen direction, then the “effect” of action by this matrix will be “large” i.e. the action by this matrix results in “large” response for this vector. The effect of the action by this matrix is high for large (absolute) eigenvalues and less for small (absolute) eigenvalues. Hence, the directions/vectors along which this action is high are called the principal directions or principal eigenvectors. The corresponding eigenvalues are called the principal values. Here are some examples: \\[ \\mathbf{\\Lambda}=\\left[\\begin{array}{cc} \\lambda_{1} &amp; 0 \\\\ 0 &amp; \\lambda_{2} \\end{array}\\right] \\\\ \\] \\[ \\mathbf{V}=\\left[\\mathbf{v}_1 \\mathbf{v}_2\\right] \\] So that \\[ \\mathbf{A V=V \\Lambda} \\] Hence, \\[ \\mathbf{A=V \\Lambda V^{-1}} \\] Eigendecomposition (a.k.a. spectral decomposition) decomposes a matrix \\(\\mathbf{A}\\) into a multiplication of a matrix of eigenvectors \\(\\mathbf{V}\\) and a diagonal matrix of eigenvalues \\(\\mathbf{\\Lambda}\\). This can only be done if a matrix is diagonalizable. In fact, the definition of a diagonalizable matrix \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\) is that it can be eigendecomposed into \\(n\\) eigenvectors, so that \\(\\mathbf{V^{-1} A V=\\Lambda}\\). Example: A = matrix(sample(1:100, 9), 3, 3) A ## [,1] [,2] [,3] ## [1,] 23 57 5 ## [2,] 80 37 7 ## [3,] 52 20 83 eigen(A) ## eigen() decomposition ## $values ## [1] 111.90282 68.93979 -37.84261 ## ## $vectors ## [,1] [,2] [,3] ## [1,] 0.3144894 -0.1732654 -0.6807399 ## [2,] 0.4156469 -0.2237782 0.7112619 ## [3,] 0.8534249 0.9591154 0.1752133 V = eigen(A)$vectors Lam = diag(eigen(A)$values) # Prove that AV = V lambda and A%*%V ## [,1] [,2] [,3] ## [1,] 35.19226 -11.94488 25.76098 ## [2,] 46.51207 -15.42722 -26.91601 ## [3,] 95.50065 66.12122 -6.63053 V%*%Lam ## [,1] [,2] [,3] ## [1,] 35.19226 -11.94488 25.76098 ## [2,] 46.51207 -15.42722 -26.91601 ## [3,] 95.50065 66.12122 -6.63053 # And decomposition V%*%Lam%*%solve(V) ## [,1] [,2] [,3] ## [1,] 23 57 5 ## [2,] 80 37 7 ## [3,] 52 20 83 And, matrix inverse with eigendecomposition \\[ \\mathbf{A^{-1}=V \\Lambda^{-1} V^{-1}} \\] Example: A = matrix(sample(1:100, 9), 3, 3) A ## [,1] [,2] [,3] ## [1,] 19 41 69 ## [2,] 96 74 6 ## [3,] 86 40 27 V = eigen(A)$vectors Lam = diag(eigen(A)$values) # Inverse of A solve(A) ## [,1] [,2] [,3] ## [1,] -0.007783238 -0.007318369 0.02151680 ## [2,] 0.009191128 0.024000531 -0.02882189 ## [3,] 0.011174569 -0.012245982 0.01120113 # And V%*%solve(Lam)%*%solve(V) ## [,1] [,2] [,3] ## [1,] -0.007783238 -0.007318369 0.02151680 ## [2,] 0.009191128 0.024000531 -0.02882189 ## [3,] 0.011174569 -0.012245982 0.01120113 The inverse of \\(\\mathbf{\\Lambda}\\) is just the inverse of each diagonal element (the eigenvalues). But, this can only be done if a matrix is diagonalizable. So if \\(\\mathbf{A}\\) is not \\(n \\times n\\), then we can use \\(\\mathbf{A&#39;A}\\) or \\(\\mathbf{AA&#39;}\\), both symmetric now. Example: \\[ \\mathbf{A}=\\left(\\begin{array}{ll} 1 &amp; 2 \\\\ 2 &amp; 4 \\end{array}\\right) \\] As \\(\\operatorname{det}(\\mathbf{A})=0, \\mathbf{A}\\) is singular and its inverse is undefined. \\(\\operatorname{Det}(\\mathbf{A})\\) equals the product of the eigenvalues \\(\\theta_{\\mathrm{j}}\\) of \\(\\mathrm{A}\\): the matrix \\(\\mathbf{A}\\) is singular if any eigenvalue of \\(\\mathbf{A}\\) is zero. To see this, consider the spectral decomposition of \\(A\\) : \\[ \\mathbf{A}=\\sum_{j=1}^{p} \\theta_{j} \\mathbf{v}_{j} \\mathbf{v}_{j}^{\\top} \\] where \\(\\mathbf{v}_{\\mathrm{j}}\\) is the eigenvector belonging to \\(\\theta_{\\mathrm{j}}\\) The inverse of \\(\\mathbf{A}\\) is then: \\[ \\mathbf{A}^{-1}=\\sum_{j=1}^{p} \\theta_{j}^{-1} \\mathbf{v}_{j} \\mathbf{v}_{j}^{\\top} \\] A has eigenvalues 5 and 0. The inverse of \\(A\\) via the spectral decomposition is then undefined: \\[ \\mathbf{A}^{-1}=\\frac{1}{5} \\mathbf{v}_{1} \\mathbf{v}_{1}^{\\top}+ \\frac{1}{0} \\mathbf{v}_{1} \\mathbf{v}_{1}^{\\top} \\] "],["singular-value-decomposition.html", "Chapter 27 Singular Value Decomposition", " Chapter 27 Singular Value Decomposition The answer is to work with \\(\\mathbf{A^{\\top} A}\\) and \\(\\mathbf{A A^{\\top}}\\), both of which are symmetric (and have \\(n\\) and \\(m\\) orthogonal eigenvectors, respectively). So we have the following decomposition: \\[ \\begin{aligned} \\mathbf{A^{\\top} A =V D V^{\\top}} \\\\ \\mathbf{A A^{\\top} =U D^{\\prime} U^{\\top}} \\end{aligned} \\] where \\(\\mathbf{V}\\) is an \\(n \\times n\\) orthogonal matrix consisting of the eigenvectors of \\(\\mathbf{A^{\\top} A, D}\\) an \\(n \\times n\\) diagonal matrix with the eigenvalues of \\(\\mathbf{A^{\\top} A}\\) on the diagonal, \\(\\mathbf{U}\\) an \\(m \\times m\\) orthogonal matrix consisting of the eigenvectors of \\(\\mathbf{A A^{\\top}}\\), and \\(\\mathbf{D^{\\prime}}\\) an \\(m \\times m\\) diagonal matrix with the eigenvalues of \\(\\mathbf{A A^{\\top}}\\) on the diagonal. It turns out that \\(\\mathrm{D}\\) and \\(\\mathbf{D^{\\prime}}\\) have the same non-zero diagonal entries except that the order might be different. Now comes a highlight of linear algebra. Any real \\(m \\times n\\) matrix can be factored as \\[ \\mathbf{A=U \\Sigma V^{\\top}} \\] where \\(\\mathbf{U}\\) is an \\(m \\times m\\) orthogonal matrix whose columns are the eigenvectors of \\(\\mathbf{A A^{\\top}}\\), \\(\\mathbf{V}\\) is an \\(n \\times n\\) orthogonal matrix whose columns are the eigenvectors of \\(\\mathbf{A^{\\top} A}\\), and \\(\\mathbf{\\Sigma}\\) is an \\(m \\times n\\) diagonal matrix of the form: \\[ \\Sigma=\\left(\\begin{array}{cccc} \\sigma_{1} &amp; &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\sigma_{n} &amp; \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp;0 \\\\ \\end{array}\\right) \\] with \\(\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{n}&gt;0\\) and \\(r=\\operatorname{rank}(\\mathbf{A})\\). In the above, \\(\\sigma_{1}, \\ldots, \\sigma_{n}\\) are the square roots of the eigenvalues of \\(\\mathbf{A^{\\top} A}\\). They are called the singular values of \\(\\mathbf{A}\\). Our basic goal is to “solve” the system \\(\\mathbf{A} x=b\\) for all matrices \\(\\mathbf{A}\\) and vectors \\(b\\). A second goal is to solve the system using a numerically stable algorithm. A third goal is to solve the system in a reasonably efficient manner. For instance, we do not want to compute \\(\\mathbf{A}^{-1}\\) using determinants. An important point is that although \\(\\mathbf{U}\\) in \\(\\mathbf{U \\Sigma V^{\\top}}\\) is \\(m \\times m\\) when its multiplied by \\(\\mathbf{\\Sigma}\\) it reduces to \\(n \\times n\\) matrix due to zeros in \\(\\mathbf{\\Sigma}\\). Hence, we can actually select only those in \\(\\mathbf{U}\\) that are not going to be zero out due to that multiplication and take only \\(n \\times n\\) from \\(\\mathbf{U}\\) matrix. This is called “Economy SVD” \\(\\mathbf{\\hat{U} \\hat{\\Sigma} V^{\\top}}\\) where all matrices will be \\(n \\times n\\) "],["rankr-approximations.html", "Chapter 28 Rank(r) Approximations", " Chapter 28 Rank(r) Approximations SVD can also be used for rank approximations a.k.a. matrix approximations. Since \\(\\mathbf{A=U \\Sigma V^{\\top}}\\) can be written as \\[ =\\sigma_{1} u_{1} v_{1}^{\\top}+\\sigma_{2} u_{2} v_{2}^{\\top}+\\ldots+\\sigma_{n} u_{n} v_{n}^{\\top}+ 0 \\] Each term in this equation is a Rank 1 matrix: \\(u_1\\) is \\(n \\times 1\\) column vector and \\(v_1\\) is \\(1 \\times n\\) row vector. Hence these are the only orthogonal entries in the resulting matrix, hence Rank1. Therefore, the first term with \\(\\sigma_1\\) is a Rank 1 \\(n \\times n\\) matrix. All other terms have the same dimension. Since \\(\\sigma\\)s are ordered, the first term is the carries the most information. So Rank 1 approximation is taking only the first term and ignoring the others. Here is a simple example: #rank-one approximation #http://cs.brown.edu/courses/cs053/current/slides/12-01-2017.pdf #https://cran.r-project.org/web/packages/matlib/vignettes/eigen-ex1.html A &lt;- matrix(c(1,5,4,2), 2 ,2) A ## [,1] [,2] ## [1,] 1 4 ## [2,] 5 2 v1 &lt;- matrix(eigen(t(A)%*%(A))$vector[,1], 1, 2) sigma &lt;- sqrt(eigen(t(A)%*%(A))$values[1]) u1 &lt;- matrix(eigen(A%*%t(A))$vector[,1], 2, 1) Atilde &lt;- sigma*u1%*%v1 Atilde ## [,1] [,2] ## [1,] -2.560369 -2.069843 ## [2,] -4.001625 -3.234977 And, Rank 2 approximation can be obtained by adding the first 2 terms and so on. As we add more terms we can get the full information in the data. But often times, we truncate the ranks at \\(r\\) by removing the terms with small \\(sigma\\). This is also called noise reduction. Here is an example for an image compression for a matrix from our own work (Graphical Network Analysis of COVID-19 Spread). A better example for an image can be found in the Github repo of package rsvd Tiger (Erichson_2019?): comt &lt;- readRDS(&quot;comt.rds&quot;) heatmap(comt, Colv = NA, Rowv = NA, main = &quot;Heatmap - Original&quot;, xlab = &quot;Lags&quot;, ylab = &quot;Starting days of 7-day rolling windows&quot;) # Regular SVD fck &lt;-svd(comt) r = 2 comt.re &lt;- as.matrix(fck$u[,1:r])%*%diag(fck$d)[1:r,1:r]%*%t(fck$v[,1:r]) heatmap(comt.re, Colv = NA, Rowv = NA, main = &quot;Heatmap Matrix - Rank(2) Approx&quot;, xlab = &quot;Lags&quot;, ylab = &quot;Startting days of 7-day rolling windows&quot;) #XX&#39; and X&#39;X SVD wtf &lt;- comt%*%t(comt) fck &lt;-svd(wtf) r = 2 comt.re2 &lt;- as.matrix(fck$u[,1:r])%*%diag(fck$d)[1:r,1:r]%*%t(fck$v[,1:r]) heatmap(comt.re2, Colv = NA, Rowv = NA, main = &quot;Row Corr. - Rank(2)&quot;, xlab = &quot;Startting days of 7-day rolling windows&quot;, ylab = &quot;Startting days of 7-day rolling windows&quot;) wtf &lt;- t(comt)%*%comt fck &lt;-svd(wtf) r = 2 comt.re3 &lt;- as.matrix(fck$u[,1:r])%*%diag(fck$d)[1:r,1:r]%*%t(fck$v[,1:r]) heatmap(comt.re3, Colv = NA, Rowv = NA, main = &quot;Column Corr. - Rank(2)&quot;, xlab = &quot;Lags&quot;, ylab = &quot;Lags&quot;) There is a series of lectures on SVD by Steve Brunton at YouTube (Brunton_2020?) that you can benefit from greatly! "],["moore-penrose-inverse.html", "Chapter 29 Moore-Penrose inverse", " Chapter 29 Moore-Penrose inverse Another example is solving OLS with SVD: \\[ \\mathbf{y = X \\beta}\\\\ \\mathbf{y = U \\Sigma V&#39; \\beta}\\\\ \\mathbf{U&#39;y = U&#39;U \\Sigma V&#39; \\beta}\\\\ \\mathbf{U&#39;y = \\Sigma V&#39; \\beta}\\\\ \\mathbf{\\Sigma^{-1}}\\mathbf{U&#39;y = V&#39; \\beta}\\\\ \\mathbf{V\\Sigma^{-1}}\\mathbf{U&#39;y = \\beta}\\\\ \\] And \\[ \\mathbf{V\\Sigma^{-1}U&#39; = M^+} \\] is called “generalized inverse” or The Moore-Penrose Pseudoinverse. Here are some application of SVD and Pseudoinverse. library(MASS) ##Simple SVD and generalized inverse a &lt;- matrix(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1), 9, 4) a.svd &lt;- svd(a) ds &lt;- diag(1/a.svd$d[1:3]) u &lt;- a.svd$u v &lt;- a.svd$v us &lt;- as.matrix(u[, 1:3]) vs &lt;- as.matrix(v[, 1:3]) (a.ginv &lt;- vs %*% ds %*% t(us)) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333 ## [2,] 0.25000000 0.25000000 0.25000000 -0.08333333 -0.08333333 -0.08333333 ## [3,] -0.08333333 -0.08333333 -0.08333333 0.25000000 0.25000000 0.25000000 ## [4,] -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333 ## [,7] [,8] [,9] ## [1,] 0.08333333 0.08333333 0.08333333 ## [2,] -0.08333333 -0.08333333 -0.08333333 ## [3,] -0.08333333 -0.08333333 -0.08333333 ## [4,] 0.25000000 0.25000000 0.25000000 ginv(a) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333 ## [2,] 0.25000000 0.25000000 0.25000000 -0.08333333 -0.08333333 -0.08333333 ## [3,] -0.08333333 -0.08333333 -0.08333333 0.25000000 0.25000000 0.25000000 ## [4,] -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333 -0.08333333 ## [,7] [,8] [,9] ## [1,] 0.08333333 0.08333333 0.08333333 ## [2,] -0.08333333 -0.08333333 -0.08333333 ## [3,] -0.08333333 -0.08333333 -0.08333333 ## [4,] 0.25000000 0.25000000 0.25000000 ##Simulated DGP x1 &lt;- rep(1, 20) x2 &lt;- rnorm(20) x3 &lt;- rnorm(20) u &lt;- matrix(rnorm(20, mean=0, sd=1), nrow=20, ncol=1) X &lt;- cbind(x1, x2, x3) beta &lt;- matrix(c(0.5, 1.5, 2), nrow=3, ncol=1) Y &lt;- X%*%beta + u ##OLS betahat_OLS &lt;- solve(t(X)%*%X)%*%t(X)%*%Y betahat_OLS ## [,1] ## x1 0.5094523 ## x2 1.4714533 ## x3 2.2450864 ##SVD X.svd &lt;- svd(X) ds &lt;- diag(1/X.svd$d) u &lt;- X.svd$u v &lt;- X.svd$v us &lt;- as.matrix(u) vs &lt;- as.matrix(v) X.ginv_mine &lt;- vs %*% ds %*% t(us) # Compare X.ginv &lt;- ginv(X) round((X.ginv_mine - X.ginv),4) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] ## [1,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [2,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [3,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [,15] [,16] [,17] [,18] [,19] [,20] ## [1,] 0 0 0 0 0 0 ## [2,] 0 0 0 0 0 0 ## [3,] 0 0 0 0 0 0 # Now OLS betahat_ginv &lt;- X.ginv %*%Y betahat_ginv ## [,1] ## [1,] 0.5094523 ## [2,] 1.4714533 ## [3,] 2.2450864 betahat_OLS ## [,1] ## x1 0.5094523 ## x2 1.4714533 ## x3 2.2450864 Now the question where and when we can use ginv? With a high-dimensional \\(\\mathbf{X}\\), where \\(p &gt; n\\), the vector \\(\\beta\\) cannot uniquely be determined from the system of equations.the solution to the normal equation is \\[ \\hat{\\boldsymbol{\\beta}}=\\left(\\mathbf{X}^{\\top} \\mathbf{X}\\right)^{+} \\mathbf{X}^{\\top} \\mathbf{Y}+\\mathbf{v} \\quad \\text { for all } \\mathbf{v} \\in \\mathcal{V} \\] where \\(\\mathbf{A}^{+}\\)denotes the Moore-Penrose inverse of the matrix \\(\\mathbf{A}\\). Therefore, there is no unique estimator of the regression parameter (See Page 7 for proof in Lecture notes on ridge regression) (Wieringen_2021?). To arrive at a unique regression estimator for studies with rank deficient design matrices, the minimum least squares estimator may be employed. The minimum least squares estimator of regression parameter minimizes the sum-of-squares criterion and is of minimum length. Formally, \\(\\hat{\\boldsymbol{\\beta}}_{\\mathrm{MLS}}=\\arg \\min _{\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}}\\|\\mathbf{Y}-\\mathbf{X} \\boldsymbol{\\beta}\\|_{2}^{2}\\) such that \\(\\left\\|\\hat{\\boldsymbol{\\beta}}_{\\mathrm{MLS}}\\right\\|_{2}^{2}&lt;\\|\\boldsymbol{\\beta}\\|_{2}^{2}\\) for all \\(\\boldsymbol{\\beta}\\) that minimize \\(\\|\\mathbf{Y}-\\mathbf{X} \\boldsymbol{\\beta}\\|_{2}^{2}\\). So \\(\\hat{\\boldsymbol{\\beta}}_{\\mathrm{MLS}}=\\left(\\mathbf{X}^{\\top} \\mathbf{X}\\right)^{+} \\mathbf{X}^{\\top} \\mathbf{Y}\\) is the minimum least squares estimator of regression parameter minimizes the sum-of-squares criterion. As we talked before in Chapter 17, an alternative (and related) estimator of the regression parameter \\(\\beta\\) that avoids the use of the Moore-Penrose inverse and is able to deal with (super)-collinearity among the columns of the design matrix is the ridge regression estimator proposed by Hoerl and Kennard (1970). They propose to simply replace \\(\\mathbf{X}^{\\top} \\mathbf{X}\\) by \\(\\mathbf{X}^{\\top} \\mathbf{X}+\\lambda \\mathbf{I}_{p p}\\) with \\(\\lambda \\in[0, \\infty)\\). The ad-hoc fix solves the singularity as it adds a positive matrix, \\(\\lambda \\mathbf{I}_{p p}\\), to a positive semi-definite one, \\(\\mathbf{X}^{\\top} \\mathbf{X}\\), making the total a positive definite matrix, which is invertible. Hence, the ad-hoc fix of the ridge regression estimator resolves the non-evaluation of the estimator in the face of super-collinearity but yields a ‘ridge fit’ that is not optimal in explaining the observation. Mathematically, this is due to the fact that the fit \\(\\widehat{Y}(\\lambda)\\) corresponding to the ridge regression estimator is not a projection of \\(Y\\) onto the covariate space. "],["principle-component-analysis.html", "Chapter 30 Principle Component Analysis", " Chapter 30 Principle Component Analysis Having seen SVD and Eigenvalue decomposition, now we can look at Principle Component Analysis (PCA), which is a statistical procedure that allows you to summarize the information content in large data tables. In other words, it helps dimension reduction in big datasets. PCA is a eigenvalue decomposition of a covariance matrix (of data matrix \\(\\mathbf{X}\\)). Since a covariance matrix is a square matrix, we can apply the eigenvalue decomposition, which reveals the unique orthogonal directions (variances) in the data so that their orthogonal linear combinations maximize the total variance. The goal is here a dimension reduction of the data matrix. Hence by selecting a few loading, we can reduce the dimension of the data but capture a substantial variation in the data at the same time. See https://www.youtube.com/watch?v=fkf4IBRSeEc (Brunton_PCA?) and https://setosa.io/ev/principal-component-analysis/ (Powell_PCA?). Principal components are the ordered (orthogonal) lines (vectors) that best account for the maximum variance in the data by their magnitude. To get the (unique) variances (direction and the magnitude) in data, we first obtain the mean-centered covariance matrix. And as you can imagine, eigenvectors (which give the unique directions) and eigenvalues (which identify those directions’ magnitude) are used for PCA: So when we use the covariance matrix of a data, we can use eigenvalue decomposition to identify the unique variation and their relative magnitude in the data. Here is a simple procedure: \\(\\mathbf{X}\\) is the data matrix, \\(\\mathbf{B}\\) is the mean-centered data matrix, \\(\\mathbf{C}\\) is the covariance matrix (\\(\\mathbf{B^TB}\\)) (note if \\(\\mathbf{B}\\) is scaled, i.e. “z-scored”, \\(\\mathbf{B^TB}\\) gives correlation matrix) Compute the eigenvectors and values of \\(\\mathbf{C}\\): \\(\\mathbf{C} = \\mathbf{VDV^{\\top}}\\) hence \\(\\mathbf{CV} = \\mathbf{VD}\\), where \\(\\mathbf{V}\\) is the eigenvectors (loadings) and \\(\\mathbf{D}\\) is eigenvalues. Using \\(\\mathbf{V}\\), the transformation of \\(\\mathbf{B}\\) with \\(\\mathbf{B} \\mathbf{V}\\) maps the data of \\(p\\) variables to a new space of \\(p\\) variables which are uncorrelated over the dataset. \\(\\mathbf{T} (=\\mathbf{B} \\mathbf{V})\\) is called the principle component or score matrix Since SVD of \\(\\mathbf{B} = \\mathbf{U} \\Sigma \\mathbf{V}^{\\top}\\), we can also get \\(\\mathbf{B}\\mathbf{V} = \\mathbf{T} = \\mathbf{U\\Sigma}\\). Hence the principle components are \\(\\mathbf{T} = \\mathbf{BV} = \\mathbf{U\\Sigma}\\). However, not all the principal components need to be kept. Keeping only the first \\(r\\) principal components, produced by using only the first \\(r\\) eigenvectors, gives the truncated transformation \\(\\mathbf{T}_{r} = \\mathbf{B} \\mathbf{V}_{r}\\). Obviously you choose those with higher variance in each directions by the order of eigenvalues. We can use \\(\\frac{\\lambda_{k}}{\\sum_{i=1} \\lambda_{k}}\\) to identify \\(r\\). Or cumulatively, we can see how much variation could be captured by \\(r\\) number of \\(\\lambda\\)s, which gives us an idea how many principle components to keep … \\[ \\frac{\\sum_{i=1}^{r} \\lambda_{k}}{\\sum_{i=1}^n \\lambda_{k}} \\] Let’s see an example. Here is the data: library(&quot;factoextra&quot;) data(decathlon2) X &lt;- as.matrix(decathlon2[, 1:10]) head(X) ## X100m Long.jump Shot.put High.jump X400m X110m.hurdle Discus ## SEBRLE 11.04 7.58 14.83 2.07 49.81 14.69 43.75 ## CLAY 10.76 7.40 14.26 1.86 49.37 14.05 50.72 ## BERNARD 11.02 7.23 14.25 1.92 48.93 14.99 40.87 ## YURKOV 11.34 7.09 15.19 2.10 50.42 15.31 46.26 ## ZSIVOCZKY 11.13 7.30 13.48 2.01 48.62 14.17 45.67 ## McMULLEN 10.83 7.31 13.76 2.13 49.91 14.38 44.41 ## Pole.vault Javeline X1500m ## SEBRLE 5.02 63.19 291.7 ## CLAY 4.92 60.15 301.5 ## BERNARD 5.32 62.77 280.1 ## YURKOV 4.72 63.44 276.4 ## ZSIVOCZKY 4.42 55.37 268.0 ## McMULLEN 4.42 56.37 285.1 n &lt;- nrow(X) B &lt;- scale(X, center = TRUE) C &lt;- t(B)%*%B/(n-1) head(C) ## X100m Long.jump Shot.put High.jump X400m ## X100m 1.0000000 -0.7377932 -0.3703180 -0.3146495 0.5703453 ## Long.jump -0.7377932 1.0000000 0.3737847 0.2682078 -0.5036687 ## Shot.put -0.3703180 0.3737847 1.0000000 0.5747998 -0.2073588 ## High.jump -0.3146495 0.2682078 0.5747998 1.0000000 -0.2616603 ## X400m 0.5703453 -0.5036687 -0.2073588 -0.2616603 1.0000000 ## X110m.hurdle 0.6699790 -0.5521158 -0.2701634 -0.2022579 0.5970140 ## X110m.hurdle Discus Pole.vault Javeline X1500m ## X100m 0.6699790 -0.3893760 0.01156433 -0.26635476 -0.17805307 ## Long.jump -0.5521158 0.3287652 0.07982045 0.28806781 0.17332597 ## Shot.put -0.2701634 0.7225179 -0.06837068 0.47558572 0.00959628 ## High.jump -0.2022579 0.4210187 -0.55129583 0.21051789 -0.15699017 ## X400m 0.5970140 -0.2545326 0.11156898 0.02350554 0.18346035 ## X110m.hurdle 1.0000000 -0.4213608 0.12118697 0.09655757 -0.10331329 #Check it head(cov(B)) ## X100m Long.jump Shot.put High.jump X400m ## X100m 1.0000000 -0.7377932 -0.3703180 -0.3146495 0.5703453 ## Long.jump -0.7377932 1.0000000 0.3737847 0.2682078 -0.5036687 ## Shot.put -0.3703180 0.3737847 1.0000000 0.5747998 -0.2073588 ## High.jump -0.3146495 0.2682078 0.5747998 1.0000000 -0.2616603 ## X400m 0.5703453 -0.5036687 -0.2073588 -0.2616603 1.0000000 ## X110m.hurdle 0.6699790 -0.5521158 -0.2701634 -0.2022579 0.5970140 ## X110m.hurdle Discus Pole.vault Javeline X1500m ## X100m 0.6699790 -0.3893760 0.01156433 -0.26635476 -0.17805307 ## Long.jump -0.5521158 0.3287652 0.07982045 0.28806781 0.17332597 ## Shot.put -0.2701634 0.7225179 -0.06837068 0.47558572 0.00959628 ## High.jump -0.2022579 0.4210187 -0.55129583 0.21051789 -0.15699017 ## X400m 0.5970140 -0.2545326 0.11156898 0.02350554 0.18346035 ## X110m.hurdle 1.0000000 -0.4213608 0.12118697 0.09655757 -0.10331329 Eigenvalues and vectors … #Eigens evalues &lt;- eigen(C)$values evalues ## [1] 3.7499727 1.7451681 1.5178280 1.0322001 0.6178387 0.4282908 0.3259103 ## [8] 0.2793827 0.1911128 0.1122959 evectors &lt;- eigen(C)$vectors evectors #Ordered ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.42290657 -0.2594748 -0.081870461 -0.09974877 0.2796419 -0.16023494 ## [2,] -0.39189495 0.2887806 0.005082180 0.18250903 -0.3355025 -0.07384658 ## [3,] -0.36926619 -0.2135552 -0.384621732 -0.03553644 0.3544877 -0.32207320 ## [4,] -0.31422571 -0.4627797 -0.003738604 -0.07012348 -0.3824125 -0.52738027 ## [5,] 0.33248297 -0.1123521 -0.418635317 -0.26554389 -0.2534755 0.23884715 ## [6,] 0.36995919 -0.2252392 -0.338027983 0.15726889 -0.2048540 -0.26249611 ## [7,] -0.37020078 -0.1547241 -0.219417086 -0.39137188 0.4319091 0.28217086 ## [8,] 0.11433982 0.5583051 -0.327177839 0.24759476 0.3340758 -0.43606610 ## [9,] -0.18341259 -0.0745854 -0.564474643 0.47792535 -0.1697426 0.42368592 ## [10,] -0.03599937 0.4300522 -0.286328973 -0.64220377 -0.3227349 -0.10850981 ## [,7] [,8] [,9] [,10] ## [1,] 0.03227949 -0.35266427 0.71190625 -0.03272397 ## [2,] -0.24902853 -0.72986071 0.12801382 -0.02395904 ## [3,] -0.23059438 0.01767069 -0.07184807 0.61708920 ## [4,] -0.03992994 0.25003572 0.14583529 -0.41523052 ## [5,] -0.69014364 0.01543618 -0.13706918 -0.12016951 ## [6,] 0.42797378 -0.36415520 -0.49550598 0.03514180 ## [7,] 0.18416631 -0.26865454 -0.18621144 -0.48037792 ## [8,] -0.12654370 0.16086549 -0.02983660 -0.40290423 ## [9,] 0.23324548 0.19922452 0.33300936 -0.02100398 ## [10,] 0.34406521 0.09752169 0.19899138 0.18954698 Now with prcomp(). First, eigenvalues: # With `prcomp()` Xpca &lt;- prcomp(X, scale = TRUE) #Eigenvalues Xpca$sdev # you can see it&#39;s ordered ## [1] 1.9364846 1.3210481 1.2320016 1.0159725 0.7860272 0.6544393 0.5708855 ## [8] 0.5285666 0.4371645 0.3351059 # They are sqrt() of eigenvalues that we calculated earlier sqrt(evalues) ## [1] 1.9364846 1.3210481 1.2320016 1.0159725 0.7860272 0.6544393 0.5708855 ## [8] 0.5285666 0.4371645 0.3351059 Loadings … #Eigenvectors (loadings) Xpca$rotation # 10x10 ## PC1 PC2 PC3 PC4 PC5 ## X100m -0.42290657 0.2594748 -0.081870461 0.09974877 -0.2796419 ## Long.jump 0.39189495 -0.2887806 0.005082180 -0.18250903 0.3355025 ## Shot.put 0.36926619 0.2135552 -0.384621732 0.03553644 -0.3544877 ## High.jump 0.31422571 0.4627797 -0.003738604 0.07012348 0.3824125 ## X400m -0.33248297 0.1123521 -0.418635317 0.26554389 0.2534755 ## X110m.hurdle -0.36995919 0.2252392 -0.338027983 -0.15726889 0.2048540 ## Discus 0.37020078 0.1547241 -0.219417086 0.39137188 -0.4319091 ## Pole.vault -0.11433982 -0.5583051 -0.327177839 -0.24759476 -0.3340758 ## Javeline 0.18341259 0.0745854 -0.564474643 -0.47792535 0.1697426 ## X1500m 0.03599937 -0.4300522 -0.286328973 0.64220377 0.3227349 ## PC6 PC7 PC8 PC9 PC10 ## X100m 0.16023494 -0.03227949 0.35266427 -0.71190625 0.03272397 ## Long.jump 0.07384658 0.24902853 0.72986071 -0.12801382 0.02395904 ## Shot.put 0.32207320 0.23059438 -0.01767069 0.07184807 -0.61708920 ## High.jump 0.52738027 0.03992994 -0.25003572 -0.14583529 0.41523052 ## X400m -0.23884715 0.69014364 -0.01543618 0.13706918 0.12016951 ## X110m.hurdle 0.26249611 -0.42797378 0.36415520 0.49550598 -0.03514180 ## Discus -0.28217086 -0.18416631 0.26865454 0.18621144 0.48037792 ## Pole.vault 0.43606610 0.12654370 -0.16086549 0.02983660 0.40290423 ## Javeline -0.42368592 -0.23324548 -0.19922452 -0.33300936 0.02100398 ## X1500m 0.10850981 -0.34406521 -0.09752169 -0.19899138 -0.18954698 loadings &lt;- Xpca$rotation Interestingly the signs of eigenvectors are flipped and opposites of what we calculated with eigen() above. There are multiple discussions about the sign reversals in eignevectores. You can find them here (Kroll_2015?) and here (Wilks_2019?) Visualize the order … plot(Xpca$sdev) # Eigenvalues fviz_eig(Xpca) # Cumulative with &quot;factoextra&quot; # Or var &lt;- (Xpca$sdev)^2 var_perc &lt;- var/sum(var) * 100 barplot(var_perc, xlab=&#39;PC&#39;, ylab=&#39;Percent Variance&#39;, names.arg=1:length(var_perc), las=1, ylim=c(0, max(var_perc)), col=&#39;lightgreen&#39;) abline(h=mean(var_perc), col=&#39;red&#39;) Since we have ten variables, if each variable contributed equally, they would each contribute 10% to the total variance (red line). This criterion suggests we should also include principal component 4 (but barely) in our interpretation. And principle component scores \\(\\mathbf{T} = \\mathbf{X}\\mathbf{V}\\) (a.k.a score matrix) with prcomp(): pc &lt;- scale(X)%*%Xpca$rotation head(pc) ## PC1 PC2 PC3 PC4 PC5 PC6 ## SEBRLE 0.2727622 -0.5264068 -1.5556058 0.10384438 1.05453531 0.7177257 ## CLAY 0.8879389 -2.0551314 -0.8249697 1.81612193 -0.40100595 -1.5039874 ## BERNARD -1.3466138 -1.3229149 -0.9439501 -1.46516144 -0.17925232 0.5996203 ## YURKOV -0.9108536 2.2390912 -1.9063730 0.09501304 0.18735823 0.3754439 ## ZSIVOCZKY -0.1018764 1.0694498 2.0596722 0.07056229 -0.03232182 -0.9321431 ## McMULLEN 0.2353742 0.9215376 0.8028425 1.17942532 1.79598700 -0.3241881 ## PC7 PC8 PC9 PC10 ## SEBRLE -0.04935537 0.02990462 -0.63079187 0.07728655 ## CLAY -0.75968352 -0.06536612 0.05920672 0.15812336 ## BERNARD -0.75032098 -0.49570997 0.07483747 -0.03288604 ## YURKOV -0.29565551 0.09332310 -0.06769776 0.13791531 ## ZSIVOCZKY -0.30752133 0.29476740 -0.48055837 0.44234659 ## McMULLEN 0.02896393 -0.53358562 0.05116850 0.37610188 dim(pc) ## [1] 27 10 # which is also given by `prcomp()` head(Xpca$x) ## PC1 PC2 PC3 PC4 PC5 PC6 ## SEBRLE 0.2727622 -0.5264068 -1.5556058 0.10384438 1.05453531 0.7177257 ## CLAY 0.8879389 -2.0551314 -0.8249697 1.81612193 -0.40100595 -1.5039874 ## BERNARD -1.3466138 -1.3229149 -0.9439501 -1.46516144 -0.17925232 0.5996203 ## YURKOV -0.9108536 2.2390912 -1.9063730 0.09501304 0.18735823 0.3754439 ## ZSIVOCZKY -0.1018764 1.0694498 2.0596722 0.07056229 -0.03232182 -0.9321431 ## McMULLEN 0.2353742 0.9215376 0.8028425 1.17942532 1.79598700 -0.3241881 ## PC7 PC8 PC9 PC10 ## SEBRLE -0.04935537 0.02990462 -0.63079187 0.07728655 ## CLAY -0.75968352 -0.06536612 0.05920672 0.15812336 ## BERNARD -0.75032098 -0.49570997 0.07483747 -0.03288604 ## YURKOV -0.29565551 0.09332310 -0.06769776 0.13791531 ## ZSIVOCZKY -0.30752133 0.29476740 -0.48055837 0.44234659 ## McMULLEN 0.02896393 -0.53358562 0.05116850 0.37610188 Now you can think that if we use evectors that we calculated earlier with filliped signs, the data would be different. It’s similar to multiply the entire data with -1. So the data would not change in a sense that that captures the variation between observations and variables. That’s why the sign of eigenvalues are arbitraray. With SVD … # With SVD Xsvd &lt;- svd(scale(X)) pc_2 &lt;- Xsvd$u%*%diag(Xsvd$d) dim(pc_2) ## [1] 27 10 head(pc_2) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.2727622 -0.5264068 -1.5556058 0.10384438 1.05453531 0.7177257 ## [2,] 0.8879389 -2.0551314 -0.8249697 1.81612193 -0.40100595 -1.5039874 ## [3,] -1.3466138 -1.3229149 -0.9439501 -1.46516144 -0.17925232 0.5996203 ## [4,] -0.9108536 2.2390912 -1.9063730 0.09501304 0.18735823 0.3754439 ## [5,] -0.1018764 1.0694498 2.0596722 0.07056229 -0.03232182 -0.9321431 ## [6,] 0.2353742 0.9215376 0.8028425 1.17942532 1.79598700 -0.3241881 ## [,7] [,8] [,9] [,10] ## [1,] -0.04935537 0.02990462 -0.63079187 0.07728655 ## [2,] -0.75968352 -0.06536612 0.05920672 0.15812336 ## [3,] -0.75032098 -0.49570997 0.07483747 -0.03288604 ## [4,] -0.29565551 0.09332310 -0.06769776 0.13791531 ## [5,] -0.30752133 0.29476740 -0.48055837 0.44234659 ## [6,] 0.02896393 -0.53358562 0.05116850 0.37610188 Here we can reduce the dimensionality by selecting only 4 PC (the first 4 PC’s are above the average, which explain more than 80% of the variation in the data - see the graph above) reduced &lt;- pc[, 1:4] dim(reduced) ## [1] 27 4 head(reduced) ## PC1 PC2 PC3 PC4 ## SEBRLE 0.2727622 -0.5264068 -1.5556058 0.10384438 ## CLAY 0.8879389 -2.0551314 -0.8249697 1.81612193 ## BERNARD -1.3466138 -1.3229149 -0.9439501 -1.46516144 ## YURKOV -0.9108536 2.2390912 -1.9063730 0.09501304 ## ZSIVOCZKY -0.1018764 1.0694498 2.0596722 0.07056229 ## McMULLEN 0.2353742 0.9215376 0.8028425 1.17942532 The individual columns of \\(\\mathbf{T}\\) successively inherit the maximum possible variance from \\(\\mathbf{X}\\), with each coefficient vector in \\(\\mathbf{V}\\) constrained to be a unit vector. The full principal components decomposition of \\(\\mathbf{X}\\), \\(\\mathbf{T}=\\mathbf{X V}\\), where \\(\\mathbf{V}\\) is a \\(p \\times p\\) matrix of weights whose columns are the eigenvectors of \\(\\mathbf{X}^{\\top} \\mathbf{X}\\). Columns of \\(\\mathbf{V}\\) multiplied by the square root of corresponding eigenvalues, that is, eigenvectors scaled up by the variances, are called loadings in PCA or in Factor analysis. Note that if we make a singular value decomposition for a covariance matrix \\[ \\begin{aligned} \\mathbf{X}^{T} \\mathbf{X} &amp;=\\mathbf{V} \\mathbf{\\Sigma}^{\\top} \\mathbf{U}^{\\top} \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^{\\top} \\\\ &amp;=\\mathbf{V} \\mathbf{\\Sigma}^{\\top} \\mathbf{\\Sigma} \\mathbf{V}^{\\top} \\\\ &amp;=\\mathbf{V} \\hat{\\mathbf{\\Sigma}}^{2} \\mathbf{V}^{\\top} \\end{aligned} \\] where \\(\\hat{\\boldsymbol{\\Sigma}}\\) is the square diagonal matrix with the singular values of \\(\\mathbf{X}\\) and the excess zeros chopped off that satisfies \\(\\hat{\\boldsymbol{\\Sigma}}^{2}=\\boldsymbol{\\Sigma}^{\\top} \\boldsymbol{\\Sigma}\\). Comparison with the eigenvector factorization of \\(\\mathbf{X}^{\\top} \\mathbf{X}\\) establishes that the right singular vectors \\(\\mathbf{V}\\) of \\(\\mathbf{X}\\) are equivalent to the eigenvectors of \\(\\mathbf{X}^{\\top} \\mathbf{X}\\), while the singular values \\(\\sigma_{(k)}\\) of \\(\\mathbf{X}\\) are equal to the square-root of the eigenvalues \\(\\lambda_{(k)}\\) of \\(\\mathbf{X}^{\\top} \\mathbf{X}\\). biplot(reduced[, 1:2], loadings[, 1:2], cex=0.7) "],["factor-analysis.html", "Chapter 31 Factor Analysis", " Chapter 31 Factor Analysis Factor Analysis (FA) is a method for modeling observed variables, and their covariance structure, in terms of a smaller number of underlying latent (unobserved) “factors”. In FA the observed variables are modeled as linear functions of the “factors.” In PCA, we create new variables that are linear combinations of the observed variables. In both PCA and FA, the dimension of the data is reduced. A factor model can be thought of as a series of multiple regressions, predicting each of the observable variables \\(X_{i}\\) from the values of the (unobservable) common factors \\(f_{i}\\): \\[ \\begin{gathered} X_{1}=\\mu_{1}+l_{11} f_{1}+l_{12} f_{2}+\\cdots+l_{1 m} f_{m}+\\epsilon_{1} \\\\ X_{2}=\\mu_{2}+l_{21} f_{1}+l_{22} f_{2}+\\cdots+l_{2 m} f_{m}+\\epsilon_{2} \\\\ \\vdots \\\\ X_{p}=\\mu_{p}+l_{p 1} f_{1}+l_{p 2} f_{2}+\\cdots+l_{p m} f_{m}+\\epsilon_{p} \\end{gathered} \\] where \\(\\mu_{i}\\) is the variable mean (intercept). The regression coefficients \\(l_{i j}\\) (the partial slopes) for all of these multiple regressions are called factor loadings: \\(l_{i j}=\\) is loading of the \\(i^{t h}\\) variable on the \\(j^{t h}\\) factor. With a matrix notation, we can show the matrix of factor loadings: \\[ \\mathbf{L}=\\left(\\begin{array}{cccc} l_{11} &amp; l_{12} &amp; \\ldots &amp; l_{1 m} \\\\ l_{21} &amp; l_{22} &amp; \\ldots &amp; l_{2 m} \\\\ \\vdots &amp; \\vdots &amp; &amp; \\vdots \\\\ l_{p 1} &amp; l_{p 2} &amp; \\ldots &amp; l_{p m} \\end{array}\\right) \\] The errors \\(\\varepsilon_{i}\\) are called the specific factors. Here, \\(\\varepsilon_{i}=\\) specific factor for variable \\(i\\). When we collect them in a vector, we can express these series of multivariate regression as follows: \\[\\begin{equation} \\mathbf{X}=\\boldsymbol{\\mu}+\\mathbf{L f}+\\boldsymbol{\\epsilon} \\tag{31.1} \\end{equation}\\] There are multiple assumptions: \\(E\\left(\\epsilon_{i}\\right)=0\\) and \\(\\operatorname{var}\\left(\\epsilon_{i}\\right)=\\psi_{i}\\) (a.k.a “specific variance”), \\(E\\left(f_{i}\\right)=0\\) and \\(\\operatorname{var}\\left(f_{i}\\right)=1\\), \\(\\operatorname{cov}\\left(f_{i}, f_{j}\\right)=0\\) for \\(i \\neq j\\), \\(\\operatorname{cov}\\left(\\epsilon_{i}, \\epsilon_{j}\\right)=0\\) for \\(i \\neq j\\), \\(\\operatorname{cov}\\left(\\epsilon_{i}, f_{j}\\right)=0\\), Hence, \\(\\operatorname{var}\\left(X_{i}\\right)=\\sigma_{i}^{2}=\\sum_{j=1}^{m} l_{i j}^{2}+\\psi_{i}\\). The term \\(\\sum_{j=1}^{m} l_{i j}^{2}\\) is called the Communality for variable \\(i\\). The larger the communality, the better the model performance for the \\(i\\) th variable. \\(\\operatorname{cov}\\left(X_{i}, X_{j}\\right)=\\sigma_{i j}=\\sum_{k=1}^{m} l_{i k} l_{j k}\\), \\(\\operatorname{cov}\\left(X_{i}, f_{j}\\right)=l_{i j}\\) The factor model for our variance-covariance matrix can then be expressed as: \\[\\begin{equation} \\Sigma=\\mathbf{L L}^{\\prime}+\\mathbf{\\Psi} \\tag{31.2} \\end{equation}\\] where, \\[ \\boldsymbol{\\Psi}=\\left(\\begin{array}{cccc} \\psi_{1} &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; \\psi_{2} &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\ldots &amp; \\psi_{p} \\end{array}\\right) \\] And, \\[ \\hat{l}_{i j}=\\hat{e}_{j i} \\sqrt{\\hat{\\lambda}_j} \\] The total variance of each variable given in the factor model (27.2) can be explained by the sum of the shared variance with another variable, \\(\\mathbf{L} \\mathbf{L}^{\\prime}\\) (the common variance or communality) and the unique variance, \\(\\mathbf{\\Psi}\\), inherent to each variable (specific variance) There are multiple methods to estimate the parameters of a factor model. In general, two methods are most common: PCA and MLE. Let’s have an example. The data set is called bfi and comes from the psych package. It is made up of 25 self-report personality items from the International Personality Item Pool, gender, education level and age for 2800 subjects and used in the Synthetic Aperture Personality Assessment: The personality items are split into 5 categories: Agreeableness (A), Conscientiousness (C), Extraversion (E), Neuroticism (N), Openness (O). Each item was answered on a six point scale: 1 Very Inaccurate, 2 Moderately Inaccurate, 3 Slightly Inaccurate, 4 Slightly Accurate, 5 Moderately Accurate, 6 Very Accurate. library(psych) library(GPArotation) data(&quot;bfi&quot;) describeData(bfi, head = 5, tail=5) ## n.obs = 2800 of which 2236 are complete cases. Number of variables = 28 of which all are numeric TRUE ## variable # n.obs type H1 H2 H3 H4 H5 T1 T2 T3 T4 T5 ## A1 1 2784 1 2 2 5 4 2 6 2 2 5 2 ## A2 2 2773 1 4 4 4 4 3 1 4 3 2 3 ## A3 3 2774 1 3 5 5 6 3 3 4 5 2 1 ## A4 4 2781 1 4 2 4 5 4 3 3 2 4 4 ## A5 5 2784 1 4 5 4 5 5 3 5 5 4 2 ## C1 6 2779 1 2 5 4 4 4 6 2 5 5 5 ## C2 7 2776 1 3 4 5 4 4 6 3 5 5 5 ## C3 8 2780 1 3 4 4 3 5 6 4 5 5 3 ## C4 9 2774 1 4 3 2 5 3 1 4 1 2 3 ## C5 10 2784 1 4 4 5 5 2 1 3 1 6 3 ## E1 11 2777 1 3 1 2 5 2 1 2 2 2 3 ## E2 12 2784 1 3 1 4 3 2 4 2 2 2 3 ## E3 13 2775 1 3 6 4 4 5 5 4 6 4 1 ## E4 14 2791 1 4 4 4 4 4 5 4 3 5 2 ## E5 15 2779 1 4 3 5 4 5 6 3 6 4 2 ## N1 16 2778 1 3 3 4 2 2 1 NA 3 5 1 ## N2 17 2779 1 4 3 5 5 3 1 3 4 5 2 ## N3 18 2789 1 2 3 4 2 4 1 2 3 6 2 ## N4 19 2764 1 2 5 2 4 4 NA 3 3 4 1 ## N5 20 2771 1 3 5 3 1 3 1 3 1 1 1 ## O1 21 2778 1 3 4 4 3 3 6 6 5 5 3 ## O2 22 2800 1 6 2 2 3 3 1 3 1 2 1 ## O3 23 2772 1 3 4 5 4 4 6 5 6 5 3 ## O4 24 2786 1 4 3 5 3 3 6 4 4 5 5 ## O5 25 2780 1 3 3 2 5 3 1 2 3 1 1 ## gender 26 2800 1 1 2 2 2 1 1 1 2 1 2 ## education 27 2577 1 NA NA NA NA NA 3 4 4 4 4 ## age 28 2800 1 16 18 17 17 17 19 27 29 31 50 To get rid of missing observations and the last three variables, df &lt;- bfi[complete.cases(bfi[,1:25]),1:25] dim(bfi[,1:25]) ## [1] 2800 25 dim(df) ## [1] 2436 25 The first decision that we need make is the number of factors that we will need to extract. For \\(p=28\\), the variance-covariance matrix \\(\\Sigma\\) contains \\[ \\frac{p(p+1)}{2}=\\frac{25 \\times 26}{2}=325 \\] unique elements or entries. With \\(m\\) factors, the number of parameters in the factor model would be \\[ p(m+1)=25(m+1) \\] Taking \\(m=5\\), we have 150 parameters in the factor model. How do we choose \\(m\\)? Although it is common to look at the results of the principal components analysis, often in social sciences, the underlying theory within the field of study indicates how many factors to expect. scree(df) Let’s use the factanal() function of the build-in stats package pa.out &lt;- factanal(df, factors = 5) pa.out ## ## Call: ## factanal(x = df, factors = 5) ## ## Uniquenesses: ## A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 E3 ## 0.830 0.576 0.466 0.691 0.512 0.660 0.569 0.677 0.510 0.557 0.634 0.454 0.558 ## E4 E5 N1 N2 N3 N4 N5 O1 O2 O3 O4 O5 ## 0.468 0.592 0.271 0.337 0.478 0.507 0.664 0.675 0.744 0.518 0.752 0.726 ## ## Loadings: ## Factor1 Factor2 Factor3 Factor4 Factor5 ## A1 0.104 -0.393 ## A2 0.191 0.144 0.601 ## A3 0.280 0.110 0.662 ## A4 0.181 0.234 0.454 -0.109 ## A5 -0.124 0.351 0.580 ## C1 0.533 0.221 ## C2 0.624 0.127 0.140 ## C3 0.554 0.122 ## C4 0.218 -0.653 ## C5 0.272 -0.190 -0.573 ## E1 -0.587 -0.120 ## E2 0.233 -0.674 -0.106 -0.151 ## E3 0.490 0.315 0.313 ## E4 -0.121 0.613 0.363 ## E5 0.491 0.310 0.120 0.234 ## N1 0.816 -0.214 ## N2 0.787 -0.202 ## N3 0.714 ## N4 0.562 -0.367 -0.192 ## N5 0.518 -0.187 0.106 -0.137 ## O1 0.182 0.103 0.524 ## O2 0.163 -0.113 0.102 -0.454 ## O3 0.276 0.153 0.614 ## O4 0.207 -0.220 0.144 0.368 ## O5 -0.512 ## ## Factor1 Factor2 Factor3 Factor4 Factor5 ## SS loadings 2.687 2.320 2.034 1.978 1.557 ## Proportion Var 0.107 0.093 0.081 0.079 0.062 ## Cumulative Var 0.107 0.200 0.282 0.361 0.423 ## ## Test of the hypothesis that 5 factors are sufficient. ## The chi square statistic is 1490.59 on 185 degrees of freedom. ## The p-value is 1.22e-202 The first chunk provides the “uniqueness” (specific variance) for each variable, which range from 0 to 1 . The uniqueness, sometimes referred to as noise, corresponds to the proportion of variability, which can not be explained by a linear combination of the factors. This is the \\(\\hat{\\Psi}\\) in the equation above. A high uniqueness for a variable indicates that the factors do not account well for its variance. The next section reports the loadings ranging from \\(-1\\) to \\(1.\\) This is the \\(\\hat{\\mathbf{L}}\\) in the equation (27.2) above. The loadings are the contribution of each original variable to the factor. Variables with a high loading are well explained by the factor. Notice there is no entry for certain variables since \\(R\\) does not print loadings less than \\(0.1\\). The communalities for the \\(i^{t h}\\) variable are computed by taking the sum of the squared loadings for that variable. This is expressed below: \\[ \\hat{h}_i^2=\\sum_{j=1}^m \\hat{l}_{i j}^2 \\] This proportion of the variability is denoted as communality. Another way to calculate the communality is to subtract the uniquenesses from 1. An appropriate factor model results in low values for uniqueness and high values for communality. apply(pa.out$loadings^2,1,sum) # communality ## A1 A2 A3 A4 A5 C1 C2 C3 ## 0.1703640 0.4237506 0.5337657 0.3088959 0.4881042 0.3401202 0.4313729 0.3227542 ## C4 C5 E1 E2 E3 E4 E5 N1 ## 0.4900773 0.4427531 0.3659303 0.5459794 0.4422484 0.5319941 0.4079732 0.7294156 ## N2 N3 N4 N5 O1 O2 O3 O4 ## 0.6630751 0.5222584 0.4932099 0.3356293 0.3253527 0.2558864 0.4815981 0.2484000 ## O5 ## 0.2740596 1 - apply(pa.out$loadings^2,1,sum) # uniqueness ## A1 A2 A3 A4 A5 C1 C2 C3 ## 0.8296360 0.5762494 0.4662343 0.6911041 0.5118958 0.6598798 0.5686271 0.6772458 ## C4 C5 E1 E2 E3 E4 E5 N1 ## 0.5099227 0.5572469 0.6340697 0.4540206 0.5577516 0.4680059 0.5920268 0.2705844 ## N2 N3 N4 N5 O1 O2 O3 O4 ## 0.3369249 0.4777416 0.5067901 0.6643707 0.6746473 0.7441136 0.5184019 0.7516000 ## O5 ## 0.7259404 The table under the loadings reports the proportion of variance explained by each factor. The row Cumulative Var gives the cumulative proportion of variance explained. These numbers range from 0 to 1; Proportion Var shows the proportion of variance explained by each factor, and the row SS loadings gives the sum of squared loadings. This is sometimes used to determine the value of a particular factor. A factor is worth keeping if the SS loading is greater than 1 (Kaiser’s rule). The last section of the output reports a significance test: The null hypothesis is that the number of factors in the model is sufficient to capture the full dimensionality of the data set. Conventionally, we reject \\(H_0\\) if the \\(p\\)-value is less than \\(0.05\\). Such a result indicates that the number of factors is too low. The low \\(p\\)-value in our example above leads us to reject the \\(H_0\\), and indicates that we fitted NOT an appropriate model. Finally, with our estimated factor model, we may calculate \\(\\hat{\\Sigma}\\) and compare it to the observed correlation matrix, \\(S\\), by simple matrix algebra. Lambda &lt;- pa.out$loadings Psi &lt;- diag(pa.out$uniquenesses) S &lt;- pa.out$correlation Sigma &lt;- Lambda %*% t(Lambda) + Psi round(head(S) - head(Sigma), 2) ## A1 A2 A3 A4 A5 C1 C2 C3 C4 C5 E1 E2 ## A1 0.00 -0.12 -0.03 0.01 0.04 0.05 0.06 0.03 0.09 0.00 0.08 0.03 ## A2 -0.12 0.00 0.03 0.02 -0.03 -0.04 -0.05 0.03 -0.03 0.02 -0.04 -0.01 ## A3 -0.03 0.03 0.00 0.02 0.02 -0.02 -0.02 -0.02 -0.01 -0.01 0.03 0.01 ## A4 0.01 0.02 0.02 0.00 -0.02 -0.04 0.04 -0.06 0.01 -0.04 0.01 0.01 ## A5 0.04 -0.03 0.02 -0.02 0.00 0.02 -0.01 0.01 0.00 0.00 0.03 0.03 ## C1 0.05 -0.04 -0.02 -0.04 0.02 0.00 0.07 0.01 0.01 0.05 0.01 0.01 ## E3 E4 E5 N1 N2 N3 N4 N5 O1 O2 O3 O4 ## A1 0.07 0.05 0.01 -0.01 -0.02 0.02 0.01 0.00 0.06 0.06 0.02 -0.02 ## A2 -0.06 -0.04 0.07 0.00 0.04 -0.03 -0.01 -0.01 -0.01 -0.01 -0.03 0.01 ## A3 0.01 -0.03 -0.01 0.02 0.01 -0.01 -0.02 -0.05 0.00 -0.02 0.00 -0.03 ## A4 -0.01 0.01 -0.02 0.02 -0.02 0.01 -0.02 0.00 0.02 -0.02 0.00 -0.02 ## A5 0.03 0.04 -0.01 0.00 0.00 -0.01 -0.01 0.00 0.00 0.00 0.00 0.00 ## C1 -0.02 0.06 0.02 -0.02 -0.01 0.02 0.01 0.01 -0.01 0.02 0.00 0.04 ## O5 ## A1 0.07 ## A2 -0.05 ## A3 -0.01 ## A4 -0.01 ## A5 0.00 ## C1 0.02 This matrix is also called as the residual matrix. For more see: https://cran.r-project.org/web/packages/factoextra/readme/README.html "],["graphical-network-analysis.html", "Graphical Network Analysis", " Graphical Network Analysis Following chapters are work in progress without a proper copyediting … "],["fundementals.html", "Chapter 32 Fundementals 32.1 Covariance 32.2 Correlation 32.3 Precision matrix 32.4 Semi-partial correlation", " Chapter 32 Fundementals Graphical modeling refers to a class of probabilistic models that uses graphs to express conditional (in)dependence relations between random variables. A graphical model represents the probabilistic relationships among a set of variables. Nodes in the graph correspond to variables, and the absence of edges (partial correlations) corresponds to conditional independence. Graphical models are becoming more popular in statistics and in its applications in many different fields for several reasons. The central idea is that each variable is represented by a node in a graph. Any pair of nodes may be joined by an edge. For most types of graph a missing edge represents some form of independency between the pair of variables. Because the independency may be either marginal or conditional on some or all of the other variables, a variety of types of graph are needed. A particularly important distinction is between directed and undirected edges. In the former an arrow indicates the direction of dependency from an explanatory variable to a response. If, however, the two variables are to be interpreted on an equal footing then an edge between them is undirected, or cyclic dependencies are permitted. Hence, we need to cover several concept related to statistical dependece, correlations 32.1 Covariance library(ppcor) library(glasso) library(glassoFast) library(corpcor) library(rags2ridges) First, the data matrix, which refers to the array of numbers: \\[ \\mathbf{X}=\\left(\\begin{array}{cccc} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1 p} \\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2 p} \\\\ x_{31} &amp; x_{32} &amp; \\cdots &amp; x_{3 p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n 1} &amp; x_{n 2} &amp; \\cdots &amp; x_{n p} \\end{array}\\right) \\] Our data set.seed(5) x &lt;- rnorm(30, sd=runif(30, 2, 50)) X &lt;- matrix(x, 10) X ## [,1] [,2] [,3] ## [1,] -1.613670 -4.436764 42.563842 ## [2,] -20.840548 36.237338 -36.942481 ## [3,] -100.484392 25.903897 -24.294407 ## [4,] 3.769073 -18.950442 -22.616651 ## [5,] -1.821506 -12.454626 -1.243431 ## [6,] 32.103933 3.693050 38.807102 ## [7,] 25.752668 22.861071 -18.452338 ## [8,] 59.864792 98.848864 -3.607105 ## [9,] 33.862342 34.853324 16.704375 ## [10,] 5.980194 62.755408 -21.841795 The Covariation of Data \\[ \\mathbf{S}=\\left(\\begin{array}{ccccc} s_{1}^{2} &amp; s_{12} &amp; s_{13} &amp; \\cdots &amp; s_{1 p} \\\\ s_{21} &amp; s_{2}^{2} &amp; s_{23} &amp; \\cdots &amp; s_{2 p} \\\\ s_{31} &amp; s_{32} &amp; s_{3}^{2} &amp; \\cdots &amp; s_{3 p} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ s_{p 1} &amp; s_{p 2} &amp; s_{p 3} &amp; \\cdots &amp; s_{p}^{2} \\end{array}\\right) \\] \\[ s_{j}^{2}=(1 / n) \\sum_{i=1}^{n}\\left(x_{i j}-\\bar{x}_{j}\\right)^{2} \\text { is the variance of the } j \\text {-th variable } \\] \\[ \\begin{aligned} &amp;s_{j k}=(1 / n) \\sum_{i=1}^{n}\\left(x_{i j}-\\bar{x}_{j}\\right)\\left(x_{i k}-\\bar{x}_{k}\\right) \\text { is the covariance between the } j \\text {-th and } k \\text {-th variables } \\end{aligned} \\] \\[ \\bar{x}_{j}=(1 / n) \\sum_{i=1}^{n} x_{i j} \\text { is the mean of the } j \\text {-th variable } \\] We can calculate the covariance matrix such as \\[ \\mathbf{S}=\\frac{1}{n} \\mathbf{X}_{c}^{\\prime} \\mathbf{X}_{c} \\] Note that the centered matrix \\(\\mathbf{X}_{c}\\) \\[ \\mathbf{X}_{c}=\\left(\\begin{array}{cccc} x_{11}-\\bar{x}_{1} &amp; x_{12}-\\bar{x}_{2} &amp; \\cdots &amp; x_{1 p}-\\bar{x}_{p} \\\\ x_{21}-\\bar{x}_{1} &amp; x_{22}-\\bar{x}_{2} &amp; \\cdots &amp; x_{2 p}-\\bar{x}_{p} \\\\ x_{31}-\\bar{x}_{1} &amp; x_{32}-\\bar{x}_{2} &amp; \\cdots &amp; x_{3 p}-\\bar{x}_{p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n 1}-\\bar{x}_{1} &amp; x_{n 2}-\\bar{x}_{2} &amp; \\cdots &amp; x_{n p}-\\bar{x}_{p} \\end{array}\\right) \\] How? # More direct n &lt;- nrow(X) m &lt;- matrix(1, n, 1)%*%colMeans(X) Xc &lt;- X-m Xc ## [,1] [,2] [,3] ## [1,] -5.2709585 -29.3678760 45.6561309 ## [2,] -24.4978367 11.3062262 -33.8501919 ## [3,] -104.1416804 0.9727849 -21.2021184 ## [4,] 0.1117842 -43.8815539 -19.5243622 ## [5,] -5.4787951 -37.3857380 1.8488577 ## [6,] 28.4466449 -21.2380620 41.8993911 ## [7,] 22.0953790 -2.0700407 -15.3600493 ## [8,] 56.2075038 73.9177518 -0.5148158 ## [9,] 30.2050530 9.9222117 19.7966643 ## [10,] 2.3229057 37.8242961 -18.7495065 # Or #http://users.stat.umn.edu/~helwig/notes/datamat-Notes.pdf C &lt;- diag(n) - matrix(1/n, n, n) XC &lt;- C %*% X XC ## [,1] [,2] [,3] ## [1,] -5.2709585 -29.3678760 45.6561309 ## [2,] -24.4978367 11.3062262 -33.8501919 ## [3,] -104.1416804 0.9727849 -21.2021184 ## [4,] 0.1117842 -43.8815539 -19.5243622 ## [5,] -5.4787951 -37.3857380 1.8488577 ## [6,] 28.4466449 -21.2380620 41.8993911 ## [7,] 22.0953790 -2.0700407 -15.3600493 ## [8,] 56.2075038 73.9177518 -0.5148158 ## [9,] 30.2050530 9.9222117 19.7966643 ## [10,] 2.3229057 37.8242961 -18.7495065 # Or Xcc &lt;- scale(X, center=TRUE, scale=FALSE) # Covariance Matrix S &lt;- t(Xc) %*% Xc / (n-1) S ## [,1] [,2] [,3] ## [1,] 1875.3209 429.8712 462.4775 ## [2,] 429.8712 1306.9817 -262.8231 ## [3,] 462.4775 -262.8231 755.5193 # Check it cov(X) ## [,1] [,2] [,3] ## [1,] 1875.3209 429.8712 462.4775 ## [2,] 429.8712 1306.9817 -262.8231 ## [3,] 462.4775 -262.8231 755.5193 32.2 Correlation \\[ \\mathbf{R}=\\left(\\begin{array}{ccccc} 1 &amp; r_{12} &amp; r_{13} &amp; \\cdots &amp; r_{1 p} \\\\ r_{21} &amp; 1 &amp; r_{23} &amp; \\cdots &amp; r_{2 p} \\\\ r_{31} &amp; r_{32} &amp; 1 &amp; \\cdots &amp; r_{3 p} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ r_{p 1} &amp; r_{p 2} &amp; r_{p 3} &amp; \\cdots &amp; 1 \\end{array}\\right) \\] where \\[ r_{j k}=\\frac{s_{j k}}{s_{j} s_{k}}=\\frac{\\sum_{i=1}^{n}\\left(x_{i j}-\\bar{x}_{j}\\right)\\left(x_{i k}-\\bar{x}_{k}\\right)}{\\sqrt{\\sum_{i=1}^{n}\\left(x_{i j}-\\bar{x}_{j}\\right)^{2}} \\sqrt{\\sum_{i=1}^{n}\\left(x_{i k}-\\bar{x}_{k}\\right)^{2}}} \\] is the Pearson correlation coefficient between variables \\(\\mathbf{X}_{j}\\) and \\(\\mathbf{X}_{k}\\) We can calculate the correlation matrix such as \\[ \\mathbf{R}=\\frac{1}{n} \\mathbf{X}_{s}^{\\prime} \\mathbf{X}_{s} \\] where \\(\\mathbf{X}_{s}=\\mathbf{C X D}^{-1}\\) with \\(\\mathbf{C}=\\mathbf{I}_{n}-n^{-1} \\mathbf{1}_{n} \\mathbf{1}_{n}^{\\prime}\\) denoting a centering matrix, \\(\\mathbf{D}=\\operatorname{diag}\\left(s_{1}, \\ldots, s_{p}\\right)\\) denoting a diagonal scaling matrix. Note that the standardized matrix \\(\\mathbf{X}_{s}\\) has the form \\[ \\mathbf{X}_{s}=\\left(\\begin{array}{cccc} \\left(x_{11}-\\bar{x}_{1}\\right) / s_{1} &amp; \\left(x_{12}-\\bar{x}_{2}\\right) / s_{2} &amp; \\cdots &amp; \\left(x_{1 p}-\\bar{x}_{p}\\right) / s_{p} \\\\ \\left(x_{21}-\\bar{x}_{1}\\right) / s_{1} &amp; \\left(x_{22}-\\bar{x}_{2}\\right) / s_{2} &amp; \\cdots &amp; \\left(x_{2 p}-\\bar{x}_{p}\\right) / s_{p} \\\\ \\left(x_{31}-\\bar{x}_{1}\\right) / s_{1} &amp; \\left(x_{32}-\\bar{x}_{2}\\right) / s_{2} &amp; \\cdots &amp; \\left(x_{3 p}-\\bar{x}_{p}\\right) / s_{p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\left(x_{n 1}-\\bar{x}_{1}\\right) / s_{1} &amp; \\left(x_{n 2}-\\bar{x}_{2}\\right) / s_{2} &amp; \\cdots &amp; \\left(x_{n p}-\\bar{x}_{p}\\right) / s_{p} \\end{array}\\right) \\] How? # More direct n &lt;- nrow(X) sdx &lt;- 1/matrix(1, n, 1)%*%apply(X, 2, sd) m &lt;- matrix(1, n, 1)%*%colMeans(X) Xs &lt;- (X-m)*sdx Xs ## [,1] [,2] [,3] ## [1,] -0.121717156 -0.81233989 1.66102560 ## [2,] -0.565704894 0.31273963 -1.23151117 ## [3,] -2.404843294 0.02690804 -0.77135887 ## [4,] 0.002581324 -1.21380031 -0.71032005 ## [5,] -0.126516525 -1.03412063 0.06726369 ## [6,] 0.656890910 -0.58746247 1.52435083 ## [7,] 0.510227259 -0.05725905 -0.55881729 ## [8,] 1.297945627 2.04462654 -0.01872963 ## [9,] 0.697496131 0.27445664 0.72022674 ## [10,] 0.053640619 1.04625151 -0.68212986 # Or #http://users.stat.umn.edu/~helwig/notes/datamat-Notes.pdf C &lt;- diag(n) - matrix(1/n, n, n) D &lt;- diag(apply(X, 2, sd)) XS &lt;- C %*% X %*% solve(D) XS ## [,1] [,2] [,3] ## [1,] -0.121717156 -0.81233989 1.66102560 ## [2,] -0.565704894 0.31273963 -1.23151117 ## [3,] -2.404843294 0.02690804 -0.77135887 ## [4,] 0.002581324 -1.21380031 -0.71032005 ## [5,] -0.126516525 -1.03412063 0.06726369 ## [6,] 0.656890910 -0.58746247 1.52435083 ## [7,] 0.510227259 -0.05725905 -0.55881729 ## [8,] 1.297945627 2.04462654 -0.01872963 ## [9,] 0.697496131 0.27445664 0.72022674 ## [10,] 0.053640619 1.04625151 -0.68212986 # Or Xss &lt;- scale(X, center=TRUE, scale=TRUE) # Covariance Matrix R &lt;- t(Xs) %*% Xs / (n-1) R ## [,1] [,2] [,3] ## [1,] 1.0000000 0.2745780 0.3885349 ## [2,] 0.2745780 1.0000000 -0.2644881 ## [3,] 0.3885349 -0.2644881 1.0000000 # Check it cor(X) ## [,1] [,2] [,3] ## [1,] 1.0000000 0.2745780 0.3885349 ## [2,] 0.2745780 1.0000000 -0.2644881 ## [3,] 0.3885349 -0.2644881 1.0000000 32.3 Precision matrix The inverse of this matrix, \\(\\mathrm{S}_{\\mathrm{XX}}^{-1}\\), if it exists, is the inverse covariance matrix, also known as the concentration matrix or precision matrix. Let us consider a 2×2 covariance matrix \\[ \\left[\\begin{array}{cc} \\sigma^{2}(x) &amp; \\rho \\sigma(x) \\sigma(y) \\\\ \\rho \\sigma(x) \\sigma(y) &amp; \\sigma^{2}(y) \\end{array}\\right] \\] Its inverse: \\[ \\frac{1}{\\sigma^{2}(x) \\sigma^{2}(y)-\\rho^{2} \\sigma^{2}(x) \\sigma^{2}(y)}\\left[\\begin{array}{cc} \\sigma^{2}(y) &amp; -\\rho \\sigma(x) \\sigma(y) \\\\ -\\rho \\sigma(x) \\sigma(y) &amp; \\sigma^{2}(x) \\end{array}\\right] \\] If call the the precision matrix \\(D\\), the correlation coefficient will be \\[ -\\frac{d_{i j}}{\\sqrt{d_{i i}} \\sqrt{d_{j j}}}, \\] Or, \\[ \\frac{-\\rho \\sigma_{x} \\sigma_{y}}{\\sigma_{x}^{2} \\sigma_{y}^{2}\\left(1-e^{2}\\right)} \\times \\sqrt{\\sigma_{x}^{2}\\left(1-\\rho^{2}\\right)} \\sqrt{\\sigma_{y}^{2}\\left(1-\\rho^{2}\\right)}=-\\rho \\] That was for a 2x2 cov-matrix. When we have more columns, the correlation coefficient reflects partial correlations. Here is an example: pm &lt;- solve(S) # precision matrix pm ## [,1] [,2] [,3] ## [1,] 0.0007662131 -0.0003723763 -0.0005985624 ## [2,] -0.0003723763 0.0010036440 0.0005770819 ## [3,] -0.0005985624 0.0005770819 0.0018907421 # Partial correlation of 1,2 -pm[1,2]/(sqrt(pm[1,1])*sqrt(pm[2,2])) ## [1] 0.4246365 # Or -cov2cor(solve(S)) ## [,1] [,2] [,3] ## [1,] -1.0000000 0.4246365 0.4973000 ## [2,] 0.4246365 -1.0000000 -0.4189204 ## [3,] 0.4973000 -0.4189204 -1.0000000 # Or ppcor::pcor(X) ## $estimate ## [,1] [,2] [,3] ## [1,] 1.0000000 0.4246365 0.4973000 ## [2,] 0.4246365 1.0000000 -0.4189204 ## [3,] 0.4973000 -0.4189204 1.0000000 ## ## $p.value ## [,1] [,2] [,3] ## [1,] 0.0000000 0.2546080 0.1731621 ## [2,] 0.2546080 0.0000000 0.2617439 ## [3,] 0.1731621 0.2617439 0.0000000 ## ## $statistic ## [,1] [,2] [,3] ## [1,] 0.000000 1.240918 1.516557 ## [2,] 1.240918 0.000000 -1.220629 ## [3,] 1.516557 -1.220629 0.000000 ## ## $n ## [1] 10 ## ## $gp ## [1] 1 ## ## $method ## [1] &quot;pearson&quot; 32.4 Semi-partial correlation With partial correlation, we find the correlation between X and Y holding Z constant for both X and Y. Sometimes, however, we want to hold Z constant for just X or just Y. In that case, we compute a semipartial correlation. A partial correlation is computed between two residuals. A semipartial is computed between one residual and another raw or unresidualized variable. One interpretation of the semipartial is that it is the correlation between one variable and the residual of another, so that the influence of a third variable is only partialed from one of two variables (hence, semipartial). Another interpretation is that the semipartial shows the increment in correlation of one variable above and beyond another. This is seen most easily with the R2 formulation. Partial \\[ r_{12.3}^{2}=\\frac{R_{1.23}^{2}-R_{1.3}^{2}}{1-R_{1.3}^{2}} \\] SemiPartial \\[ r_{1(2.3)}^{2}=R_{1.23}^{2}-R_{1.3}^{2} \\] The difference between a slope coefficient, semi-partial and partial correlation can be seen by looking their definitions: Partial: \\[ x_{12,3}=\\frac{r_{12}-r_{13} r_{23}}{\\sqrt{1-r_{12}^{2}} \\sqrt{1-r_{23}^{2}}} \\] Regression: \\[ x_{1}=b_{1}+b_{2} x_{2}+b_{2} X_{3} \\] and \\[ b_{2}=\\frac{\\sum x_{3}^{2} \\sum x_{1} x_{2}-\\sum x_{1} x_{3} \\sum x_{2} x_{3}}{\\sum x_{2}^{2} \\sum x_{3}^{2}-\\left(\\sum x_{2} x_{3}\\right)^{2}} \\] With standardized variables: \\[ b_{2}=\\frac{r_{12}-r_{13} r_{23}}{1-r_{23}^{2}} \\] Semi-partial (or “part”) correlation: \\[ r_{1(2.3)}=\\frac{r_{1 2}-r_{1_{3}} r_{23}}{\\sqrt{1-r_{23}^{2}}} \\] see http://faculty.cas.usf.edu/mbrannick/regression/Partial.html for very nice Venn diagrams. The difference is the square root in the denominator. The regression coefficient can exceed 1.0 in absolute value; the correlation cannot. The difference between “beta” coefficients and semipartial is that semipartial normalizes the coefficient between -1 and +1. The function spcor can calculate the pairwise semi-partial (part) correlations for each pair of variables given others. In addition, it gives us the p value as well as statistic for each pair of variables. Xx &lt;- X[,1] Y &lt;- X[,2] Z &lt;- X[,3] ppcor::spcor(X) ## $estimate ## [,1] [,2] [,3] ## [1,] 1.0000000 0.3912745 0.4781862 ## [2,] 0.4095148 1.0000000 -0.4028191 ## [3,] 0.4795907 -0.3860075 1.0000000 ## ## $p.value ## [,1] [,2] [,3] ## [1,] 0.0000000 0.2977193 0.1929052 ## [2,] 0.2737125 0.0000000 0.2824036 ## [3,] 0.1914134 0.3048448 0.0000000 ## ## $statistic ## [,1] [,2] [,3] ## [1,] 0.000000 1.124899 1.440535 ## [2,] 1.187625 0.000000 -1.164408 ## [3,] 1.446027 -1.107084 0.000000 ## ## $n ## [1] 10 ## ## $gp ## [1] 1 ## ## $method ## [1] &quot;pearson&quot; lm(Xx~Y+Z) ## ## Call: ## lm(formula = Xx ~ Y + Z) ## ## Coefficients: ## (Intercept) Y Z ## -6.0434 0.4860 0.7812 For more information and matrix solutions to partial correlations, see https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4681537/pdf/nihms740182.pdf (Kim_2015?). "],["regularized-covariance-matrix.html", "Chapter 33 Regularized covariance matrix 33.1 MLE 33.2 High-dimensional data 33.3 Ridge (\\(\\ell_{2}\\)) and glasso (\\(\\ell_{1}\\)) 33.4 What’s graphical - graphical ridge or glasso?", " Chapter 33 Regularized covariance matrix Why is a covariance matrix \\(S\\) is singular when \\(n&lt;p\\) in the data matrix of \\(X\\)? Consider the \\(n \\times p\\) matrix of sample data, \\(X\\). From the above, the rank of \\(X\\) is at most \\(\\min (n, p)\\). Since \\[ \\mathbf{S}=\\frac{1}{n} \\mathbf{X}_{c}^{\\prime} \\mathbf{X}_{c} \\] \\(\\operatorname{rank}(X_c)\\) will be \\(n\\) (\\(n&lt;p\\)). Since \\(\\operatorname{rank}(A B) \\leq \\min (\\operatorname{rank}(A), \\operatorname{rank}(B))\\). Clearly the rank of \\(S\\) won’t be larger than the rank of \\(X_c\\). Since \\(S\\) is \\(p \\times p\\) and its rank is \\(n\\), \\(S\\) will be singular. That’s, if \\(n&lt;p\\) then \\(\\operatorname{rank}(X)&lt;p\\) in which case \\(\\operatorname{rank}(S)&lt;p\\). 33.1 MLE Before understanding L1 or L2 regularization, we need to see the multivariate Gaussian distribution, its parameterization and MLE solutions. The multivariate Gaussian distribution of a random vector \\(X \\in \\mathbf{R}^{p}\\) is commonly expressed in terms of the parameters \\(\\mu\\) and \\(\\Sigma\\), where \\(\\mu\\) is an \\(p \\times 1\\) vector and \\(\\Sigma\\) is an \\(p \\times p\\), a nonsingular symmetric covariance matrix. Hence, we have the following form for the density function (the Wishart distribution arises as the distribution of the sample covariance matrix for a sample from a multivariate normal distribution - See Wishard Distribution): \\[ f(x \\mid \\mu, \\Sigma)=\\frac{1}{(2 \\pi)^{p / 2}|\\Sigma|^{1 / 2}} \\exp \\left\\{-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right\\}, \\] where \\(|\\Sigma|\\) is the determinant of the covariance matrix. The likelihood function is: \\[ \\mathcal{L}(\\mu, \\Sigma)=(2 \\pi)^{-\\frac{n p}{2}} \\prod_{i=1}^{n} \\operatorname{det}(\\Sigma)^{-\\frac{1}{2}} \\exp \\left(-\\frac{1}{2}\\left(x_{i}-\\mu\\right)^{\\mathrm{T}} \\Sigma^{-1}\\left(x_{i}-\\mu\\right)\\right) \\] Since the estimate \\(\\bar{x}\\) does not depend on \\(\\Sigma\\), we can just substitute it for \\(\\mu\\) in the likelihood function, getting \\[ \\mathcal{L}(\\bar{x}, \\Sigma) \\propto \\operatorname{det}(\\Sigma)^{-\\frac{n}{2}} \\exp \\left(-\\frac{1}{2} \\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{\\mathrm{T}} \\Sigma^{-1}\\left(x_{i}-\\bar{x}\\right)\\right) \\] and then seek the value of \\(\\Sigma\\) that maximizes the likelihood of the data (in practice it is easier to work with \\(\\log \\mathcal{L}\\) ). Regard the scalar \\(\\left(x_{i}-\\bar{x}\\right)^{\\mathrm{T}} \\Sigma^{-1}\\left(x_{i}-\\bar{x}\\right)\\) as the trace of a \\(1 \\times 1\\) matrix. This makes it possible to use the identity \\(\\operatorname{tr}(A B)=\\operatorname{tr}(B A)\\) \\[ \\begin{aligned} \\mathcal{L}(\\bar{x}, \\Sigma) &amp; \\propto \\operatorname{det}(\\Sigma)^{-\\frac{n}{2}} \\exp \\left(-\\frac{1}{2} \\sum_{i=1}^{n}\\left(\\left(x_{i}-\\bar{x}\\right)^{\\mathrm{T}} \\Sigma^{-1}\\left(x_{i}-\\bar{x}\\right)\\right)\\right) \\\\ &amp;=\\operatorname{det}(\\Sigma)^{-\\frac{n}{2}} \\exp \\left(-\\frac{1}{2} \\sum_{i=1}^{n} \\operatorname{tr}\\left(\\left(x_{i}-\\bar{x}\\right)\\left(x_{i}-\\bar{x}\\right)^{\\mathrm{T}} \\Sigma^{-1}\\right)\\right) \\\\ &amp;=\\operatorname{det}(\\Sigma)^{-\\frac{n}{2}} \\exp \\left(-\\frac{1}{2} \\operatorname{tr}\\left(\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\left(x_{i}-\\bar{x}\\right)^{\\mathrm{T}} \\Sigma^{-1}\\right)\\right) \\\\ &amp;=\\operatorname{det}(\\Sigma)^{-\\frac{n}{2}} \\exp \\left(-\\frac{1}{2} \\operatorname{tr}\\left(S \\Sigma^{-1}\\right)\\right) \\end{aligned} \\] where \\[ S=\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\left(x_{i}-\\bar{x}\\right)^{\\mathrm{T}} \\in \\mathbf{R}^{p \\times p} \\] And finally, re-write the likelihood in the log form using the trace trick: \\[ \\ln \\mathcal{L}(\\mu, \\Sigma)=\\text { const }-\\frac{n}{2} \\ln \\operatorname{det}(\\Sigma)-\\frac{1}{2} \\operatorname{tr}\\left[\\Sigma^{-1} \\sum_{i=1}^{n}\\left(x_{i}-\\mu\\right)\\left(x_{i}-\\mu\\right)^{\\mathrm{T}}\\right] \\] or, for a multivariate normal model with mean 0 and covariance \\(\\Sigma\\), the likelihood function in this case is given by \\[ \\ell(\\Omega ; S)=\\ln |\\Omega|-\\operatorname{tr}(S \\Omega) \\] where \\(\\Omega=\\Sigma^{-1}\\) is the so-called precision matrix (also sometimes called the concentration matrix). It is precisely this \\(\\Omega\\) for which we seek an estimate, which we will denote \\(P\\). Indeed, one can naturally try to use the inverse of \\(S\\) for this. The differential of this log-likelihood is \\[ d \\ln \\mathcal{L}(\\mu, \\Sigma)=\\\\-\\frac{n}{2} \\operatorname{tr}\\left[\\Sigma^{-1}\\{d \\Sigma\\}\\right]-\\frac{1}{2} \\operatorname{tr}\\left[-\\Sigma^{-1}\\{d \\Sigma\\} \\Sigma^{-1} \\sum_{i=1}^{n}\\left(x_{i}-\\mu\\right)\\left(x_{i}-\\mu\\right)^{\\mathrm{T}}-2 \\Sigma^{-1} \\sum_{i=1}^{n}\\left(x_{i}-\\mu\\right)\\{d \\mu\\}^{\\mathrm{T}}\\right] \\] and \\[ \\sum_{i=1}^{n}\\left(x_{i}-\\mu\\right)\\left(x_{i}-\\mu\\right)^{\\mathrm{T}}=\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\left(x_{i}-\\bar{x}\\right)^{\\mathrm{T}}=S \\] Then the terms involving \\(d \\Sigma\\) in \\(d \\ln \\mathcal{L}\\) can be combined as \\[ -\\frac{1}{2} \\operatorname{tr}\\left(\\Sigma^{-1}\\{d \\Sigma\\}\\left[n I_{p}-\\Sigma^{-1} S\\right]\\right) \\] See the rest from https://en.wikipedia.org/wiki/Estimation_of_covariance_matrices. For \\(n &lt; p\\), the empirical estimate of the covariance matrix becomes singular, i.e. it cannot be inverted to compute the precision matrix. There is also another intuitive way to see the whole algebra (see the post here) (Intui_Cross?): Let’s start with the univariate standard normal density (parameter free) which is \\[ \\frac{1}{\\sqrt{2 \\pi}} \\exp \\left(-\\frac{1}{2} t^{2}\\right) \\] When we extend (parameterize) it to \\(x=\\sigma t+\\mu\\), the change of variable requires \\(d t=\\frac{1}{\\sigma} d x\\) making the general normal density \\[ \\frac{1}{\\sqrt{2 \\pi}} \\frac{1}{\\sigma} \\exp \\left(-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}\\right) \\] The log-likelihood is \\[ \\text { A constant }-\\frac{n}{2} \\log \\left(\\sigma^{2}\\right)-\\frac{1}{2} \\sum_{i=1}^{n}\\left(\\frac{x_{i}-\\mu}{\\sigma}\\right)^{2}, \\] maximization of which is equivalent to minimizing \\[ n \\log \\left(\\sigma^{2}\\right)+\\sum_{i=1}^{n}\\left(\\frac{x_{i}-\\mu}{\\sigma}\\right)^{2} \\] Multivariate (say number of dimensions \\(=d\\) ) counterpart behaves the similar way. Starting with the generating (standard) density \\[ (\\sqrt{2 \\pi})^{-d} \\exp \\left(-\\frac{1}{2} \\mathbf{z}^{t} \\mathbf{z}\\right) \\] and the general multivariate normal (MVN) density is \\[ (\\sqrt{2 \\pi})^{-d}|\\boldsymbol{\\Sigma}|^{-1 / 2} \\exp \\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^{t} \\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right) \\] Observe that \\(|\\boldsymbol{\\Sigma}|^{-1 / 2}\\) (which is the reciprocal of the square root of the determinant of the covariance matrix \\(\\boldsymbol{\\Sigma}\\) ) in the multivariate case does what \\(1 / \\sigma\\) does in the univariate case and \\(\\boldsymbol{\\Sigma}^{-1}\\) does what \\(1 / \\sigma^{2}\\) does in the univariate case. In simpler terms, \\(|\\boldsymbol{\\Sigma}|^{-1 / 2}\\) is the change of variable “adjustment”. The maximization of likelihood would lead to minimizing (analogous to the univariate case) \\[ n \\log |\\boldsymbol{\\Sigma}|+\\sum_{i=1}^{n}(\\mathbf{x}-\\boldsymbol{\\mu})^{t} \\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}) \\] Again, in simpler terms, \\(n \\log |\\mathbf{\\Sigma}|\\) takes the spot of \\(n \\log \\left(\\sigma^{2}\\right)\\) which was there in the univariate case. These terms account for corresponding change of variable adjustments in each scenario. Let’s start with a data matrix of 10x6, where no need for regularization. n = 10 p = 6 X &lt;- matrix (rnorm(n*p), n, p) # Cov. &amp; Precision Matrices S &lt;- cov(X) pm &lt;- solve(S) # precision -pm[1,2]/(sqrt(pm[1,1])*sqrt(pm[2,2])) ## [1] -0.3761039 -cov2cor(pm) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] -1.000000000 -0.3761039 0.75733464 -0.742262100 -0.008589893 -0.05638894 ## [2,] -0.376103886 -1.0000000 0.42071649 -0.503593629 0.369656950 -0.21253696 ## [3,] 0.757334638 0.4207165 -1.00000000 0.899556831 0.082741691 -0.14425448 ## [4,] -0.742262100 -0.5035936 0.89955683 -1.000000000 0.009503286 -0.02858780 ## [5,] -0.008589893 0.3696569 0.08274169 0.009503286 -1.000000000 0.45766004 ## [6,] -0.056388945 -0.2125370 -0.14425448 -0.028587801 0.457660041 -1.00000000 # ppcor pc &lt;- ppcor::pcor(X) pc$estimate ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1.000000000 -0.3761039 0.75733464 -0.742262100 -0.008589893 -0.05638894 ## [2,] -0.376103886 1.0000000 0.42071649 -0.503593629 0.369656950 -0.21253696 ## [3,] 0.757334638 0.4207165 1.00000000 0.899556831 0.082741691 -0.14425448 ## [4,] -0.742262100 -0.5035936 0.89955683 1.000000000 0.009503286 -0.02858780 ## [5,] -0.008589893 0.3696569 0.08274169 0.009503286 1.000000000 0.45766004 ## [6,] -0.056388945 -0.2125370 -0.14425448 -0.028587801 0.457660041 1.00000000 # glasso glassoFast::glassoFast(S,rho=0) ## $w ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1.066265707 -0.005231957 0.29450734 -0.11452030 -0.03539292 -0.22942515 ## [2,] -0.005231957 0.807021234 0.01166286 -0.17816736 0.26998413 -0.04780001 ## [3,] 0.294507338 0.011662864 0.65019637 0.53298298 0.01622787 -0.34817189 ## [4,] -0.114520302 -0.178167358 0.53298298 0.73032440 -0.03539357 -0.25756187 ## [5,] -0.035392916 0.269984128 0.01622787 -0.03539357 0.67044676 0.34574204 ## [6,] -0.229425151 -0.047800009 -0.34817189 -0.25756187 0.34574204 1.29620992 ## ## $wi ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 2.40091766 0.8293192 -3.8703842 3.43740240 0.01964285 0.09369095 ## [2,] 0.82931917 2.0254031 -1.9746520 2.14186241 -0.77814567 0.32421492 ## [3,] -3.87038419 -1.9746520 10.8784484 -8.86746443 -0.40358316 0.50985808 ## [4,] 3.43740240 2.1418624 -8.8674644 8.93255772 -0.04202985 0.09163187 ## [5,] 0.01964285 -0.7781457 -0.4035832 -0.04202985 2.18760326 -0.72548033 ## [6,] 0.09369095 0.3242149 0.5098581 0.09163187 -0.72548033 1.14868499 ## ## $errflag ## [1] 0 ## ## $niter ## [1] 1 Rl &lt;- glassoFast::glassoFast(S,rho=0)$wi # -Rl[1,2]/(sqrt(Rl[1,1])*sqrt(Rl[2,2])) ## [1] -0.3760775 -cov2cor(Rl) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] -1.000000000 -0.3760775 0.75732443 -0.742256187 -0.008571002 -0.05641681 ## [2,] -0.376077488 -1.0000000 0.42067950 -0.503556167 0.369675686 -0.21255767 ## [3,] 0.757324429 0.4206795 -1.00000000 0.899555705 0.082730429 -0.14423324 ## [4,] -0.742256187 -0.5035562 0.89955570 -1.000000000 0.009507922 -0.02860607 ## [5,] -0.008571002 0.3696757 0.08273043 0.009507922 -1.000000000 0.45765782 ## [6,] -0.056416812 -0.2125577 -0.14423324 -0.028606069 0.457657818 -1.00000000 33.2 High-dimensional data Now with a data matrix of 6x10: n = 6 p = 10 set.seed(1) X &lt;- matrix (rnorm(n*p), n, p) # Cov. &amp; Precision Matrices S &lt;- cov(X) S ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.889211221 -0.17223814 -0.36660043 0.35320957 -0.629545741 -0.27978848 ## [2,] -0.172238139 0.34416306 -0.09280183 -0.04282613 0.139236591 -0.26060435 ## [3,] -0.366600426 -0.09280183 1.46701338 -0.50796342 -0.024550727 -0.11504405 ## [4,] 0.353209573 -0.04282613 -0.50796342 1.24117592 -0.292005017 0.42646139 ## [5,] -0.629545741 0.13923659 -0.02455073 -0.29200502 0.553562287 0.26275658 ## [6,] -0.279788479 -0.26060435 -0.11504405 0.42646139 0.262756584 0.81429052 ## [7,] 0.143364328 -0.14895377 0.29598156 0.30839120 -0.275296303 0.04418159 ## [8,] -0.273835576 0.17201439 -0.31052657 -0.39667581 0.376175973 -0.02536104 ## [9,] -0.008919669 0.24390178 -0.50198614 0.52741301 0.008044799 -0.01297542 ## [10,] -0.304722895 0.33936685 -1.08854590 0.20441696 0.499437080 0.20218868 ## [,7] [,8] [,9] [,10] ## [1,] 0.14336433 -0.27383558 -0.008919669 -0.3047229 ## [2,] -0.14895377 0.17201439 0.243901782 0.3393668 ## [3,] 0.29598156 -0.31052657 -0.501986137 -1.0885459 ## [4,] 0.30839120 -0.39667581 0.527413006 0.2044170 ## [5,] -0.27529630 0.37617597 0.008044799 0.4994371 ## [6,] 0.04418159 -0.02536104 -0.012975416 0.2021887 ## [7,] 0.37576405 -0.40476558 0.046294293 -0.4691147 ## [8,] -0.40476558 0.46612332 -0.026813818 0.5588965 ## [9,] 0.04629429 -0.02681382 0.540956259 0.5036908 ## [10,] -0.46911465 0.55889647 0.503690786 1.3107637 try(solve(S), silent = FALSE) ## Error in solve.default(S) : ## system is computationally singular: reciprocal condition number = 3.99819e-19 The standard definition for the inverse of a matrix fails if the matrix is not square or singular. However, one can generalize the inverse using singular value decomposition. Any rectangular real matrix \\(\\mathbf{M}\\) can be decomposed as \\(\\mathbf{M=U \\Sigma V^{&#39;}}\\), where \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are orthogonal and \\(\\mathbf{D}\\) is a diagonal matrix containing only the positive singular values. The pseudoinverse, also known as Moore-Penrose or generalized inverse is then obtained as \\[ \\mathbf{M^+} = \\mathbf{V \\Sigma^{-1} U&#39;} \\] Don’t be confused due to notation: \\(\\Sigma\\) is not the covariance matrix here With using the method of generalized inverse by ppcor and corpcorhere (Schafer_2005?): #https://rdrr.io/cran/corpcor/man/pseudoinverse.html Si &lt;- corpcor::pseudoinverse(S) -Si[1,2]/(sqrt(Si[1,1])*sqrt(Si[2,2])) ## [1] -0.4823509 # ppcor pc &lt;- ppcor::pcor(X) ## Warning in ppcor::pcor(X): The inverse of variance-covariance matrix is ## calculated using Moore-Penrose generalized matrix invers due to its determinant ## of zero. ## Warning in sqrt((n - 2 - gp)/(1 - pcor^2)): NaNs produced pc$estimate ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1.00000000 -0.48235089 -0.43471080 -0.6132218 0.59239395 -0.1515785108 ## [2,] -0.48235089 1.00000000 -0.85835176 -0.7984656 0.08341783 0.1922476120 ## [3,] -0.43471080 -0.85835176 1.00000000 -0.8107355 -0.06073205 -0.1395456329 ## [4,] -0.61322177 -0.79846556 -0.81073546 1.0000000 0.11814582 -0.3271223659 ## [5,] 0.59239395 0.08341783 -0.06073205 0.1181458 1.00000000 -0.4056046405 ## [6,] -0.15157851 0.19224761 -0.13954563 -0.3271224 -0.40560464 1.0000000000 ## [7,] 0.81227748 0.76456650 0.76563183 0.7861380 -0.07927500 0.2753626258 ## [8,] -0.74807903 -0.67387820 -0.64812735 -0.6321303 -0.04063566 -0.2660628754 ## [9,] 0.79435763 0.32542381 0.52481792 0.5106454 -0.08284875 0.5458020595 ## [10,] 0.01484899 -0.34289348 0.01425498 -0.2181704 -0.41275254 0.0006582396 ## [,7] [,8] [,9] [,10] ## [1,] 0.8122775 -0.74807903 0.79435763 0.0148489929 ## [2,] 0.7645665 -0.67387820 0.32542381 -0.3428934821 ## [3,] 0.7656318 -0.64812735 0.52481792 0.0142549759 ## [4,] 0.7861380 -0.63213032 0.51064540 -0.2181703890 ## [5,] -0.0792750 -0.04063566 -0.08284875 -0.4127525424 ## [6,] 0.2753626 -0.26606288 0.54580206 0.0006582396 ## [7,] 1.0000000 0.96888026 -0.84167300 0.2703213517 ## [8,] 0.9688803 1.00000000 0.84455999 -0.3746342510 ## [9,] -0.8416730 0.84455999 1.00000000 -0.0701428715 ## [10,] 0.2703214 -0.37463425 -0.07014287 1.0000000000 # corpcor with pseudo inverse corpcor::cor2pcor(S) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1.00000000 -0.48235089 -0.43471080 -0.6132218 0.59239395 -0.1515785108 ## [2,] -0.48235089 1.00000000 -0.85835176 -0.7984656 0.08341783 0.1922476120 ## [3,] -0.43471080 -0.85835176 1.00000000 -0.8107355 -0.06073205 -0.1395456329 ## [4,] -0.61322177 -0.79846556 -0.81073546 1.0000000 0.11814582 -0.3271223659 ## [5,] 0.59239395 0.08341783 -0.06073205 0.1181458 1.00000000 -0.4056046405 ## [6,] -0.15157851 0.19224761 -0.13954563 -0.3271224 -0.40560464 1.0000000000 ## [7,] 0.81227748 0.76456650 0.76563183 0.7861380 -0.07927500 0.2753626258 ## [8,] -0.74807903 -0.67387820 -0.64812735 -0.6321303 -0.04063566 -0.2660628754 ## [9,] 0.79435763 0.32542381 0.52481792 0.5106454 -0.08284875 0.5458020595 ## [10,] 0.01484899 -0.34289348 0.01425498 -0.2181704 -0.41275254 0.0006582396 ## [,7] [,8] [,9] [,10] ## [1,] 0.8122775 -0.74807903 0.79435763 0.0148489929 ## [2,] 0.7645665 -0.67387820 0.32542381 -0.3428934821 ## [3,] 0.7656318 -0.64812735 0.52481792 0.0142549759 ## [4,] 0.7861380 -0.63213032 0.51064540 -0.2181703890 ## [5,] -0.0792750 -0.04063566 -0.08284875 -0.4127525424 ## [6,] 0.2753626 -0.26606288 0.54580206 0.0006582396 ## [7,] 1.0000000 0.96888026 -0.84167300 0.2703213517 ## [8,] 0.9688803 1.00000000 0.84455999 -0.3746342510 ## [9,] -0.8416730 0.84455999 1.00000000 -0.0701428715 ## [10,] 0.2703214 -0.37463425 -0.07014287 1.0000000000 33.3 Ridge (\\(\\ell_{2}\\)) and glasso (\\(\\ell_{1}\\)) The effect of the ridge penalty is also studied from the perspective of singular values. When \\(\\mathbf{X}\\) is high-dimensional the regression parameter \\(\\beta\\) cannot be estimated. This is only the practical consequence of high-dimensionality: the expression \\(\\left(\\mathbf{X}^{\\top} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\top} \\mathbf{Y}\\) cannot be evaluated numerically. But the problem arising from the high-dimensionality of the data is more fundamental. To appreciate this, consider the normal equations: \\(\\mathbf{X}^{\\top} \\mathbf{X} \\boldsymbol{\\beta}=\\mathbf{X}^{\\top} \\mathbf{Y}\\). The matrix \\(\\mathbf{X}^{\\top} \\mathbf{X}\\) is of rank \\(n\\), while \\(\\boldsymbol{\\beta}\\) is a vector of length \\(p\\). Hence, while there are \\(p\\) unknowns, the system of linear equations from which these are to be solved effectively comprises \\(n\\) degrees of freedom. If \\(p&gt;n\\), the vector \\(\\boldsymbol{\\beta}\\) cannot uniquely be determined from this system of equations. We can express the effect of the ridge penalty from the perspective of singular values. In case of singular \\(\\mathbf{X}^{T} \\mathbf{X}\\) its inverse \\(\\left(\\mathbf{X}^{T} \\mathbf{X}\\right)^{-1}\\) is not defined. Consequently, the OLS estimator \\[ \\hat{\\boldsymbol{\\beta}}=\\left(\\mathbf{X}^{\\top} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\top} \\mathbf{Y} \\] does not exist. This happens in high-dimensional data. An ad-hoc solution adds \\(\\lambda \\mathbf{I}\\) to \\(\\mathbf{X}^{T} \\mathbf{X}\\), leading to: \\[ \\hat{\\boldsymbol{\\beta}}(\\lambda)=\\left(\\mathbf{X}^{\\top} \\mathbf{X}+\\lambda \\mathbf{I}_{p p}\\right)^{-1} \\mathbf{X}^{\\top} \\mathbf{Y} \\] This is called the ridge estimator. Let the columns of \\(X\\) be standardized, as well as \\(y\\) itself. (This means we no longer need a constant column in \\(X\\)). The ad-hoc ridge estimator minimizes the loss function: \\[ \\mathcal{L}(\\boldsymbol{\\beta} ; \\lambda)=\\|\\mathbf{Y}-\\mathbf{X} \\boldsymbol{\\beta}\\|_{2}^{2}+\\lambda\\|\\boldsymbol{\\beta}\\|_{2}^{2} \\] Or constrained optimization problem \\[ \\arg \\min _{\\beta}\\|\\mathbf{y}-\\mathbf{X} \\beta\\|^{2}+\\lambda\\|\\beta\\|^{2} \\quad \\lambda&gt;0 \\] Take the derivative of the loss function: \\[ \\frac{\\partial}{\\partial \\boldsymbol{\\beta}} \\mathcal{L}(\\boldsymbol{\\beta} ; \\lambda)=-2 \\mathbf{X}^{\\top} \\mathbf{y}+2\\left(\\mathbf{X}^{\\top} \\mathbf{X}+\\lambda \\mathbf{I}_{p}\\right) \\boldsymbol{\\beta} \\] Hence: \\[ \\begin{aligned} \\hat{\\beta}_{R} &amp;=\\left(X^{\\prime} X+\\lambda I_{p}\\right)^{-1} X^{\\prime} y \\\\ &amp;=\\left(V \\Sigma^{2} V^{\\prime}+\\lambda I_{p}\\right)^{-1} V \\Sigma U^{\\prime} y \\\\ &amp;=\\left(V \\Sigma^{2} V^{\\prime}+\\lambda V V^{\\prime}\\right)^{-1} V \\Sigma U^{\\prime} y \\\\ &amp;=\\left(V\\left(\\Sigma^{2}+\\lambda I_p\\right) V^{\\prime}\\right)^{-1} V \\Sigma U^{\\prime} y \\\\ &amp;=V\\left(\\Sigma^{2}+\\lambda I_p\\right)^{-1} V^{\\prime} V \\Sigma U^{\\prime} y \\\\ &amp;=V\\left(\\Sigma^{2}+\\lambda I_p\\right)^{-1} \\Sigma U^{\\prime} y . \\end{aligned} \\] The columns of \\(\\mathbf{U}_{x}\\) and \\(\\mathbf{V}_{x}\\) are orthogonal: \\(\\mathbf{U}_{x}^{\\top} \\mathbf{U}_{x}=\\mathbf{I}_{n n}=\\mathbf{U}_{x} \\mathbf{U}_{x}^{\\top}\\) and \\(\\mathbf{V}_{x}^{\\top} \\mathbf{V}_{x}=\\mathbf{I}_{p p}=\\mathbf{V}_{x} \\mathbf{V}_{x}^{\\top}\\). The difference with OLS? \\[ V\\Sigma^{-1}U&#39;y = \\beta_{OLS}\\\\ V\\Sigma^{-2}\\Sigma U&#39;y = \\beta_{OLS} \\] The difference between this and \\(\\beta_{OLS}\\) and \\(\\beta_{R}\\) is the replacement of \\(\\Sigma^{-1}=\\Sigma^{-2} \\Sigma\\) by \\(\\left(\\Sigma^{2}+\\lambda I_p\\right)^{-1} \\Sigma\\). In effect, this multiplies the original by the fraction \\(\\Sigma^{2} /\\left(\\Sigma^{2}+\\lambda\\right) .\\) Because (when \\(\\left.\\lambda&gt;0\\right)\\) the denominator is obviously greater than the numerator, the parameter estimates shrink towards zero, i.e., write \\((\\mathbf{\\Sigma})_{j j}=d_{j j}\\) to obtain \\(d_{i i}/\\left(d_{i i}^{2}+\\lambda\\right)\\). So that \\[ \\frac{d_{j j}^{-1}}{\\text { OLS }} \\geq \\frac{d_{j j} /\\left(d_{j j}^{2}+\\lambda\\right)}{\\text { ridge }} \\] As such, the rotated coefficients must shrink, but it is possible, when \\(\\lambda\\) is sufficiently small, for some of the \\(\\hat{\\beta}_{R}\\) themselves actually to increase in size. Interest in graphical models that combine a probabilistic description (through a multivariate distribution) of a system with a graph that depicts the system’s structure (capturing dependence relationships), has surged in recent years11. In its trail this has renewed the attention to the estimation of precision matrices as they harbor the conditional (in)dependencies among jointly distributed variates. In particular, with the advent of high-dimensional data, for which traditional precision estimators are not well-defined, this brought about several novel precision estimators. Generally, these novel estimators overcome the undersampling by maximization of the log-likelihood augmented with a so-called penalty. A penalty discourages large (in some sense) values among the elements of the precision matrix estimate. This reduces the risk of overfitting but also yields a well-defined penalized precision matrix estimator. Datasets where \\(p&gt;n\\) are starting to be common, so what now? To solve the problem, penalized estimators, like rags2ridges (see, Introduction to rags2ridges), adds a so-called ridge penalty to the likelihood above (this method is also called \\(\\ell_{2}\\) shrinkage and works by “shrinking” the eigenvalues of \\(S\\) in a particular manner to combat that they “explode” when \\(p \\geq n\\). “Shrinking” is a “biased estimation” as a means of variance reduction of S. Their algorithm solves the following: \\[ \\ell(\\Omega ; S)=\\ln |\\Omega|-\\operatorname{tr}(S \\Omega)-\\frac{\\lambda}{2}\\|\\Omega-T\\|_{2}^{2} \\] where \\(\\lambda&gt;0\\) is the ridge penalty parameter, \\(T\\) is a \\(p \\times p\\) known target matrix and \\(\\|\\cdot\\|_{2}\\) is the \\(\\ell_{2}\\)-norm. Assume for now the target matrix is an all zero matrix and thus out of the equation. The core function of rags2ridges is ridgeP which computes this estimate in a fast manner. The ridge precision estimation can be summarized with the following steps See: The ridge penalty: \\[ \\frac{1}{2} \\lambda_{2}\\left\\|\\boldsymbol{\\Sigma}^{-1}\\right\\|_{2}^{2} \\] When writing \\(\\Omega=\\Sigma^{-1}\\) the ridge penalty is: \\[ \\|\\boldsymbol{\\Omega}\\|_{2}^{2}=\\sum_{j_{1}, j_{2}=1}^{p}\\left[(\\boldsymbol{\\Omega})_{j_{1}, j_{2}}\\right]^{2} \\] For a 2x2 precision matrix this penalized estimation problems can be viewed as constrained optimization problem: \\[ \\begin{aligned} &amp;{\\left[\\Omega_{11}\\right]^{2}+2\\left[\\Omega_{12}\\right]^{2}} +\\left[\\Omega_{22}\\right]^{2} \\leq c\\left(\\lambda_{2}\\right) \\end{aligned} \\] Consider the ridge loss function: \\[ \\log (|\\boldsymbol{\\Omega}|)-\\operatorname{tr}(\\mathbf{S} \\boldsymbol{\\Omega})-\\frac{1}{2} \\lambda_{2} \\operatorname{tr}\\left(\\boldsymbol{\\Omega} \\boldsymbol{\\Omega}^{\\mathrm{T}}\\right) \\] Equation of the derivative w.r.t. the precision matrix to zero yields the estimating equation: \\[ \\boldsymbol{\\Omega}^{-1}-\\mathbf{S}-\\lambda_{2} \\boldsymbol{\\Omega}=\\mathbf{0}_{p \\times p} \\] Matrix algebra then yields: \\[ \\widehat{\\boldsymbol{\\Omega}}\\left(\\lambda_{2}\\right)=\\left[\\frac{1}{2} \\mathbf{S}+\\left(\\lambda_{2} \\mathbf{I}_{p \\times p}+\\frac{1}{4} \\mathbf{S}^{2}\\right)^{1 / 2}\\right]^{-1} \\] Thus, \\[ \\widehat{\\boldsymbol{\\Sigma}}\\left(\\lambda_{2}\\right)=\\frac{1}{2} \\mathbf{S}+\\left(\\lambda_{2} \\mathbf{I}_{p \\times p}+\\frac{1}{4} \\mathbf{S}^{2}\\right)^{1 / 2} \\] The derived ridge covariance estimator is positive definite, ie it’s symmetric and all its eigenvalues are positive. (Remember, when the matrix is symmetric, its trace is the sum of eigenvalues. Since the diagonal entries are all positive - variances - the trace of this covariance matrix is positive - see) For \\(\\lambda_{2}=0\\), we obtain: \\[ \\begin{aligned} \\widehat{\\boldsymbol{\\Sigma}}(0) &amp;=\\frac{1}{2} \\mathbf{S}+\\left(\\frac{1}{4} \\mathbf{S}^{2}\\right)^{1 / 2} \\\\ &amp;=\\frac{1}{2} \\mathbf{S}+\\frac{1}{2} \\mathbf{S}=\\mathbf{S} \\end{aligned} \\] For large enough \\(\\lambda_{2}\\): \\[ \\widehat{\\boldsymbol{\\Sigma}}\\left(\\lambda_{2}\\right) \\approx \\lambda_{2} \\mathbf{I}_{p \\times p} \\] The penalty parameter \\(\\lambda\\) shrinks the values of \\(P\\) such toward 0 (when \\(T=0\\) ), i.e. very larges values of \\(\\lambda\\) makes \\(P\\) “small” and more stable whereas smaller values of \\(\\lambda\\) makes the \\(P\\) tend toward the (possibly nonexistent) \\(S^{-1}\\). Let’s try some simulations: # We did this before n = 6 p = 10 set.seed(1) X &lt;- matrix (rnorm(n*p), n, p) # Cov. &amp; Precision Matrices S &lt;- cov(X) S ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.889211221 -0.17223814 -0.36660043 0.35320957 -0.629545741 -0.27978848 ## [2,] -0.172238139 0.34416306 -0.09280183 -0.04282613 0.139236591 -0.26060435 ## [3,] -0.366600426 -0.09280183 1.46701338 -0.50796342 -0.024550727 -0.11504405 ## [4,] 0.353209573 -0.04282613 -0.50796342 1.24117592 -0.292005017 0.42646139 ## [5,] -0.629545741 0.13923659 -0.02455073 -0.29200502 0.553562287 0.26275658 ## [6,] -0.279788479 -0.26060435 -0.11504405 0.42646139 0.262756584 0.81429052 ## [7,] 0.143364328 -0.14895377 0.29598156 0.30839120 -0.275296303 0.04418159 ## [8,] -0.273835576 0.17201439 -0.31052657 -0.39667581 0.376175973 -0.02536104 ## [9,] -0.008919669 0.24390178 -0.50198614 0.52741301 0.008044799 -0.01297542 ## [10,] -0.304722895 0.33936685 -1.08854590 0.20441696 0.499437080 0.20218868 ## [,7] [,8] [,9] [,10] ## [1,] 0.14336433 -0.27383558 -0.008919669 -0.3047229 ## [2,] -0.14895377 0.17201439 0.243901782 0.3393668 ## [3,] 0.29598156 -0.31052657 -0.501986137 -1.0885459 ## [4,] 0.30839120 -0.39667581 0.527413006 0.2044170 ## [5,] -0.27529630 0.37617597 0.008044799 0.4994371 ## [6,] 0.04418159 -0.02536104 -0.012975416 0.2021887 ## [7,] 0.37576405 -0.40476558 0.046294293 -0.4691147 ## [8,] -0.40476558 0.46612332 -0.026813818 0.5588965 ## [9,] 0.04629429 -0.02681382 0.540956259 0.5036908 ## [10,] -0.46911465 0.55889647 0.503690786 1.3107637 try(solve(S), silent = FALSE) ## Error in solve.default(S) : ## system is computationally singular: reciprocal condition number = 3.99819e-19 # With Ridge, lambda = 0 lambda = 0 SRidge &lt;- 0.5*S + expm::sqrtm(lambda*diag(1, p)+0.25*(S%*%S)) SRidge ## [,1] [,2] [,3] [,4] ## [1,] 0.889211226+0i -0.17223814+0i -0.36660042+0i 0.35320957+0i ## [2,] -0.172238138+0i 0.34416307+0i -0.09280183-0i -0.04282613+0i ## [3,] -0.366600424+0i -0.09280183-0i 1.46701339+0i -0.50796342-0i ## [4,] 0.353209571+0i -0.04282613+0i -0.50796342-0i 1.24117592+0i ## [5,] -0.629545737+0i 0.13923659+0i -0.02455073-0i -0.29200502+0i ## [6,] -0.279788477-0i -0.26060434-0i -0.11504405+0i 0.42646139-0i ## [7,] 0.143364330+0i -0.14895377+0i 0.29598156+0i 0.30839120+0i ## [8,] -0.273835576+0i 0.17201439+0i -0.31052657+0i -0.39667581+0i ## [9,] -0.008919667-0i 0.24390178-0i -0.50198614+0i 0.52741301-0i ## [10,] -0.304722894+0i 0.33936685+0i -1.08854590-0i 0.20441696+0i ## [,5] [,6] [,7] [,8] ## [1,] -6.295457e-01+0e+00i -0.27978848+0i 0.14336433+0i -0.27383558+0i ## [2,] 1.392366e-01+0e+00i -0.26060434-0i -0.14895377+0i 0.17201439+0i ## [3,] -2.455073e-02-0e+00i -0.11504405+0i 0.29598156+0i -0.31052657+0i ## [4,] -2.920050e-01+0e+00i 0.42646139-0i 0.30839120+0i -0.39667581+0i ## [5,] 5.535623e-01+0e+00i 0.26275658-0i -0.27529630+0i 0.37617597-0i ## [6,] 2.627566e-01-0e+00i 0.81429053+0i 0.04418159-0i -0.02536104+0i ## [7,] -2.752963e-01+0e+00i 0.04418159-0i 0.37576405+0i -0.40476558+0i ## [8,] 3.761760e-01-0e+00i -0.02536104+0i -0.40476558+0i 0.46612332+0i ## [9,] 8.044802e-03-1e-09i -0.01297542+0i 0.04629429-0i -0.02681382-0i ## [10,] 4.994371e-01+0e+00i 0.20218868-0i -0.46911465+0i 0.55889646+0i ## [,9] [,10] ## [1,] -8.919667e-03+0e+00i -0.3047229+0i ## [2,] 2.439018e-01-0e+00i 0.3393668+0i ## [3,] -5.019861e-01+0e+00i -1.0885459-0i ## [4,] 5.274130e-01-0e+00i 0.2044170+0i ## [5,] 8.044802e-03-1e-09i 0.4994371+0i ## [6,] -1.297542e-02+0e+00i 0.2021887-0i ## [7,] 4.629429e-02-0e+00i -0.4691147+0i ## [8,] -2.681382e-02-0e+00i 0.5588965+0i ## [9,] 5.409563e-01+0e+00i 0.5036908-0i ## [10,] 5.036908e-01-0e+00i 1.3107637+0i # With Ridge, lambda = 0 lambda = 0.5 SRidge &lt;- 0.5*S + expm::sqrtm(lambda*diag(1, p)+0.25*(S%*%S)) SRidge ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1.37069495 -0.13010676 -0.26194680 0.29808331 -0.48369828 -0.20441616 ## [2,] -0.13010676 0.94030145 -0.10511356 -0.04366041 0.11696761 -0.16508806 ## [3,] -0.26194680 -0.10511356 1.88019154 -0.44099096 -0.04204807 -0.10402368 ## [4,] 0.29808331 -0.04366041 -0.44099096 1.65839433 -0.23659110 0.31883220 ## [5,] -0.48369828 0.11696761 -0.04204807 -0.23659110 1.13995922 0.19056877 ## [6,] -0.20441616 -0.16508806 -0.10402368 0.31883220 0.19056877 1.27387916 ## [7,] 0.13649132 -0.12065058 0.24458809 0.23574080 -0.22740740 0.02961578 ## [8,] -0.23352284 0.14255435 -0.25606790 -0.30118441 0.30653069 -0.01411260 ## [9,] 0.01451734 0.16994890 -0.43236453 0.41233338 0.01096246 0.01961695 ## [10,] -0.24477072 0.27432427 -0.90767449 0.17451629 0.40902909 0.17254457 ## [,7] [,8] [,9] [,10] ## [1,] 0.13649132 -0.2335228409 0.0145173438 -0.2447707 ## [2,] -0.12065058 0.1425543547 0.1699489031 0.2743243 ## [3,] 0.24458809 -0.2560679036 -0.4323645280 -0.9076745 ## [4,] 0.23574080 -0.3011844066 0.4123333844 0.1745163 ## [5,] -0.22740740 0.3065306880 0.0109624604 0.4090291 ## [6,] 0.02961578 -0.0141126044 0.0196169539 0.1725446 ## [7,] 0.98901314 -0.3131878342 0.0113671901 -0.3860672 ## [8,] -0.31318783 1.0735641056 -0.0002152917 0.4596644 ## [9,] 0.01136719 -0.0002152917 1.1067644836 0.4153144 ## [10,] -0.38606724 0.4596643602 0.4153143722 1.7902418 solve(SRidge) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 0.96296745 0.08426275 0.20930726 -0.11025252 0.291694924 0.15074465 ## [2,] 0.08426275 1.19227678 -0.02462345 -0.00166857 -0.044537957 0.19103259 ## [3,] 0.20930726 -0.02462345 0.82635632 0.13394492 -0.034994680 0.02204074 ## [4,] -0.11025252 -0.00166857 0.13394492 0.83443682 0.110827841 -0.21525839 ## [5,] 0.29169492 -0.04453796 -0.03499468 0.11082784 1.172793866 -0.14437564 ## [6,] 0.15074465 0.19103259 0.02204074 -0.21525839 -0.144375635 0.91917727 ## [7,] -0.01374603 0.05660639 -0.10278693 -0.14530081 0.095777812 -0.02913162 ## [8,] 0.08062547 -0.05892007 0.10891732 0.19098281 -0.139290570 0.02249687 ## [9,] 0.04687403 -0.14790576 0.13924322 -0.23015924 0.005835323 0.06518474 ## [10,] 0.11990434 -0.13008516 0.36174283 -0.05980134 -0.180815973 -0.05928821 ## [,7] [,8] [,9] [,10] ## [1,] -0.01374603 0.08062547 0.046874026 0.11990434 ## [2,] 0.05660639 -0.05892007 -0.147905757 -0.13008516 ## [3,] -0.10278693 0.10891732 0.139243218 0.36174283 ## [4,] -0.14530081 0.19098281 -0.230159243 -0.05980134 ## [5,] 0.09577781 -0.13929057 0.005835323 -0.18081597 ## [6,] -0.02913162 0.02249687 0.065184740 -0.05928821 ## [7,] 1.22649819 0.18315549 -0.069854206 0.16609483 ## [8,] 0.18315549 1.21488158 0.053197052 -0.19846421 ## [9,] -0.06985421 0.05319705 1.131616449 -0.17675283 ## [10,] 0.16609483 -0.19846421 -0.176752827 0.95895605 There are many ways to regularize covariance estimation. Some of these “ad-hoc” estimators are often referred to as “ridge” estimates: \\[ \\mathbf{S}+\\lambda_{a} \\mathbf{I}_{p \\times p} \\quad \\text { for } \\quad \\lambda_{a}&gt;0 \\] and: \\[ \\left(1-\\lambda_{a}\\right) \\mathbf{S}+\\lambda_{a} \\mathbf{T} \\quad \\text { for } \\quad \\lambda_{a} \\in(0,1) \\] where \\(\\mathrm{T}\\) is some nonrandom, positive definite matrix. Both are not derived from a penalized loss function, but are simply ad-hoc fixes to resolve the singularity of the estimate. To evaluate ad-hoc and Ridge estimators, we compare the eigenvalues of the ad-hoc and ridge estimator of the covariance matrix. Consider the eigen-decomposition: \\(\\mathbf{S}=\\mathbf{V D V}^{\\mathrm{T}}\\) with \\(\\mathbf{V}\\) and \\(\\mathrm{D}\\) the eigenvalue and -vector matrices. The eigenvalues of \\(\\widehat{\\mathbf{\\Sigma}}\\left(\\lambda_{2}\\right)\\) are then: \\[ d_{j}\\left[\\widehat{\\boldsymbol{\\Sigma}}\\left(\\lambda_{2}\\right)\\right]=\\frac{1}{2} d_{j}+\\left(\\lambda_{2}+\\frac{1}{4} d_{j}^{2}\\right)^{1 / 2} \\] Writing \\(d_{j}=(\\mathbf{D})_{j j}\\) it is easily seen that: \\[ \\lambda_{a}+d_{j} \\geq \\frac{1}{2} d_{j}+\\sqrt{\\lambda_{a}^{2}+\\frac{1}{4} d_{j}^{2}} \\] Thus, the ad-hoc estimator shrinks the eigenvalues of the sample covariance matrix more than the ridge estimator. Why a target \\(T\\)? Both the ad-hoc and ridge covariance estimator converge to: \\[ \\widehat{\\boldsymbol{\\Sigma}}\\left(\\lambda_{2}\\right) \\approx \\lambda_{2} \\mathbf{I}_{p \\times p} \\quad \\text {for large enough} \\quad\\lambda_{2} \\] Its inverse (the precision matrix) converges to the zero matrix including the diagonal elements! Consequently, the partial correlation of this matrix are undefined. If signal-to-noise ratio is poor, why not provide a hint. The target matrix \\(T\\) is a matrix the same size as \\(P\\) which the estimate is “shrunken” toward, i.e. for large values of \\(\\lambda\\) the estimate goes toward \\(T\\). The choice of the target is another subject. While one might first think that the all-zeros \\(T=[0]\\) would be a default it is intuitively not a good target. This is because we’d like an estimate that is positive definite (the matrix-equivalent to at positive number) and the null-matrix is not positive definite. If one has a very good prior estimate or some other information this might used to construct the target. In the absence of such knowledge, the default could be a data-driven diagonal matrix. The function default.target() offers some different approaches to selecting this. A good choice here is often the diagonal matrix times the reciprocal mean of the eigenvalues of the sample covariance as entries. See ?default.target for more choices. To ensure the ridge precision estimate converges to a positive definite target matrix \\(\\mathbf{T}\\) , the latter is incorporated in the penalty: \\[ \\frac{1}{2} \\lambda_{2} \\operatorname{tr}\\left[(\\boldsymbol{\\Omega}-\\mathbf{T})(\\boldsymbol{\\Omega}-\\mathbf{T})^{\\mathrm{T}}\\right] \\] Clearly, the penalty is minimized for \\(\\Omega=\\mathbf{T}\\). One expects that, for large \\(\\lambda_{2}\\), the maximization of the penalized log-likelihood requires the minimization of the penalty: the optimum moves close to \\(\\mathbf{T}\\). The log-likelihood augmented with this “target”-penalty is maximized by: \\[ \\left\\{\\frac{1}{2}\\left(\\mathbf{S}-\\lambda_{2} \\mathbf{T}\\right)+\\left[\\lambda_{2} \\mathbf{I}_{p \\times p}+\\frac{1}{4}\\left(\\mathbf{S}-\\lambda_{2} \\mathbf{T}\\right)^{2}\\right]^{1 / 2}\\right\\}^{-1} \\] For generalized ridge precision estimator one can show that: \\[ \\lim _{\\lambda_{2} \\rightarrow \\infty} \\widehat{\\boldsymbol{\\Omega}}\\left(\\lambda_{2}\\right)=\\mathbf{T} \\] and \\(\\widehat{\\Omega}\\left(\\lambda_{2}\\right) \\succ 0\\) for all \\(\\lambda_{2}&gt;0\\) What Lambda should you choose? One strategy for choosing \\(\\lambda\\) is selecting it to be stable yet precise (a bias-variance trade-off). Automatic k-fold cross-validation can be done with optPenalty.kCVauto() is well suited for this. LOOCV Simulation: Define a banded \\(p \\times p\\) precision matrix, \\(p=100\\). Draw \\(n=10\\) samples. Determine optimal lambda by LOOCV. Estimate precision matrix with and without target. Target is true precision. Summary from their paper From the paper (Wessel_Ridge?), which defines a kind of sparsity method similar to L1 (glasso) but using L2: Let \\(\\mathbf{Y}_{i}, i=1, \\ldots, n\\), be a \\(p\\)-dimensional random variate drawn from \\(\\mathcal{N}_{p}(\\mathbf{0}, \\mathbf{\\Sigma})\\). The maximum likelihood (ML) estimator of the precision matrix \\(\\boldsymbol{\\Omega}=\\boldsymbol{\\Sigma}^{-1}\\) maximizes: \\[ \\mathcal{L}(\\boldsymbol{\\Omega} ; \\mathbf{S}) \\propto \\ln |\\boldsymbol{\\Omega}|-\\operatorname{tr}(\\mathbf{S} \\boldsymbol{\\Omega}) ~~~~~~~~~~~~~~~ (1) \\] where \\(\\mathbf{S}\\) is the sample covariance estimate. If \\(n&gt;p\\), the log-likelihood achieves its maximum for \\(\\hat{\\boldsymbol{\\Omega}}^{\\mathrm{ML}}=\\mathbf{S}^{-1}\\). In the high-dimensional setting where \\(p&gt;n\\), the sample covariance matrix is singular and its inverse is undefined. Consequently, so is \\(\\hat{\\boldsymbol{\\Omega}}^{\\mathrm{ML}}\\). A common workaround is the addition of a penalty to the \\(\\log\\)-likelihood (1). The \\(\\ell_{1}\\)-penalized estimation of the precision matrix was considered almost simultaneously. This (graphical) lasso estimate of \\(\\Omega\\) has attracted much attention due to the resulting sparse solution and has grown into an active area of research. Juxtaposed to situations in which sparsity is an asset are situations in which one is intrinsically interested in more accurate representations of the high-dimensional precision matrix. In addition, the true (graphical) model need not be (extremely) sparse in terms of containing many zero elements. In these cases we may prefer usage of a regularization method that shrinks the estimated elements of the precision matrix proportionally Ridge estimators of the precision matrix currently in use can be roughly divided into two archetypes (cf. Ledoit and Wolf, 2004; Schäfer and Strimmer, 2005a). The first archetypal form of ridge estimator commonly is a convex combination of \\(\\mathbf{S}\\) and a positive definite (p.d.) target matrix \\(\\boldsymbol{\\Gamma}: \\hat{\\mathbf{\\Omega}}^{\\mathrm{I}}\\left(\\lambda_{\\mathrm{I}}\\right)=\\left[\\left(1-\\lambda_{\\mathrm{I}}\\right) \\mathbf{S}+\\lambda_{\\mathrm{I}} \\boldsymbol{\\Gamma}\\right]^{-1}\\), with \\(\\lambda_{\\mathrm{I}} \\in(0,1]\\). A common (low-dimensional) target choice is \\(\\Gamma\\) diagonal with \\((\\Gamma)_{j j}=(\\mathbf{S})_{j j}\\) for \\(j=1, \\ldots, p .\\) This estimator has the desirable property of shrinking to \\(\\Gamma^{-}\\) when \\(\\lambda_{\\mathrm{I}}=1\\) (maximum penalization). The estimator can be motivated from the bias-variance tradeoff as it seeks to balance the high-variance, low-bias matrix \\(\\mathbf{S}\\) with the lower-variance, higher-bias matrix \\(\\mathbf{\\Gamma}\\). It can also be viewed as resulting from the maximization of the following penalized log-likelihood: \\[ \\ln |\\boldsymbol{\\Omega}|-\\left(1-\\lambda_{\\mathrm{I}}\\right) \\operatorname{tr}(\\mathbf{S} \\boldsymbol{\\Omega})-\\lambda_{\\mathrm{I}} \\operatorname{tr}(\\boldsymbol{\\Omega} \\boldsymbol{\\Gamma}) \\] The penalized log-likelihood is obtained from the original log-likelihood (1) by the replacement of \\(\\mathbf{S}\\) by \\(\\left(1-\\lambda_{\\mathrm{I}}\\right) \\mathbf{S}\\) and the addition of a penalty. The estimate \\(\\hat{\\boldsymbol{\\Omega}}^{\\mathrm{I}}\\left(\\lambda_{\\mathrm{I}}\\right)\\) can thus be viewed as a penalized ML estimate. 33.4 What’s graphical - graphical ridge or glasso? A contemporary use for precision matrices is found in network reconstruction through graphical modeling (Network Analysis). Graphical modeling refers to a class of probabilistic models that uses graphs to express conditional (in)dependence relations between random variables. In a multivariate normal model, \\(p_{i j}=p_{j i}=0\\) if and only if \\(X_{i}\\) and \\(X_{j}\\) are conditionally independent when condition on all other variables. I.e. \\(X_{i}\\) and \\(X_{j}\\) are conditionally independent given all \\(X_{k}\\) where \\(k \\neq i\\) and \\(k \\neq j\\) if and when the \\(i j\\) th and \\(j i\\) th elements of \\(P\\) are zero. In real world applications, this means that \\(P\\) is often relatively sparse (lots of zeros). This also points to the close relationship between \\(P\\) and the partial correlations. The non-zero entries of the symmetric P matrix can be interpreted the edges of a graph where nodes correspond to the variables. The graphical lasso (gLasso) is the L1-equivalent to graphical ridge. A nice feature of the L1 penalty automatically induces sparsity and thus also select the edges in the underlying graph. The L2 penalty of rags2ridges relies on an extra step that selects the edges after \\(P\\) is estimated. While some may argue this as a drawback (typically due to a lack of perceived simplicity), it is often beneficial to separate the “variable selection” and estimation. First, a separate post-hoc selection step allows for greater flexibility. Secondly, when co-linearity is present the L1 penalty is “unstable” in the selection between the items. I.e. if 2 covariances are co-linear only one of them will typically be selected in a unpredictable way whereas the L2 will put equal weight on both and “average” their effect. Ultimately, this means that the L2 estimate is typically more stable than the L1. At last point to mention here is also that the true underlying graph might not always be very sparse (or sparse at all). The sparsify() functions lets you select the non-zero entries of \\(P\\) corresponding to edges. It supports a handful different approaches ranging from simple thresholding to false discovery rate based selection. After edge selection, GGMnetworkStats() can be utilized to get summary statistics of the resulting graph topology. Now, we will apply some packages on both glass and ridge. First LASSO: # glasso gl &lt;- glasso::glasso(S,rho=0.2641,approx=FALSE)[c(&#39;w&#39;,&#39;wi&#39;)] gl ## $w ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1.153311221 -0.0019492424 -0.10250046 0.089115084 -0.365445494 ## [2,] -0.001949242 0.6082630590 -0.03940240 0.004528138 0.011247377 ## [3,] -0.102500456 -0.0394024012 1.73111338 -0.243862447 -0.082905020 ## [4,] 0.089115084 0.0045281381 -0.24386245 1.505275920 -0.027903824 ## [5,] -0.365445494 0.0112473773 -0.08290502 -0.027903824 0.817662287 ## [6,] -0.015688470 0.0005372775 -0.02436263 0.162361394 0.005007582 ## [7,] 0.013032583 -0.0097981855 0.09957364 0.044291165 -0.041565070 ## [8,] -0.045950765 0.0140890956 -0.13200901 -0.132575754 0.112076056 ## [9,] 0.016042995 0.0114506691 -0.23788553 0.263312936 0.026628021 ## [10,] -0.040785435 0.0752668494 -0.82444547 0.094745570 0.235337162 ## [,6] [,7] [,8] [,9] [,10] ## [1,] -0.0156884699 0.013032583 -0.04595076 0.01604299 -0.04078544 ## [2,] 0.0005372775 -0.009798185 0.01408910 0.01145067 0.07526685 ## [3,] -0.0243626295 0.099573642 -0.13200901 -0.23788553 -0.82444547 ## [4,] 0.1623613944 0.044291165 -0.13257575 0.26331294 0.09474557 ## [5,] 0.0050075816 -0.041565070 0.11207606 0.02662802 0.23533716 ## [6,] 1.0783905237 0.004547886 -0.01346013 0.02839133 0.01124185 ## [7,] 0.0045478865 0.639864046 -0.14066560 -0.02176493 -0.20501465 ## [8,] -0.0134601285 -0.140665596 0.73022332 0.01972098 0.29479653 ## [9,] 0.0283913276 -0.021764935 0.01972098 0.80505626 0.23959079 ## [10,] 0.0112418534 -0.205014654 0.29479653 0.23959079 1.57486374 ## ## $wi ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1.02453397 0.0000000 0.07730448 -0.041768030 0.464189834 0.02078441 ## [2,] 0.00000000 1.6538060 0.00000000 0.000000000 0.000000000 0.00000000 ## [3,] 0.07729822 0.0000000 0.79699479 0.084060598 0.000000000 0.00000000 ## [4,] -0.04176456 0.0000000 0.08406127 0.743589130 0.001284129 -0.10295591 ## [5,] 0.46418826 0.0000000 0.00000000 0.001284557 1.502048141 0.00000000 ## [6,] 0.02078392 0.0000000 0.00000000 -0.102955958 0.000000000 0.94311116 ## [7,] 0.00000000 0.0000000 0.00000000 -0.038678985 0.000000000 0.00000000 ## [8,] 0.00000000 0.0000000 0.00000000 0.143926311 -0.124728257 0.00000000 ## [9,] 0.00000000 0.0000000 0.08715415 -0.218520346 0.000000000 0.00000000 ## [10,] 0.00000000 -0.0790397 0.40091494 0.000000000 -0.189163291 0.00000000 ## [,7] [,8] [,9] [,10] ## [1,] 0.00000000 0.0000000 0.00000000 0.00000000 ## [2,] 0.00000000 0.0000000 0.00000000 -0.07903969 ## [3,] 0.00000000 0.0000000 0.08715362 0.40091441 ## [4,] -0.03867901 0.1439262 -0.21852024 0.00000000 ## [5,] 0.00000000 -0.1247273 0.00000000 -0.18916474 ## [6,] 0.00000000 0.0000000 0.00000000 0.00000000 ## [7,] 1.67533935 0.2452522 0.00000000 0.17451303 ## [8,] 0.24525219 1.5631718 0.00000000 -0.25070153 ## [9,] 0.00000000 0.0000000 1.38457190 -0.15186887 ## [10,] 0.17451306 -0.2507014 -0.15186929 0.96965130 -cov2cor(gl$wi) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] -1.00000000 0.0000000 -0.08554877 0.047853564 -0.374188570 -0.02114429 ## [2,] 0.00000000 -1.0000000 0.00000000 0.000000000 0.000000000 0.00000000 ## [3,] -0.08554184 0.0000000 -1.00000000 -0.109193860 0.000000000 0.00000000 ## [4,] 0.04784959 0.0000000 -0.10919473 -1.000000000 -0.001215067 0.12294286 ## [5,] -0.37418730 0.0000000 0.00000000 -0.001215471 -1.000000000 0.00000000 ## [6,] -0.02114379 0.0000000 0.00000000 0.122942921 0.000000000 -1.00000000 ## [7,] 0.00000000 0.0000000 0.00000000 0.034654302 0.000000000 0.00000000 ## [8,] 0.00000000 0.0000000 0.00000000 -0.133496635 0.081399093 0.00000000 ## [9,] 0.00000000 0.0000000 -0.08296646 0.215361272 0.000000000 0.00000000 ## [10,] 0.00000000 0.0624159 -0.45605446 0.000000000 0.156742635 0.00000000 ## [,7] [,8] [,9] [,10] ## [1,] 0.00000000 0.00000000 0.00000000 0.00000000 ## [2,] 0.00000000 0.00000000 0.00000000 0.06241589 ## [3,] 0.00000000 0.00000000 -0.08296595 -0.45605385 ## [4,] 0.03465432 -0.13349651 0.21536117 0.00000000 ## [5,] 0.00000000 0.08139844 0.00000000 0.15674384 ## [6,] 0.00000000 0.00000000 0.00000000 0.00000000 ## [7,] -1.00000000 -0.15155075 0.00000000 -0.13692056 ## [8,] -0.15155077 -1.00000000 0.00000000 0.20363191 ## [9,] 0.00000000 0.00000000 -1.00000000 0.13106997 ## [10,] -0.13692058 0.20363183 0.13107034 -1.00000000 # glassoFast glf &lt;- glassoFast::glassoFast(S,rho=0.2641) -cov2cor(glf$wi) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] -1.00000000 0.00000000 -0.08554575 0.047851718 -0.374188717 -0.02114399 ## [2,] 0.00000000 -1.00000000 0.00000000 0.000000000 0.000000000 0.00000000 ## [3,] -0.08554575 0.00000000 -1.00000000 -0.109194550 0.000000000 0.00000000 ## [4,] 0.04785172 0.00000000 -0.10919455 -1.000000000 -0.001215165 0.12294285 ## [5,] -0.37418872 0.00000000 0.00000000 -0.001215165 -1.000000000 0.00000000 ## [6,] -0.02114399 0.00000000 0.00000000 0.122942854 0.000000000 -1.00000000 ## [7,] 0.00000000 0.00000000 0.00000000 0.034654196 0.000000000 0.00000000 ## [8,] 0.00000000 0.00000000 0.00000000 -0.133496568 0.081401944 0.00000000 ## [9,] 0.00000000 0.00000000 -0.08296614 0.215361164 0.000000000 0.00000000 ## [10,] 0.00000000 0.06241583 -0.45605417 0.000000000 0.156742467 0.00000000 ## [,7] [,8] [,9] [,10] ## [1,] 0.0000000 0.00000000 0.00000000 0.00000000 ## [2,] 0.0000000 0.00000000 0.00000000 0.06241583 ## [3,] 0.0000000 0.00000000 -0.08296614 -0.45605417 ## [4,] 0.0346542 -0.13349657 0.21536116 0.00000000 ## [5,] 0.0000000 0.08140194 0.00000000 0.15674247 ## [6,] 0.0000000 0.00000000 0.00000000 0.00000000 ## [7,] -1.0000000 -0.15155079 0.00000000 -0.13692053 ## [8,] -0.1515508 -1.00000000 0.00000000 0.20363189 ## [9,] 0.0000000 0.00000000 -1.00000000 0.13107029 ## [10,] -0.1369205 0.20363189 0.13107029 -1.00000000 And Ridge: n = 6 p = 10 set.seed(1) X &lt;- matrix (rnorm(n*p), n, p) S &lt;- cov(X) # corpcor cpr &lt;- corpcor::pcor.shrink(X) ## Estimating optimal shrinkage intensity lambda (correlation matrix): 0.7365 cpr ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1.00000000 -0.063781216 -0.101406692 0.063120988 -0.194493134 ## [2,] -0.06378122 1.000000000 0.007994094 -0.000768278 0.045367893 ## [3,] -0.10140669 0.007994094 1.000000000 -0.079446872 0.016525671 ## [4,] 0.06312099 -0.000768278 -0.079446872 1.000000000 -0.062013066 ## [5,] -0.19449313 0.045367893 0.016525671 -0.062013066 1.000000000 ## [6,] -0.08453161 -0.142765543 -0.023156121 0.118733605 0.101284666 ## [7,] 0.01012585 -0.066403162 0.078386268 0.085900860 -0.090559866 ## [8,] -0.05718524 0.062980841 -0.073635683 -0.106514843 0.121121615 ## [9,] -0.01423788 0.137888867 -0.113559683 0.147784876 -0.001232215 ## [10,] -0.03863764 0.081744909 -0.163495477 0.045875669 0.096163827 ## [,6] [,7] [,8] [,9] [,10] ## [1,] -0.08453161 0.01012585 -0.05718524 -0.014237882 -0.03863764 ## [2,] -0.14276554 -0.06640316 0.06298084 0.137888867 0.08174491 ## [3,] -0.02315612 0.07838627 -0.07363568 -0.113559683 -0.16349548 ## [4,] 0.11873361 0.08590086 -0.10651484 0.147784876 0.04587567 ## [5,] 0.10128467 -0.09055987 0.12112162 -0.001232215 0.09616383 ## [6,] 1.00000000 0.02022953 -0.01359508 -0.016265809 0.04534151 ## [7,] 0.02022953 1.00000000 -0.18507676 0.050393624 -0.11283317 ## [8,] -0.01359508 -0.18507676 1.00000000 -0.031721445 0.12046550 ## [9,] -0.01626581 0.05039362 -0.03172144 1.000000000 0.12181207 ## [10,] 0.04534151 -0.11283317 0.12046550 0.121812073 1.00000000 ## attr(,&quot;lambda&quot;) ## [1] 0.7364502 ## attr(,&quot;lambda.estimated&quot;) ## [1] TRUE ## attr(,&quot;class&quot;) ## [1] &quot;shrinkage&quot; ## attr(,&quot;spv&quot;) ## [1] 0.9163043 0.9261594 0.9159745 0.9151906 0.8824546 0.9457673 0.8900877 ## [8] 0.8719793 0.9159940 0.8735017 # rags2ridges opt &lt;- rags2ridges::optPenalty.kCVauto(X, lambdaMin = 0.01, lambdaMax = 30) opt ## $optLambda ## [1] 3.670373 ## ## $optPrec ## A 10 x 10 ridge precision matrix estimate with lambda = 3.670373 ## [,1] [,2] [,3] [,4] [,5] [,6] … ## [1,] 2.07949563 0.036633612 0.078479879 -0.074714732 0.133955417 0.05979073 … ## [2,] 0.03663361 2.195421836 0.019061938 0.009073605 -0.029391000 0.05604418 … ## [3,] 0.07847988 0.019061938 1.958521859 0.107011686 0.004540494 0.02397844 … ## [4,] -0.07471473 0.009073605 0.107011686 2.004976912 0.062008715 -0.09090978 … ## [5,] 0.13395542 -0.029391000 0.004540494 0.062008715 2.151438332 -0.05607991 … ## [6,] 0.05979073 0.056044183 0.023978439 -0.090909777 -0.056079908 2.09478152 … ## … 4 more rows and 4 more columns -cov2cor(opt$optPrec) ## A 10 x 10 ridge precision matrix estimate with lambda = 3.670373 ## [,1] [,2] [,3] [,4] [,5] … ## [1,] -1.00000000 -0.017145168 -0.038887955 0.036590854 -0.063331070 … ## [2,] -0.01714517 -1.000000000 -0.009192716 -0.004324801 0.013523557 … ## [3,] -0.03888796 -0.009192716 -1.000000000 -0.054002306 -0.002211945 … ## [4,] 0.03659085 -0.004324801 -0.054002306 -1.000000000 -0.029856136 … ## [5,] -0.06333107 0.013523557 -0.002211945 -0.029856136 -1.000000000 … ## [6,] -0.02864742 -0.026133777 -0.011838245 0.044359489 0.026416390 … ## [,6] … ## [1,] -0.02864742 … ## [2,] -0.02613378 … ## [3,] -0.01183825 … ## [4,] 0.04435949 … ## [5,] 0.02641639 … ## [6,] -1.00000000 … ## … 4 more rows and 4 more columns Si &lt;- rags2ridges::ridgeP(S, lambda=opt$optLambda) Si ## A 10 x 10 ridge precision matrix estimate with lambda = 3.670373 ## [,1] [,2] [,3] [,4] [,5] [,6] … ## [1,] 1.71127936 0.04250957 0.091807414 -0.08619734 0.155561828 0.06978509 … ## [2,] 0.04250957 1.84587475 0.021184682 0.01052115 -0.033800444 0.06587097 … ## [3,] 0.09180741 0.02118468 1.573665988 0.12278371 0.004278519 0.02711906 … ## [4,] -0.08619734 0.01052115 0.122783713 1.62525274 0.071878756 -0.10579068 … ## [5,] 0.15556183 -0.03380044 0.004278519 0.07187876 1.795540421 -0.06533859 … ## [6,] 0.06978509 0.06587097 0.027119056 -0.10579068 -0.065338591 1.72817504 … ## … 4 more rows and 4 more columns -cov2cor(Si) ## A 10 x 10 ridge precision matrix estimate with lambda = 3.670373 ## [,1] [,2] [,3] [,4] [,5] … ## [1,] -1.00000000 -0.023918013 -0.055944967 0.051686004 -0.088745223 … ## [2,] -0.02391801 -1.000000000 -0.012429812 -0.006074373 0.018566229 … ## [3,] -0.05594497 -0.012429812 -1.000000000 -0.076775744 -0.002545304 … ## [4,] 0.05168600 -0.006074373 -0.076775744 -1.000000000 -0.042076781 … ## [5,] -0.08874522 0.018566229 -0.002545304 -0.042076781 -1.000000000 … ## [6,] -0.04057965 -0.036880692 -0.016444642 0.063123791 0.037091834 … ## [,6] … ## [1,] -0.04057965 … ## [2,] -0.03688069 … ## [3,] -0.01644464 … ## [4,] 0.06312379 … ## [5,] 0.03709183 … ## [6,] -1.00000000 … ## … 4 more rows and 4 more columns -Si[1,2]/(sqrt(Si[1,1])*sqrt(Si[2,2])) ## [1] -0.02391801 spr &lt;- rags2ridges::sparsify(opt$optPrec, threshold = &quot;connected&quot;) ## - Retained elements: 16 ## - Corresponding to 35.56 % of possible edges ## spr ## $sparseParCor ## A 10 x 10 ridge precision matrix estimate with lambda = 3.670373 ## [,1] [,2] [,3] [,4] [,5] [,6] … ## [1,] 1.00000000 0 -0.03888796 0.03659085 -0.06333107 0.00000000 … ## [2,] 0.00000000 1 0.00000000 0.00000000 0.00000000 0.00000000 … ## [3,] -0.03888796 0 1.00000000 -0.05400231 0.00000000 0.00000000 … ## [4,] 0.03659085 0 -0.05400231 1.00000000 0.00000000 0.04435949 … ## [5,] -0.06333107 0 0.00000000 0.00000000 1.00000000 0.00000000 … ## [6,] 0.00000000 0 0.00000000 0.04435949 0.00000000 1.00000000 … ## … 4 more rows and 4 more columns ## ## $sparsePrecision ## A 10 x 10 ridge precision matrix estimate with lambda = 3.670373 ## [,1] [,2] [,3] [,4] [,5] [,6] … ## [1,] 2.07949563 0.000000 0.07847988 -0.07471473 0.1339554 0.00000000 … ## [2,] 0.00000000 2.195422 0.00000000 0.00000000 0.0000000 0.00000000 … ## [3,] 0.07847988 0.000000 1.95852186 0.10701169 0.0000000 0.00000000 … ## [4,] -0.07471473 0.000000 0.10701169 2.00497691 0.0000000 -0.09090978 … ## [5,] 0.13395542 0.000000 0.00000000 0.00000000 2.1514383 0.00000000 … ## [6,] 0.00000000 0.000000 0.00000000 -0.09090978 0.0000000 2.09478152 … ## … 4 more rows and 4 more columns rags2ridges::GGMnetworkStats(spr$sparseParCor, as.table = TRUE) ## degree betweenness closeness eigenCentrality nNeg nPos mutualInfo ## [1,] 3 1.500000 0.05882353 0.6272576 2 1 0.006771079 ## [2,] 1 0.000000 0.04761905 0.2661848 0 1 0.001206142 ## [3,] 4 3.083333 0.07142857 0.8538060 4 0 0.019887529 ## [4,] 5 10.250000 0.07142857 0.8583712 2 3 0.010460258 ## [5,] 3 2.000000 0.06250000 0.6442970 1 2 0.007847332 ## [6,] 1 0.000000 0.04545455 0.2284853 0 1 0.001986481 ## [7,] 2 0.000000 0.05555556 0.4773306 2 0 0.003664627 ## [8,] 4 5.333333 0.07142857 0.7932303 2 2 0.007450242 ## [9,] 3 1.333333 0.06666667 0.7219403 1 2 0.007439661 ## [10,] 6 13.500000 0.07692308 1.0000000 2 4 0.024757853 ## variance partialVar ## [1,] 1.006794 1 ## [2,] 1.001207 1 ## [3,] 1.020087 1 ## [4,] 1.010515 1 ## [5,] 1.007878 1 ## [6,] 1.001988 1 ## [7,] 1.003671 1 ## [8,] 1.007478 1 ## [9,] 1.007467 1 ## [10,] 1.025067 1 #rags2ridges::fullMontyS(X, lambdaMin = 0.01, lambdaMax = 30) - Gives an error rags2ridges::Ugraph(spr$sparseParCor, type = &quot;weighted&quot;) ## Warning in type.convert.default(X[[i]], ...): &#39;as.is&#39; should be specified by ## the caller; using TRUE ## Warning in type.convert.default(X[[i]], ...): &#39;as.is&#39; should be specified by ## the caller; using TRUE ## [,1] [,2] ## [1,] 1.000000 0.000000e+00 ## [2,] 0.809017 5.877853e-01 ## [3,] 0.309017 9.510565e-01 ## [4,] -0.309017 9.510565e-01 ## [5,] -0.809017 5.877853e-01 ## [6,] -1.000000 1.224647e-16 ## [7,] -0.809017 -5.877853e-01 ## [8,] -0.309017 -9.510565e-01 ## [9,] 0.309017 -9.510565e-01 ## [10,] 0.809017 -5.877853e-01 See: The Generalized Ridge Estimator of the Inverse Covariance Matrix (Wessel_2019?), Ridge estimation of inverse covariance matrices from high-dimensional data (Wessel_Ridge?)↩︎ "],["r-lab-1---basics-i.html", "Chapter 34 R Lab 1 - Basics I 34.1 R, RStudio, and R Packages 34.2 RStudio 34.3 Working directory 34.4 Data Types and Stuctures 34.5 Vectors 34.6 Subsetting Vectors 34.7 Vectorization or vector operations 34.8 Matrices 34.9 Matrix Operations 34.10 Subsetting Matrix 34.11 R-Style Guide", " Chapter 34 R Lab 1 - Basics I What we will review in this lab: R, RStudio, and R Packages, Starting with RStudio, Working Directory, Data Types and Structures (Vectors and Matrices), R-Style Guide 34.1 R, RStudio, and R Packages R is both a programming language and software environment for statistical computing, which is free and open-source. With ever increasing availability of large amounts of data, it is critical to have the ability to analyze the data and learn from it for making informed decisions. Familiarity with software such as R allows users to visualize data, run statistical tests, and apply machine learning algorithms. Even if you already know other software, there are still good reasons to learn R: R is free. If your future employer does not already have R installed, you can always download it for free, unlike other proprietary software packages that require expensive licenses. You can always have access to R on your computer. R gives you access to cutting-edge technology. Top researchers develop statistical learning methods in R, and new algorithms are constantly added to the list of packages you can download. R is a useful skill. Employers that value analytics recognize R as useful and important. If for no other reason, learning R is worthwhile to help boost your resume. To get started, you will need to install two pieces of software: R, the actual programming language: Download it from here. – Chose your operating system, and select the most recent version. RStudio, an excellent integrated development environment (IDE) for working with R, an interface used to interact with R: Download it from here. The following notes will serve as an introduction to the R basics that we will need in this book. At the beginning, these introductory R subjects may feel like an overwhelming amount of information. You are not expected to pick up everything the first time through. You should try all of the code from these examples and solve the practice exercises. R is used both for software development and data analysis. We will not use it for software development but apply some concepts in that area. Our main goal will be to analyze data, but we will also perform programming exercises that help illustrate certain algorithmic concepts. Here is a very good article about R and Programming that everybody should read: 7 Reasons for policy professionals to get into R programming in 2019 (Jones_2019?). 34.2 RStudio Source Pane, click on the plus sign in the top left corner. From the drop-down menu, select R Script . As shown in that dropdown menu, you can also open an R Script by pressing Ctrl+Shift+N. You should now see the screen above. The Console Pane is the interface to R. If you opened R directly instead of opening RStudio, you would see just this console. You can type commands directly in the console. The console displays the results of any command you run. For example, type 2+4 in the command line and press enter. You should see the command you typed, the result of the command, and a new command line. To clear the console, you press Ctrl+L or type cat(“\\014”) in the command line. R code can be entered into the command line directly (in Console Pane) or saved to a script (Source Pane). Let’s try some coding. 2 + 3 #write this on the command line and hit Enter ## [1] 5 Now write the same line into the script in Source Pane and run it The Source Pane is a text editor where you can type your code before running it. You can save your code in a text file called a script. Scripts have typically file names with the extension .R. Any text shown in green is a comment in the script. You write a comment by adding a # to an RScript. Anything to the right of a # is considered a comment and is thus ignored by R when running code. Place your cursor anywhere on the first few lines of code and click Run. You can also run code by pressing Ctrl+Enter. The Environment Pane includes an Environment, a History tab, and a Connections tab. The Connections tab makes it easy to connect to any data source on your system. The Environment tab displays any objects that you have created during your R session. For example, we created three variables: \\(x\\), \\(y\\), and \\(z\\). R stored those variables as objects, and you can see them in the Environment pane. We will discuss R objects in more detail later. If you want to see a list of all objects in the current session, type ls() in the command line. You can remove an individual object from the environment with the rm() command. For example, remove x by typing rm(x) in the command line. You can remove all objects from the environment by clicking or typing rm(list=ls()) in the command line. The History tab keeps a record of all the commands you have run. To copy a command from the history into the console, select the command and press Enter. The Files Pane includes several tabs that provide useful information. The Files tab displays the contents of your working directory. The Plot tab shows all graphs that you have created. The Packages tab displays the R packages that you have installed in your System Library. An R package typically includes code, data, documentation for the package and functions inside, and tests to check everything works as it should. Check to see if the package moments has been installed. If you cannot find it, you need to install it by using the command install.packages(\"moments\"). Once you have installed the package, you need to load it using the command library(moments). Or you can use install tab and follow the instructions and the go to package to check it to activate as shown below. The help tab has built-in documentation for packages and functions in R. The help is automatically available for any loaded packages. You can access that file by typing help(mean) or ?mean in the command line. You can also use the search bar in the help tab. One of the most difficult things to do when learning R is to know how to find help. Your very first helper should be Google where you post your error message or a short description of your issue. The ability to solve problems using this method is quickly becoming an extremely valuable skill. Ask your tutor or instructor in the lab, only after you use all other available sources. Do not be discouraged by running into errors and difficulties when learning R. (Or any technical skill.) It is simply part of the learning process. The Viewer tab displays HTML output. R packages such as R Markdown and Shiny create HTML outputs that you can view in the Viewer tab. We’ll see it later. 34.3 Working directory Without further specification, files will be loaded from and saved to the working directory. The functions getwd() and setwd() will get and set the working directory, respectively. getwd() ## [1] &quot;/Users/yigitaydede/Dropbox/toolbox&quot; #setwd(&quot;Book2022&quot;) #List all the objects in your local workspace using ls() ## character(0) #List all the files in your working directory using list.files() or dir() ## [1] &quot;_bookdown_files&quot; &quot;_bookdown.yml&quot; ## [3] &quot;_main_files&quot; &quot;_output.yml&quot; ## [5] &quot;01-Intro.md&quot; &quot;01-Intro.Rmd&quot; ## [7] &quot;02-cross-refs_files&quot; &quot;02-Preliminaries_files&quot; ## [9] &quot;02-Preliminaries.md&quot; &quot;02-Preliminaries.Rmd&quot; ## [11] &quot;03-Bias-VarianceTradeoff_cache&quot; &quot;03-Bias-VarianceTradeoff_files&quot; ## [13] &quot;03-Bias-VarianceTradeoff.md&quot; &quot;03-Bias-VarianceTradeoff.Rmd&quot; ## [15] &quot;04-Overfitting_cache&quot; &quot;04-Overfitting_files&quot; ## [17] &quot;04-Overfitting.md&quot; &quot;04-Overfitting.Rmd&quot; ## [19] &quot;05-ParametricEstimations_files&quot; &quot;05-ParametricEstimations.md&quot; ## [21] &quot;05-ParametricEstimations.Rmd&quot; &quot;06-Basics_cache&quot; ## [23] &quot;06-Basics_files&quot; &quot;06-Basics.md&quot; ## [25] &quot;06-Basics.Rmd&quot; &quot;07-Smoothing_files&quot; ## [27] &quot;07-Smoothing.md&quot; &quot;07-Smoothing.Rmd&quot; ## [29] &quot;08-Nonparametric_kNN_cache&quot; &quot;08-Nonparametric_kNN_files&quot; ## [31] &quot;08-Nonparametric_kNN.md&quot; &quot;08-Nonparametric_kNN.Rmd&quot; ## [33] &quot;09-HyperTuning_cache&quot; &quot;09-HyperTuning_files&quot; ## [35] &quot;09-HyperTuning.md&quot; &quot;09-HyperTuning.Rmd&quot; ## [37] &quot;10-TuningClass_cache&quot; &quot;10-TuningClass_files&quot; ## [39] &quot;10-TuningClass.md&quot; &quot;10-TuningClass.Rmd&quot; ## [41] &quot;11-CART_files&quot; &quot;11-CART.md&quot; ## [43] &quot;11-CART.Rmd&quot; &quot;12-Ensemble_cache&quot; ## [45] &quot;12-Ensemble_files&quot; &quot;12-Ensemble.md&quot; ## [47] &quot;12-Ensemble.Rmd&quot; &quot;13-EnsembleApplication_cache&quot; ## [49] &quot;13-EnsembleApplication_files&quot; &quot;13-EnsembleApplication.md&quot; ## [51] &quot;13-EnsembleApplication.Rmd&quot; &quot;14-SVM_cache&quot; ## [53] &quot;14-SVM_files&quot; &quot;14-SVM.md&quot; ## [55] &quot;14-SVM.Rmd&quot; &quot;15-NN_cache&quot; ## [57] &quot;15-NN_files&quot; &quot;15-NN.md&quot; ## [59] &quot;15-NN.Rmd&quot; &quot;16-Ridge_cache&quot; ## [61] &quot;16-Ridge_files&quot; &quot;16-Ridge.md&quot; ## [63] &quot;16-Ridge.Rmd&quot; &quot;17-Lasso_cache&quot; ## [65] &quot;17-Lasso_files&quot; &quot;17-Lasso.md&quot; ## [67] &quot;17-Lasso.Rmd&quot; &quot;18-TimeSeriesArima_cache&quot; ## [69] &quot;18-TimeSeriesArima_files&quot; &quot;18-TimeSeriesArima.md&quot; ## [71] &quot;18-TimeSeriesArima.Rmd&quot; &quot;19-TSGrid_cache&quot; ## [73] &quot;19-TSGrid_files&quot; &quot;19-TSGrid.md&quot; ## [75] &quot;19-TSGrid.Rmd&quot; &quot;20-TSEmbedding_files&quot; ## [77] &quot;20-TSEmbedding.md&quot; &quot;20-TSEmbedding.Rmd&quot; ## [79] &quot;21-TSRandomForest_cache&quot; &quot;21-TSRandomForest_files&quot; ## [81] &quot;21-TSRandomForest.md&quot; &quot;21-TSRandomForest.Rmd&quot; ## [83] &quot;22-TSNeural_cache&quot; &quot;22-TSNeural_files&quot; ## [85] &quot;22-TSNeural.md&quot; &quot;22-TSNeural.Rmd&quot; ## [87] &quot;23-DimensionReduction.md&quot; &quot;23-DimensionReduction.Rmd&quot; ## [89] &quot;24-SingValueDecomp.md&quot; &quot;24-SingValueDecomp.Rmd&quot; ## [91] &quot;25-RankrApprox_files&quot; &quot;25-RankrApprox.md&quot; ## [93] &quot;25-RankrApprox.Rmd&quot; &quot;26-MoorePenroseInv.md&quot; ## [95] &quot;26-MoorePenroseInv.Rmd&quot; &quot;27-PrincipalCompAnalysis_files&quot; ## [97] &quot;27-PrincipalCompAnalysis.md&quot; &quot;27-PrincipalCompAnalysis.Rmd&quot; ## [99] &quot;28-FactorAnalysis_files&quot; &quot;28-FactorAnalysis.md&quot; ## [101] &quot;28-FactorAnalysis.Rmd&quot; &quot;29-NetworkAnalysis.md&quot; ## [103] &quot;29-NetworkAnalysis.Rmd&quot; &quot;30-RegularizedCovMatrix_files&quot; ## [105] &quot;30-RegularizedCovMatrix.md&quot; &quot;30-RegularizedCovMatrix.Rmd&quot; ## [107] &quot;31-RLabs.Rmd&quot; &quot;32-RLab2_files&quot; ## [109] &quot;32-RLab2.Rmd&quot; &quot;33-RLab3DataPrep_files&quot; ## [111] &quot;33-RLab3DataPrep.Rmd&quot; &quot;34-RLab4SimLab_cache&quot; ## [113] &quot;34-RLab4SimLab_files&quot; &quot;34-RLab4SimLab.Rmd&quot; ## [115] &quot;35-Appendix1_cache&quot; &quot;35-Appendix1_files&quot; ## [117] &quot;35-Appendix1.Rmd&quot; &quot;36-Appendix2_cache&quot; ## [119] &quot;36-Appendix2_files&quot; &quot;36-Appendix2.Rmd&quot; ## [121] &quot;44-citations.Rmd&quot; &quot;45-blocks.Rmd&quot; ## [123] &quot;50-references.Rmd&quot; &quot;adult_names.txt&quot; ## [125] &quot;adult_test.csv&quot; &quot;adult_train.csv&quot; ## [127] &quot;auto-mpg.csv&quot; &quot;book.bib&quot; ## [129] &quot;comt.rds&quot; &quot;creditcard10.RData&quot; ## [131] &quot;dftoronto.RData&quot; &quot;docs&quot; ## [133] &quot;fes73.rds&quot; &quot;hedonic.dat&quot; ## [135] &quot;index.md&quot; &quot;index.Rmd&quot; ## [137] &quot;irates.dat&quot; &quot;mnist.Rdata&quot; ## [139] &quot;myocarde.csv&quot; &quot;packages.bib&quot; ## [141] &quot;png&quot; &quot;preamble.tex&quot; ## [143] &quot;README.md&quot; &quot;renderbc4a2dfbfcbc.rds&quot; ## [145] &quot;style.css&quot; &quot;table1.text&quot; ## [147] &quot;toolbox.Rproj&quot; &quot;toronto2.rds&quot; ## [149] &quot;wineQualityReds.csv&quot; &quot;YA_TextBook.md&quot; ## [151] &quot;YA_TextBook.rds&quot; #As we go through this lesson, you should be examining the help page #for each new function. Check out the help page for list.files with the #command ?list.files #or help(&quot;list.files&quot;) #Using the args() function on a function name is also a handy way to #see what arguments a function can take. args(list.files) ## function (path = &quot;.&quot;, pattern = NULL, all.files = FALSE, full.names = FALSE, ## recursive = FALSE, ignore.case = FALSE, include.dirs = FALSE, ## no.. = FALSE) ## NULL 34.4 Data Types and Stuctures R has a number of basic data types. Numeric: Also known as Double. The default type when dealing with numbers. 1,1.0,42.5 Integer: 1L,2L,42L Complex: 4 + 2i Logical: Two possible values: TRUE and FALSE. NA is also considered logical. Character:“a”,“Statistics”,“1plus2.” R also has a number of basic data structures. A data structure is either homogeneous (all elements are of the same data type) or heterogeneous (elements can be of more than one data type): You can think each data structure as data container where you data is stored. Here are the main “container” or data structures. Think it as Stata or Excel spread-sheets. Vector: 1 dimension (column OR row) and homogeneous. That is every element of the vector has to be the same type. Each vector can be thought of as a variable. Matrix: 2 dimensions (column AND row) and homogeneous. That is every element of the matrix has to be the same type. Data Frame: 2 dimensions (column AND row) and heterogeneous. That is every element of the data frame doesn’t have to be the same type. This is the main difference between a matrix and a data frame. Data frames are the most common data structure in any data analysis. List: 1 dimension and heterogeneous. Data can be multiple data structures. Array: 3+ dimensions and homogeneous. 34.5 Vectors Many operations in R make heavy use of vectors. Possibly the most common way to create a vector in R is using the c() function, which is short for “combine.” As the name suggests, it combines a list of elements separated by commas. c(1, 5, 0, -1) ## [1] 1 5 0 -1 If we would like to store this vector in a variable we can do so with the assignment operator &lt;- or =. But the convention is &lt;- x &lt;- c(1, 5, 0, -1) z = c(1, 5, 0, -1) x ## [1] 1 5 0 -1 z ## [1] 1 5 0 -1 Because vectors must contain elements that are all the same type, R will automatically coerce to a single type when attempting to create a vector that combines multiple types. c(10, &quot;Machine Learning&quot;, FALSE) ## [1] &quot;10&quot; &quot;Machine Learning&quot; &quot;FALSE&quot; c(10, FALSE) ## [1] 10 0 c(10, TRUE) ## [1] 10 1 x &lt;- c(10, &quot;Machine Learning&quot;, FALSE) str(x) #this tells us the structure of the object ## chr [1:3] &quot;10&quot; &quot;Machine Learning&quot; &quot;FALSE&quot; class(x) ## [1] &quot;character&quot; y &lt;- c(10, FALSE) str(y) ## num [1:2] 10 0 class(y) ## [1] &quot;numeric&quot; If you want to create a vector based on a sequence of numbers, you can do it easily with an operator, which creates a sequence of integers between two specified integers. y &lt;- c(1:15) y ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #or y &lt;- 1:8 y ## [1] 1 2 3 4 5 6 7 8 Note that scalars do not exists in R. They are simply vectors of length 1. y &lt;- 24 #this a vector with 1 element, 24 If you want to create a vector based on a specific sequence of numbers increasing or decreasing, you can use seq() y &lt;- seq(from = 1.5, to = 13, by = 0.9) #increasing y ## [1] 1.5 2.4 3.3 4.2 5.1 6.0 6.9 7.8 8.7 9.6 10.5 11.4 12.3 y &lt;- seq(1.5, -13, -0.9) #decreasing. Note that you can ignore the argument labels y ## [1] 1.5 0.6 -0.3 -1.2 -2.1 -3.0 -3.9 -4.8 -5.7 -6.6 -7.5 -8.4 ## [13] -9.3 -10.2 -11.1 -12.0 -12.9 The other useful tool is rep() rep(&quot;ML&quot;, times = 10) ## [1] &quot;ML&quot; &quot;ML&quot; &quot;ML&quot; &quot;ML&quot; &quot;ML&quot; &quot;ML&quot; &quot;ML&quot; &quot;ML&quot; &quot;ML&quot; &quot;ML&quot; #or x &lt;- c(1, 5, 0, -1) rep(x, times = 2) ## [1] 1 5 0 -1 1 5 0 -1 And we can use them as follows. wow &lt;- c(x, rep(seq(1, 9, 2), 3), c(1, 2, 3), 42, 2:4) wow ## [1] 1 5 0 -1 1 3 5 7 9 1 3 5 7 9 1 3 5 7 9 1 2 3 42 2 3 ## [26] 4 Another one, which can be used to create equal intervals. g &lt;- seq(6, 60, length = 4) g ## [1] 6 24 42 60 And we can use longer names and calculate the number of elements in a vector: length(wow) ## [1] 26 34.6 Subsetting Vectors One of the most confusing subjects in R is subsetting the data containers. It’s an important part in data management and if it is done in 2 steps, the whole operation becomes quite easy: Identifying the index of the element that satisfies the required condition, Calling the index to subset the vector. But before we start lets see a simple subsetting. (Note the square brackets) #Suppose we have the following vector myvector &lt;- c(1, 2, 3, 4, 5, 8, 4, 10, 12) #I can call each element with its index number: myvector[c(1,6)] ## [1] 1 8 myvector[4:7] ## [1] 4 5 8 4 myvector[-6] ## [1] 1 2 3 4 5 4 10 12 Okay, we are ready … #Let&#39;s look at this vector myvector &lt;- c(1, 2, 3, 4, 5, 8, 4, 10, 12) #We want to subset only those less than 5 #Step 1: use a logical operator to identify the elements #meeting the condition. logi &lt;- myvector &lt; 5 logi ## [1] TRUE TRUE TRUE TRUE FALSE FALSE TRUE FALSE FALSE #logi is a logical vector class(logi) ## [1] &quot;logical&quot; #Step 2: use it for subsetting newvector &lt;- myvector[logi==TRUE] newvector ## [1] 1 2 3 4 4 #or better newvector &lt;- myvector[logi] newvector ## [1] 1 2 3 4 4 This is good as it shows those 2 steps. Perhaps, we can combine these 2 steps as follows: newvector &lt;- myvector[myvector &lt; 5] newvector ## [1] 1 2 3 4 4 Another way to do this is to use of which(), which gives us the index of each element that satisfies the condition. ind &lt;- which(myvector &lt; 5) # Step 1 ind ## [1] 1 2 3 4 7 newvector &lt;- myvector[ind] # Step 2 newvector ## [1] 1 2 3 4 4 Or we can combine these 2 steps: newvector &lt;- myvector[which(myvector &lt; 5)] newvector ## [1] 1 2 3 4 4 Last one: find the 4’s in myvector make them 8 (I know hard, but after a couple of tries it will seem easier): myvector &lt;- c(1, 2, 3, 4, 5, 8, 4, 10, 12) #I&#39;ll show you 3 ways to do that. #1st way to show the steps ind &lt;- which(myvector==4) #identifying the index with 4 newvector &lt;- myvector[ind] + 4 # adding them 4 myvector[ind] &lt;- newvector #replacing those with the new values myvector ## [1] 1 2 3 8 5 8 8 10 12 #2nd and easier way myvector[which(myvector==4)] &lt;- myvector[which(myvector==4)] + 4 myvector ## [1] 1 2 3 8 5 8 8 10 12 #3nd and easiest way myvector[myvector==4] &lt;- myvector[myvector==4] + 4 myvector ## [1] 1 2 3 8 5 8 8 10 12 What happens if the vector is a character vector? How can we subset it? We can use grep() as shown below: m &lt;- c(&quot;about&quot;, &quot;aboard&quot;, &quot;board&quot;, &quot;bus&quot;, &quot;cat&quot;, &quot;abandon&quot;) #Now suppose that we need to pick the elements that contain &quot;ab&quot; #Same steps again a &lt;- grep(&quot;ab&quot;, m) #similar to which() that gives us index numbers a ## [1] 1 2 6 newvector &lt;- m[a] newvector ## [1] &quot;about&quot; &quot;aboard&quot; &quot;abandon&quot; 34.7 Vectorization or vector operations One of the biggest strengths of R is its use of vectorized operations. Lets see it in action! x &lt;- 1:10 x ## [1] 1 2 3 4 5 6 7 8 9 10 x+1 ## [1] 2 3 4 5 6 7 8 9 10 11 2 * x ## [1] 2 4 6 8 10 12 14 16 18 20 2 ^ x ## [1] 2 4 8 16 32 64 128 256 512 1024 x ^ 2 ## [1] 1 4 9 16 25 36 49 64 81 100 sqrt(x) ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 ## [9] 3.000000 3.162278 log(x) ## [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 1.7917595 1.9459101 ## [8] 2.0794415 2.1972246 2.3025851 Its like a calculator! y &lt;- 1:10 y ## [1] 1 2 3 4 5 6 7 8 9 10 x + y ## [1] 2 4 6 8 10 12 14 16 18 20 How about this: y &lt;- 1:11 x + y ## Warning in x + y: longer object length is not a multiple of shorter object ## length ## [1] 2 4 6 8 10 12 14 16 18 20 12 OK, the warning is self-explanatory. But what’s “12” at the end? It’s the sum of the first element of x, which is 1 and the last element of y, which is 11. 34.8 Matrices R stores matrices and arrays in a similar manner as vectors, but with the attribute called dimension. A matrix is an array that has two dimensions. Data in a matrix are organized into rows and columns. Matrices are commonly used while arrays are rare. We will not see arrays in this book. Matrices are homogeneous data structures, just like atomic vectors, but they can have 2 dimensions, rows and columns, unlike vectors. Matrices can be created using the matrix function. #Let&#39;s create 5 x 4 numeric matrix containing numbers from 1 to 20 mymatrix &lt;- matrix(1:20, nrow = 5, ncol = 4) #Here we order the number by columns mymatrix ## [,1] [,2] [,3] [,4] ## [1,] 1 6 11 16 ## [2,] 2 7 12 17 ## [3,] 3 8 13 18 ## [4,] 4 9 14 19 ## [5,] 5 10 15 20 class(mymatrix) ## [1] &quot;matrix&quot; &quot;array&quot; dim(mymatrix) ## [1] 5 4 mymatrix &lt;- matrix(1:20, nrow = 5, ncol = 4, byrow = TRUE) mymatrix ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 ## [4,] 13 14 15 16 ## [5,] 17 18 19 20 We will be using two different variables. Following the usual mathematical convention, lower-case x (or any other letter), which stores a vector and capital X, which stores a matrix. We can do this because R is case sensitive. 34.9 Matrix Operations Now some key matrix operations: X &lt;- matrix(1:9, nrow = 3, ncol = 3) Y &lt;- matrix(11:19, nrow = 3, ncol = 3) A &lt;- X + Y A ## [,1] [,2] [,3] ## [1,] 12 18 24 ## [2,] 14 20 26 ## [3,] 16 22 28 B &lt;- X * Y B ## [,1] [,2] [,3] ## [1,] 11 56 119 ## [2,] 24 75 144 ## [3,] 39 96 171 #The symbol %*% is called pipe operator. #And it carries out a matrix multiplication #different than a simple multiplication. C &lt;- X%*%Y C ## [,1] [,2] [,3] ## [1,] 150 186 222 ## [2,] 186 231 276 ## [3,] 222 276 330 Note that X * Y is not a matrix multiplication. It is element by element multiplication. (Same for X / Y). Instead, matrix multiplication uses %*%. Other matrix functions include t() which gives the transpose of a matrix and solve() which returns the inverse of a square matrix if it is invertible. matrix() function is not the only way to create a matrix. Matrices can also be created by combining vectors as columns, using cbind(), or combining vectors as rows, using rbind(). Look at this: #Let&#39;s create 2 vectors. x &lt;- rev(c(1:9)) #this can be done by c(9:1). I wanted to show rev() x ## [1] 9 8 7 6 5 4 3 2 1 y &lt;- rep(2, 9) y ## [1] 2 2 2 2 2 2 2 2 2 A &lt;- rbind(x, y) A ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## x 9 8 7 6 5 4 3 2 1 ## y 2 2 2 2 2 2 2 2 2 B &lt;- cbind(x, y) B ## x y ## [1,] 9 2 ## [2,] 8 2 ## [3,] 7 2 ## [4,] 6 2 ## [5,] 5 2 ## [6,] 4 2 ## [7,] 3 2 ## [8,] 2 2 ## [9,] 1 2 #You can label each column and row colnames(B) &lt;- c(&quot;column1&quot;, &quot;column2&quot;) B ## column1 column2 ## [1,] 9 2 ## [2,] 8 2 ## [3,] 7 2 ## [4,] 6 2 ## [5,] 5 2 ## [6,] 4 2 ## [7,] 3 2 ## [8,] 2 2 ## [9,] 1 2 Here are some operations very useful when using matrices: rowMeans(A) ## x y ## 5 2 colMeans(B) ## column1 column2 ## 5 2 rowSums(B) ## [1] 11 10 9 8 7 6 5 4 3 colSums(A) ## [1] 11 10 9 8 7 6 5 4 3 Last thing: When vectors are coerced to become matrices, they are column vectors. So a vector of length n becomes an \\(n \\times 1\\) matrix after coercion. x ## [1] 9 8 7 6 5 4 3 2 1 X &lt;- as.matrix(x) X ## [,1] ## [1,] 9 ## [2,] 8 ## [3,] 7 ## [4,] 6 ## [5,] 5 ## [6,] 4 ## [7,] 3 ## [8,] 2 ## [9,] 1 34.10 Subsetting Matrix Like vectors, matrices can be subsetted using square brackets, [ ]. However, since matrices are two-dimensional, we need to specify both row and column indices when subsetting. Y ## [,1] [,2] [,3] ## [1,] 11 14 17 ## [2,] 12 15 18 ## [3,] 13 16 19 Y[1,3] ## [1] 17 Y[,3] ## [1] 17 18 19 Y[2,] ## [1] 12 15 18 Y[2, c(1, 3)] # If we need more than a column (row), we use c() ## [1] 12 18 Conditional subsetting is the same as before in vectors. Let’s solve this problem: what’s the number in column 1 in Y when the number in column 3 is 18? Y ## [,1] [,2] [,3] ## [1,] 11 14 17 ## [2,] 12 15 18 ## [3,] 13 16 19 Y[Y[,3]==18, 1] ## [1] 12 #What are the numbers in a row when the number in column 3 is 18? Y[Y[,3]==19, ] ## [1] 13 16 19 #Print the rows in Y when the number in column 3 is more than 17? Y[Y[,3] &gt; 17, ] ## [,1] [,2] [,3] ## [1,] 12 15 18 ## [2,] 13 16 19 We will see later how these conditional subsetting can be done much smoother with data frames. 34.11 R-Style Guide The idea is simple: your R code, or any other code in different languages, should be written in a readable and maintainable style. Here is a blog by Roman Pahl that may help you develop a better styling in your codes. (You may find in some chapters and labs that my codes are not following the “good” styling practices. I am trying to improve!) Next: Lists and data frames "],["r-lab-2---basics-ii.html", "Chapter 35 R Lab 2 - Basics II 35.1 Data frames and lists 35.2 Programming Basics", " Chapter 35 R Lab 2 - Basics II What we will review in this lab: Data frames and lists, Programming Basics. 35.1 Data frames and lists We will begin with lists first to get them out of the way. Although they are not used much in the book, you should know about them. 35.1.1 Lists A list is a one-dimensional heterogeneous data structure. So it is indexed like a vector with a single integer value, but each element can contain an element of any type. Lets look at some examples of working with them: # creation A &lt;- list(42, &quot;Hello&quot;, TRUE) dim(A) ## NULL str(A) ## List of 3 ## $ : num 42 ## $ : chr &quot;Hello&quot; ## $ : logi TRUE class(A) ## [1] &quot;list&quot; # Another one B &lt;- list( a = c(1, 2, 3, 4), b = TRUE, c = &quot;Hello!&quot;, d = function(arg = 1) {print(&quot;Hello World!&quot;)}, X = matrix(0, 4 , 4) ) B ## $a ## [1] 1 2 3 4 ## ## $b ## [1] TRUE ## ## $c ## [1] &quot;Hello!&quot; ## ## $d ## function(arg = 1) {print(&quot;Hello World!&quot;)} ## ## $X ## [,1] [,2] [,3] [,4] ## [1,] 0 0 0 0 ## [2,] 0 0 0 0 ## [3,] 0 0 0 0 ## [4,] 0 0 0 0 dim(B) ## NULL dim(B$X) ## [1] 4 4 str(B) ## List of 5 ## $ a: num [1:4] 1 2 3 4 ## $ b: logi TRUE ## $ c: chr &quot;Hello!&quot; ## $ d:function (arg = 1) ## ..- attr(*, &quot;srcref&quot;)= &#39;srcref&#39; int [1:8] 12 15 12 55 15 55 12 12 ## .. ..- attr(*, &quot;srcfile&quot;)=Classes &#39;srcfilecopy&#39;, &#39;srcfile&#39; &lt;environment: 0x7f929d4b55b0&gt; ## $ X: num [1:4, 1:4] 0 0 0 0 0 0 0 0 0 0 ... class(B) ## [1] &quot;list&quot; Source: https://stats.idre.ucla.edu/stat/data/intro_r/intro_r.html#(27) Lists can be subset using two syntaxes, the $ operator, and square brackets [ ]. The $ operator returns a named element of a list. The [ ] syntax returns a list, while the [[ ]] returns an element of a list. #For example to get the matrix in our list B$X ## [,1] [,2] [,3] [,4] ## [1,] 0 0 0 0 ## [2,] 0 0 0 0 ## [3,] 0 0 0 0 ## [4,] 0 0 0 0 #or B[5] ## $X ## [,1] [,2] [,3] [,4] ## [1,] 0 0 0 0 ## [2,] 0 0 0 0 ## [3,] 0 0 0 0 ## [4,] 0 0 0 0 #or B[[5]] ## [,1] [,2] [,3] [,4] ## [1,] 0 0 0 0 ## [2,] 0 0 0 0 ## [3,] 0 0 0 0 ## [4,] 0 0 0 0 #And to get the (1,3) element of matrix X in list B B[[5]][1,3] ## [1] 0 35.1.2 Data Frames We have seen vectors, matrices, and lists for storing data. We will now introduce a data frame that is the most common way to store and interact with data in this book. Datasets for statistical analysis are typically stored in data frames in R. Unlike a matrix, a data frame can have different data types for each elements (columns). A data frame is a list of vectors (columns - you can think of them as “variables”). So, each vector (column) must contain the same data type, but the different vectors (columns) can store different data types. However, unlike a list, the columns (elements) of a data frame must all be vectors and have the same length (number of observations) Data frames combine the features of matrices and lists. Like matrices, dataframes are rectangular, where the columns are variables and the rows are observations of those variables. like lists, dataframes can have elements (column vectors) of different data types (some double, some character, etc.) – but they must be equal length. Real datasets usually combine variables of different types, so data frames are well suited for storage. #One way to do that mydata &lt;- data.frame(diabetic = c(TRUE, FALSE, TRUE, FALSE), height = c(65, 69, 71, 73)) mydata ## diabetic height ## 1 TRUE 65 ## 2 FALSE 69 ## 3 TRUE 71 ## 4 FALSE 73 str(mydata) ## &#39;data.frame&#39;: 4 obs. of 2 variables: ## $ diabetic: logi TRUE FALSE TRUE FALSE ## $ height : num 65 69 71 73 dim(mydata) ## [1] 4 2 #Or create vectors for each column diabetic = c(TRUE, FALSE, TRUE, FALSE) height = c(65, 69, 71, 73) #And include them in a dataframe as follows mydata &lt;- data.frame(diabetic, height) mydata ## diabetic height ## 1 TRUE 65 ## 2 FALSE 69 ## 3 TRUE 71 ## 4 FALSE 73 str(mydata) ## &#39;data.frame&#39;: 4 obs. of 2 variables: ## $ diabetic: logi TRUE FALSE TRUE FALSE ## $ height : num 65 69 71 73 dim(mydata) ## [1] 4 2 #And more importantly, you can extend it by adding more columns weight = c(103, 45, 98.4, 70.5) mydata &lt;- data.frame(mydata, weight) mydata ## diabetic height weight ## 1 TRUE 65 103.0 ## 2 FALSE 69 45.0 ## 3 TRUE 71 98.4 ## 4 FALSE 73 70.5 You will have the following mistake a lot. Let’s see it now so you can avoid it later. #Try running the code below separately without the comment # and see what happens #mydata &lt;- data.frame(diabetic = c(TRUE, FALSE, TRUE, FALSE, FALSE), #height = c(65, 69, 71, 73)) The problem in the example above is that there are a different number of rows and columns. Here are some useful tools for diagnosing this problem: #Number of columns ncol(mydata) ## [1] 3 nrow(mydata) ## [1] 4 Often data you’re working with has abstract column names, such as (x1, x2, x3…).The dataset cars is data from the 1920s on “Speed and Stopping Distances of Cars”. There is only 2 columns shown below. colnames(datasets::cars) ## [1] &quot;speed&quot; &quot;dist&quot; #Using Base r: colnames(cars)[1:2] &lt;- c(&quot;Speed (mph)&quot;, &quot;Stopping Distance (ft)&quot;) colnames(cars) ## [1] &quot;Speed (mph)&quot; &quot;Stopping Distance (ft)&quot; #Using GREP: colnames(cars)[grep(&quot;dist&quot;, colnames(cars))] &lt;- &quot;Stopping Distance (ft)&quot; colnames(cars) ## [1] &quot;Speed (mph)&quot; &quot;Stopping Distance (ft)&quot; 35.1.3 Reading (importing) and writting (exporting) data files When you read a data in other formats, they may also be imported as a data frame. How can we import data into R? The simple way is here: If the data is a .csv file, for example, we would also use the read_csv() function from the readr package. Note that R has a built in function read.csv() that operates very similarly. The readr function read_csv() has a number of advantages. For example, it is much faster reading larger data. It also uses the tibble package to read the data as a tibble. # library(readr) # library(RCurl) # x &lt;- getURL(&quot;https://raw.githubusercontent.com/tidyverse/readr/main/inst/extdata/mtcars.csv&quot;) # example_csv = read_csv(x, show_col_types = FALSE) # head(example_csv) # str(example_csv) A tibble is simply a data frame that prints with sanity. Notice in the output above that we are given additional information such as dimension and variable type. The as_tibble() function can be used to coerce a regular data frame to a tibble. # library(tibble) # example_data = as_tibble(example_csv) # example_data It is important to note that while matrices have rows and columns, data frames (tibbles) instead have observations and variables. When displayed in the console or viewer, each row is an observation and each column is a variable. read_csv() assigns the data frame to a class called tibble, a tidyverse structure that slightly alters how data frames behave, such as when they are being created or printed. Tibbles are still data frames, and will work in most functions that require data frame inputs. By default, tibbles only show the first ten rows of as many columns as will fit on screen. We also see the column types. We can create tibbles manually in a nearly identical manner to data frames with tibble(). To convert a tibble to a regular data frame (to print the whole data set, for example), use as.data.frame(). To understand more about the data set, we use the ? operator to pull up the documentation for the data. (You can use ?? to search the Internet for more info) # ?mtcars # #Results are not shown here # # #To access each column # example_data$mpg # #or # example_data[,1] After importing our data, a quick glance at the dataset can often tell us if the data were read in correctly. Use head() and tail() to look at a specified number of rows at the beginning or end of a dataset, respectively. Use View() on a dataset to open a spreadsheet-style view of a dataset. In RStuido, clicking on a dataset in the Environment pane will View() it. We can export our data in a number of formats, including text, Excel .xlsx, and in other statistical software formats like Stata .dta, using write_functions that reverse the operations of the read_functions. Multiple objects can be stored in an R binary file (usally extension “.Rdata”) with save() and then later loaded with load(). I did not specify realistic path names below. Excel .csv file: write_csv(dat_csv, file = \"path/to/save/filename.csv\") Stata .dta file: write_dta(dat_csv, file = \"path/to/save/filename.dta\") save these objects to an .Rdata file: save(dat_csv, mydata, file=\"path/to/save/filename.Rdata\") One last thing: if you want to save the entire workspace, save.image() is just a short-cut for ‘save my current workspace’, i.e., save(list = ls(all.names = TRUE), file = \".RData\", envir = .GlobalEnv). It is also what happens with q(\"yes\"). 35.1.4 Subsetting Data Frames Subsetting data frames can work much like subsetting matrices using square brackets, [,]. Let’s use another data given in the ggplot2 library. library(ggplot2) head(mpg, n = 10) ## # A tibble: 10 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto… f 18 29 p comp… ## 2 audi a4 1.8 1999 4 manu… f 21 29 p comp… ## 3 audi a4 2 2008 4 manu… f 20 31 p comp… ## 4 audi a4 2 2008 4 auto… f 21 30 p comp… ## 5 audi a4 2.8 1999 6 auto… f 16 26 p comp… ## 6 audi a4 2.8 1999 6 manu… f 18 26 p comp… ## 7 audi a4 3.1 2008 6 auto… f 18 27 p comp… ## 8 audi a4 quattro 1.8 1999 4 manu… 4 18 26 p comp… ## 9 audi a4 quattro 1.8 1999 4 auto… 4 16 25 p comp… ## 10 audi a4 quattro 2 2008 4 manu… 4 20 28 p comp… mpg[mpg$hwy &gt; 35, c(&quot;manufacturer&quot;, &quot;model&quot;, &quot;year&quot;)] ## # A tibble: 6 × 3 ## manufacturer model year ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 honda civic 2008 ## 2 honda civic 2008 ## 3 toyota corolla 2008 ## 4 volkswagen jetta 1999 ## 5 volkswagen new beetle 1999 ## 6 volkswagen new beetle 1999 An alternative would be to use the subset() function, which has a much more readable syntax. subset(mpg, subset = hwy &gt; 35, select = c(&quot;manufacturer&quot;, &quot;model&quot;, &quot;year&quot;)) ## # A tibble: 6 × 3 ## manufacturer model year ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 honda civic 2008 ## 2 honda civic 2008 ## 3 toyota corolla 2008 ## 4 volkswagen jetta 1999 ## 5 volkswagen new beetle 1999 ## 6 volkswagen new beetle 1999 Lastly, we could use the filter and select functions from the dplyr package which introduces the %&gt;% operator from the magrittr package. This is not necessary for this book, however the dplyr package is something you should be aware of as it is becoming a popular tool in the R world. # library(dplyr) # mpg %&gt;% filter(hwy &gt; 35) %&gt;% select(manufacturer, model, year) 35.1.5 Plotting from data frame There are many good ways and packages for plotting. I’ll show you one here. Visualizing the relationship between multiple variables can get messy very quickly. Here is the ggpairs() function in the GGally package (Tay_2019?). library(fueleconomy) #install.packages(&quot;fueleconomy&quot;) data(vehicles) df &lt;- vehicles[1:100, ] str(df) ## tibble [100 × 12] (S3: tbl_df/tbl/data.frame) ## $ id : num [1:100] 13309 13310 13311 14038 14039 ... ## $ make : chr [1:100] &quot;Acura&quot; &quot;Acura&quot; &quot;Acura&quot; &quot;Acura&quot; ... ## $ model: chr [1:100] &quot;2.2CL/3.0CL&quot; &quot;2.2CL/3.0CL&quot; &quot;2.2CL/3.0CL&quot; &quot;2.3CL/3.0CL&quot; ... ## $ year : num [1:100] 1997 1997 1997 1998 1998 ... ## $ class: chr [1:100] &quot;Subcompact Cars&quot; &quot;Subcompact Cars&quot; &quot;Subcompact Cars&quot; &quot;Subcompact Cars&quot; ... ## $ trans: chr [1:100] &quot;Automatic 4-spd&quot; &quot;Manual 5-spd&quot; &quot;Automatic 4-spd&quot; &quot;Automatic 4-spd&quot; ... ## $ drive: chr [1:100] &quot;Front-Wheel Drive&quot; &quot;Front-Wheel Drive&quot; &quot;Front-Wheel Drive&quot; &quot;Front-Wheel Drive&quot; ... ## $ cyl : num [1:100] 4 4 6 4 4 6 4 4 6 5 ... ## $ displ: num [1:100] 2.2 2.2 3 2.3 2.3 3 2.3 2.3 3 2.5 ... ## $ fuel : chr [1:100] &quot;Regular&quot; &quot;Regular&quot; &quot;Regular&quot; &quot;Regular&quot; ... ## $ hwy : num [1:100] 26 28 26 27 29 26 27 29 26 23 ... ## $ cty : num [1:100] 20 22 18 19 21 17 20 21 17 18 ... Let’s see how GGally::ggpairs() visualizes relationships between quantitative variables: library(GGally) #install.packages(&quot;GGally&quot;) new_df &lt;- df[, c(&quot;cyl&quot;, &quot;hwy&quot;, &quot;cty&quot;)] ggpairs(new_df) The visualization changes a little when we have a mix of quantitative and categorical variables. Below, fuel is a categorical variable while hwy is a quantitative variable. mixed_df &lt;- df[, c(&quot;fuel&quot;, &quot;hwy&quot;)] ggpairs(mixed_df) 35.1.6 Some useful functions my_data &lt;- data.frame(a=1:10, b=rnorm(10, 2, 5)) # Cut a continuous variable into intervals with new integer my_data$c &lt;- cut(my_data$b, 3) str(my_data) ## &#39;data.frame&#39;: 10 obs. of 3 variables: ## $ a: int 1 2 3 4 5 6 7 8 9 10 ## $ b: num 3.21 4.7 8.15 -4.07 5.92 ... ## $ c: Factor w/ 3 levels &quot;(-6.75,-0.881]&quot;,..: 2 2 3 1 3 1 2 1 3 1 # Standardizes variable (subtracts mean and divides by standard deviation) my_data$d &lt;- scale(my_data$a) str(my_data) ## &#39;data.frame&#39;: 10 obs. of 4 variables: ## $ a: int 1 2 3 4 5 6 7 8 9 10 ## $ b: num 3.21 4.7 8.15 -4.07 5.92 ... ## $ c: Factor w/ 3 levels &quot;(-6.75,-0.881]&quot;,..: 2 2 3 1 3 1 2 1 3 1 ## $ d: num [1:10, 1] -1.486 -1.156 -0.826 -0.495 -0.165 ... ## ..- attr(*, &quot;scaled:center&quot;)= num 5.5 ## ..- attr(*, &quot;scaled:scale&quot;)= num 3.03 # lag, lead and cumulative sum a variable library(dplyr) my_data$f &lt;- lag(my_data$d, n=1L) my_data$g &lt;- lead(my_data$d, n=1L) my_data$h &lt;- cumsum(my_data$d) my_data ## a b c d f g h ## 1 1 3.207734 (-0.881,4.97] -1.4863011 NA -1.1560120 -1.486301 ## 2 2 4.700946 (-0.881,4.97] -1.1560120 -1.4863011 -0.8257228 -2.642313 ## 3 3 8.154757 (4.97,10.8] -0.8257228 -1.1560120 -0.4954337 -3.468036 ## 4 4 -4.067326 (-6.75,-0.881] -0.4954337 -0.8257228 -0.1651446 -3.963470 ## 5 5 5.922703 (4.97,10.8] -0.1651446 -0.4954337 0.1651446 -4.128614 ## 6 6 -6.727742 (-6.75,-0.881] 0.1651446 -0.1651446 0.4954337 -3.963470 ## 7 7 0.660900 (-0.881,4.97] 0.4954337 0.1651446 0.8257228 -3.468036 ## 8 8 -3.014230 (-6.75,-0.881] 0.8257228 0.4954337 1.1560120 -2.642313 ## 9 9 10.811752 (4.97,10.8] 1.1560120 0.8257228 1.4863011 -1.486301 ## 10 10 -1.575263 (-6.75,-0.881] 1.4863011 1.1560120 NA 0.000000 35.1.7 Categorical Variables in Data Frames Let’s create a very simple data frame. b &lt;- data.frame(gender=c(&quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;), numbers=c(1,2,3, 0.5, 11, -1)) b ## gender numbers ## 1 male 1.0 ## 2 female 2.0 ## 3 female 3.0 ## 4 female 0.5 ## 5 female 11.0 ## 6 male -1.0 dim(b) ## [1] 6 2 str(b) ## &#39;data.frame&#39;: 6 obs. of 2 variables: ## $ gender : chr &quot;male&quot; &quot;female&quot; &quot;female&quot; &quot;female&quot; ... ## $ numbers: num 1 2 3 0.5 11 -1 Why does say “levels” in the output? Because by default, character vectors are converted to factors in data.frame(). If you want to convert them to back to characters. table(b$gender) # Distribution of gender ## ## female male ## 4 2 # back to character b$gender &lt;- as.character(b$gender) str(b) ## &#39;data.frame&#39;: 6 obs. of 2 variables: ## $ gender : chr &quot;male&quot; &quot;female&quot; &quot;female&quot; &quot;female&quot; ... ## $ numbers: num 1 2 3 0.5 11 -1 # back to factor b$gender &lt;- as.factor(b$gender) str(b) ## &#39;data.frame&#39;: 6 obs. of 2 variables: ## $ gender : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 1 1 1 2 ## $ numbers: num 1 2 3 0.5 11 -1 35.2 Programming Basics In this section we see three main applications: conditional flows, loops, and functions, that are main pillars of any type of programming. 35.2.1 if/Else The main syntax is as follows if (condition) { some R code } else { more R code } Here is a simple example: x &lt;- c(&quot;what&quot;,&quot;is&quot;,&quot;truth&quot;) if(&quot;Truth&quot; %in% x) { print(&quot;Truth is found&quot;) } else { print(&quot;Truth is not found&quot;) } ## [1] &quot;Truth is not found&quot; How about this: x &lt;- c(1, 4, 4) a &lt;- 3 #Here is a nice if/Else if(length(x[x==a])&gt;0) { print(paste(&quot;x has&quot;, length(x[x==a]), a)) } else { print(paste(&quot;x doesn&#39;t have any&quot;, a)) } ## [1] &quot;x doesn&#39;t have any 3&quot; #Another one with pipping a &lt;- 4 if(a %in% x) { print(paste(&quot;x has&quot;, length(x[x==a]), a)) } else { print(paste(&quot;x doesn&#39;t have any&quot;, a)) } ## [1] &quot;x has 2 4&quot; Nested conditions: #Change the numbers to see all conditions x &lt;- 0 y &lt;- 4 if (x == 0 &amp; y!= 0) { print(&quot;a number cannot be divided by zero&quot;) } else if (x == 0 &amp; y == 0) { print(&quot;a zero cannot be divided by zero&quot;) } else { a &lt;- y/x print(paste(&quot;y/x = &quot;, a)) } ## [1] &quot;a number cannot be divided by zero&quot; A simpler, one-line ifelse! #Change the numbers x &lt;- 0 y &lt;- 4 ifelse (x &gt; y, &quot;x is bigger than y&quot;, &quot;y is bigger than x&quot;) ## [1] &quot;y is bigger than x&quot; #more, because the ifelse will fail if x = y. Try it! ifelse (x == y, &quot;x is the same as y&quot;, ifelse(x &gt; y, &quot;x is bigger than y&quot;, &quot;y is bigger than x&quot;)) ## [1] &quot;y is bigger than x&quot; A simpler, without else! z &lt;- 0 w &lt;- 4 if(z &gt; w) print(&quot;w is bigger than z&quot;) #Change the numbers x &lt;- 5 y &lt;- 3 if(x &gt; y) print(&quot;x is bigger than y&quot;) ## [1] &quot;x is bigger than y&quot; #See that both of them moves to the next line. Building multiple conditions without else (it’s a silly example!): z &lt;- 0 w &lt;- 4 x &lt;- 5 y &lt;- 3 if(z &gt; w) print(&quot;z is bigger than w&quot;) if(w &gt; z) print(&quot;w is bigger than z&quot;) ## [1] &quot;w is bigger than z&quot; if(x &gt; y) print(&quot;x is bigger than y&quot;) ## [1] &quot;x is bigger than y&quot; if(y &gt; x) print(&quot;y is bigger than x&quot;) if(z &gt; x) print(&quot;z is bigger than x&quot;) if(x &gt; z) print(&quot;x is bigger than z&quot;) ## [1] &quot;x is bigger than z&quot; if(w &gt; y) print(&quot;w is bigger than y&quot;) ## [1] &quot;w is bigger than y&quot; if(y &gt; w) print(&quot;y is bigger than w&quot;) #Try it with if-else. The ifelse() function only allows for one “if” statement, two cases. You could add nested “if” statements, but that’s just a pain, especially if the 3+ conditions you want to use are all on the same level, conceptually. Is there a way to specify multiple conditions at the same time? dplyr() is the most powerful library for data management. When you have time, read more about it! Source: https://stats.idre.ucla.edu/stat/data/intro_r/intro_r.html#(35) #Let&#39;s create a data frame: df &lt;- data.frame(&quot;name&quot;=c(&quot;Kaija&quot;, &quot;Ella&quot;, &quot;Andis&quot;), &quot;test1&quot; = c(FALSE, TRUE, TRUE), &quot;test2&quot; = c(FALSE, FALSE, TRUE)) df ## name test1 test2 ## 1 Kaija FALSE FALSE ## 2 Ella TRUE FALSE ## 3 Andis TRUE TRUE # Suppose we want separate the people into three groups: #People who passed both tests: Group A #People who passed one test: Group B #People who passed neither test: Group C #dplyr has a function for exactly this purpose: case_when(). library(dplyr) df &lt;- df %&gt;% mutate(group = case_when(test1 &amp; test2 ~ &quot;A&quot;, # both tests: group A xor(test1, test2) ~ &quot;B&quot;, # one test: group B !test1 &amp; !test2 ~ &quot;C&quot; # neither test: group C )) df ## name test1 test2 group ## 1 Kaija FALSE FALSE C ## 2 Ella TRUE FALSE B ## 3 Andis TRUE TRUE A 35.2.2 Loops What would you do if you needed to execute a block of code multiple times? In general, statements are executed sequentially. A loop statement allows us to execute a statement or group of statements multiple times and the following is the general form of a loop statement in most programming languages. There are 3 main loop types: while(), for(), repeat(). Here are some examples for for() loop: x &lt;- c(3, -1, 4, 2, 10, 5) for (i in 1:length(x)) { x[i] &lt;- x[i] * 2 } x ## [1] 6 -2 8 4 20 10 Note that this just for an example. If we want to multiply each element of a vector by 2, a loop isn’t the best way. Although it is very normal in many programming languages, we would simply use a vectorized operation in R. x &lt;- c(3, -1, 4, 2, 10, 5) x &lt;- x * 2 x ## [1] 6 -2 8 4 20 10 But some times it would be very handy: # If the element in x is not zero, multiply it with the subsequent element x &lt;- c(3, -1, 4, 2, 10, 5) for (i in 1:(length(x)-1)) { ifelse(x[i] &gt; 0, x[i] &lt;- x[i] * x[i + 1], x[i] &lt;- 0) } x ## [1] -3 0 8 20 50 5 Here are some examples for while() loop: # Let&#39;s use our first example x &lt;- 3 cnt &lt;- 1 while (cnt &lt; 11) { x = x * 2 cnt = cnt + 1 } x ## [1] 3072 Here are some examples for repeat() loop: # Let&#39;s use our first example x &lt;- 3 cnt &lt;- 1 repeat { x = x * 2 cnt = cnt + 1 if(cnt &gt; 10) break } x ## [1] 3072 35.2.3 The apply() family The apply() family is one of the R base packages and is populated with functions to manipulate slices of data from matrices, arrays, lists and data frames in a repetitive way. These functions allow crossing the data in a number of ways and avoid explicit use of loop constructs. They act on an input list, matrix or array and apply a named function with one or several optional arguments. The family is made up of the apply(), lapply() , sapply(), vapply(), mapply(), rapply(), and tapply() functions. 35.2.3.1 apply() The R base manual tells you that it’s called as follows: apply(X, MARGIN, FUN, ...), where, X is an array or a matrix if the dimension of the array is 2; MARGIN is a variable defining how the function is applied: when MARGIN=1, it applies over rows, whereas with MARGIN=2, it works over columns. Note that when you use the construct MARGIN=c(1,2), it applies to both rows and columns; and FUN, which is the function that you want to apply to the data. It can be any R function, including a User Defined Function (UDF). # Construct a 5x6 matrix X &lt;- matrix(rnorm(30), nrow=5, ncol=6) # Sum the values of each column with `apply()` apply(X, 2, sum) ## [1] -1.6528570 -5.0955632 -0.2748814 0.7576102 3.6357951 1.4207988 apply(X, 2, length) ## [1] 5 5 5 5 5 5 apply(X, 1, length) ## [1] 6 6 6 6 6 apply(X, 2, function (x) length(x)-1) ## [1] 4 4 4 4 4 4 #If you don’t want to write a function inside of the arguments len &lt;- function(x){ length(x)-1 } apply(X,2, len) ## [1] 4 4 4 4 4 4 #It can also be used to repeat a function on cells within a matrix X_new &lt;- apply(X, 1:2, function(x) x+3) X_new ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 3.330239 2.6694940 2.099657 2.992762 3.736205 3.883987 ## [2,] 1.545708 2.7098516 3.272923 2.778570 4.706624 3.951384 ## [3,] 4.313855 0.6029971 3.130178 3.081559 3.573729 3.441011 ## [4,] 1.350984 1.9613254 3.764653 3.089278 3.234427 2.486582 ## [5,] 2.806356 1.9607687 2.457708 3.815441 3.384810 2.657834 Since apply() is used only for matrices, if you apply apply() to a data frame, it first coerces your data.frame to an array which means all the columns must have the same type. Depending on your context, this could have unintended consequences. For a safer practice in data frames, we can use lappy() and sapply(): 35.2.3.2 lapply() You want to apply a given function to every element of a list and obtain a list as a result. When you execute ?lapply, you see that the syntax looks like the apply() function. The difference is that it can be used for other objects like data frames, lists or vectors. And the output returned is a list (which explains the “l” in the function name), which has the same number of elements as the object passed to it. lapply() function does not need MARGIN. A&lt;-c(1:9) B&lt;-c(1:12) C&lt;-c(1:15) my.lst&lt;-list(A,B,C) lapply(my.lst, sum) ## [[1]] ## [1] 45 ## ## [[2]] ## [1] 78 ## ## [[3]] ## [1] 120 35.2.3.3 sapply() sapply works just like lapply, but will simplify the output if possible. This means that instead of returning a list like lapply, it will return a vector instead if the data is simplifiable. A&lt;-c(1:9) B&lt;-c(1:12) C&lt;-c(1:15) my.lst&lt;-list(A,B,C) sapply(my.lst, sum) ## [1] 45 78 120 35.2.3.4 tapply() Sometimes you may want to perform the apply function on some data, but have it separated by factor. In that case, you should use tapply. Let’s take a look at the information for tapply. X &lt;- matrix(c(1:10, 11:20, 21:30), nrow = 10, ncol = 3) tdata &lt;- as.data.frame(cbind(c(1,1,1,1,1,2,2,2,2,2), X)) colnames(tdata) ## [1] &quot;V1&quot; &quot;V2&quot; &quot;V3&quot; &quot;V4&quot; tapply(tdata$V2, tdata$V1, mean) ## 1 2 ## 3 8 What we have here is an important tool: We have a conditional mean of column 2 (V2) with respect to column 1 (V1). You can use tapply to do some quick summary statistics on a variable split by condition. summary &lt;- tapply(tdata$V2, tdata$V1, function(x) c(mean(x), sd(x))) summary ## $`1` ## [1] 3.000000 1.581139 ## ## $`2` ## [1] 8.000000 1.581139 35.2.3.5 mapply() mapply() would be used to create a new variable. For example, using dataset tdata, we could divide one column by another column to create a new value. This would be useful for creating a ratio of two variables as shown in the example below. tdata$V5 &lt;- mapply(function(x, y) x/y, tdata$V2, tdata$V4) tdata$V5 ## [1] 0.04761905 0.09090909 0.13043478 0.16666667 0.20000000 0.23076923 ## [7] 0.25925926 0.28571429 0.31034483 0.33333333 35.2.4 Functions An R function is created by using the keyword function. The basic syntax of an R function definition is as follows: To use a function, you simply type its name, followed by an open parenthesis, then specify values of its arguments, then finish with a closing parenthesis. An argument is a variable which is used in the body of the function. Specifying the values of the arguments is essentially providing the inputs to the function. Let’s write our first function: first &lt;- function(a){ b &lt;- a ^ 2 return(b) } first(1675) ## [1] 2805625 Let’s have a function that find the z-score (standardization). That’s subtracting the sample mean, and dividing by the sample standard deviation. \\[ \\frac{x-\\overline{x}}{s} \\] z_score &lt;- function(x){ m &lt;- mean(x) std &lt;- sd(x) z &lt;- (x - m)/std z } set.seed(1) x &lt;- rnorm(10, 3, 30) z &lt;- z_score(x) z ## [1] -0.97190653 0.06589991 -1.23987805 1.87433300 0.25276523 -1.22045645 ## [7] 0.45507643 0.77649606 0.56826358 -0.56059319 #to check it out round(mean(z), 4) ## [1] 0 sd(z) ## [1] 1 We can do a better job in writing z_score() z_score &lt;- function(x){ z &lt;- (x - mean(x))/sd(x) } set.seed(1) x &lt;- rnorm(10, 3, 30) z &lt;- z_score(x) z ## [1] -0.97190653 0.06589991 -1.23987805 1.87433300 0.25276523 -1.22045645 ## [7] 0.45507643 0.77649606 0.56826358 -0.56059319 Lets create a function that prints the factorials: fact &lt;- function(a){ b &lt;- 1 for (i in 1:(a-1)) { b &lt;- b*(i+1) } b } fact(5) ## [1] 120 Creating loops is an act of art and requires very careful thinking. The same loop can be done by many different structures. And it always takes more time to understand somebody else’s loop than your own! 35.2.4.1 outer() outer() takes two vectors and a function (that itself takes two arguments) and builds a matrix by calling the given function for each combination of the elements in the two vectors. x &lt;- c(0, 1, 2) y &lt;- c(0, 1, 2, 3, 4) m &lt;- outer ( y, # First dimension: the columns (y) x, # Second dimension: the rows (x) function (x, y) x+2*y ) m ## [,1] [,2] [,3] ## [1,] 0 2 4 ## [2,] 1 3 5 ## [3,] 2 4 6 ## [4,] 3 5 7 ## [5,] 4 6 8 In place of the function, an operator can be given, which makes it easy to create a matrix with simple calculations (such as multiplying): m &lt;- outer(c(10, 20, 30, 40), c(2, 4, 6), &quot;*&quot;) m ## [,1] [,2] [,3] ## [1,] 20 40 60 ## [2,] 40 80 120 ## [3,] 60 120 180 ## [4,] 80 160 240 It becomes very handy when we build a polynomial model: x &lt;- sample(0:20, 10, replace = TRUE) x ## [1] 8 14 20 4 8 13 4 4 1 9 m &lt;- outer(x, 1:4, &quot;^&quot;) m ## [,1] [,2] [,3] [,4] ## [1,] 8 64 512 4096 ## [2,] 14 196 2744 38416 ## [3,] 20 400 8000 160000 ## [4,] 4 16 64 256 ## [5,] 8 64 512 4096 ## [6,] 13 169 2197 28561 ## [7,] 4 16 64 256 ## [8,] 4 16 64 256 ## [9,] 1 1 1 1 ## [10,] 9 81 729 6561 35.2.5 dplyr() Taking the time to prepare your data before analysis can save you frustration and time spent cleaning up errors. The package dplyr contains several easy-to-use data management functions that we will learn to use to manage our data. "],["r-lab-3---preparing-the-data.html", "Chapter 36 R Lab 3 - Preparing the data 36.1 Preparing the data for a regression analysis with lm() 36.2 “DUMMY” variable models", " Chapter 36 R Lab 3 - Preparing the data 36.1 Preparing the data for a regression analysis with lm() In chapter 2 we estimated a simple linear regression by writing a set of simple R commands to perform the necessary calculations. But we can use the lm() command, instead. We will use three datasets in this lab. Let’s start with the first one, vehicles. library(fueleconomy) #install.packages(&quot;fueleconomy&quot;) data(vehicles) df &lt;- as.data.frame(vehicles) head(df) ## id make model year class trans ## 1 13309 Acura 2.2CL/3.0CL 1997 Subcompact Cars Automatic 4-spd ## 2 13310 Acura 2.2CL/3.0CL 1997 Subcompact Cars Manual 5-spd ## 3 13311 Acura 2.2CL/3.0CL 1997 Subcompact Cars Automatic 4-spd ## 4 14038 Acura 2.3CL/3.0CL 1998 Subcompact Cars Automatic 4-spd ## 5 14039 Acura 2.3CL/3.0CL 1998 Subcompact Cars Manual 5-spd ## 6 14040 Acura 2.3CL/3.0CL 1998 Subcompact Cars Automatic 4-spd ## drive cyl displ fuel hwy cty ## 1 Front-Wheel Drive 4 2.2 Regular 26 20 ## 2 Front-Wheel Drive 4 2.2 Regular 28 22 ## 3 Front-Wheel Drive 6 3.0 Regular 26 18 ## 4 Front-Wheel Drive 4 2.3 Regular 27 19 ## 5 Front-Wheel Drive 4 2.3 Regular 29 21 ## 6 Front-Wheel Drive 6 3.0 Regular 26 17 str(df) ## &#39;data.frame&#39;: 33442 obs. of 12 variables: ## $ id : num 13309 13310 13311 14038 14039 ... ## $ make : chr &quot;Acura&quot; &quot;Acura&quot; &quot;Acura&quot; &quot;Acura&quot; ... ## $ model: chr &quot;2.2CL/3.0CL&quot; &quot;2.2CL/3.0CL&quot; &quot;2.2CL/3.0CL&quot; &quot;2.3CL/3.0CL&quot; ... ## $ year : num 1997 1997 1997 1998 1998 ... ## $ class: chr &quot;Subcompact Cars&quot; &quot;Subcompact Cars&quot; &quot;Subcompact Cars&quot; &quot;Subcompact Cars&quot; ... ## $ trans: chr &quot;Automatic 4-spd&quot; &quot;Manual 5-spd&quot; &quot;Automatic 4-spd&quot; &quot;Automatic 4-spd&quot; ... ## $ drive: chr &quot;Front-Wheel Drive&quot; &quot;Front-Wheel Drive&quot; &quot;Front-Wheel Drive&quot; &quot;Front-Wheel Drive&quot; ... ## $ cyl : num 4 4 6 4 4 6 4 4 6 5 ... ## $ displ: num 2.2 2.2 3 2.3 2.3 3 2.3 2.3 3 2.5 ... ## $ fuel : chr &quot;Regular&quot; &quot;Regular&quot; &quot;Regular&quot; &quot;Regular&quot; ... ## $ hwy : num 26 28 26 27 29 26 27 29 26 23 ... ## $ cty : num 20 22 18 19 21 17 20 21 17 18 ... 36.1.1 Factor variables If we want to estimate a model, we need to check the structure of the model variables in the data. For example, you can see that some of the variables are “characters”, such as make. It is always a good practice to transform them into factor variables also known as indicator, categorical, or dummy variables. You can still use “character” variables in lm() or use them in descriptive analyses, but we will lose many features. There are many different ways to convert one type of vector to another type, here are two simple ways: #First way: df &lt;- as.data.frame(vehicles) for (i in 1:ncol(df)) { if(is.character(df[,i])) df[,i] &lt;- as.factor(df[,i]) } #2nd way: df &lt;- as.data.frame(vehicles) colms &lt;- sapply(df, is.character) df[colms] &lt;- lapply(df[colms], as.factor) str(df) ## &#39;data.frame&#39;: 33442 obs. of 12 variables: ## $ id : num 13309 13310 13311 14038 14039 ... ## $ make : Factor w/ 128 levels &quot;Acura&quot;,&quot;Alfa Romeo&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ model: Factor w/ 3198 levels &quot;1-Ton Truck 2WD&quot;,..: 28 28 28 29 29 29 29 29 29 30 ... ## $ year : num 1997 1997 1997 1998 1998 ... ## $ class: Factor w/ 34 levels &quot;Compact Cars&quot;,..: 29 29 29 29 29 29 29 29 29 1 ... ## $ trans: Factor w/ 47 levels &quot;Auto (AV-S6)&quot;,..: 33 44 33 33 44 33 33 44 33 33 ... ## $ drive: Factor w/ 7 levels &quot;2-Wheel Drive&quot;,..: 5 5 5 5 5 5 5 5 5 5 ... ## $ cyl : num 4 4 6 4 4 6 4 4 6 5 ... ## $ displ: num 2.2 2.2 3 2.3 2.3 3 2.3 2.3 3 2.5 ... ## $ fuel : Factor w/ 13 levels &quot;CNG&quot;,&quot;Diesel&quot;,..: 12 12 12 12 12 12 12 12 12 8 ... ## $ hwy : num 26 28 26 27 29 26 27 29 26 23 ... ## $ cty : num 20 22 18 19 21 17 20 21 17 18 ... #We won&#39;t learn the advance use of &#39;apply&#39; family in this text. #But this is a helpful line. apply() works only with matrices. #sapply() and lapply() are for lists. But a data frame is also a list #Therefore df[2] instead of df[, 2] can work very well. #How about df[1:10, 2] vs df[[2]][1:10]. Same! You can also numeric or integer types for indicator variables. Again the good practice is to convert them to factor variables. 36.1.2 Dummy Coding Let’s look at drive in our data. In dummy coding, you will always have a contrast matrix with one less column than levels of the original variable. In our example, our categorical variable has 7 levels so we will have contrast matrices with 6 columns and 7 rows. tapply(df$hwy, df$drive, mean) # Mean hwy MPG for each drive type. ## 2-Wheel Drive 4-Wheel Drive ## 20.22091 22.14592 ## 4-Wheel or All-Wheel Drive All-Wheel Drive ## 19.62193 24.75927 ## Front-Wheel Drive Part-time 4-Wheel Drive ## 28.32944 18.72917 ## Rear-Wheel Drive ## 20.98899 #This is also a nice function. Try this: #This is similar to &quot;egen&quot; in Stata. tapply(df$hwy, df$drive, function(x) c(mean(x), sd(x))) ## $`2-Wheel Drive` ## [1] 20.220907 7.524566 ## ## $`4-Wheel Drive` ## [1] 22.145923 3.742637 ## ## $`4-Wheel or All-Wheel Drive` ## [1] 19.62193 3.68364 ## ## $`All-Wheel Drive` ## [1] 24.759274 4.046489 ## ## $`Front-Wheel Drive` ## [1] 28.329437 5.536714 ## ## $`Part-time 4-Wheel Drive` ## [1] 18.729167 2.069728 ## ## $`Rear-Wheel Drive` ## [1] 20.988994 4.949314 Dummy coding is a very commonly used coding scheme. It compares each level of the categorical variable to a fixed reference level. For example, we can choose drive = 1 as the reference group and compare the mean of each level of drive to the reference level of 1. This is the default for disordered factors in R. #assigning the treatment contrasts to drive contrasts(df$drive) ## 4-Wheel Drive 4-Wheel or All-Wheel Drive ## 2-Wheel Drive 0 0 ## 4-Wheel Drive 1 0 ## 4-Wheel or All-Wheel Drive 0 1 ## All-Wheel Drive 0 0 ## Front-Wheel Drive 0 0 ## Part-time 4-Wheel Drive 0 0 ## Rear-Wheel Drive 0 0 ## All-Wheel Drive Front-Wheel Drive ## 2-Wheel Drive 0 0 ## 4-Wheel Drive 0 0 ## 4-Wheel or All-Wheel Drive 0 0 ## All-Wheel Drive 1 0 ## Front-Wheel Drive 0 1 ## Part-time 4-Wheel Drive 0 0 ## Rear-Wheel Drive 0 0 ## Part-time 4-Wheel Drive Rear-Wheel Drive ## 2-Wheel Drive 0 0 ## 4-Wheel Drive 0 0 ## 4-Wheel or All-Wheel Drive 0 0 ## All-Wheel Drive 0 0 ## Front-Wheel Drive 0 0 ## Part-time 4-Wheel Drive 1 0 ## Rear-Wheel Drive 0 1 contrasts(df$drive) &lt;- contr.treatment(7, base=4) #Changing the base contrasts(df$drive) ## 1 2 3 5 6 7 ## 2-Wheel Drive 1 0 0 0 0 0 ## 4-Wheel Drive 0 1 0 0 0 0 ## 4-Wheel or All-Wheel Drive 0 0 1 0 0 0 ## All-Wheel Drive 0 0 0 0 0 0 ## Front-Wheel Drive 0 0 0 1 0 0 ## Part-time 4-Wheel Drive 0 0 0 0 1 0 ## Rear-Wheel Drive 0 0 0 0 0 1 We can make good tables as well: summary(df$drive) ## 2-Wheel Drive 4-Wheel Drive ## 507 699 ## 4-Wheel or All-Wheel Drive All-Wheel Drive ## 6647 1267 ## Front-Wheel Drive Part-time 4-Wheel Drive ## 12233 96 ## Rear-Wheel Drive ## 11993 table(df$fuel, df$drive) ## ## 2-Wheel Drive 4-Wheel Drive ## CNG 0 0 ## Diesel 72 21 ## Electricity 14 0 ## Gasoline or E85 0 93 ## Gasoline or natural gas 0 0 ## Gasoline or propane 0 0 ## Midgrade 0 12 ## Premium 1 223 ## Premium and Electricity 0 0 ## Premium Gas or Electricity 0 0 ## Premium or E85 0 18 ## Regular 420 332 ## Regular Gas and Electricity 0 0 ## ## 4-Wheel or All-Wheel Drive All-Wheel Drive ## CNG 2 0 ## Diesel 225 30 ## Electricity 0 0 ## Gasoline or E85 147 97 ## Gasoline or natural gas 4 0 ## Gasoline or propane 4 0 ## Midgrade 0 6 ## Premium 1480 679 ## Premium and Electricity 0 0 ## Premium Gas or Electricity 0 1 ## Premium or E85 1 32 ## Regular 4784 422 ## Regular Gas and Electricity 0 0 ## ## Front-Wheel Drive Part-time 4-Wheel Drive ## CNG 23 0 ## Diesel 200 0 ## Electricity 25 0 ## Gasoline or E85 238 28 ## Gasoline or natural gas 5 0 ## Gasoline or propane 0 0 ## Midgrade 0 0 ## Premium 2088 8 ## Premium and Electricity 0 0 ## Premium Gas or Electricity 5 0 ## Premium or E85 0 0 ## Regular 9641 60 ## Regular Gas and Electricity 8 0 ## ## Rear-Wheel Drive ## CNG 33 ## Diesel 326 ## Electricity 16 ## Gasoline or E85 440 ## Gasoline or natural gas 9 ## Gasoline or propane 4 ## Midgrade 25 ## Premium 4138 ## Premium and Electricity 1 ## Premium Gas or Electricity 1 ## Premium or E85 37 ## Regular 6963 ## Regular Gas and Electricity 0 36.1.3 Column (Variable) names Before any type of data analysis, we need to take care of several things. One of these is that we usually do not use the whole data, but a subset of the data. For example, you may want to remove some observations or keep only some types. And most importantly we need to take care of missing values. We will look at these now. First, look at the column (variable) names. Do they have generic names (that is, \\(x_1\\), $x_2$, etc)? Or do the names have typo problems, or are too long/short? In our vehicles data the names seem fine. Let’s use another data, Autompg. library(tidyverse) autompg &lt;- read_csv(&quot;auto-mpg.csv&quot;, show_col_types = FALSE) #autompg = read.table(&quot;http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data&quot;, #quote = &quot;\\&quot;&quot;, comment.char = &quot;&quot;, stringsAsFactors = FALSE) colnames(autompg) ## [1] &quot;mpg&quot; &quot;cyl&quot; &quot;displ&quot; &quot;hp&quot; &quot;weight&quot; &quot;accel&quot; &quot;yr&quot; &quot;origin&quot; ## [9] &quot;name&quot; str(autompg) ## spc_tbl_ [392 × 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ mpg : num [1:392] 18 15 18 16 17 15 14 14 14 15 ... ## $ cyl : num [1:392] 8 8 8 8 8 8 8 8 8 8 ... ## $ displ : num [1:392] 307 350 318 304 302 429 454 440 455 390 ... ## $ hp : num [1:392] 130 165 150 150 140 198 220 215 225 190 ... ## $ weight: num [1:392] 3504 3693 3436 3433 3449 ... ## $ accel : num [1:392] 12 11.5 11 12 10.5 10 9 8.5 10 8.5 ... ## $ yr : num [1:392] 70 70 70 70 70 70 70 70 70 70 ... ## $ origin: num [1:392] 1 1 1 1 1 1 1 1 1 1 ... ## $ name : chr [1:392] &quot;chevrolet chevelle malibu&quot; &quot;buick skylark 320&quot; &quot;plymouth satellite&quot; &quot;amc rebel sst&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. mpg = col_double(), ## .. cyl = col_double(), ## .. displ = col_double(), ## .. hp = col_double(), ## .. weight = col_double(), ## .. accel = col_double(), ## .. yr = col_double(), ## .. origin = col_double(), ## .. name = col_character() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; #I don&#39;t like them! How about this: colnames(autompg) = c(&quot;mpg&quot;, &quot;cyl&quot;, &quot;disp&quot;, &quot;hp&quot;, &quot;wt&quot;, &quot;acc&quot;, &quot;year&quot;, &quot;origin&quot;, &quot;name&quot;) colnames(autompg) ## [1] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; &quot;wt&quot; &quot;acc&quot; &quot;year&quot; &quot;origin&quot; ## [9] &quot;name&quot; str(autompg) ## spc_tbl_ [392 × 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ mpg : num [1:392] 18 15 18 16 17 15 14 14 14 15 ... ## $ cyl : num [1:392] 8 8 8 8 8 8 8 8 8 8 ... ## $ disp : num [1:392] 307 350 318 304 302 429 454 440 455 390 ... ## $ hp : num [1:392] 130 165 150 150 140 198 220 215 225 190 ... ## $ wt : num [1:392] 3504 3693 3436 3433 3449 ... ## $ acc : num [1:392] 12 11.5 11 12 10.5 10 9 8.5 10 8.5 ... ## $ year : num [1:392] 70 70 70 70 70 70 70 70 70 70 ... ## $ origin: num [1:392] 1 1 1 1 1 1 1 1 1 1 ... ## $ name : chr [1:392] &quot;chevrolet chevelle malibu&quot; &quot;buick skylark 320&quot; &quot;plymouth satellite&quot; &quot;amc rebel sst&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. mpg = col_double(), ## .. cyl = col_double(), ## .. displ = col_double(), ## .. hp = col_double(), ## .. weight = col_double(), ## .. accel = col_double(), ## .. yr = col_double(), ## .. origin = col_double(), ## .. name = col_character() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; 36.1.4 Data subsetting and missing values OK, they are fine now. Let’s see if they have any missing value. Missing values are defined by NA in R. We’ll see later NaN as well. any(is.na(autompg)) ## [1] FALSE #But we have to be careful. HP is a character vector. Why? Perhaps it contains a character? which(autompg$hp == &quot;?&quot;) ## integer(0) #Pay attention to subset(). This will be a time-saver subset_hp &lt;- subset(autompg, autompg$hp != &quot;?&quot;) dim(subset_hp) ## [1] 392 9 dim(autompg) ## [1] 392 9 #Those 6 observations are dropped and the new data frame is &quot;subset_hp&quot; #You can drop columns (variables) as well autompg_less = subset(autompg, select = c(&quot;mpg&quot;, &quot;cyl&quot;, &quot;disp&quot;, &quot;hp&quot;, &quot;wt&quot;, &quot;acc&quot;, &quot;year&quot;)) dim(autompg_less) ## [1] 392 7 #Traditional way to do those subselections subset_hp &lt;- autompg[autompg$hp != &quot;?&quot;,] dim(subset_hp) ## [1] 392 9 dim(autompg) ## [1] 392 9 #And autompg_less = autompg[, c(1:7)] dim(autompg_less) ## [1] 392 7 Look at the help(subset): “This is a convenience function intended for use interactively. For programming it is better to use the standard subsetting functions like [], and in particular the non-standard evaluation of argument subset can have unanticipated consequences”. 36.2 “DUMMY” variable models We can use our dataset, df, which is a cleaned version of vehicles, to try out a “dummy” variable model. #Remember we had this: tapply(df$hwy, df$drive, mean) ## 2-Wheel Drive 4-Wheel Drive ## 20.22091 22.14592 ## 4-Wheel or All-Wheel Drive All-Wheel Drive ## 19.62193 24.75927 ## Front-Wheel Drive Part-time 4-Wheel Drive ## 28.32944 18.72917 ## Rear-Wheel Drive ## 20.98899 contrasts(df$drive) &lt;- contr.treatment(7, base=1) #Setting the base back to 1 Lets try to make a regression on highway fuel economy based upon our drive dummy variable. model_nocons &lt;- lm(hwy ~ drive + 0, data = df) # &quot;0&quot; means no constant in lm() summary(model_nocons) ## ## Call: ## lm(formula = hwy ~ drive + 0, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.329 -3.146 -0.329 2.671 80.671 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## drive2-Wheel Drive 20.22091 0.21973 92.03 &lt;2e-16 *** ## drive4-Wheel Drive 22.14592 0.18714 118.34 &lt;2e-16 *** ## drive4-Wheel or All-Wheel Drive 19.62193 0.06069 323.34 &lt;2e-16 *** ## driveAll-Wheel Drive 24.75927 0.13900 178.12 &lt;2e-16 *** ## driveFront-Wheel Drive 28.32944 0.04473 633.29 &lt;2e-16 *** ## drivePart-time 4-Wheel Drive 18.72917 0.50497 37.09 &lt;2e-16 *** ## driveRear-Wheel Drive 20.98899 0.04518 464.57 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.948 on 33435 degrees of freedom ## Multiple R-squared: 0.9587, Adjusted R-squared: 0.9587 ## F-statistic: 1.11e+05 on 7 and 33435 DF, p-value: &lt; 2.2e-16 We can add in an intercept now: model_withcons &lt;- lm(hwy ~ drive, data = df) summary(model_withcons) ## ## Call: ## lm(formula = hwy ~ drive, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.329 -3.146 -0.329 2.671 80.671 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 20.2209 0.2197 92.025 &lt; 2e-16 *** ## drive2 1.9250 0.2886 6.670 2.6e-11 *** ## drive3 -0.5990 0.2280 -2.628 0.008605 ** ## drive4 4.5384 0.2600 17.455 &lt; 2e-16 *** ## drive5 8.1085 0.2242 36.160 &lt; 2e-16 *** ## drive6 -1.4917 0.5507 -2.709 0.006757 ** ## drive7 0.7681 0.2243 3.424 0.000618 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.948 on 33435 degrees of freedom ## Multiple R-squared: 0.3656, Adjusted R-squared: 0.3655 ## F-statistic: 3212 on 6 and 33435 DF, p-value: &lt; 2.2e-16 You can see that the intercept is the first category, 2WDrive. The rest of the coefficient are the difference of each drive type from 2wdrive. Let’s change the base to 4: contrasts(df$drive) &lt;- contr.treatment(7, base=4) #Changing the base model_withcons &lt;- lm(hwy ~ drive, data = df) summary(model_withcons) ## ## Call: ## lm(formula = hwy ~ drive, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.329 -3.146 -0.329 2.671 80.671 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 24.7593 0.1390 178.12 &lt;2e-16 *** ## drive1 -4.5384 0.2600 -17.45 &lt;2e-16 *** ## drive2 -2.6134 0.2331 -11.21 &lt;2e-16 *** ## drive3 -5.1373 0.1517 -33.87 &lt;2e-16 *** ## drive5 3.5702 0.1460 24.45 &lt;2e-16 *** ## drive6 -6.0301 0.5238 -11.51 &lt;2e-16 *** ## drive7 -3.7703 0.1462 -25.80 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.948 on 33435 degrees of freedom ## Multiple R-squared: 0.3656, Adjusted R-squared: 0.3655 ## F-statistic: 3212 on 6 and 33435 DF, p-value: &lt; 2.2e-16 36.2.1 mtcars example Now it’s time to estimate a model that defines mpg (fuel efficiency, in miles per gallon), as a function of hp (horsepower - in foot-pounds per second), and am (transmission, Automatic or Manual). #First, let&#39;s plot it (play with parameter in plot() to see the difference) plot(mpg ~ hp, data = mtcars, cex = 2, col=&quot;darkgrey&quot;) let’s start with a simple linear regression where \\(Y\\) is mpg and \\(x_{1}\\) is hp. This is our first and simplest lm() application. For the sake of simplicity, let’s drop the index \\(i\\) for observations. \\[ Y=\\beta_{0}+\\beta_{1} x_{1}+\\epsilon \\] model1 &lt;- lm(mpg ~ hp, data = mtcars) model1 ## ## Call: ## lm(formula = mpg ~ hp, data = mtcars) ## ## Coefficients: ## (Intercept) hp ## 30.09886 -0.06823 plot(mpg ~ hp, data = mtcars, col = am + 1, cex = 2) abline(model1, lwd = 3, col = &quot;green&quot;) As you see the red, manual, observations are mostly above the line, while the black, automatic, observations are mostly below the line. This means not only our model underestimates (overestimates) the fuel efficiency of manual (automatic) transmissions, but also the effect of hp on mpg will be biased. This is because OLS tries to minimize the MSE for all observation not for manual and automatic transmissions separately. To correct for this, we add a predictor to our model, namely, am as \\(x_{2}\\), as follows: \\[ Y=\\beta_{0}+\\beta_{1} x_{1}+\\beta_{2} x_{2}+\\epsilon \\] where \\(x_{1}\\) and \\(Y\\) remain the same, but now: \\[ x_{2}=\\left\\{\\begin{array}{ll}{1} &amp; {\\text { manual transmission }} \\\\ {0} &amp; {\\text { automatic transmission }}\\end{array}\\right. \\] We call \\(x_{2}\\) as a “dummy” variable, which is a numerical variable that is used in a regression analysis to “code” for a binary categorical variable. Note that am is already a dummy variable, since it uses the values 0 and 1 to represent automatic and manual transmissions. As we have seen earlier, often, a variable like am would be stored as a character vector. Converting them to factor variables will take care of creating dummy variables. model2 &lt;- lm(mpg ~ hp + am, data = mtcars) model2 ## ## Call: ## lm(formula = mpg ~ hp + am, data = mtcars) ## ## Coefficients: ## (Intercept) hp am ## 26.58491 -0.05889 5.27709 Note the difference in \\(\\hat{\\beta}_{1}\\). Since \\(x_{2}\\) can only take values 0 and 1, we can write two different models, one for manual and one for automatic transmissions. For automatic transmissions, that is \\(x_{2}\\) = 0, we have, \\[ Y=\\beta_{0}+\\beta_{1} x_{1}+\\epsilon \\] Then for manual transmissions, that is \\(x_2 = 1\\), we have, \\[ Y=\\left(\\beta_{0}+\\beta_{2}\\right)+\\beta_{1} x_{1}+\\epsilon \\] Here is our interpretations: \\(\\hat{\\beta}_{0} = 26.5849137\\) is the estimated average mpg for a car with an automatic transmission and 0 hp. \\(\\hat{\\beta}_{0} + \\hat{\\beta}_{2} = 31.8619991\\) is the estimated average mpg for a car with a manual transmission and 0 hp. \\(\\hat{\\beta}_{2} = 5.2770853\\) is the estimated difference in average mpg for cars with manual transmissions as compared to those with automatic transmission, for any hp. \\(\\hat{\\beta}_{1} = −0.0588878\\) is the estimated change in average mpg for an increase in one hp, for either transmission types. To show them on a plot, we can combine the coefficients from Model2 to calculate the estimated slope and intercepts, as we already described above int_auto = coef(model2)[1] int_manu = coef(model2)[1] + coef(model2)[3] slope = coef(model2)[2] #And re-plot them plot(mpg ~ hp, data = mtcars, col = am + 1, cex = 2) abline(int_auto, slope, col = 1, lty = 1, lwd = 2) # add line for auto abline(int_manu, slope, col = 2, lty = 2, lwd = 2) # add line for manual legend(&quot;topright&quot;, c(&quot;Automatic&quot;, &quot;Manual&quot;), col = c(1, 2), pch = c(1, 1)) The above picture makes it clear that \\(\\beta_{2}\\) is significant, which you can verify mathematically with a hypothesis test. In the model, \\[ Y=\\beta_{0}+\\beta_{1} x_{1}+\\beta_{2} x_{2}+\\epsilon \\] We see that the effect of hp (\\(x_{1}\\)) is the same irrespective of whether the car is manual or automatic. This is captured by \\(\\beta_{1}\\) which is the average change in \\(Y\\) for an increase in \\(x_{1}\\), no matter the value of \\(x_{2}\\). Although \\(\\beta_{2}\\) captures the difference in the average of Y for manual cars (remember \\(x_{2} = 1\\) for manuals), we do not know if the effect of hp would be different for manual cars. This is a restriction that we may not want to have and might venture a more flexible model. To remove the “same slope” restriction, we will now discuss interaction. Essentially, we would like a model that allows for two different slopes one for each transmission type. Consider the following model, \\[ Y=\\beta_{0}+\\beta_{1} x_{1}+\\beta_{2} x_{2}+\\beta_{3} x_{1} x_{2}+\\epsilon \\] where \\(x_{1}\\), \\(x_{2}\\), and \\(Y\\) are the same as before, but we have added a new interaction term \\(x_{1}x_{2}\\) which is the product of \\(x_{1}\\) and \\(x_{2}\\). So its effect on mpg is captured by the additional parameter \\(\\beta_{3}\\). This model estimates differences in two slopes and two intercepts. Let’s see this mathematically: For manual cars, that is \\(x_{2}\\) = 0, we have: \\[ Y=\\beta_{0}+\\beta_{1} x_{1}+\\epsilon \\] For automatic cars, that is \\(x_{2}\\) = 1, we have \\[ Y=\\left(\\beta_{0}+\\beta_{2}\\right)+\\left(\\beta_{1}+\\beta_{3}\\right) x_{1}+\\epsilon \\] These two models have both different slopes and intercepts. \\(\\beta_{0}\\) is the average mpg for a manual car with 0 hp. \\(\\beta_{1}\\) is the change in average mpg for an increase of one hp, for manual cars. \\(\\beta_{0} + \\beta_{2}\\) is the average mpg for a automatic car with 0 hp. \\(\\beta_{1} + \\beta_{3}\\) is the change in average mpg for an increase of one hp, for manual cars. How do we fit this model in R? There are a number of ways. 36.2.2 model.matrix() # These 2 are the same models model1 = lm(mpg ~ hp + am + hp:am, data = mtcars) model2 = lm(mpg ~ hp + am + hp*am, data = mtcars) #Important note: even our am is a &quot;numerical&quot; variable, 0 and 1 are #indicators, not numbers. So converting them to a factor variable is the #proper way to handle categorical variables. The reason is simple. When you have a #large dataset with many X variables, some of the indicator variables #are going to be &quot;numeric&quot; not &quot;character&quot;. For example, you amy have a #variable with 10 categories identified with numbers from 1 to 10. #R will take it as a continuous variable. If you convert it to factor variable #everything will be easy. LET&quot;S SEE: mtcars$am &lt;- as.factor(mtcars$am) str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 1 1 1 1 1 1 1 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... #Now we can use a better way to build a model, specially for larger datasets: X &lt;- model.matrix(~ hp + am + hp:am, data = mtcars) head(X) ## (Intercept) hp am1 hp:am1 ## Mazda RX4 1 110 1 110 ## Mazda RX4 Wag 1 110 1 110 ## Datsun 710 1 93 1 93 ## Hornet 4 Drive 1 110 0 0 ## Hornet Sportabout 1 175 0 0 ## Valiant 1 105 0 0 #Or, even better X &lt;- model.matrix(~ hp*am, data = mtcars) head(X) ## (Intercept) hp am1 hp:am1 ## Mazda RX4 1 110 1 110 ## Mazda RX4 Wag 1 110 1 110 ## Datsun 710 1 93 1 93 ## Hornet 4 Drive 1 110 0 0 ## Hornet Sportabout 1 175 0 0 ## Valiant 1 105 0 0 #Here the &quot;base&quot; for am is 0. We can make it without the intercept #REMEMBER the &quot;DUMMY TRAP&quot; X &lt;- model.matrix(~ hp*am + 0, data = mtcars) head(X) ## hp am0 am1 hp:am1 ## Mazda RX4 110 0 1 110 ## Mazda RX4 Wag 110 0 1 110 ## Datsun 710 93 0 1 93 ## Hornet 4 Drive 110 1 0 0 ## Hornet Sportabout 175 1 0 0 ## Valiant 105 1 0 0 #How about changing the base for am to 1 (0, manual is the base in ma, remember)? #The level which is chosen for the reference level is the level which is contrasted against. #By default, this is simply the first level alphabetically. #We can specify that we want to be the reference level by using the relevel function: table(mtcars$am) ## ## 0 1 ## 19 13 levels(mtcars$am) ## [1] &quot;0&quot; &quot;1&quot; str(mtcars$am) ## Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 1 1 1 1 1 1 1 ... levels(mtcars$am) &lt;- c(&quot;Manual&quot;, &quot;Auto&quot;) table(mtcars$am) ## ## Manual Auto ## 19 13 levels(mtcars$am) ## [1] &quot;Manual&quot; &quot;Auto&quot; mtcars$am &lt;- relevel(mtcars$am, &quot;Auto&quot;) str(mtcars$am) ## Factor w/ 2 levels &quot;Auto&quot;,&quot;Manual&quot;: 1 1 1 2 2 2 2 2 2 2 ... X &lt;- model.matrix(~ hp*am, data = mtcars) head(X) ## (Intercept) hp amManual hp:amManual ## Mazda RX4 1 110 0 0 ## Mazda RX4 Wag 1 110 0 0 ## Datsun 710 1 93 0 0 ## Hornet 4 Drive 1 110 1 110 ## Hornet Sportabout 1 175 1 175 ## Valiant 1 105 1 105 #More on this https://hopstat.wordpress.com/2014/06/26/be-careful-with-using-model-design-in-r/ #and https://genomicsclass.github.io/book/pages/expressing_design_formula.html #Final Note: if use model.matrix() for lm() you have to be careful about the X1, which is 1 Y &lt;- mtcars$mpg model3 &lt;- lm(Y ~ X) summary(model3) #Pay attention to F, R-squared etc) ## ## Call: ## lm(formula = Y ~ X) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.3818 -2.2696 0.1344 1.7058 5.8752 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 31.8425012 1.5288820 20.827 &lt; 2e-16 *** ## X(Intercept) NA NA NA NA ## Xhp -0.0587341 0.0101671 -5.777 3.34e-06 *** ## XamManual -5.2176534 2.6650931 -1.958 0.0603 . ## Xhp:amManual -0.0004029 0.0164602 -0.024 0.9806 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.961 on 28 degrees of freedom ## Multiple R-squared: 0.782, Adjusted R-squared: 0.7587 ## F-statistic: 33.49 on 3 and 28 DF, p-value: 2.112e-09 #Becasue X has also have 1, lm() drops one of them model4 &lt;- lm(Y ~ X - 1) summary(model4) #Pay attention to F, R-squared etc) ## ## Call: ## lm(formula = Y ~ X - 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.3818 -2.2696 0.1344 1.7058 5.8752 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## X(Intercept) 31.8425012 1.5288820 20.827 &lt; 2e-16 *** ## Xhp -0.0587341 0.0101671 -5.777 3.34e-06 *** ## XamManual -5.2176534 2.6650931 -1.958 0.0603 . ## Xhp:amManual -0.0004029 0.0164602 -0.024 0.9806 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.961 on 28 degrees of freedom ## Multiple R-squared: 0.9825, Adjusted R-squared: 0.98 ## F-statistic: 393.5 on 4 and 28 DF, p-value: &lt; 2.2e-16 # So model4 is the correct one. 1 should be removed from lm() # If you remove 1 from the &quot;design&quot; matrix X &lt;- model.matrix(~ hp*am - 1, data = mtcars) #Remove 1 from the &quot;design&quot; matrix model5 &lt;- lm(Y ~ X) summary(model5) ## ## Call: ## lm(formula = Y ~ X) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.3818 -2.2696 0.1344 1.7058 5.8752 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 26.6248479 2.1829432 12.197 1.01e-12 *** ## Xhp -0.0587341 0.0101671 -5.777 3.34e-06 *** ## XamAuto 5.2176534 2.6650931 1.958 0.0603 . ## XamManual NA NA NA NA ## Xhp:amManual -0.0004029 0.0164602 -0.024 0.9806 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.961 on 28 degrees of freedom ## Multiple R-squared: 0.782, Adjusted R-squared: 0.7587 ## F-statistic: 33.49 on 3 and 28 DF, p-value: 2.112e-09 #It doesn&#39;t work! In concluding this section, we can see the effect of hp on mpg is almost identical for both manual and auto. #First let&#39;s use a new model with the original levels in am str(mtcars$am) ## Factor w/ 2 levels &quot;Auto&quot;,&quot;Manual&quot;: 1 1 1 2 2 2 2 2 2 2 ... mtcars$am &lt;- relevel(mtcars$am, &quot;Manual&quot;) str(mtcars$am) ## Factor w/ 2 levels &quot;Manual&quot;,&quot;Auto&quot;: 2 2 2 1 1 1 1 1 1 1 ... Y &lt;- mtcars$mpg X &lt;- model.matrix(~ hp*am, data = mtcars) head(X) ## (Intercept) hp amAuto hp:amAuto ## Mazda RX4 1 110 1 110 ## Mazda RX4 Wag 1 110 1 110 ## Datsun 710 1 93 1 93 ## Hornet 4 Drive 1 110 0 0 ## Hornet Sportabout 1 175 0 0 ## Valiant 1 105 0 0 model &lt;- lm(Y ~ X - 1) summary(model) ## ## Call: ## lm(formula = Y ~ X - 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.3818 -2.2696 0.1344 1.7058 5.8752 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## X(Intercept) 26.6248479 2.1829432 12.197 1.01e-12 *** ## Xhp -0.0591370 0.0129449 -4.568 9.02e-05 *** ## XamAuto 5.2176534 2.6650931 1.958 0.0603 . ## Xhp:amAuto 0.0004029 0.0164602 0.024 0.9806 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.961 on 28 degrees of freedom ## Multiple R-squared: 0.9825, Adjusted R-squared: 0.98 ## F-statistic: 393.5 on 4 and 28 DF, p-value: &lt; 2.2e-16 36.2.3 Example with a bigger data set: Autompg Our results could be different in a larger and more realistic dataset such as Autompg, that we downloaded and cleaned earlier. Lets give it a go: # remove the plymouth reliant, as it causes some issues autompg = subset(autompg, autompg$name != &quot;plymouth reliant&quot;) #Change horsepower from character to numeric #We should have converted it to a factor variable #But for the sake of this example, we keep it as numeric. #And we manually create a dummary variable for #foreign vs domestic cars: domestic = 1. autompg$hp = as.numeric(autompg$hp) autompg$domestic = as.numeric(autompg$origin == 1) #Remove 3 and 5 cylinder cars (which are very rare.) autompg = autompg[autompg$cyl != 5,] autompg = autompg[autompg$cyl != 3,] #Change cyl to a factor variable autompg$cyl = as.factor(autompg$cyl) str(autompg) ## tibble [383 × 10] (S3: tbl_df/tbl/data.frame) ## $ mpg : num [1:383] 18 15 18 16 17 15 14 14 14 15 ... ## $ cyl : Factor w/ 3 levels &quot;4&quot;,&quot;6&quot;,&quot;8&quot;: 3 3 3 3 3 3 3 3 3 3 ... ## $ disp : num [1:383] 307 350 318 304 302 429 454 440 455 390 ... ## $ hp : num [1:383] 130 165 150 150 140 198 220 215 225 190 ... ## $ wt : num [1:383] 3504 3693 3436 3433 3449 ... ## $ acc : num [1:383] 12 11.5 11 12 10.5 10 9 8.5 10 8.5 ... ## $ year : num [1:383] 70 70 70 70 70 70 70 70 70 70 ... ## $ origin : num [1:383] 1 1 1 1 1 1 1 1 1 1 ... ## $ name : chr [1:383] &quot;chevrolet chevelle malibu&quot; &quot;buick skylark 320&quot; &quot;plymouth satellite&quot; &quot;amc rebel sst&quot; ... ## $ domestic: num [1:383] 1 1 1 1 1 1 1 1 1 1 ... We’ll now be concerned with three variables: mpg, disp, and domestic. We will use mpg as the response. We can fit a model, \\[ Y=\\beta_{0}+\\beta_{1} disp+\\beta_{2} domestic+\\epsilon \\] where,\\(Y\\) is mpg, the fuel efficiency in miles per gallon, disp is the displacement in cubic inches,and domestic as described below, which is a dummy variable. \\[ domestic=\\left\\{\\begin{array}{ll}{1} &amp; {\\text { Domestic }} \\\\ {0} &amp; {\\text { Foreign }}\\end{array}\\right. \\] We will fit this model, extract the slope and intercept for the “two lines,” plot the data and add the lines. #lm() model1 = lm(mpg ~ disp + domestic, data = autompg) #Extracting slope and intercept coefficents int_for = coef(model1)[1] int_dom = coef(model1)[1] + coef(model1)[3] slope_for = coef(model1)[2] slope_dom = coef(model1)[2] #Plot plot(mpg ~ disp, data = autompg, col = domestic + 1, pch = domestic + 1) abline(int_for, slope_for, col = 1, lty = 1, lwd = 2) # add line for foreign cars abline(int_dom, slope_dom, col = 2, lty = 2, lwd = 2) # add line for domestic cars legend(&quot;topright&quot;, c(&quot;Foreign&quot;, &quot;Domestic&quot;), pch = c(1, 2), col = c(1, 2)) This is a model that allows for two parallel lines, meaning the mpg can be different on average between foreign and domestic cars of the same engine displacement, but the change in average mpg for an increase in displacement is the same for both. We can see this model isn’t doing very well here. The red line fits the red points fairly well, but the black line isn’t doing very well for the black points, it should clearly have a more negative slope. Essentially, we would like a model that allows for two different slopes. Consider the following model, \\[ Y=\\beta_{0}+\\beta_{1} disp+\\beta_{2} domestic+\\beta_{3} disp*domestic+\\epsilon \\] Now we have added a new interaction term \\(disp*domestic\\), as we described earlier. model2 = lm(mpg ~ disp * domestic, data = autompg) summary(model2) ## ## Call: ## lm(formula = mpg ~ disp * domestic, data = autompg) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.8332 -2.8956 -0.8332 2.2828 18.7749 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 46.05484 1.80582 25.504 &lt; 2e-16 *** ## disp -0.15692 0.01668 -9.407 &lt; 2e-16 *** ## domestic -12.57547 1.95644 -6.428 3.90e-10 *** ## disp:domestic 0.10252 0.01692 6.060 3.29e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.308 on 379 degrees of freedom ## Multiple R-squared: 0.7011, Adjusted R-squared: 0.6987 ## F-statistic: 296.3 on 3 and 379 DF, p-value: &lt; 2.2e-16 #Extracting slope and intercept coefficents int_for = coef(model2)[1] int_dom = coef(model2)[1] + coef(model2)[3] slope_for = coef(model2)[2] slope_dom = coef(model2)[2] + coef(model2)[4] #Plot plot(mpg ~ disp, data = autompg, col = domestic + 1, pch = domestic + 1) abline(int_for, slope_for, col = 1, lty = 1, lwd = 2) abline(int_dom, slope_dom, col = 2, lty = 2, lwd = 2) legend(&quot;topright&quot;, c(&quot;Foreign&quot;, &quot;Domestic&quot;), pch = c(1, 2), col = c(1, 2)) We see that these lines fit the data much better 36.2.4 Some more data management tools for subsetting: complete.cases(), is.na(), and within() Let’s try out some new tools using our first dataset: vehicles. str(vehicles) ## tibble [33,442 × 12] (S3: tbl_df/tbl/data.frame) ## $ id : num [1:33442] 13309 13310 13311 14038 14039 ... ## $ make : chr [1:33442] &quot;Acura&quot; &quot;Acura&quot; &quot;Acura&quot; &quot;Acura&quot; ... ## $ model: chr [1:33442] &quot;2.2CL/3.0CL&quot; &quot;2.2CL/3.0CL&quot; &quot;2.2CL/3.0CL&quot; &quot;2.3CL/3.0CL&quot; ... ## $ year : num [1:33442] 1997 1997 1997 1998 1998 ... ## $ class: chr [1:33442] &quot;Subcompact Cars&quot; &quot;Subcompact Cars&quot; &quot;Subcompact Cars&quot; &quot;Subcompact Cars&quot; ... ## $ trans: chr [1:33442] &quot;Automatic 4-spd&quot; &quot;Manual 5-spd&quot; &quot;Automatic 4-spd&quot; &quot;Automatic 4-spd&quot; ... ## $ drive: chr [1:33442] &quot;Front-Wheel Drive&quot; &quot;Front-Wheel Drive&quot; &quot;Front-Wheel Drive&quot; &quot;Front-Wheel Drive&quot; ... ## $ cyl : num [1:33442] 4 4 6 4 4 6 4 4 6 5 ... ## $ displ: num [1:33442] 2.2 2.2 3 2.3 2.3 3 2.3 2.3 3 2.5 ... ## $ fuel : chr [1:33442] &quot;Regular&quot; &quot;Regular&quot; &quot;Regular&quot; &quot;Regular&quot; ... ## $ hwy : num [1:33442] 26 28 26 27 29 26 27 29 26 23 ... ## $ cty : num [1:33442] 20 22 18 19 21 17 20 21 17 18 ... #First, let&#39;s check if there is any NA in the data head(is.na(vehicles)) # you can see from here what is.na() does. So: ## id make model year class trans drive cyl displ fuel hwy cty ## [1,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [2,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [3,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [4,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [5,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [6,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE index &lt;- which(rowSums(is.na(vehicles))&gt;0) #Dropping observations with NA and assigning it toa new dataset, &quot;data&quot; data &lt;- vehicles[-index, ] #We can also use complete.cases to identify row index with NA index &lt;- which(!complete.cases(vehicles)) index ## [1] 1232 1233 2347 3246 3247 3248 6115 6116 6533 7783 7784 8472 ## [13] 10613 10614 11696 11697 11728 12411 12412 12413 12928 12929 12934 12935 ## [25] 12944 13669 16429 16430 21070 23472 23473 23474 24485 24486 24487 24488 ## [37] 24489 26150 28628 28704 28705 28706 28707 28708 28709 29314 29315 30023 ## [49] 30024 30025 30026 30027 30028 31063 31064 31065 31066 31067 31068 31069 #Much easier option. Let&#39;s used here &quot;df&quot;, since we cleaned it earlier dim(df) ## [1] 33442 12 data &lt;- df[complete.cases(df), ] dim(data) ## [1] 33382 12 Our “cleaned” data is now ready. We would like make hwy an indicator variable. Let’s name the new variable mpg and if hyw &gt; 23, mpg = 1 and 0 otherwise. Let’s see how we can do it? #1st way: mpg &lt;- c(rep(0, nrow(data))) #Create vector mpg data2 &lt;- cbind(data, mpg) # add it to data data2 &lt;- within(data2, mpg[hwy &gt; 23] &lt;- 1) #You can add more conditions here with &amp; #2nd way rm(data2) mpg &lt;- c(rep(0, nrow(data))) #Create vector mpg data2 &lt;- cbind(data, mpg) # add it to data data2$mpg[data2$hwy &gt; 23] &lt;- 1 #3nd way data$mpg[data$hwy &gt; 23] &lt;- 1 data$mpg[is.na(data$mpg)] &lt;- 0 str(data) ## &#39;data.frame&#39;: 33382 obs. of 13 variables: ## $ id : num 13309 13310 13311 14038 14039 ... ## $ make : Factor w/ 128 levels &quot;Acura&quot;,&quot;Alfa Romeo&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ model: Factor w/ 3198 levels &quot;1-Ton Truck 2WD&quot;,..: 28 28 28 29 29 29 29 29 29 30 ... ## $ year : num 1997 1997 1997 1998 1998 ... ## $ class: Factor w/ 34 levels &quot;Compact Cars&quot;,..: 29 29 29 29 29 29 29 29 29 1 ... ## $ trans: Factor w/ 47 levels &quot;Auto (AV-S6)&quot;,..: 33 44 33 33 44 33 33 44 33 33 ... ## $ drive: Factor w/ 7 levels &quot;2-Wheel Drive&quot;,..: 5 5 5 5 5 5 5 5 5 5 ... ## ..- attr(*, &quot;contrasts&quot;)= num [1:7, 1:6] 1 0 0 0 0 0 0 0 1 0 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:7] &quot;2-Wheel Drive&quot; &quot;4-Wheel Drive&quot; &quot;4-Wheel or All-Wheel Drive&quot; &quot;All-Wheel Drive&quot; ... ## .. .. ..$ : chr [1:6] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;5&quot; ... ## $ cyl : num 4 4 6 4 4 6 4 4 6 5 ... ## $ displ: num 2.2 2.2 3 2.3 2.3 3 2.3 2.3 3 2.5 ... ## $ fuel : Factor w/ 13 levels &quot;CNG&quot;,&quot;Diesel&quot;,..: 12 12 12 12 12 12 12 12 12 8 ... ## $ hwy : num 26 28 26 27 29 26 27 29 26 23 ... ## $ cty : num 20 22 18 19 21 17 20 21 17 18 ... ## $ mpg : num 1 1 1 1 1 1 1 1 1 0 ... #Now with(): This is more like subset() #Let&#39;s see the mean of hwy for diesel and cyl == 4 mean(with(data, hwy[cyl == 4 &amp; fuel ==&quot;Diesel&quot;])) ## [1] 36.17483 #Ofcourse you can do it with index &lt;- which(data$cyl == 4 &amp; data$fuel ==&quot;Diesel&quot;) mean(data$hwy[index]) ## [1] 36.17483 In our next lab, we will work with simulated data and see how they can give us useful insights when modelling. "],["r-lab-4---simulation-in-r.html", "Chapter 37 R Lab 4 - Simulation in R 37.1 Sampling in R: sample() 37.2 Random number generating with probablity distributions 37.3 Simulation for statistical inference 37.4 Creataing data with a Data Generating Model (DGM) 37.5 Bootstrapping 37.6 Monty Hall - Fun example", " Chapter 37 R Lab 4 - Simulation in R In this lab we will learn how to simulate data and illustrate their use in several examples. More specifically we’ll cover the following subjects: Sampling in R: sample(), Random number generating with probability distributions, Simulation for statistical inference, Creating data with a DGM, Bootstrapping, Power of simulation - A fun example. Why would we want to simulate data? Why not just use real data? Because with real data, we don’t know what the right answer is. Suppose we use real data and we apply a method to extract information, how do we know that we applied the method correctly? Now suppose we create artificial data by simulating a “Data Generating Model”. Since we can know the correct answer, we can check whether or not our methods work to extract the information we wish to have. If our method is correct, than we can apply it to real data. 37.1 Sampling in R: sample() Let’s play with sample() for simple random sampling. We will see the arguments of sample() function. sample(c(&quot;H&quot;,&quot;T&quot;), size = 8, replace = TRUE) # fair coin ## [1] &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; sample(1:6, size = 2, replace = TRUE, prob=c(3,3,3,4,4,4)) ## [1] 6 4 #let&#39;s do it again sample(c(&quot;H&quot;,&quot;T&quot;), size = 8, replace = TRUE) # fair coin ## [1] &quot;T&quot; &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; sample(1:6, size = 2, replace = TRUE, prob=c(3,3,3,4,4,4)) ## [1] 5 6 The results are different. If we use set.seed() then we can get the same results each time. Lets try now: set.seed(123) sample(c(&quot;H&quot;,&quot;T&quot;), size = 8, replace = TRUE) # fair coin ## [1] &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; sample(1:6, size = 2, replace = TRUE, prob=c(3,3,3,4,4,4)) ## [1] 4 4 #let&#39;s do it again set.seed(123) sample(c(&quot;H&quot;,&quot;T&quot;), size = 8, replace = TRUE) # fair coin ## [1] &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; sample(1:6, size = 2, replace = TRUE, prob=c(3,3,3,4,4,4)) ## [1] 4 4 We use replace=TRUE to override the default sample without replacement. This means the same thing can get selected from the population multiple times. And, prob= to sample elements with different probabilities, e.g. over sample based on some factor. The set.seed() function allow you to make a reproducible set of random numbers. Let’s see the difference. x &lt;- 1:12 # a random permutation set.seed(123) sample(x) ## [1] 3 12 10 2 6 11 5 4 9 8 1 7 # This shuffles the numbers set.seed(123) sample(x, replace = TRUE) ## [1] 3 3 10 2 6 11 5 4 6 9 10 11 # This shuffles the numbers and replacing them More… # sample()&#39;s surprise -- example x &lt;- 1:10 sample(x[x &gt; 3]) # length 2 ## [1] 8 6 9 4 7 10 5 sample(x[x &gt; 9]) # oops -- length 10! So this doesn&#39;t work ## [1] 3 8 2 7 9 1 6 10 4 5 sample(x[x &gt; 10]) # length 0 ## integer(0) Here is an example: let’s generate 501 coin flips. In the true model, this should generate heads half of the time, and tails half of the time. set.seed(123) coins &lt;- sample(c(&quot;Heads&quot;,&quot;Tails&quot;), 501, replace = TRUE) Now let’s take that data as given and analyze it in our standard way! The proportion of heads is: mean(coins==&#39;Heads&#39;) ## [1] 0.5209581 barplot(prop.table(table(coins)), col = c(&quot;lightskyblue3&quot;,&quot;mistyrose3&quot;), cex.axis = 0.8, cex.names = 0.8) So what’s our conclusion? We came to the conclusion that the true model generates heads 0.493014 of the time. But it is NOT 0.50 , so pretty close, but not exact. Did this whole thing work or not? What if it always errs on the same side? In other words, what if it’s always bias towards heads in every sample with 501 flips? We will do our first simulation to answer it momentarily. One more useful application: sample(letters, 10, replace = TRUE) ## [1] &quot;p&quot; &quot;z&quot; &quot;o&quot; &quot;s&quot; &quot;c&quot; &quot;n&quot; &quot;a&quot; &quot;x&quot; &quot;a&quot; &quot;p&quot; 37.2 Random number generating with probablity distributions Here are the common probability distributions in R. Search help in R for more detail. beta(shape1, shape2, ncp), binom(size, prob), chisq(df, ncp), exp(rate), gamma(shape, scale), logis(location, scale), norm(mean, sd), pois(lambda), t(df, ncp), unif(min, max), dnorm(x,) returns the density or the value on the y-axis of a probability distribution for a discrete value of x, pnorm(q,) returns the cumulative density function (CDF) or the area under the curve to the left of an x value on a probability distribution curve, qnorm(p,) returns the quantile value, i.e. the standardized z value for x, rnorm(n,) returns a random simulation of size n rnorm(6) # 6 std nrml distribution values ## [1] -0.2645952 -0.9472983 0.7395213 0.8967787 -0.3460009 -1.7820571 rnorm(10, mean = 50, sd = 19) # set parameters ## [1] 58.83389 12.93042 40.19385 59.29253 67.13847 62.16690 68.07297 38.61666 ## [9] 24.71680 38.74801 runif(n = 10, min = 0, max = 1) #uniform distribution ## [1] 0.96415257 0.08146656 0.85436475 0.80223822 0.38517360 0.32759740 ## [7] 0.20493870 0.56938266 0.88805519 0.52971409 rpois(n = 10, lambda = 15) # Poisson distribution ## [1] 15 15 6 17 16 13 15 15 15 21 # toss coin 8 times using binomial distribution rbinom(n = 8, size = 1, p = 0.5) ## [1] 0 0 1 0 1 1 1 0 rbinom(8,1,.5) # args correct order ## [1] 1 0 0 1 1 1 1 0 # 18 trials, sample size 10, prob success =.2 rbinom(18, 10, 0.2) ## [1] 5 1 0 4 2 1 4 0 1 3 1 1 1 3 1 3 1 1 Can we replicate our coin-flip example here with probability distributions? Yes, we can! set.seed(123) coins &lt;- rbinom(n = 501, size = 1, p = 0.5) mean(coins==0) ## [1] 0.5309381 barplot(prop.table(table(coins)), col = c(&quot;lightskyblue3&quot;,&quot;mistyrose3&quot;), cex.axis = 0.8, cex.names = 0.8) 37.3 Simulation for statistical inference Let’s predict number of girls in 400 births, where probability of female birth is 48.8% n.girls &lt;- rbinom(1, 400, 0.488) n.girls ## [1] 201 n.girls/400 ## [1] 0.5025 Now, to get distribution of the simulations, repeat the simulation many times. n.sims &lt;- 1000 n.girls &lt;- rbinom(n.sims, 400, .488) hist(n.girls, col = &quot;slategray3&quot;, cex.axis = 0.75) mean(n.girls)/400 ## [1] 0.4872775 This is called as sampling distribution. Can we do same thing with a loop? n.sims &lt;- 1000 n.girls &lt;- rep(NA, n.sims) # create vector to store simulations for (i in 1:n.sims){ n.girls[i] &lt;- rbinom(1, 400, 0.488) } hist(n.girls, col = &quot;lavender&quot;, cex.axis = 0.75) Let’s apply a similar simulation to our coin flipping. n.sims &lt;- 1000 n.heads &lt;- rep(NA, n.sims) # create vector to store simulations for (i in 1:n.sims){ n.heads[i] &lt;- mean(rbinom(n = 501, size = 1, p = 0.5)) } hist(n.heads, col=&quot;aliceblue&quot;, cex.axis = 0.75) mean(n.heads) ## [1] 0.4997705 Here is another way for the same simulation: n.heads &lt;- replicate(1000, mean(rbinom(n = 501, size = 1, p = 0.5))) hist(n.heads, col=&quot;lightpink&quot;,cex.axis = 0.75) mean(n.heads) ## [1] 0.4987265 What’s the 95% confidence interval for the mean? sd &lt;- sd(n.heads) CI95 &lt;- c(-2*sd+mean(n.heads), 2*sd+mean(n.heads)) CI95 ## [1] 0.4538446 0.5436085 What happens if we use a “wrong” estimator for the mean, like sum(heads)/300? n.sims &lt;- 1000 n.heads &lt;- rep(NA, n.sims) # create vector to store simulations for (i in 1:n.sims){ n.heads[i] &lt;- sum(rbinom(n = 501, size = 1, p = 0.5))/300 } mean(n.heads) ## [1] 0.83496 Because we are working with a simulation, identifying that the result from this incorrect estimator is wrong becomes easy. 37.4 Creataing data with a Data Generating Model (DGM) One of the major tasks of statistics is to obtain information about populations. In most of cases, the population is unknown and the only thing that is known for the researcher is a finite subset of observations drawn from the population. The main aim of the statistical analysis is to obtain information about the population through analysis of the sample. Since very little information is known about the population characteristics, one has to establish some assumptions about the behavior of this unknown population. For example, for a regression analysis, we can state that the whole population regression function (PRF) is a linear function of the different values of \\(X\\). One important issue related to the PRF is the error term (\\(u_i\\)) in the regression equation. For a pair of realizations \\((x_i,y_i)\\) from the random variables \\((X,Y)\\), we can write the following equalities: \\[ y_{i}=E\\left(Y | X=x_{i}\\right)+u_{i}=\\alpha+\\beta x_{i}+u_{i} \\] and \\[ E\\left(u | X=x_{i}\\right)=0 \\] This result implies that for \\(X=x_i\\), the divergences of all values of \\(Y\\) with respect to the conditional expectation \\(E(Y\\vert X=x_i)\\) are averaged out. There are several reasons for the existence of the error term in the regression: (1) the error term is taking into account variables which are not in the model; (2) we do not have great confidence about the correctness of the model; and (3) we do not know if there are measurement errors in the variables. In a regression analysis, the PRF is a Data Generating Model for \\(y_i\\), which is unknown to us. Because it is unknown, we must try to learn about it from a sample since that is the only available data for us. If we assume that there is a specific PRF that generates the data, then given any estimator of \\(\\alpha\\) and \\(\\beta\\), namely \\(\\hat{\\beta}\\) and \\(\\hat{\\alpha}\\), we can estimate them from our sample with the sample regression function (SRF): \\[ \\hat{y}_{i}=\\hat{\\alpha}+\\hat{\\beta} x_{i}, \\quad i=1, \\cdots, n \\] The relationship between the PRF and SRF is: \\[ y_{i}=\\hat{y}_{i}+\\hat{u}_{i}, \\quad i=1, \\cdots, n \\] where \\(\\hat{u_i}\\) is denoted the residuals from SRF. With a data generating process (DGP) at hand, it is possible to create new simulated data. With \\(\\alpha\\), \\(\\beta\\) and the vector of exogenous variables \\(X\\) (fixed), a sample of size \\(n\\) can be used to obtain \\(N\\) values of \\(Y\\) with random variable \\(u\\). This yields one complete population of size \\(N\\). Note that this artificially generated set of data could be viewed as an example of real-world data that a researcher would be faced with when dealing with the kind of estimation problem this model represents. Note especially that the set of data obtained depends crucially on the particular set of error terms drawn. A different set of error terms would create a different data set of \\(Y\\) for the same problem. With the artificial data we generated, DGM is now known and the whole population is accessible. That is, we can test many models on different samples drawn from this population in order to see whether their inferential properties are in line with DGM. We’ll have several examples below. Here is our DGM: \\[ Y_{i}=\\beta_{1}+\\beta_{2} X_{2 i}+\\beta_{3} X_{3 i}+\\beta_{4} X_{2 i} X_{3 i}+\\beta_{5} X_{5 i}, \\] with the following coefficient vector: \\(\\beta = (12, -0.7, 34, -0.17, 5.4)\\). Moreover \\(x_2\\) is binary variable with values of 0 and 1 and \\(x_5\\) and \\(x_3\\) are highly correlated with \\(\\rho = 0.65\\). When we add the error term, \\(u\\), which is independently and identically (i.i.d) distributed with \\(N(0,1)\\), we can get the whole population of 10,000 observations. DGM plus the error term is called the data generating process (DGP) library(MASS) N &lt;- 10000 x_2 &lt;- sample(c(0,1), N, replace = TRUE) #Dummy variable #mvrnorm() creates a matrix of correlated variables X_corr &lt;- mvrnorm(N, mu = c(0,0), Sigma = matrix(c(1,0.65,0.65,1), ncol = 2), empirical = TRUE) #We can check their correlation cor(X_corr) ## [,1] [,2] ## [1,] 1.00 0.65 ## [2,] 0.65 1.00 #Each column is one of our variables x_3 &lt;- X_corr[,1] x_5 &lt;- X_corr[,2] #interaction x_23 &lt;- x_2*x_3 # Now DGM beta &lt;- c(12, -0.7, 34, -0.17, 5.4) dgm &lt;- beta[1] + beta[2]*x_2 + beta[3]*x_3 + beta[4]*x_23 + beta[5]*x_5 #And our Yi y &lt;- dgm + rnorm(N,0,1) pop &lt;- data.frame(y, x_2, x_3, x_23, x_5) str(pop) ## &#39;data.frame&#39;: 10000 obs. of 5 variables: ## $ y : num -37.09 8.41 12.84 44.55 31.87 ... ## $ x_2 : num 0 0 1 1 0 1 1 1 0 0 ... ## $ x_3 : num -1.3163 -0.1002 0.0558 0.7737 0.6297 ... ## $ x_23: num 0 0 0.0558 0.7737 0 ... ## $ x_5 : num -0.6134 -0.0465 -0.0857 1.5022 -0.3612 ... #Here is new thing/trick to learn #for better looking tables install.packages(&quot;stargazer&quot;) library(stargazer) stargazer(pop, type = &quot;text&quot;, title = &quot;Descriptive Statistics&quot;, digits = 1, out = &quot;table1.text&quot;) ## ## Descriptive Statistics ## ============================================ ## Statistic N Mean St. Dev. Min Max ## -------------------------------------------- ## y 10,000 11.7 37.7 -168.8 164.3 ## x_2 10,000 0.5 0.5 0 1 ## x_3 10,000 0.0 1.0 -4.7 3.9 ## x_23 10,000 0.004 0.7 -4.7 3.9 ## x_5 10,000 -0.0 1.0 -4.4 3.8 ## -------------------------------------------- #The table will be saved in the working directory #with whatever name you write in the out option. #You can open this file with any word processor Now we are going to sample this population and run a SRF. library(stargazer) n &lt;- 500 #sample size ind &lt;- sample(nrow(pop), n, replace = FALSE) sample &lt;- pop[ind, ] str(sample) ## &#39;data.frame&#39;: 500 obs. of 5 variables: ## $ y : num 10.7 44.7 -47.1 -30.6 24 ... ## $ x_2 : num 0 1 0 0 1 0 1 1 1 0 ... ## $ x_3 : num -0.111 0.705 -1.62 -1.153 0.358 ... ## $ x_23: num 0 0.705 0 0 0.358 ... ## $ x_5 : num 0.627 1.528 -0.889 -0.722 0.281 ... model &lt;- lm(y ~ ., data = sample) stargazer(model, type = &quot;text&quot;, title = &quot;G O O D - M O D E L&quot;, dep.var.labels = &quot;Y&quot;, digits = 3) ## ## G O O D - M O D E L ## ================================================ ## Dependent variable: ## ---------------------------- ## Y ## ------------------------------------------------ ## x_2 -0.718*** ## (0.087) ## ## x_3 34.025*** ## (0.068) ## ## x_23 -0.162* ## (0.085) ## ## x_5 5.357*** ## (0.058) ## ## Constant 12.075*** ## (0.060) ## ## ------------------------------------------------ ## Observations 500 ## R2 0.999 ## Adjusted R2 0.999 ## Residual Std. Error 0.974 (df = 495) ## F Statistic 196,907.500*** (df = 4; 495) ## ================================================ ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 As you can see the coefficients are very close to our “true” coefficients specified in DGM. Now we can test what happens if we omit \\(x_5\\) in our SRF and estimate it. library(stargazer) n &lt;- 500 #sample size sample &lt;- pop[sample(nrow(pop), n, replace = FALSE), ] str(sample) ## &#39;data.frame&#39;: 500 obs. of 5 variables: ## $ y : num -21.42 35.02 -2.79 -12.15 -10.37 ... ## $ x_2 : num 0 1 1 0 0 0 0 1 0 1 ... ## $ x_3 : num -0.828 0.65 -0.387 -0.907 -0.423 ... ## $ x_23: num 0 0.65 -0.387 0 0 ... ## $ x_5 : num -1.035 0.194 -0.298 1.311 -1.467 ... model_bad &lt;- lm(y ~ x_2 + x_3 + x_23, data = sample) stargazer(model_bad, type = &quot;text&quot;, title = &quot;B A D - M O D E L&quot;, dep.var.labels = &quot;Y&quot;, digits = 3) ## ## B A D - M O D E L ## =============================================== ## Dependent variable: ## --------------------------- ## Y ## ----------------------------------------------- ## x_2 -0.717* ## (0.379) ## ## x_3 37.534*** ## (0.275) ## ## x_23 -0.581 ## (0.406) ## ## Constant 11.861*** ## (0.266) ## ## ----------------------------------------------- ## Observations 500 ## R2 0.986 ## Adjusted R2 0.986 ## Residual Std. Error 4.231 (df = 496) ## F Statistic 11,326.660*** (df = 3; 496) ## =============================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 Now it seems that none of the coefficients are as good as before, except for the intercept. This is a so-called omitted variable bias (OVB) problem, also known as a model underfitting or specification error. Would be the case that that this is a problem for only one sample? We can simulate the results many times and see whether on average \\(\\hat{\\beta_3}\\) is biased or not. n.sims &lt;- 500 n &lt;- 500 #sample size beta_3 &lt;- c(NA, n.sims) for (i in 1:n.sims){ sample &lt;- pop[sample(nrow(pop), n, replace = FALSE), ] model_bad &lt;- lm(y ~ x_2 + x_3 + x_23, data = sample) beta_3[i] &lt;- model_bad$coefficients[&quot;x_3&quot;] } summary(beta_3) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 36.56 37.33 37.49 37.50 37.68 38.19 As we can see the OVB problem is not a problem in one sample. We withdrew a sample and estimated the same underfitting model 500 times with a simulation. Therefore we collected 500 \\(\\hat{\\beta_3}\\). The average is 37.47. If we do the same simulation with a model that is correctly specified, you can see the results: the average of 500 \\(\\hat{\\beta_3}\\) is 34, which is the “correct”true” coefficent in our DGM. n.sims &lt;- 500 n &lt;- 500 #sample size beta_3 &lt;- c(NA, n.sims) for (i in 1:n.sims){ sample &lt;- pop[sample(nrow(pop), n, replace = FALSE), ] model_good &lt;- lm(y ~ x_2 + x_3 + x_23 + x_5, data = sample) beta_3[i] &lt;- model_good$coefficients[&quot;x_3&quot;] } summary(beta_3) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 33.81 33.96 34.02 34.01 34.06 34.23 37.5 Bootstrapping Bootstrapping is the process of resampling with replacement (all values in the sample have an equal probability of being selected, including multiple times, so a value could have duplicates). Resample, calculate a statistic (e.g. the mean), repeat this hundreds or thousands of times and you are able to estimate a precise/accurate uncertainty of the mean (confidence interval) of the data’s distribution. There are less assumptions about the underlying distribution using bootstrap compared to calculating the standard error directly. Generally bootstrapping follows the same basic steps: Resample a given data set a specified number of times, Calculate a specific statistic from each sample, Find the standard deviation of the distribution of that statistic. In the following bootstrapping example we would like to obtain a standard error for the estimate of the mean. We will be using the lapply(), sapply() functions in combination with the sample function. (see https://stats.idre.ucla.edu/r/library/r-library-introduction-to-bootstrapping/)(UCLA_2021?) #creating the data set by taking 100 observations #from a normal distribution with mean 5 and stdev 3 set.seed(123) data &lt;- rnorm(100, 5, 3) #rounding each observation to nearest integer data[1:10] ## [1] 3.318573 4.309468 9.676125 5.211525 5.387863 10.145195 6.382749 ## [8] 1.204816 2.939441 3.663014 #obtaining 20 bootstrap samples and storing in a list resamples &lt;- lapply(1:20, function(i) sample(data, replace = T)) #display the first of the bootstrap samples resamples[1] ## [[1]] ## [1] 8.76144476 3.11628177 4.02220524 10.36073941 6.30554447 9.10580685 ## [7] 2.93597415 3.60003394 3.58162578 6.34462934 5.71619521 7.06592076 ## [13] 4.91435973 4.34607526 7.33989536 4.37624817 5.37156273 6.93312965 ## [19] 8.67224539 4.32268704 1.20481630 1.63067425 4.33854031 5.91058592 ## [25] 4.14568098 1.63067425 11.15025406 -1.92750663 11.50686790 4.11478555 ## [31] 7.06592076 8.62388599 5.33204815 10.36073941 8.29051704 7.68537698 ## [37] 3.85858700 3.85858700 3.66301409 4.02220524 -1.92750663 6.15584120 ## [43] 2.93944144 6.38274862 6.38274862 6.75384125 6.13891845 2.87239771 ## [49] 2.81332631 4.00037785 9.10580685 1.92198666 -0.06007993 7.68537698 ## [55] 0.35374159 1.58558919 3.66301409 4.87138863 9.10580685 4.14568098 ## [61] 8.67224539 3.12488220 4.91435973 -0.06007993 6.38274862 3.12488220 ## [67] 8.29051704 8.44642286 4.11478555 6.93312965 2.81332631 0.35374159 ## [73] 8.01721557 1.92198666 5.33204815 10.14519496 7.98051157 3.31857306 ## [79] 8.44642286 6.75384125 5.01729256 1.58558919 -0.06007993 7.51336113 ## [85] 4.33854031 6.38274862 5.64782471 -0.06007993 2.91587906 6.93312965 ## [91] 10.14519496 3.11628177 6.27939266 5.71619521 6.49355143 1.94427385 ## [97] 6.66175296 0.35374159 3.58162578 7.46474324 Here is another way to do the same thing: set.seed(123) data &lt;- rnorm(100, 5, 3) resamples_2 &lt;- matrix(NA, nrow = 100, ncol = 20) for (i in 1:20) { resamples_2[,i] &lt;- sample(data, 100, replace = TRUE) } str(resamples_2) ## num [1:100, 1:20] 8.76 3.12 4.02 10.36 6.31 ... #display the first of the bootstrap samples resamples_2[, 1] ## [1] 8.76144476 3.11628177 4.02220524 10.36073941 6.30554447 9.10580685 ## [7] 2.93597415 3.60003394 3.58162578 6.34462934 5.71619521 7.06592076 ## [13] 4.91435973 4.34607526 7.33989536 4.37624817 5.37156273 6.93312965 ## [19] 8.67224539 4.32268704 1.20481630 1.63067425 4.33854031 5.91058592 ## [25] 4.14568098 1.63067425 11.15025406 -1.92750663 11.50686790 4.11478555 ## [31] 7.06592076 8.62388599 5.33204815 10.36073941 8.29051704 7.68537698 ## [37] 3.85858700 3.85858700 3.66301409 4.02220524 -1.92750663 6.15584120 ## [43] 2.93944144 6.38274862 6.38274862 6.75384125 6.13891845 2.87239771 ## [49] 2.81332631 4.00037785 9.10580685 1.92198666 -0.06007993 7.68537698 ## [55] 0.35374159 1.58558919 3.66301409 4.87138863 9.10580685 4.14568098 ## [61] 8.67224539 3.12488220 4.91435973 -0.06007993 6.38274862 3.12488220 ## [67] 8.29051704 8.44642286 4.11478555 6.93312965 2.81332631 0.35374159 ## [73] 8.01721557 1.92198666 5.33204815 10.14519496 7.98051157 3.31857306 ## [79] 8.44642286 6.75384125 5.01729256 1.58558919 -0.06007993 7.51336113 ## [85] 4.33854031 6.38274862 5.64782471 -0.06007993 2.91587906 6.93312965 ## [91] 10.14519496 3.11628177 6.27939266 5.71619521 6.49355143 1.94427385 ## [97] 6.66175296 0.35374159 3.58162578 7.46474324 Calculating the mean for each bootstrap sample: colMeans(resamples_2) ## [1] 5.095470 5.611315 5.283893 4.930731 4.804722 5.187125 4.946582 4.952693 ## [9] 5.470162 5.058354 4.790996 5.357154 5.479364 5.366046 5.454458 5.474732 ## [17] 5.566421 5.229395 5.111966 5.262666 #and the mean of all means mean(colMeans(resamples_2)) ## [1] 5.221712 Calculating the standard deviation of the distribution of means: sqrt(var(colMeans(resamples_2))) ## [1] 0.2523254 37.6 Monty Hall - Fun example The Monty Hall problem is a brain teaser, in the form of a probability puzzle, loosely based on the American television game show Let’s Make a Deal and named after its original host, Monty Hall. The problem was originally posed (and solved) in a letter by Steve Selvin to the American Statistician in 1975 (Selvin 1975a), (Selvin 1975b). It became famous as a question from a reader’s letter quoted in Marilyn vos Savant’s “Ask Marilyn” column in Parade magazine in 1990: Suppose you’re on a game show, and you’re given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows what’s behind the doors, opens another door, say No. 3, which has a goat. He then says to you, “Do you want to pick door No. 2?” Is it to your advantage to switch your choice? Vos Savant’s response was that the contestant should switch to the other door (vos Savant 1990a). Under the standard assumptions, contestants who switch have a 2/3 chance of winning the car, while contestants who stick to their initial choice have only a 1/3 chance. Many readers of vos Savant’s column refused to believe switching is beneficial despite her explanation. After the problem appeared in Parade, approximately 10,000 readers, including nearly 1,000 with PhDs, wrote to the magazine, most of them claiming vos Savant was wrong. Even when given explanations, simulations, and formal mathematical proofs, many people still do not accept that switching is the best strategy. Paul Erdős, one of the most prolific mathematicians in history, remained unconvinced until he was shown a computer simulation demonstrating the predicted result. The given probabilities depend on specific assumptions about how the host and contestant choose their doors. A key insight is that, under these standard conditions, there is more information about doors 2 and 3 that was not available at the beginning of the game, when door 1 was chosen by the player: the host’s deliberate action adds value to the door he did not choose to eliminate, but not to the one chosen by the contestant originally. Another insight is that switching doors is a different action than choosing between the two remaining doors at random, as the first action uses the previous information and the latter does not. Other possible behaviors than the one described can reveal different additional information, or none at all, and yield different probabilities. Here is the simple Bayes rule: \\(Pr(A|B) = Pr(B|A)Pr(A)/Pr(B)\\). Let’s play it: The player picks Door 1, Monty Hall opens Door 3. My question is this: \\(Pr(CAR = 1|Open = 3) &lt; Pr(CAR = 2|Open = 3)\\)? If this is true the player should always switch. Here is the Bayesian answer: \\(Pr(Car=1|Open=3) = Pr(Open=3|Car=1)Pr(Car=1)/Pr(Open=3)\\) = 1/2 x (1/3) / (1/2) = 1/3 Let’s see each number. Given that the player picks Door 1, if the car is behind Door 1, Monty should be indifferent between opening Doors 2 and 3. So the first term is 1/2. The second term is easy: Probability that the car is behind Door 1 is 1/3. The third term is also simple and usualy overlooked. This is not a conditional probability. If the car were behind Door 2, the probability that Monty opens Door 3 would be 1. And this explains why the second option is different, below: \\(Pr(Car=2|Open=3) = Pr(Open=3|Car=2)Pr(Car=2)/Pr(Open=3)\\) = 1 x (1/3) / (1/2) = 2/3 Image taken from http://media.graytvinc.com/images/690*388/mon+tyhall.jpg Simulation to prove it Step 1: Decide the number of plays n &lt;- 100000 Step 2: Define all possible door combinations 3 doors, the first one has the car. All possible outcomes for the game: outcomes &lt;- c(123,132,213,231,312,321) Step 3: Create empty containers where you store the outcomes from each game car &lt;- rep(0, n) goat1 &lt;- rep(0, n) goat2 &lt;- rep(0, n) choice &lt;- rep(0,n) monty &lt;- rep(0, n) winner &lt;- rep(0, n) Step 4: Loop for (i in 1:n){ doors &lt;- sample(outcomes,1) #The game&#39;s door combination car[i] &lt;- substring(doors, first = c(1,2,3), last = c(1,2,3))[1] #the right door goat1[i] &lt;- substring(doors, first = c(1,2,3), last = c(1,2,3))[2] #The first wrong door goat2[i] &lt;- substring(doors, first = c(1,2,3), last = c(1,2,3))[3] #The second wrong door #Person selects a random door choice[i] &lt;- sample(1:3,1) #Now Monty opens a door if (choice[i] == car[i]) {monty[i] = sample(c(goat1[i],goat2[i]),1)} else if (choice[i] == goat1[i]) {monty[i] = goat2[i]} else {monty[i] = goat1[i]} # 1 represents the stayer who remains by her initial choice # 2 represents the switcher who changes her initial choice if (choice[i] == car[i]){winner[i] = 1} else {winner[i] = 2} } Step 5: Chart hist(winner, breaks = 2, main = &quot;Who would win the most?&quot;, ylim = c(0,70000), labels = c(&quot;Stayer&quot;, &quot;Switcher&quot;), col = c(&quot;aliceblue&quot;, &quot;pink&quot;), cex.axis = 0.75, cex.lab = 0.75, cex.main = 0.85) The simulation is inspired by https://theressomethingaboutr.wordpress.com/2019/02/12/in-memory-of-monty-hall/ (Rajter_2019?) "],["algorithmic-optimization.html", "Chapter 38 Algorithmic Optimization 38.1 Brute-force optimization 38.2 Derivative-based methods 38.3 ML Estimation with logistic regression 38.4 Gradient Descent Algorithm 38.5 Optimization with R", " Chapter 38 Algorithmic Optimization Here is a definition of algorithmic optimization from Wikipedia: An optimization algorithm is a procedure which is executed iteratively by comparing various solutions until an optimum or a satisfactory solution is found. Optimization algorithms help us to minimize or maximize an objective function \\(F(x)\\) with respect to the internal parameters of a model mapping a set of predictors (\\(X\\)) to target values(\\(Y\\)). There are three types of optimization algorithms which are widely used; Zero-Order Algorithms, First-Order Optimization Algorithms, and Second-Order Optimization Algorithms. Zero-order (or derivative-free) algorithms use only the criterion value at some positions. It is popular when the gradient and Hessian information are difficult to obtain, e.g., no explicit function forms are given. First Order Optimization Algorithms minimize or maximize a Loss function \\(F(x)\\) using its Gradient values with respect to the parameters. Most widely used First order optimization algorithm is Gradient Descent. The First order derivative displays whether the function is decreasing or increasing at a particular point. In this appendix, we will review some important concepts in algorithmic optimization. 38.1 Brute-force optimization Let’s look at a simplified example about optimal retirement-plan and solve it with a zero-order algorithm. Suppose that there are 2 groups of workers who are planning for their retirement at the age of 65. Both consider spending 40,000 dollars each year for the rest of their lives after retirement. On average, people in both groups expect to live 20 more years after retirement with some uncertainty. The people in the first group (A) have the following risk profile: 85% chance to live 20 years and 15% chance to live 30 years. The same risk profile for the people in the second group (B) is: 99% for 20 years and 1% for 30 years. Suppose that in each group, their utility (objective) function is \\(U=C^{0.5}\\). What’s the maximum premium (lump-sum payment) that a person in each group would be willing to pay for a life-time annuity of 40K? Without a pension plan, people in each group have the following utilities: #For people in group A U_A = 0.85*sqrt(40000*20) + 0.15*sqrt(10*0) U_A ## [1] 760.2631 #For people in group B U_B = 0.99*sqrt(40000*20) + 0.01*sqrt(10*0) U_B ## [1] 885.4829 For example, they would not pay 200,000 dollars to cover their retirement because that would make them worse than their current situation (without a pension plan). #For people in group A U_A = 0.85*sqrt(40000*20 - 200000) + 0.15*sqrt(40000*10 - 200000) U_A ## [1] 725.4892 #For people in group B U_B = 0.99*sqrt(40000*20 - 200000) + 0.01*sqrt(40000*10 - 200000) U_B ## [1] 771.3228 Hence, the payment they would be willing to make for reduction in uncertainty during their retirement should not make them worse off. Or more technically, their utility should not be lower than their current utility levels. Therefore Pmax, the maximum premium that a person would be willing to pay, can be found by minimizing the following cost function for people, for example, in Group A: \\[ f(Pmax) = p \\times \\sqrt{40000 \\times 20~\\text{years}-Pmax}+ \\\\ (1-p) \\times \\sqrt{40000 \\times 10~ \\text{years}-Pmax} - p \\times \\sqrt{ 40000 \\times 20~\\text{years}} \\] Here is the iteration to solve for Pmax for people in Group A. We created a cost function, costf, that we try to minimize. Change the parameters to play with it. The same algorithm can be used to find Pmax for people in Group B. library(stats) p &lt;- 0.85 w1 &lt;- 800000 w2 &lt;- 400000 converged = F iterations = 0 maxiter &lt;- 600000 learnrate &lt;- 0.5 Pmax &lt;- 10000 while(converged == FALSE){ costf &lt;- p*sqrt(w1 - Pmax) + (1 - p)*sqrt(w2 - Pmax) - p*sqrt(w1) if(costf &gt; 0){ Pmax &lt;- Pmax + learnrate iterations = iterations + 1 if(iterations &gt; maxiter) { print(&quot;It cannot converge before finding the optimal Pmax&quot;) break } converged = FALSE }else{ converged = TRUE print(paste(&quot;Maximum Premium:&quot;, Pmax, &quot;achieved with&quot;, iterations, &quot;iterations&quot;)) } } ## [1] &quot;Maximum Premium: 150043 achieved with 280086 iterations&quot; #let&#39;s verify it by `uniroot()` which finds the roots for f(x) = 0 costf &lt;- function(x){ p * sqrt(800000 - x) + (1-p) * sqrt(400000 - x) - p*sqrt(800000) } paste(&quot;Unitroot for f(x) = 0 is &quot;, uniroot(costf, c(10000, 200000))$root) ## [1] &quot;Unitroot for f(x) = 0 is 150042.524874307&quot; There are better functions that we could use for this purpose, but this example works well for our experiment. There several of important parameters in our algorithm. The first one is the starting Pmax, which can be set up manually. If the starting value is too low, iteration could not converge. If it’s too high, it can give us an error. Another issue is that our iteration does not know if the learning rate should increase or decrease when the starting value is too high or too low. This can be done with additional lines of code, but we will not address it here. This situation leads us to the learning rate: the incremental change in the value of the parameter. This parameter should be conditioned on the value of cost function. If the cost function for a given Pmax is negative, for example, the learning rate should be negative. Secondly, the number of maximum iterations must be set properly, otherwise the algorithm may not converge or take too long to converge. In the next section, we will address these issues with a smarter algorithm. There are other types of approaches. For example, the algorithm may create a grid of Pmax and then try all the possible values to see which one approximately makes the cost function minimum. 38.2 Derivative-based methods One of the derivative-based methods is the Newton-Raphson method. If we assume that the function is differentiable and has only one minimum (maximum), we can develop an optimization algorithm that looks for the point in parameter space where the derivative of the function is zero. There are other methods, like Fisher Scoring and Iteratively Reweighted Least Squares, that we will not see here. First, let’s see the Newton-Raphson method. This is a well-known extension of your calculus class about derivatives in High School. The method is very simple and used to find the roots of \\(f(x)=0\\) by iterations. In first-year computer science courses, this method is used to teach loop algorithms that calculate the value of, for example, \\(e^{0.71}\\) or \\(\\sqrt{12}\\). It is a simple iteration that converges in a few steps. \\[ x_{n+1}=x_{n}-\\frac{f\\left(x_{n}\\right)}{f^{\\prime}\\left(x_{n}\\right)} \\] To understand it, let’s look at the function \\(y=f(x)\\) shown in the following graph: It has a zero at \\(x=x_r\\), which is not known. To find it, we start with \\(x_0\\) as an initial estimate of \\(X_r\\). The tangent line to the graph at the point \\(\\left(x_0, f\\left(x_0\\right)\\right)\\) has the point \\(x_1\\) at which the tangent crosses the \\(x\\)-axis. The slope of this line can be defined as \\[ \\frac{y-f\\left(x_0\\right)}{x-x_0}=f^{\\prime}\\left(x_0\\right) \\] Hence, \\[ y-f\\left(x_0\\right)=f^{\\prime}\\left(x_0\\right)\\left(x-x_0\\right) \\] At the point where the tangent line cross the \\(x\\)-axis, \\(y=0\\) and \\(x=x_1\\). Hence solving the equation for \\(x_1\\), we get \\[ x_{1}=x_{0}-\\frac{f\\left(x_{0}\\right)}{f^{\\prime}\\left(x_{0}\\right)} \\] And the second approximations: \\[ x_{2}=x_{1}-\\frac{f\\left(x_{1}\\right)}{f^{\\prime}\\left(x_{1}\\right)} \\] And with multiple iterations one can find the solution. Here is the example: newton &lt;- function(f, x0, tol = 1e-5, n = 1000) { require(numDeriv) # Package for computing f&#39;(x) for (i in 1:n) { dx &lt;- genD(func = f, x = x0)$D[1] # First-order derivative f&#39;(x0) x1 &lt;- x0 - (f(x0) / dx) # Calculate next value x1 if (abs(x1 - x0) &lt; tol) { res &lt;- paste(&quot;Root approximation is &quot;, x1, &quot; in &quot;, i, &quot; iterations&quot;) return(res) } # If Newton-Raphson has not yet reached convergence set x1 as x0 and continue x0 &lt;- x1 } print(&#39;Too many iterations in method&#39;) } func2 &lt;- function(x) { x^15 - 2 } newton(func2, 1) ## [1] &quot;Root approximation is 1.04729412282063 in 5 iterations&quot; #Check it paste(&quot;Calculator result: &quot;, 2^(1/15)) ## [1] &quot;Calculator result: 1.04729412282063&quot; Newton’s method is often used to solve two different, but related, problems: Finding \\(x\\) such that \\(f(x)=0\\) (try to solve our insurance problem with this method) Finding \\(x\\) that \\(g&#39;(x)=0\\), or find \\(x\\) that minimizes/maximizes \\(g(x)\\). The relation between these two problems is obvious when we define \\(f(x) = g&#39;(x)\\). Hence, for the second problem, the Newton-Raphson method becomes: \\[ x_{n+1}=x_{n}-\\frac{g^{\\prime}\\left(x_{n}\\right)}{g^{\\prime \\prime}\\left(x_{n}\\right)} \\] Connection between these two problems are defined in this post (Gulzar_2018?) very nicely. Let’s pretend that we are interested in determining the parameters of a random variable \\(X \\sim N(\\mu, \\sigma^{2})\\). Here is the log-likelihood function for \\(X\\): \\[ \\log (\\mathcal{L}(\\mu, \\sigma))=-\\frac{n}{2} \\log \\left(2 \\pi \\sigma^{2}\\right)-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n}\\left(x_{i}-\\mu\\right)^{2} \\] We have seen it in Chapter 2 before. But this time we will use dnorm() which calculates the pdf of a normal variable. First let’s have the data and the log-likelihood: # Let&#39;s create a sample of normal variables set.seed(2019) X &lt;- rnorm(100, 0, 1) # And the log-likelihood of this function. # Remember likelihood function would be prod(dnorm()) with log=F normalLL &lt;- function(prmt){ sum(dnorm(X, mean = prmt[1], sd = prmt[2], log = TRUE)) } # Let&#39;s try several parameters normalLL(prmt = c(1,1.5)) ## [1] -176.078 normalLL(prmt = c(2,1)) ## [1] -347.4119 normalLL(prmt = c(mean(X),sd(X))) ## [1] -131.4619 As you can see, the last one is the best. And we can verify it because we had created \\(X\\) with 0 mean and 1 sd, approximately. Now we will use the Newton-Raphson method to calculate those parameters that minimize the negative log-likelihood. First, let’s build a function that estimates the slope of the function (first-derivative) numerically at any arbitrary point in parameter space for mean and sd, separately. Don’t forget, the log-likelihood is a function of parameters (mean and sd) not X. # First partial (numerical) derivative w.r.t. mean firstM &lt;- function(p1, p2, change = 0.0001){ prmt &lt;- c(p1, p2) high &lt;- normalLL(prmt + c(change,0)) low &lt;- normalLL(prmt - c(change,0)) slope &lt;- (high-low)/(change*2) return(slope) } firstM(mean(X), sd(X)) ## [1] 0 # First partial (numerical) derivative w.r.t. sd firstSD &lt;- function(p1, p2, change = 0.0001){ prmt &lt;- c(p1, p2) high &lt;- normalLL(prmt + c(0, change)) low &lt;- normalLL(prmt - c(0, change)) slope &lt;- (high-low)/(change*2) return(slope) } firstSD(mean(X), sd(X)) ## [1] -1.104417 #Verify them with the grad() library(numDeriv) f &lt;- function(x) { a &lt;- x[1]; b &lt;- x[2] sum(dnorm(X, mean = a, sd = b, log = TRUE)) } grad(f,c(mean(X),sd(X)))[1] ## [1] 0 grad(f,c(mean(X),sd(X)))[2] ## [1] -1.104419 # Or better round(jacobian(f,c(mean(X),sd(X))), 4) #First derivatives ## [,1] [,2] ## [1,] 0 -1.1044 round(hessian(f,c(mean(X),sd(X))), 4) #Second derivatives ## [,1] [,2] ## [1,] -121.9741 0.000 ## [2,] 0.0000 -240.289 Let’s try them now in the Newton-Raphson method. \\[ x_{n+1}=x_{n}-\\frac{g^{\\prime}\\left(x_{n}\\right)}{g^{\\prime \\prime}\\left(x_{n}\\right)} \\] Similar to the first one, we can also develop a function that calculates the second derivatives. However, instead of using our own functions, let’s use grad() and hessian() from the numDeriv package. set.seed(2019) X &lt;- rnorm(100, 2, 2) NR &lt;- function(f, x0, y0, tol = 1e-5, n = 1000) { for (i in 1:n) { dx &lt;- grad(f,c(x0, y0))[1] # First-order derivative f&#39;(x0) ddx &lt;- hessian(f,c(x0, y0))[1,1] # Second-order derivative f&#39;&#39;(x0) x1 &lt;- x0 - (dx / ddx) # Calculate next value x1 if (abs(x1 - x0) &lt; tol) { res &lt;- paste(&quot;The mean approximation is &quot;, x1, &quot; in &quot;, i, &quot; iterations&quot;) return(res) } # If Newton-Raphson has not yet reached convergence set x1 as x0 and continue x0 &lt;- x1 } print(&#39;Too many iterations in method&#39;) } func &lt;- function(x) { a &lt;- x[1]; b &lt;- x[2] sum(dnorm(X, mean = a, sd = b, log = TRUE)) } NR(func, -3, 1.5) ## [1] &quot;The mean approximation is 1.85333200308383 in 2 iterations&quot; #Let;s verify it mean(X) ## [1] 1.853332 Finding sd is left to the practice questions. But the way to do it should be obvious. Use our approximation of the mean (1.853332) as a fixed parameter in the function and run the same algorithm for finding sd. When the power of computers and the genius of mathematics intercepts, beautiful magics happen. 38.3 ML Estimation with logistic regression The pdf of Bernoulli distribution is \\[ P(Y=y)=p^y(1-p)^{1-y} \\] It’s likelihood \\[ \\begin{aligned} L(\\boldsymbol{\\beta} \\mid \\mathbf{y} ; \\mathbf{x}) &amp;=L\\left(\\beta_0, \\beta_1 \\mid\\left(y_1, \\ldots, y_n\\right) ;\\left(x_1, \\ldots, x_n\\right)\\right) \\\\ &amp;=\\prod_{i=1}^n p_i^{y_i}\\left(1-p_i\\right)^{1-y_i} \\end{aligned} \\] And log-likelihood \\[ \\begin{aligned} \\ell(\\boldsymbol{\\beta} \\mid \\mathbf{y} ; \\mathbf{x}) &amp;=\\log \\left(\\prod_{i=1}^n p_i^{y_i}\\left(1-p_i\\right)^{1-y_i}\\right) \\\\ &amp;=\\sum_{i=1}^n\\left( \\log \\left(p_i^{y_i}\\right)+\\log \\left(1-p_i\\right)^{1-y_i}\\right) \\\\ &amp;=\\sum_{i=1}^n y_i \\left(\\log \\left(p_i\\right)+\\left(1-y_i\\right) \\log \\left(1-p_i\\right)\\right) \\end{aligned} \\] where \\[ \\begin{aligned} \\operatorname{L}\\left(p_i\\right) &amp;=\\log \\left(\\frac{p_i}{1-p_i}\\right) \\\\ &amp;=\\beta_0+\\beta_1 x_1 \\end{aligned} \\] So, \\[ p_i=\\frac{\\exp \\left(\\beta_0+x_1 \\beta_1\\right)}{1+\\exp \\left(\\beta_0+x_1 \\beta_1\\right)} \\] First partial derivative with respect to \\(\\beta_0\\) \\[ \\begin{aligned} \\frac{\\partial p_i}{\\partial \\beta_0} &amp;=\\frac{\\exp \\left(\\beta_0+x_1 \\beta_1\\right)}{\\left(1+\\exp \\left(\\beta_0+x_1 \\beta_1\\right)\\right)^2} \\\\ &amp;=p_i\\left(1-p_i\\right) \\end{aligned} \\] And \\[ \\begin{aligned} \\frac{\\partial p_i}{\\partial \\beta_1} &amp;=\\frac{x_1 \\exp \\left(\\beta_0+x_1 \\beta_1\\right)}{\\left(1+\\exp \\left(\\beta_0+x_1 \\beta_1\\right)\\right)^2} \\\\ &amp;=x_1 p_i\\left(1-p_i\\right) \\end{aligned} \\] Newton-Raphson’s equation is \\[ \\boldsymbol{\\beta}^{(t+1)}=\\boldsymbol{\\beta}^{(t)}-\\left(\\boldsymbol{H}^{(t)}\\right)^{-1} \\boldsymbol{u}^{(t)}, \\] where \\[ \\boldsymbol{\\beta}^{(t)}=\\left[\\begin{array}{c} \\beta_0^{(t)} \\\\ \\beta_1^{(t)} \\end{array}\\right] \\] \\[ \\boldsymbol{u}^{(t)}=\\left[\\begin{array}{c} u_0^{(t)} \\\\ u_1^{(t)} \\end{array}\\right]=\\left[\\begin{array}{c} \\frac{\\partial \\mathscr{C}\\left(\\beta^{(t)} \\mid y ; x\\right)}{\\partial \\beta_0} \\\\ \\frac{\\partial \\mathscr{C}\\left(\\beta^{(t)} \\mid y ; x\\right)}{\\partial \\beta_1} \\end{array}\\right]=\\left[\\begin{array}{c} \\sum_{i=1}^n \\left(y_i-p_i^{(t)}\\right) \\\\ \\sum_{i=1}^n x_i\\left(y_i-p_i^{(t)}\\right) \\end{array}\\right] \\] where, \\[ p_i^{(t)}=\\frac{\\exp \\left(\\beta_0^{(t)}+x_1 \\beta_1^{(t)}\\right)}{1+\\exp \\left(\\beta_0^{(t)}+x_1 \\beta_1^{(t)}\\right)} \\] \\(\\boldsymbol{H}^{(t)}\\) can be considered as Jacobian matrix of \\(\\boldsymbol{u}(\\cdot)\\), \\[ \\boldsymbol{H}^{(t)}=\\left[\\begin{array}{ll} \\frac{\\partial u_0^{(t)}}{\\partial \\beta_0} &amp; \\frac{\\partial u_0^{(t)}}{\\partial \\beta_1} \\\\ \\frac{\\partial u_1^{(t)}}{\\partial \\beta_0} &amp; \\frac{\\partial u_1^{(t)}}{\\partial \\beta_1} \\end{array}\\right] \\] Let’s simulate data and solve it the Newton-Raphson’s method described above. rm(list=ls()) #Simulating data set.seed(1) n &lt;- 500 X = rnorm(n) # this is our x z = -2 + 3 * X #Prob. is defined by logistic function p = 1 / (1 + exp(-z)) #Bernoulli is the special case of the binomial distribution with size = 1 y = rbinom(n, size = 1, prob = p) #And we create our data df &lt;- data.frame(y, X) head(df) ## y X ## 1 0 -0.6264538 ## 2 0 0.1836433 ## 3 0 -0.8356286 ## 4 0 1.5952808 ## 5 0 0.3295078 ## 6 0 -0.8204684 logis &lt;- glm(y ~ X, data = df, family = binomial) summary(logis) ## ## Call: ## glm(formula = y ~ X, family = binomial, data = df) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.3813 -0.4785 -0.2096 0.2988 2.4274 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.8253 0.1867 -9.776 &lt;2e-16 *** ## X 2.7809 0.2615 10.635 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 605.69 on 499 degrees of freedom ## Residual deviance: 328.13 on 498 degrees of freedom ## AIC: 332.13 ## ## Number of Fisher Scoring iterations: 6 library(numDeriv) func_u &lt;- function(b) { c(sum(df$y - exp(b[1] + b[2] * df$X)/ (1 + exp(b[1] + b[2] * df$X))), sum(df$X * (df$y - exp(b[1] + b[2] * df$X)/ (1 + exp(b[1] + b[2] * df$X))))) } # Starting points delta &lt;- matrix(1:2, nrow = 2) # starting delta container (with any number &gt; 0) b &lt;- array(c(-2,3)) while(abs(sum(delta)) &gt; 0.0001){ B &lt;- b #current b b &lt;- as.matrix(b) - solve(jacobian(func_u, x = b)) %*% func_u(b) #new b delta &lt;- b - as.matrix(B) } b ## [,1] ## [1,] -1.825347 ## [2,] 2.780929 38.4 Gradient Descent Algorithm Let’s start with a regression problem. The cost function in OLS is the residual sum of squares, \\(\\mathrm{RSS}=\\sum_{i=1}^n\\left(\\widehat{e}_i\\right)^2=\\sum_{i=1}^n\\left(y_i-\\hat{y}\\right)^2=\\sum_{i=1}^n\\left(y_i-\\left(b_1+b_2 x_i\\right)\\right)^2\\), which is a convex function. Our objective to find \\(b_1\\) and \\(b_2\\) that minimize RSS. How can we find those parameters to minimize a cost function if we don’t know much about it? The trick is to start with some point and move a bit (locally) in the direction that reduces the value of the cost function. In general, this search process for finding the minimizing point has two components: the direction and the step size. The direction tells us which direction we move next, and the step size determines how far we move in that direction. For example, the iterative search for \\(b_2\\) of gradient descent can be described by the following recursive rule: \\[ b_2^{(k+1)}=b_2^{(k)}-lr \\nabla RSS^{k} \\] Here, \\(lr\\) is learning rate and \\(\\nabla RSS^{k}\\) is the slope of RSS at step \\(k\\). Hence, \\(lr \\nabla RSS^{k}\\) is the total step size at step \\(k\\). Note that, as we move from either directions towards \\(b^*_2\\), \\(\\nabla RSS^{k}\\) gets smaller. In fact, it becomes zero at \\(b^*_2\\). Therefore, \\(\\nabla RSS^{k}\\) helps iterations find the proper adjustment in each step in terms of direction and magnitude. Since RSS is a convex function, it’s easy to see how sign of \\(\\nabla RSS^{k}\\) will direct the arbitrary \\(b_2^{&#39;&#39;}\\) towards the optimal \\(b_2\\). Since first-order approximation at \\(b_2^{&#39;&#39;}\\) is good only for small \\(\\Delta b_2\\), a small \\(lr&gt;0\\) is needed to o make \\(\\Delta b_2\\) small in magnitude. Moreover, when a high learning rate used it leads to “overshooting” past the local minima and may result in diverging algorithm. Below, we first use a simple linear regression function on simulated data and estimate its parameters with lm(). Let’s simulate a sample with our DGM. set.seed(1001) N &lt;- 100 int &lt;- rep(1, N) x1 &lt;- rnorm(N, mean = 10, sd = 2) Y &lt;- rnorm(N, 2*x1 + int, 1) model &lt;- lm(Y ~ x1) b &lt;- coef(model) b ## (Intercept) x1 ## 1.209597 1.979643 plot(x1, Y, col = &quot;blue&quot;, pch = 20) abline(b) The cost function that we want to minimize is \\[ y_i = 1 + 2x_i + \\epsilon_i \\\\ RSS = \\sum{\\epsilon_i^2}=\\sum{(y_i-1-2x_i)^2} \\] And, its plot for a range of coefficients is already shown earlier. 38.4.1 One-variable Below, we create a function, grdescent, to show how sensitive gradient descent algorithms would be to different calibrations: grdescent &lt;- function(x, y, lr, thresh, maxiter) { #starting points set.seed(234) b &lt;- runif(1, 0, 1) c &lt;- runif(1, 0, 1) n &lt;- length(x) #function yhat &lt;- c + b * x #gradient MSE &lt;- sum((y - yhat) ^ 2) / n converged = F iterations = 0 #while loop while (converged == F) { b_new &lt;- b - ((lr * (1 / n)) * (sum((y - yhat) * x * (-1)))) c_new &lt;- c - ((lr * (1 / n)) * (sum(y - yhat) * (-1))) b &lt;- b_new c &lt;- c_new yhat &lt;- b * x + c MSE_new &lt;- sum((y - yhat) ^ 2) / n if (abs(MSE - MSE_new) &lt;= thresh) { converged = T return(paste(&quot;Intercept:&quot;, c, &quot;Slope:&quot;, b)) } iterations = iterations + 1 if (iterations &gt; maxiter) { converged = T return(paste(&quot;Max. iter. reached, &quot;, &quot;Intercept:&quot;, c, &quot;Slope:&quot;, b)) } } } Note that the key part in this algorithm is b_new &lt;- b + (learnrate * (1 / n)) * sum((y - yhat) * x*(-1). The first \\(b\\) that is picked randomly by b &lt;- runif(1, 0, 1) is adjusted by learnrate * (1 / n) * (sum((y - yhat) * -x)). Note that sum((y - yhat) * x) is the first order condition of the cost function (RSS - Residual Sum of Squares) for the slope coefficient. The cost function (RSS) is a convex function where the minimum can be achieved by the optimal \\(b\\). It is a linear Taylor approximation of RSS at \\(b\\) that provides the steepest descent, that is just a simple adjustment for identifying the direction of the adjustment of \\(b\\) until the minimum RSS is reached. Now we will see if this function will give us the same intercept and slope coefficients already calculated with lm() above. grdescent(x1, Y, 0.003, 0.001, 1000000) # Perfect ## [1] &quot;Max. iter. reached, Intercept: 1.20959654028984 Slope: 1.97964316494708&quot; This is good. But, if start a very low number with a small learning rate, then we need more iteration grdescent(x1, Y, 0.00004, 0.001, 4000000) ## [1] &quot;Max. iter. reached, Intercept: 1.20947317953422 Slope: 1.97965489994805&quot; Yes, the main question is how do we find out what the learning rate should be? It is an active research question and to answer it is beyond this chapter. A general suggestion, however, is to keep it small and tune it within the training process. 38.4.2 Multivariable We will expand the gradient descent algorithms with an multivariable example using matrix algebra. First, the data and model simulation: set.seed(1001) N &lt;- 100 int &lt;- rep(1, N) x1 &lt;- rnorm(N, mean = 10, sd = 2) x2 &lt;- rnorm(N, mean = 5, sd = 1) x3 &lt;- rbinom(N, 1, 0.5) x4 &lt;- rbinom(N, 1, 0.5) x5 &lt;- rbinom(N, 1, 0.5) x6 &lt;- rnorm(N, 1, 0.25) x7 &lt;- rnorm(N, 1, 0.2) x2x3 &lt;- x2*x3 x4x5 &lt;- x4*x5 x4x6 &lt;- x5*x6 x3x7 &lt;- x3*x7 Y &lt;- rnorm(N, 2*x1 + -0.5*x2 - 1.75*x2x3 + 2*x4x5 - 3*x4x6 + 1.2*x3x7 + int, 1) X &lt;- cbind(int, x1, x2, x2x3, x4x5, x4x6, x3x7) We can solve it with linear algebra manually: betaOLS &lt;- solve(t(X)%*%X)%*%t(X)%*%Y print(betaOLS) ## [,1] ## int 0.4953323 ## x1 1.9559022 ## x2 -0.3511182 ## x2x3 -1.9112623 ## x4x5 1.7424723 ## x4x6 -2.8323934 ## x3x7 2.1015442 We can also solve it with lm() model1.lm &lt;- lm(Y ~ X -1) summary(model1.lm) ## ## Call: ## lm(formula = Y ~ X - 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.84941 -0.45289 -0.09686 0.57679 2.07154 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## Xint 0.49533 0.75410 0.657 0.51290 ## Xx1 1.95590 0.03868 50.571 &lt; 2e-16 *** ## Xx2 -0.35112 0.12600 -2.787 0.00645 ** ## Xx2x3 -1.91126 0.13358 -14.308 &lt; 2e-16 *** ## Xx4x5 1.74247 0.24396 7.143 2.01e-10 *** ## Xx4x6 -2.83239 0.18831 -15.041 &lt; 2e-16 *** ## Xx3x7 2.10154 0.64210 3.273 0.00149 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8448 on 93 degrees of freedom ## Multiple R-squared: 0.9972, Adjusted R-squared: 0.997 ## F-statistic: 4677 on 7 and 93 DF, p-value: &lt; 2.2e-16 Now the function for gradient descent: grdescentM &lt;- function(x, y, lr, thresh, maxiter) { set.seed(123) b &lt;- runif(ncol(x), 0, 1) yhat &lt;- x%*%b e &lt;- y - yhat RSS &lt;- t(e)%*%e converged = F iterations = 0 n &lt;- length(y) while(converged == F) { b_new &lt;- b - (lr*(1/n))*t(x)%*%(x%*%b - y) b &lt;- b_new yhat &lt;- x%*%b e &lt;- y - yhat RSS_new &lt;- t(e)%*%e if(RSS - RSS_new &lt;= thresh) { converged = T return(b) } iterations = iterations + 1 if(iterations &gt; maxiter) { converged = T return(b) } } } grdescentM(X, Y, 0.003, 0.001, 1000000) ## [,1] ## int 0.4953323 ## x1 1.9559022 ## x2 -0.3511182 ## x2x3 -1.9112623 ## x4x5 1.7424723 ## x4x6 -2.8323934 ## x3x7 2.1015442 38.5 Optimization with R A good summary of tools for optimization in R given in this guide: Optimization and Mathematical Programming. There are many optimization methods, each of which would only be appropriate for specific cases. In choosing a numerical optimization method, we need to consider following points: We need to know if it is a constrained or unconstrained problem. For example, the MLE method is an unconstrained problem. Most regularization problems, like Lasso or Ridge, are constraint optimization problems. Do we know how the objective function is shaped a priori? MLE and OLS methods have well-known objective functions (Residual Sum of Squares and Log-Likelihood). Maximization and minimization problems can be used in both cases by flipping the sign of the objective function. Multivariate optimization problems are much harder than single-variable optimization problems. There is, however, a large set of available optimization methods for multivariate problems. In multivariate cases, the critical point is whether the objective function has available gradients. If only the objective function is available without gradient or Hessian, the Nelder-Mead algorithm is the most common method for numerical optimization. If gradients are available, the best and most used method is the gradient descent algorithm. We have seen its application for OLS. This method can be applied to MLE as well. It is also called a Steepest Descent Algorithm. In general, the gradient descent method has three types: Batch Gradient Descent, Stochastic Gradient Descent, and Mini-Batch Gradient Descent. If the gradient and Hessian are available, we can also use the Newton-Raphson Method. This is only possible if the dataset is not high-dimensional, as calculating the Hessian would otherwise be very expensive. Usually the Nelder-Mead method is slower than the Gradient Descent method. Optim() uses the Nelder-Mead method, but the optimization method can be chosen in its arguments. More details can be found in this educational slides. The most detailed and advance source is Numerical Recipes, which uses C++ and R. "],["imbalanced-data.html", "Chapter 39 Imbalanced Data 39.1 SMOTE 39.2 Fraud detection", " Chapter 39 Imbalanced Data Classification with imbalanced data is characterized by the uneven proportion of cases that are available for each class, and causes problems in many learning algorithms. An imbalance in the data is usually considered an issue when the distribution of classes is skewed more than 60-40% ratio. There are two simple methods to overcome imbalance data in classification problems: oversampling and undersampling, both of which are used to adjust the class distribution in a data set. While oversampling simply randomly replicates the minority class, undersampling randomly selects a subset of majority class and reduces the overall sample size. Thus, it can discard useful data. There are also more complex oversampling techniques, including the creation of artificial data points. The most common technique is known as SMOTE, Synthetic Minority Oversampling Technique Chawla et al. [-(Chawla_2002?). This method generates synthetic data based on the feature space similarities between existing minority instances. In order to create a synthetic instance, it finds the k-nearest neighbors of each minority instance, randomly selects one of them, and then calculate linear interpolations to produce a new minority instance in the neighborhood. The other methods is the adaptive synthetic sampling approach (ADASYN), which builds on the methodology of SMOTE, by shifting the importance of the classification boundary to those minority classes. In this chapter we will see some examples using only SMOTE. 39.1 SMOTE We will use Credit Card Fraud Detection dataset on Kaggle {-(Kaggle_Cred?)}. The dataset has about 300K anonymized credit card transfers labeled as fraudulent or genuine. The features are numerical and anonymized (V1, V2, … , V28). They are the principal components obtained with principal component analysis (PCA). The only features which have not been transformed with PCA are Time and Amount. Feature Time contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature Amount is the transaction Amount and Class is the response variable and it takes value 1 in case of fraud and 0 otherwise. The prediction problem is to label transactions as fraud or not. We will use only a subset of data with roughly 10K observations, representing transactions. library(tidyverse) library(ROCR) library(smotefamily) library(randomForest) head(creditcard10) ## # A tibble: 6 × 31 ## Time V1 V2 V3 V4 V5 V6 V7 V8 V9 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 77319 -0.278 0.924 1.40 0.833 0.0318 -0.619 0.592 0.0361 -0.751 ## 2 130219 0.916 -2.70 -3.26 -0.00660 -0.504 -1.14 1.26 -0.609 -1.18 ## 3 42328 -2.25 -1.03 0.937 0.198 1.01 -2.00 -0.754 0.691 -0.423 ## 4 51453 -0.386 0.766 0.850 0.195 0.850 0.188 0.702 0.0700 0.0516 ## 5 48711 -1.04 0.240 1.53 -0.0509 1.80 0.650 0.556 0.172 -0.288 ## 6 125697 1.52 -1.92 -2.33 -0.586 -0.468 -0.837 0.387 -0.440 -0.456 ## # … with 21 more variables: V10 &lt;dbl&gt;, V11 &lt;dbl&gt;, V12 &lt;dbl&gt;, V13 &lt;dbl&gt;, ## # V14 &lt;dbl&gt;, V15 &lt;dbl&gt;, V16 &lt;dbl&gt;, V17 &lt;dbl&gt;, V18 &lt;dbl&gt;, V19 &lt;dbl&gt;, ## # V20 &lt;dbl&gt;, V21 &lt;dbl&gt;, V22 &lt;dbl&gt;, V23 &lt;dbl&gt;, V24 &lt;dbl&gt;, V25 &lt;dbl&gt;, ## # V26 &lt;dbl&gt;, V27 &lt;dbl&gt;, V28 &lt;dbl&gt;, Amount &lt;dbl&gt;, Class &lt;dbl&gt; table(creditcard10$Class) ## ## 0 1 ## 28427 53 prop.table(table(creditcard10$Class)) ## ## 0 1 ## 0.998139045 0.001860955 The class balance is way off! The split is approximately 99.83% to 0.017%. df &lt;- creditcard10 plot(df$V1, df$V2, col = (df$Class+1) + 4, lwd = 0.5) ind &lt;- which(df$Class == 1, arr.ind = TRUE) plot(df$V1[ind], df$V2[ind], col = &quot;red&quot;, lwd = 0.7) The idea behind SMOTE is very simple: take a red point (fraud) say Jason, find the k nearest neighbors to Jason by using all features. Then we randomly pick one observation among these, let’s say, 5 neighbors. Suppose that person is Mervin. Now, also suppose that we have only two features: V1 and V2 Observations V1 V2 Jason -12.5 -5.0 Mervin -10.5 -2.5 Then the new synthetic point will be created by \\(V1 = -12.5 + r(-10.5-(-12.5)) = -12.5 +r2.0\\) and \\(V2 = -5 + r(-2.5-(-5)) = -5 +r2.5\\), where \\(r\\) is a random number between 0 and 1. If it’s 0.7, for example, the new synthetic observation will be added to data: Observations V1 V2 Jason -12.5 -5.0 Mervin -10.5 -2.5 Synthetic -11.1 -3.25 This is one synthetic observation created form a real observation, Tim. We can repeat it 10, 20 times and create many synthetic observations from Tim. And then we can repeat it for each real cases of fraud. library(smotefamily) df$Class &lt;- as.factor(df$Class) outc &lt;- SMOTE(X = df[, -31], target = df$Class, K = 4, dup_size = 10) over_df = outc$data table(over_df$class) ## ## 0 1 ## 28427 583 prop.table(table(over_df$class)) ## ## 0 1 ## 0.97990348 0.02009652 Or, with higher K and dup_size: library(smotefamily) df$Class &lt;- as.factor(df$Class) outc &lt;- SMOTE(X = df[, -31], target = df$Class, K = 4, dup_size = 50) over_df = outc$data table(over_df$class) ## ## 0 1 ## 28427 2703 prop.table(table(over_df$class)) ## ## 0 1 ## 0.91317058 0.08682942 And, here is the new plot with expanded “fraud” cases: ind &lt;- which(over_df$class == 1, arr.ind = TRUE) plot(over_df$V1[ind], over_df$V2[ind], col = &quot;red&quot;, lwd = 0.7) All together: plot(over_df$V1, over_df$V2, col = (as.numeric(over_df$class)+1) + 4, lwd = 0.5) 39.2 Fraud detection Here is what we will do with the fraud data in the following script: Apply SMOTE on training set to balance the class distribution Train a Random Forest model on re-balanced training set Test performance on (original) test set In addition, we will compare with and without balancing. library(ROCR) library(smotefamily) library(randomForest) rm(list = ls()) load(&quot;creditcard10.RData&quot;) df$Class &lt;- as.factor(df$Class) AUCb &lt;- c() AUCimb &lt;- c() n = 10 # Could be 50, since the data is large for RF B = 100 for (i in 1:n) { set.seed(i) ind &lt;- sample(nrow(df), nrow(df), replace = TRUE) ind &lt;- unique(ind) # Otherwise it oversamples 0&#39;s train &lt;- df[ind, ] test &lt;- df[-ind, ] # Balancing outdf &lt;- SMOTE(X = train[, -31], target = train$Class, K = 10, dup_size = 50) trainn &lt;- outdf$data trainn$class &lt;- as.factor(trainn$class) #SMOTE makes factor to &quot;chr&quot;! colnames(trainn)[31] &lt;- &quot;Class&quot; #SMOTE made it lower case! modelb &lt;- randomForest(Class~., ntree = B, data = trainn) phatb &lt;- predict(modelb, test, type = &quot;prob&quot;) # Without Balancing modelimb &lt;- randomForest(Class~., ntree = B, data = train) phatimb &lt;- predict(modelimb, test, type = &quot;prob&quot;) #AUCb pred_rocr1 &lt;- prediction(phatb[,2], test$Class) auc_ROCR1 &lt;- performance(pred_rocr1, measure = &quot;auc&quot;) AUCb[i] &lt;- auc_ROCR1@y.values[[1]] #AUCimb pred_rocr1 &lt;- prediction(phatimb[,2], test$Class) auc_ROCR1 &lt;- performance(pred_rocr1, measure = &quot;auc&quot;) AUCimb[i] &lt;- auc_ROCR1@y.values[[1]] } model &lt;- c(&quot;Balanced&quot;, &quot;Imbalanced&quot;) AUCs &lt;- c(mean(AUCb), mean(AUCimb)) sd &lt;- c(sqrt(var(AUCb)), sqrt(var(AUCimb))) data.frame(model, AUCs, sd) ## model AUCs sd ## 1 Balanced 0.9682024 0.02968400 ## 2 Imbalanced 0.9340113 0.03698929 Our results show improved AUC results and lower sd for our model built on the balanced data. "],["footnotes-and-citations.html", "Chapter 40 Footnotes and citations 40.1 Footnotes 40.2 Citations", " Chapter 40 Footnotes and citations 40.1 Footnotes Footnotes are put inside the square brackets after a caret ^[]. Like this one 12. 40.2 Citations Reference items in your bibliography file(s) using @key. For example, we are using the bookdown package (Xie 2023a) (check out the last code chunk in index.Rmd to see how this citation key was added) in this sample book, which was built on top of R Markdown and knitr (Xie 2015a) (this citation was added manually in an external file book.bib). Note that the .bib files need to be listed in the index.Rmd with the YAML bibliography key. The RStudio Visual Markdown Editor can also make it easier to insert citations: https://rstudio.github.io/visual-markdown-editing/#/citations References "],["blocks.html", "Chapter 41 Blocks 41.1 Equations 41.2 Theorems and proofs 41.3 Callout blocks", " Chapter 41 Blocks 41.1 Equations Here is an equation. \\[\\begin{equation} f\\left(k\\right) = \\binom{n}{k} p^k\\left(1-p\\right)^{n-k} \\tag{41.1} \\end{equation}\\] You may refer to using \\@ref(eq:binom), like see Equation (41.1). 41.2 Theorems and proofs Labeled theorems can be referenced in text using \\@ref(thm:tri), for example, check out this smart theorem 41.1. Theorem 41.1 For a right triangle, if \\(c\\) denotes the length of the hypotenuse and \\(a\\) and \\(b\\) denote the lengths of the other two sides, we have \\[a^2 + b^2 = c^2\\] Read more here https://bookdown.org/yihui/bookdown/markdown-extensions-by-bookdown.html. 41.3 Callout blocks The R Markdown Cookbook provides more help on how to use custom blocks to design your own callouts: https://bookdown.org/yihui/rmarkdown-cookbook/custom-blocks.html "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
