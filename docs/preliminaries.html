<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Preliminaries | Toolbox for Social Scientists and Policy Analysts</title>
  <meta name="description" content="Chapter 2 Preliminaries | Toolbox for Social Scientists and Policy Analysts" />
  <meta name="generator" content="bookdown 0.32.1 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Preliminaries | Toolbox for Social Scientists and Policy Analysts" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://yaydede.github.io/toolbox//png/cover2.png" />
  
  <meta name="github-repo" content="yaydede/ToolShed_draft" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Preliminaries | Toolbox for Social Scientists and Policy Analysts" />
  
  
  <meta name="twitter:image" content="https://yaydede.github.io/toolbox//png/cover2.png" />

<meta name="author" content="Yigit Aydede" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="intro.html"/>
<link rel="next" href="learning-systems.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Toolbox</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who"><i class="fa fa-check"></i>Who</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> How we define Machine Learning</a></li>
<li class="chapter" data-level="2" data-path="preliminaries.html"><a href="preliminaries.html"><i class="fa fa-check"></i><b>2</b> Preliminaries</a>
<ul>
<li class="chapter" data-level="2.1" data-path="preliminaries.html"><a href="preliminaries.html#data-and-dataset-types"><i class="fa fa-check"></i><b>2.1</b> Data and dataset types</a></li>
<li class="chapter" data-level="2.2" data-path="preliminaries.html"><a href="preliminaries.html#plots"><i class="fa fa-check"></i><b>2.2</b> Plots</a></li>
<li class="chapter" data-level="2.3" data-path="preliminaries.html"><a href="preliminaries.html#probability-distributions-with-r"><i class="fa fa-check"></i><b>2.3</b> Probability Distributions with R</a></li>
<li class="chapter" data-level="2.4" data-path="preliminaries.html"><a href="preliminaries.html#regressions"><i class="fa fa-check"></i><b>2.4</b> Regressions</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="preliminaries.html"><a href="preliminaries.html#ordinary-least-squares-ols"><i class="fa fa-check"></i><b>2.4.1</b> Ordinary Least Squares (OLS)</a></li>
<li class="chapter" data-level="2.4.2" data-path="preliminaries.html"><a href="preliminaries.html#maximum-likelihood-estimators"><i class="fa fa-check"></i><b>2.4.2</b> Maximum Likelihood Estimators</a></li>
<li class="chapter" data-level="2.4.3" data-path="preliminaries.html"><a href="preliminaries.html#estimating-mle-with-r"><i class="fa fa-check"></i><b>2.4.3</b> Estimating MLE with R</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="preliminaries.html"><a href="preliminaries.html#blue"><i class="fa fa-check"></i><b>2.5</b> BLUE</a></li>
<li class="chapter" data-level="2.6" data-path="preliminaries.html"><a href="preliminaries.html#modeling-the-data"><i class="fa fa-check"></i><b>2.6</b> Modeling the data</a></li>
<li class="chapter" data-level="2.7" data-path="preliminaries.html"><a href="preliminaries.html#causal-vs.-predictive-models"><i class="fa fa-check"></i><b>2.7</b> Causal vs. Predictive Models</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="preliminaries.html"><a href="preliminaries.html#causal-models"><i class="fa fa-check"></i><b>2.7.1</b> Causal Models</a></li>
<li class="chapter" data-level="2.7.2" data-path="preliminaries.html"><a href="preliminaries.html#predictive-models"><i class="fa fa-check"></i><b>2.7.2</b> Predictive Models</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="preliminaries.html"><a href="preliminaries.html#simulation"><i class="fa fa-check"></i><b>2.8</b> Simulation</a></li>
</ul></li>
<li class="part"><span><b>I Formal Look at Prediction</b></span></li>
<li class="chapter" data-level="" data-path="learning-systems.html"><a href="learning-systems.html"><i class="fa fa-check"></i>Learning Systems</a></li>
<li class="chapter" data-level="3" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html"><i class="fa fa-check"></i><b>3</b> Bias-Variance Tradeoff</a>
<ul>
<li class="chapter" data-level="3.1" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#estimator-and-mse"><i class="fa fa-check"></i><b>3.1</b> Estimator and MSE</a></li>
<li class="chapter" data-level="3.2" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#prediction---mspe"><i class="fa fa-check"></i><b>3.2</b> Prediction - MSPE</a></li>
<li class="chapter" data-level="3.3" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#biased-estimator-as-a-predictor"><i class="fa fa-check"></i><b>3.3</b> Biased estimator as a predictor</a></li>
<li class="chapter" data-level="3.4" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#dropping-a-variable-in-a-regression"><i class="fa fa-check"></i><b>3.4</b> Dropping a variable in a regression</a></li>
<li class="chapter" data-level="3.5" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#uncertainty-in-estimations-and-predictions"><i class="fa fa-check"></i><b>3.5</b> Uncertainty in estimations and predictions</a></li>
<li class="chapter" data-level="3.6" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#prediction-interval-for-unbiased-ols-predictor"><i class="fa fa-check"></i><b>3.6</b> Prediction interval for unbiased OLS predictor</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="overfitting.html"><a href="overfitting.html"><i class="fa fa-check"></i><b>4</b> Overfitting</a></li>
<li class="part"><span><b>II Nonparametric Estimations</b></span></li>
<li class="chapter" data-level="" data-path="parametric-vs.-nonparametric-methods.html"><a href="parametric-vs.-nonparametric-methods.html"><i class="fa fa-check"></i>Parametric vs. Nonparametric methods</a></li>
<li class="chapter" data-level="5" data-path="parametric-estimations.html"><a href="parametric-estimations.html"><i class="fa fa-check"></i><b>5</b> Parametric Estimations</a>
<ul>
<li class="chapter" data-level="5.1" data-path="parametric-estimations.html"><a href="parametric-estimations.html#linear-probability-models-lpm"><i class="fa fa-check"></i><b>5.1</b> Linear Probability Models (LPM)</a></li>
<li class="chapter" data-level="5.2" data-path="parametric-estimations.html"><a href="parametric-estimations.html#logistic-regression"><i class="fa fa-check"></i><b>5.2</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="parametric-estimations.html"><a href="parametric-estimations.html#estimating-logistic-regression"><i class="fa fa-check"></i><b>5.2.1</b> Estimating Logistic Regression</a></li>
<li class="chapter" data-level="5.2.2" data-path="parametric-estimations.html"><a href="parametric-estimations.html#cost-functions"><i class="fa fa-check"></i><b>5.2.2</b> Cost functions</a></li>
<li class="chapter" data-level="5.2.3" data-path="parametric-estimations.html"><a href="parametric-estimations.html#deviance"><i class="fa fa-check"></i><b>5.2.3</b> Deviance</a></li>
<li class="chapter" data-level="5.2.4" data-path="parametric-estimations.html"><a href="parametric-estimations.html#predictive-accuracy"><i class="fa fa-check"></i><b>5.2.4</b> Predictive accuracy</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html"><i class="fa fa-check"></i><b>6</b> Nonparametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="6.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#density-estimations"><i class="fa fa-check"></i><b>6.1</b> Density Estimations</a></li>
<li class="chapter" data-level="6.2" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#kernel-regressions"><i class="fa fa-check"></i><b>6.2</b> Kernel regressions</a></li>
<li class="chapter" data-level="6.3" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#regression-splines"><i class="fa fa-check"></i><b>6.3</b> Regression Splines</a></li>
<li class="chapter" data-level="6.4" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#mars---multivariate-adaptive-regression-splines"><i class="fa fa-check"></i><b>6.4</b> MARS - Multivariate Adaptive Regression Splines</a></li>
<li class="chapter" data-level="6.5" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#gam---generalized-additive-model"><i class="fa fa-check"></i><b>6.5</b> GAM - Generalized Additive Model</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>7</b> Smoothing</a>
<ul>
<li class="chapter" data-level="7.1" data-path="smoothing.html"><a href="smoothing.html#using-bins"><i class="fa fa-check"></i><b>7.1</b> Using bins</a></li>
<li class="chapter" data-level="7.2" data-path="smoothing.html"><a href="smoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>7.2</b> Kernel smoothing</a></li>
<li class="chapter" data-level="7.3" data-path="smoothing.html"><a href="smoothing.html#locally-weighted-regression-loess"><i class="fa fa-check"></i><b>7.3</b> Locally weighted regression <code>loess()</code></a></li>
<li class="chapter" data-level="7.4" data-path="smoothing.html"><a href="smoothing.html#smooth-spline-regression"><i class="fa fa-check"></i><b>7.4</b> Smooth Spline Regression</a></li>
<li class="chapter" data-level="7.5" data-path="smoothing.html"><a href="smoothing.html#multivariate-loess"><i class="fa fa-check"></i><b>7.5</b> Multivariate Loess</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html"><i class="fa fa-check"></i><b>8</b> Nonparametric Classifier - kNN</a>
<ul>
<li class="chapter" data-level="8.1" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#mnist-dataset"><i class="fa fa-check"></i><b>8.1</b> <code>mnist</code> Dataset</a></li>
<li class="chapter" data-level="8.2" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#linear-classifiers-again"><i class="fa fa-check"></i><b>8.2</b> Linear classifiers (again)</a></li>
<li class="chapter" data-level="8.3" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>8.3</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="8.4" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#knn-with-caret"><i class="fa fa-check"></i><b>8.4</b> kNN with caret</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#mnist_27"><i class="fa fa-check"></i><b>8.4.1</b> <code>mnist_27</code></a></li>
<li class="chapter" data-level="8.4.2" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#adult-dataset"><i class="fa fa-check"></i><b>8.4.2</b> Adult dataset</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Self-Learning</b></span></li>
<li class="chapter" data-level="9" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html"><i class="fa fa-check"></i><b>9</b> Hyperparameter Tuning</a>
<ul>
<li class="chapter" data-level="9.1" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#training-validation-and-test-datasets"><i class="fa fa-check"></i><b>9.1</b> Training, validation, and test datasets</a></li>
<li class="chapter" data-level="9.2" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#splitting-the-data-randomly"><i class="fa fa-check"></i><b>9.2</b> Splitting the data randomly</a></li>
<li class="chapter" data-level="9.3" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>9.3</b> k-fold cross validation</a></li>
<li class="chapter" data-level="9.4" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#cross-validated-grid-search"><i class="fa fa-check"></i><b>9.4</b> Cross-validated grid search</a></li>
<li class="chapter" data-level="9.5" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#bootstrapped-grid-search"><i class="fa fa-check"></i><b>9.5</b> Bootstrapped grid search</a></li>
<li class="chapter" data-level="9.6" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#when-the-data-is-time-series"><i class="fa fa-check"></i><b>9.6</b> When the data is time-series</a></li>
<li class="chapter" data-level="9.7" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#speed"><i class="fa fa-check"></i><b>9.7</b> Speed</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html"><i class="fa fa-check"></i><b>10</b> Tuning in Classification</a>
<ul>
<li class="chapter" data-level="10.1" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html#confusion-matrix"><i class="fa fa-check"></i><b>10.1</b> Confusion matrix</a></li>
<li class="chapter" data-level="10.2" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html#performance-measures"><i class="fa fa-check"></i><b>10.2</b> Performance measures</a></li>
<li class="chapter" data-level="10.3" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html#roc-curve"><i class="fa fa-check"></i><b>10.3</b> ROC Curve</a></li>
<li class="chapter" data-level="10.4" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html#auc---area-under-the-curve"><i class="fa fa-check"></i><b>10.4</b> AUC - Area Under the Curve</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="classification-example.html"><a href="classification-example.html"><i class="fa fa-check"></i><b>11</b> Classification Example</a>
<ul>
<li class="chapter" data-level="11.1" data-path="classification-example.html"><a href="classification-example.html#lpm"><i class="fa fa-check"></i><b>11.1</b> LPM</a></li>
<li class="chapter" data-level="11.2" data-path="classification-example.html"><a href="classification-example.html#logistic-regression-1"><i class="fa fa-check"></i><b>11.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="11.3" data-path="classification-example.html"><a href="classification-example.html#knn"><i class="fa fa-check"></i><b>11.3</b> kNN</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="classification-example.html"><a href="classification-example.html#knn-10-fold-cv"><i class="fa fa-check"></i><b>11.3.1</b> kNN 10-fold CV</a></li>
<li class="chapter" data-level="11.3.2" data-path="classification-example.html"><a href="classification-example.html#knn-with-caret-1"><i class="fa fa-check"></i><b>11.3.2</b> kNN with <code>caret</code></a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Tree-based Models</b></span></li>
<li class="chapter" data-level="12" data-path="cart.html"><a href="cart.html"><i class="fa fa-check"></i><b>12</b> CART</a>
<ul>
<li class="chapter" data-level="12.1" data-path="cart.html"><a href="cart.html#cart---classification-tree"><i class="fa fa-check"></i><b>12.1</b> CART - Classification Tree</a></li>
<li class="chapter" data-level="12.2" data-path="cart.html"><a href="cart.html#rpart---recursive-partitioning"><i class="fa fa-check"></i><b>12.2</b> <code>rpart</code> - Recursive Partitioning</a></li>
<li class="chapter" data-level="12.3" data-path="cart.html"><a href="cart.html#pruning"><i class="fa fa-check"></i><b>12.3</b> Pruning</a></li>
<li class="chapter" data-level="12.4" data-path="cart.html"><a href="cart.html#classification-with-titanic"><i class="fa fa-check"></i><b>12.4</b> Classification with Titanic</a></li>
<li class="chapter" data-level="12.5" data-path="cart.html"><a href="cart.html#regression-tree"><i class="fa fa-check"></i><b>12.5</b> Regression Tree</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ensemble-learning.html"><a href="ensemble-learning.html"><i class="fa fa-check"></i><b>13</b> Ensemble Learning</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ensemble-learning.html"><a href="ensemble-learning.html#bagging"><i class="fa fa-check"></i><b>13.1</b> Bagging</a></li>
<li class="chapter" data-level="13.2" data-path="ensemble-learning.html"><a href="ensemble-learning.html#random-forest"><i class="fa fa-check"></i><b>13.2</b> Random Forest</a></li>
<li class="chapter" data-level="13.3" data-path="ensemble-learning.html"><a href="ensemble-learning.html#boosting"><i class="fa fa-check"></i><b>13.3</b> Boosting</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="ensemble-learning.html"><a href="ensemble-learning.html#sequential-ensemble-with-gbm"><i class="fa fa-check"></i><b>13.3.1</b> Sequential ensemble with <code>gbm</code></a></li>
<li class="chapter" data-level="13.3.2" data-path="ensemble-learning.html"><a href="ensemble-learning.html#adaboost"><i class="fa fa-check"></i><b>13.3.2</b> AdaBoost</a></li>
<li class="chapter" data-level="13.3.3" data-path="ensemble-learning.html"><a href="ensemble-learning.html#xgboost"><i class="fa fa-check"></i><b>13.3.3</b> XGBoost</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ensemble-applications.html"><a href="ensemble-applications.html"><i class="fa fa-check"></i><b>14</b> Ensemble Applications</a>
<ul>
<li class="chapter" data-level="14.1" data-path="ensemble-applications.html"><a href="ensemble-applications.html#classification"><i class="fa fa-check"></i><b>14.1</b> Classification</a></li>
<li class="chapter" data-level="14.2" data-path="ensemble-applications.html"><a href="ensemble-applications.html#regression"><i class="fa fa-check"></i><b>14.2</b> Regression</a></li>
<li class="chapter" data-level="14.3" data-path="ensemble-applications.html"><a href="ensemble-applications.html#exploration"><i class="fa fa-check"></i><b>14.3</b> Exploration</a></li>
<li class="chapter" data-level="14.4" data-path="ensemble-applications.html"><a href="ensemble-applications.html#boosting-applications"><i class="fa fa-check"></i><b>14.4</b> Boosting Applications</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="ensemble-applications.html"><a href="ensemble-applications.html#regression-1"><i class="fa fa-check"></i><b>14.4.1</b> Regression</a></li>
<li class="chapter" data-level="14.4.2" data-path="ensemble-applications.html"><a href="ensemble-applications.html#random-search-with-parallel-processing"><i class="fa fa-check"></i><b>14.4.2</b> Random search with parallel processing</a></li>
<li class="chapter" data-level="14.4.3" data-path="ensemble-applications.html"><a href="ensemble-applications.html#boosting-vs.-others"><i class="fa fa-check"></i><b>14.4.3</b> Boosting vs. Others</a></li>
<li class="chapter" data-level="14.4.4" data-path="ensemble-applications.html"><a href="ensemble-applications.html#classification-1"><i class="fa fa-check"></i><b>14.4.4</b> Classification</a></li>
<li class="chapter" data-level="14.4.5" data-path="ensemble-applications.html"><a href="ensemble-applications.html#adaboost.m1"><i class="fa fa-check"></i><b>14.4.5</b> AdaBoost.M1</a></li>
<li class="chapter" data-level="14.4.6" data-path="ensemble-applications.html"><a href="ensemble-applications.html#classification-with-xgboost"><i class="fa fa-check"></i><b>14.4.6</b> Classification with XGBoost</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>V SVM &amp; Neural Networks</b></span></li>
<li class="chapter" data-level="15" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>15</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="15.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#optimal-separating-classifier"><i class="fa fa-check"></i><b>15.1</b> Optimal Separating Classifier</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#the-margin"><i class="fa fa-check"></i><b>15.1.1</b> The Margin</a></li>
<li class="chapter" data-level="15.1.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#the-non-separable-case"><i class="fa fa-check"></i><b>15.1.2</b> The Non-Separable Case</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#nonlinear-boundary-with-kernels"><i class="fa fa-check"></i><b>15.2</b> Nonlinear Boundary with Kernels</a></li>
<li class="chapter" data-level="15.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#application-with-svm"><i class="fa fa-check"></i><b>15.3</b> Application with SVM</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html"><i class="fa fa-check"></i><b>16</b> Artificial Neural Networks</a>
<ul>
<li class="chapter" data-level="16.1" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#neural-network---the-idea"><i class="fa fa-check"></i><b>16.1</b> Neural Network - the idea</a></li>
<li class="chapter" data-level="16.2" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#backpropagation"><i class="fa fa-check"></i><b>16.2</b> Backpropagation</a></li>
<li class="chapter" data-level="16.3" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#neural-network---more-inputs"><i class="fa fa-check"></i><b>16.3</b> Neural Network - More inputs</a></li>
<li class="chapter" data-level="16.4" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#deep-learning"><i class="fa fa-check"></i><b>16.4</b> Deep Learning</a></li>
</ul></li>
<li class="part"><span><b>VI Penalized Regressions</b></span></li>
<li class="chapter" data-level="" data-path="parametric-models-in-prediction.html"><a href="parametric-models-in-prediction.html"><i class="fa fa-check"></i>Parametric models in prediction</a></li>
<li class="chapter" data-level="17" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>17</b> Ridge</a></li>
<li class="chapter" data-level="18" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>18</b> Lasso</a></li>
<li class="chapter" data-level="19" data-path="adaptive-lasso.html"><a href="adaptive-lasso.html"><i class="fa fa-check"></i><b>19</b> Adaptive Lasso</a></li>
<li class="chapter" data-level="20" data-path="sparsity.html"><a href="sparsity.html"><i class="fa fa-check"></i><b>20</b> Sparsity</a></li>
<li class="part"><span><b>VII Time Series</b></span></li>
<li class="chapter" data-level="" data-path="forecasting.html"><a href="forecasting.html"><i class="fa fa-check"></i>Forecasting</a></li>
<li class="chapter" data-level="21" data-path="arima-models.html"><a href="arima-models.html"><i class="fa fa-check"></i><b>21</b> ARIMA models</a>
<ul>
<li class="chapter" data-level="21.1" data-path="arima-models.html"><a href="arima-models.html#hyndman-khandakar-algorithm"><i class="fa fa-check"></i><b>21.1</b> Hyndman-Khandakar algorithm</a></li>
<li class="chapter" data-level="21.2" data-path="arima-models.html"><a href="arima-models.html#ts-plots"><i class="fa fa-check"></i><b>21.2</b> TS Plots</a></li>
<li class="chapter" data-level="21.3" data-path="arima-models.html"><a href="arima-models.html#box-cox-transformation"><i class="fa fa-check"></i><b>21.3</b> Box-Cox transformation</a></li>
<li class="chapter" data-level="21.4" data-path="arima-models.html"><a href="arima-models.html#stationarity"><i class="fa fa-check"></i><b>21.4</b> Stationarity</a></li>
<li class="chapter" data-level="21.5" data-path="arima-models.html"><a href="arima-models.html#modeling-arima"><i class="fa fa-check"></i><b>21.5</b> Modeling ARIMA</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="grid-search-for-arima.html"><a href="grid-search-for-arima.html"><i class="fa fa-check"></i><b>22</b> Grid search for ARIMA</a></li>
<li class="chapter" data-level="23" data-path="time-series-embedding.html"><a href="time-series-embedding.html"><i class="fa fa-check"></i><b>23</b> Time Series Embedding</a>
<ul>
<li class="chapter" data-level="23.1" data-path="time-series-embedding.html"><a href="time-series-embedding.html#var-for-recursive-forecasting"><i class="fa fa-check"></i><b>23.1</b> VAR for Recursive Forecasting</a></li>
<li class="chapter" data-level="23.2" data-path="time-series-embedding.html"><a href="time-series-embedding.html#embedding-for-direct-forecast"><i class="fa fa-check"></i><b>23.2</b> Embedding for Direct Forecast</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="random-forest-1.html"><a href="random-forest-1.html"><i class="fa fa-check"></i><b>24</b> Random Forest</a>
<ul>
<li class="chapter" data-level="24.1" data-path="random-forest-1.html"><a href="random-forest-1.html#univariate"><i class="fa fa-check"></i><b>24.1</b> Univariate</a></li>
<li class="chapter" data-level="24.2" data-path="random-forest-1.html"><a href="random-forest-1.html#multivariate"><i class="fa fa-check"></i><b>24.2</b> Multivariate</a></li>
<li class="chapter" data-level="24.3" data-path="random-forest-1.html"><a href="random-forest-1.html#rolling-and-expanding-windows"><i class="fa fa-check"></i><b>24.3</b> Rolling and expanding windows</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html"><i class="fa fa-check"></i><b>25</b> Recurrent Neural Networks</a>
<ul>
<li class="chapter" data-level="25.1" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#keras"><i class="fa fa-check"></i><b>25.1</b> Keras</a></li>
<li class="chapter" data-level="25.2" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#input-tensors"><i class="fa fa-check"></i><b>25.2</b> Input Tensors</a></li>
<li class="chapter" data-level="25.3" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#plain-rnn"><i class="fa fa-check"></i><b>25.3</b> Plain RNN</a></li>
<li class="chapter" data-level="25.4" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#lstm"><i class="fa fa-check"></i><b>25.4</b> LSTM</a></li>
</ul></li>
<li class="part"><span><b>VIII Dimension Reduction Methods</b></span></li>
<li class="chapter" data-level="" data-path="matrix-decompositions.html"><a href="matrix-decompositions.html"><i class="fa fa-check"></i>Matrix Decompositions</a></li>
<li class="chapter" data-level="26" data-path="eigenvectors-and-eigenvalues.html"><a href="eigenvectors-and-eigenvalues.html"><i class="fa fa-check"></i><b>26</b> Eigenvectors and eigenvalues</a></li>
<li class="chapter" data-level="27" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html"><i class="fa fa-check"></i><b>27</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="28" data-path="rankr-approximations.html"><a href="rankr-approximations.html"><i class="fa fa-check"></i><b>28</b> Rank(r) Approximations</a></li>
<li class="chapter" data-level="29" data-path="moore-penrose-inverse.html"><a href="moore-penrose-inverse.html"><i class="fa fa-check"></i><b>29</b> Moore-Penrose inverse</a></li>
<li class="chapter" data-level="30" data-path="principle-component-analysis.html"><a href="principle-component-analysis.html"><i class="fa fa-check"></i><b>30</b> Principle Component Analysis</a></li>
<li class="chapter" data-level="31" data-path="factor-analysis.html"><a href="factor-analysis.html"><i class="fa fa-check"></i><b>31</b> Factor Analysis</a></li>
<li class="part"><span><b>IX Network Analysis</b></span></li>
<li class="chapter" data-level="" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html"><i class="fa fa-check"></i>Graphical Network Analysis</a></li>
<li class="chapter" data-level="32" data-path="fundementals.html"><a href="fundementals.html"><i class="fa fa-check"></i><b>32</b> Fundementals</a>
<ul>
<li class="chapter" data-level="32.1" data-path="fundementals.html"><a href="fundementals.html#covariance"><i class="fa fa-check"></i><b>32.1</b> Covariance</a></li>
<li class="chapter" data-level="32.2" data-path="fundementals.html"><a href="fundementals.html#correlation"><i class="fa fa-check"></i><b>32.2</b> Correlation</a></li>
<li class="chapter" data-level="32.3" data-path="fundementals.html"><a href="fundementals.html#precision-matrix"><i class="fa fa-check"></i><b>32.3</b> Precision Matrix</a></li>
<li class="chapter" data-level="32.4" data-path="fundementals.html"><a href="fundementals.html#semi-partial-correlation"><i class="fa fa-check"></i><b>32.4</b> Semi-partial Correlation</a></li>
</ul></li>
<li class="chapter" data-level="33" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html"><i class="fa fa-check"></i><b>33</b> Regularized Covariance Matrix</a>
<ul>
<li class="chapter" data-level="33.1" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html#multivariate-gaussian-distribution"><i class="fa fa-check"></i><b>33.1</b> Multivariate Gaussian Distribution</a></li>
<li class="chapter" data-level="33.2" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html#high-dimensional-data"><i class="fa fa-check"></i><b>33.2</b> High-dimensional data</a></li>
<li class="chapter" data-level="33.3" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html#ridge-ell_2-and-glasso-ell_1"><i class="fa fa-check"></i><b>33.3</b> Ridge (<span class="math inline">\(\ell_{2}\)</span>) and glasso (<span class="math inline">\(\ell_{1}\)</span>)</a></li>
</ul></li>
<li class="part"><span><b>X Labs</b></span></li>
<li class="chapter" data-level="34" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html"><i class="fa fa-check"></i><b>34</b> R Lab 1 - Basics I</a>
<ul>
<li class="chapter" data-level="34.1" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#r-rstudio-and-r-packages"><i class="fa fa-check"></i><b>34.1</b> R, RStudio, and R Packages</a></li>
<li class="chapter" data-level="34.2" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#rstudio"><i class="fa fa-check"></i><b>34.2</b> RStudio</a></li>
<li class="chapter" data-level="34.3" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#working-directory"><i class="fa fa-check"></i><b>34.3</b> Working directory</a></li>
<li class="chapter" data-level="34.4" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#data-types-and-stuctures"><i class="fa fa-check"></i><b>34.4</b> Data Types and Stuctures</a></li>
<li class="chapter" data-level="34.5" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#vectors"><i class="fa fa-check"></i><b>34.5</b> Vectors</a></li>
<li class="chapter" data-level="34.6" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#subsetting-vectors"><i class="fa fa-check"></i><b>34.6</b> Subsetting Vectors</a></li>
<li class="chapter" data-level="34.7" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#vectorization-or-vector-operations"><i class="fa fa-check"></i><b>34.7</b> Vectorization or vector operations</a></li>
<li class="chapter" data-level="34.8" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#matrices"><i class="fa fa-check"></i><b>34.8</b> Matrices</a></li>
<li class="chapter" data-level="34.9" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#matrix-operations"><i class="fa fa-check"></i><b>34.9</b> Matrix Operations</a></li>
<li class="chapter" data-level="34.10" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#subsetting-matrix"><i class="fa fa-check"></i><b>34.10</b> Subsetting Matrix</a></li>
<li class="chapter" data-level="34.11" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#r-style-guide"><i class="fa fa-check"></i><b>34.11</b> R-Style Guide</a></li>
</ul></li>
<li class="chapter" data-level="35" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html"><i class="fa fa-check"></i><b>35</b> R Lab 2 - Basics II</a>
<ul>
<li class="chapter" data-level="35.1" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#data-frames-and-lists"><i class="fa fa-check"></i><b>35.1</b> Data frames and lists</a>
<ul>
<li class="chapter" data-level="35.1.1" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#lists"><i class="fa fa-check"></i><b>35.1.1</b> Lists</a></li>
<li class="chapter" data-level="35.1.2" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#data-frames"><i class="fa fa-check"></i><b>35.1.2</b> Data Frames</a></li>
<li class="chapter" data-level="35.1.3" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#reading-importing-and-writting-exporting-data-files"><i class="fa fa-check"></i><b>35.1.3</b> Reading (importing) and writting (exporting) data files</a></li>
<li class="chapter" data-level="35.1.4" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#subsetting-data-frames"><i class="fa fa-check"></i><b>35.1.4</b> Subsetting Data Frames</a></li>
<li class="chapter" data-level="35.1.5" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#plotting-from-data-frame"><i class="fa fa-check"></i><b>35.1.5</b> Plotting from data frame</a></li>
<li class="chapter" data-level="35.1.6" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#some-useful-functions"><i class="fa fa-check"></i><b>35.1.6</b> Some useful functions</a></li>
<li class="chapter" data-level="35.1.7" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#categorical-variables-in-data-frames"><i class="fa fa-check"></i><b>35.1.7</b> Categorical Variables in Data Frames</a></li>
</ul></li>
<li class="chapter" data-level="35.2" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#programming-basics"><i class="fa fa-check"></i><b>35.2</b> Programming Basics</a>
<ul>
<li class="chapter" data-level="35.2.1" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#ifelse"><i class="fa fa-check"></i><b>35.2.1</b> if/Else</a></li>
<li class="chapter" data-level="35.2.2" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#loops"><i class="fa fa-check"></i><b>35.2.2</b> Loops</a></li>
<li class="chapter" data-level="35.2.3" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#the-apply-family"><i class="fa fa-check"></i><b>35.2.3</b> The <code>apply()</code> family</a></li>
<li class="chapter" data-level="35.2.4" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#functions"><i class="fa fa-check"></i><b>35.2.4</b> Functions</a></li>
<li class="chapter" data-level="35.2.5" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#dplyr"><i class="fa fa-check"></i><b>35.2.5</b> <code>dplyr()</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="36" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html"><i class="fa fa-check"></i><b>36</b> R Lab 3 - Preparing the data</a>
<ul>
<li class="chapter" data-level="36.1" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#preparing-the-data-for-a-regression-analysis-with-lm"><i class="fa fa-check"></i><b>36.1</b> Preparing the data for a regression analysis with <code>lm()</code></a>
<ul>
<li class="chapter" data-level="36.1.1" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#factor-variables"><i class="fa fa-check"></i><b>36.1.1</b> Factor variables</a></li>
<li class="chapter" data-level="36.1.2" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#dummy-coding"><i class="fa fa-check"></i><b>36.1.2</b> Dummy Coding</a></li>
<li class="chapter" data-level="36.1.3" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#column-variable-names"><i class="fa fa-check"></i><b>36.1.3</b> Column (Variable) names</a></li>
<li class="chapter" data-level="36.1.4" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#data-subsetting-and-missing-values"><i class="fa fa-check"></i><b>36.1.4</b> Data subsetting and missing values</a></li>
</ul></li>
<li class="chapter" data-level="36.2" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#dummy-variable-models"><i class="fa fa-check"></i><b>36.2</b> “DUMMY” variable models</a>
<ul>
<li class="chapter" data-level="36.2.1" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#mtcars-example"><i class="fa fa-check"></i><b>36.2.1</b> <code>mtcars</code> example</a></li>
<li class="chapter" data-level="36.2.2" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#model.matrix"><i class="fa fa-check"></i><b>36.2.2</b> <code>model.matrix()</code></a></li>
<li class="chapter" data-level="36.2.3" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#example-with-a-bigger-data-set-autompg"><i class="fa fa-check"></i><b>36.2.3</b> Example with a bigger data set: <code>Autompg</code></a></li>
<li class="chapter" data-level="36.2.4" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#some-more-data-management-tools-for-subsetting-complete.cases-is.na-and-within"><i class="fa fa-check"></i><b>36.2.4</b> Some more data management tools for subsetting: <code>complete.cases()</code>, <code>is.na()</code>, and <code>within()</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="37" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html"><i class="fa fa-check"></i><b>37</b> R Lab 4 - Simulation in R</a>
<ul>
<li class="chapter" data-level="37.1" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#sampling-in-r-sample"><i class="fa fa-check"></i><b>37.1</b> Sampling in R: <code>sample()</code></a></li>
<li class="chapter" data-level="37.2" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#random-number-generating-with-probablity-distributions"><i class="fa fa-check"></i><b>37.2</b> Random number generating with probablity distributions</a></li>
<li class="chapter" data-level="37.3" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#simulation-for-statistical-inference"><i class="fa fa-check"></i><b>37.3</b> Simulation for statistical inference</a></li>
<li class="chapter" data-level="37.4" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#creataing-data-with-a-data-generating-model-dgm"><i class="fa fa-check"></i><b>37.4</b> Creataing data with a Data Generating Model (DGM)</a></li>
<li class="chapter" data-level="37.5" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#bootstrapping"><i class="fa fa-check"></i><b>37.5</b> Bootstrapping</a></li>
<li class="chapter" data-level="37.6" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#monty-hall---fun-example"><i class="fa fa-check"></i><b>37.6</b> Monty Hall - Fun example</a></li>
</ul></li>
<li class="part"><span><b>XI Appendix</b></span></li>
<li class="chapter" data-level="38" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html"><i class="fa fa-check"></i><b>38</b> Algorithmic Optimization</a>
<ul>
<li class="chapter" data-level="38.1" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#brute-force-optimization"><i class="fa fa-check"></i><b>38.1</b> Brute-force optimization</a></li>
<li class="chapter" data-level="38.2" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#derivative-based-methods"><i class="fa fa-check"></i><b>38.2</b> Derivative-based methods</a></li>
<li class="chapter" data-level="38.3" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#ml-estimation-with-logistic-regression"><i class="fa fa-check"></i><b>38.3</b> ML Estimation with logistic regression</a></li>
<li class="chapter" data-level="38.4" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#gradient-descent-algorithm"><i class="fa fa-check"></i><b>38.4</b> Gradient Descent Algorithm</a>
<ul>
<li class="chapter" data-level="38.4.1" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#one-variable"><i class="fa fa-check"></i><b>38.4.1</b> One-variable</a></li>
<li class="chapter" data-level="38.4.2" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#multivariable"><i class="fa fa-check"></i><b>38.4.2</b> Multivariable</a></li>
</ul></li>
<li class="chapter" data-level="38.5" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#optimization-with-r"><i class="fa fa-check"></i><b>38.5</b> Optimization with R</a></li>
</ul></li>
<li class="chapter" data-level="39" data-path="imbalanced-data.html"><a href="imbalanced-data.html"><i class="fa fa-check"></i><b>39</b> Imbalanced Data</a>
<ul>
<li class="chapter" data-level="39.1" data-path="imbalanced-data.html"><a href="imbalanced-data.html#smote"><i class="fa fa-check"></i><b>39.1</b> <code>SMOTE</code></a></li>
<li class="chapter" data-level="39.2" data-path="imbalanced-data.html"><a href="imbalanced-data.html#fraud-detection"><i class="fa fa-check"></i><b>39.2</b> Fraud detection</a></li>
</ul></li>
<li class="chapter" data-level="40" data-path="footnotes-and-citations.html"><a href="footnotes-and-citations.html"><i class="fa fa-check"></i><b>40</b> Footnotes and citations</a>
<ul>
<li class="chapter" data-level="40.1" data-path="footnotes-and-citations.html"><a href="footnotes-and-citations.html#footnotes"><i class="fa fa-check"></i><b>40.1</b> Footnotes</a></li>
<li class="chapter" data-level="40.2" data-path="footnotes-and-citations.html"><a href="footnotes-and-citations.html#citations"><i class="fa fa-check"></i><b>40.2</b> Citations</a></li>
</ul></li>
<li class="chapter" data-level="41" data-path="blocks.html"><a href="blocks.html"><i class="fa fa-check"></i><b>41</b> Blocks</a>
<ul>
<li class="chapter" data-level="41.1" data-path="blocks.html"><a href="blocks.html#equations"><i class="fa fa-check"></i><b>41.1</b> Equations</a></li>
<li class="chapter" data-level="41.2" data-path="blocks.html"><a href="blocks.html#theorems-and-proofs"><i class="fa fa-check"></i><b>41.2</b> Theorems and proofs</a></li>
<li class="chapter" data-level="41.3" data-path="blocks.html"><a href="blocks.html#callout-blocks"><i class="fa fa-check"></i><b>41.3</b> Callout blocks</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/yaydede/toolbox" target="blank"> 2023 Yigit Aydede - Bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Toolbox for Social Scientists and Policy Analysts</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="preliminaries" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Preliminaries<a href="preliminaries.html#preliminaries" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>We will start with reviewing some basic concepts in statistics. For now, don’t stress about understanding the code chunks. As you study and practice R more (See R Labs at the end of the book), the codes will become clearer.</p>
<div id="data-and-dataset-types" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Data and dataset types<a href="preliminaries.html#data-and-dataset-types" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>R has a number of basic data types.</p>
<ul>
<li><strong>Numeric</strong> : Also known as Double. The default type when dealing with numbers. 1,1.0,42.5<br />
</li>
<li><strong>Integer</strong>: 1L,2L,42L<br />
</li>
<li><strong>Complex</strong>: 4 + 2i<br />
</li>
<li><strong>Logical</strong>: Two possible values: TRUE and FALSE. <code>NA</code> is also considered logical.<br />
</li>
<li><strong>Character</strong>: “a”,“Statistics”,“1plus2.”</li>
</ul>
<p>Data can also be classified as <strong>numeric</strong> (what’s your age?) and <strong>categorical</strong> (Do you have a car?)</p>
<p>R also has a number of basic data (“container”) structures. A data structure is either <strong>homogeneous</strong> (all elements are of the same data type) or <strong>heterogeneous</strong> (elements can be of more than one data type): You can think each data structure as <strong>data container</strong> where your data are stored. Here are the main “container” or data structures:</p>
<ul>
<li><strong>Vector</strong>: One dimension (column or row) and homogeneous. That is, every element of a vector has to be the same type. Each vector can be thought of as a variable.</li>
<li><strong>Matrix</strong>: Two dimensions (columns and rows) and homogeneous. That is, every element of a matrix has to be the same type.</li>
<li><strong>Data Frame</strong>: Two dimensions (columns and rows) and heterogeneous. That is, every column of a data frame doesn’t have to be the same type. This is the main difference between a matrix and a data frame. Data frames are the most common data structures in any data analysis.</li>
<li><strong>List</strong>: One dimension and heterogeneous. A list can have multiple data structures.</li>
<li><strong>Array</strong>: 3+ dimensions and homogeneous.</li>
</ul>
<p>In this book, we most frequently work with data frames.</p>
<p>When using data, there are three things we like to do:</p>
<ul>
<li>Look at the raw data.<br />
</li>
<li>Understand the data. (What are the variables and their types?)<br />
</li>
<li>Visualize the data.</li>
</ul>
<p>To look at the data, we have two useful commands: <code>head()</code> (<code>tail()</code>) and <code>str()</code>. As seen in the following examples, <code>head()</code> and <code>tail()</code> allow us to see the first few data points in a dataset. <code>str()</code> allows us to see the structure of the data, including what data types are used. Using <code>str()</code> we can identify the <code>mtcars</code> dataset, which includes only numeric variables and is structured in a data frame.</p>
<p><strong>Cross-Sectional</strong>
In cross-sectional data, we have one dimension: subjects. Since the order of observations are not important, we can shuffle the data.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="preliminaries.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(datasets)</span>
<span id="cb1-2"><a href="preliminaries.html#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="preliminaries.html#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(mtcars)</span></code></pre></div>
<pre><code>##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1</code></pre>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="preliminaries.html#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(mtcars)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    32 obs. of  11 variables:
##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...
##  $ disp: num  160 160 108 258 360 ...
##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...
##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...
##  $ qsec: num  16.5 17 18.6 19.4 17 ...
##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...
##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...
##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...
##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...</code></pre>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="preliminaries.html#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mtcars)</span></code></pre></div>
<pre><code>##       mpg             cyl             disp             hp       
##  Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  
##  1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  
##  Median :19.20   Median :6.000   Median :196.3   Median :123.0  
##  Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  
##  3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  
##  Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  
##       drat             wt             qsec             vs        
##  Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  
##  1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  
##  Median :3.695   Median :3.325   Median :17.71   Median :0.0000  
##  Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  
##  3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  
##  Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  
##        am              gear            carb      
##  Min.   :0.0000   Min.   :3.000   Min.   :1.000  
##  1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  
##  Median :0.0000   Median :4.000   Median :2.000  
##  Mean   :0.4062   Mean   :3.688   Mean   :2.812  
##  3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  
##  Max.   :1.0000   Max.   :5.000   Max.   :8.000</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="preliminaries.html#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mtcars[,<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">4</span>)])</span></code></pre></div>
<p><img src="02-Preliminaries_files/figure-html/s1-1.png" width="672" /></p>
<p><strong>Time-series</strong><br />
Time-series data have also one dimension with subject followed through the time. We cannot shuffle the data, as it follows a sequential order.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="preliminaries.html#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(airquality)</span></code></pre></div>
<pre><code>##   Ozone Solar.R Wind Temp Month Day
## 1    41     190  7.4   67     5   1
## 2    36     118  8.0   72     5   2
## 3    12     149 12.6   74     5   3
## 4    18     313 11.5   62     5   4
## 5    NA      NA 14.3   56     5   5
## 6    28      NA 14.9   66     5   6</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="preliminaries.html#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(airquality)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    153 obs. of  6 variables:
##  $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...
##  $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...
##  $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...
##  $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...
##  $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...
##  $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...</code></pre>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="preliminaries.html#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(airquality)</span></code></pre></div>
<pre><code>##      Ozone           Solar.R           Wind             Temp      
##  Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  
##  1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  
##  Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  
##  Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  
##  3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  
##  Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  
##  NA&#39;s   :37       NA&#39;s   :7                                       
##      Month            Day      
##  Min.   :5.000   Min.   : 1.0  
##  1st Qu.:6.000   1st Qu.: 8.0  
##  Median :7.000   Median :16.0  
##  Mean   :6.993   Mean   :15.8  
##  3rd Qu.:8.000   3rd Qu.:23.0  
##  Max.   :9.000   Max.   :31.0  
## </code></pre>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="preliminaries.html#cb14-1" aria-hidden="true" tabindex="-1"></a>airquality<span class="sc">$</span>date <span class="ot">&lt;-</span> airquality<span class="sc">$</span>Month<span class="sc">*</span><span class="dv">10</span><span class="sc">+</span>airquality<span class="sc">$</span>Day</span>
<span id="cb14-2"><a href="preliminaries.html#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(airquality<span class="sc">$</span>date, airquality<span class="sc">$</span>Ozone)</span></code></pre></div>
<p><img src="02-Preliminaries_files/figure-html/stat3-1.png" width="672" /></p>
<p>This was just a simple time-series data presentation. In Part VII, we will see more advance ways to handle time-series data. Moreover, dates and times can be handled by various packages/functions in R, like <code>lubridate</code> (see <a href="https://lubridate.tidyverse.org" class="uri">https://lubridate.tidyverse.org</a>).</p>
<p>Here are some examples:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="preliminaries.html#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lubridate)</span>
<span id="cb15-2"><a href="preliminaries.html#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="co"># get current system date</span></span>
<span id="cb15-3"><a href="preliminaries.html#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="fu">Sys.Date</span>() </span></code></pre></div>
<pre><code>## [1] &quot;2023-03-12&quot;</code></pre>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="preliminaries.html#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get current system time</span></span>
<span id="cb17-2"><a href="preliminaries.html#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">Sys.time</span>()</span></code></pre></div>
<pre><code>## [1] &quot;2023-03-12 15:13:51 ADT&quot;</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="preliminaries.html#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co">#lubridate</span></span>
<span id="cb19-2"><a href="preliminaries.html#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">now</span>()</span></code></pre></div>
<pre><code>## [1] &quot;2023-03-12 15:13:51 ADT&quot;</code></pre>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="preliminaries.html#cb21-1" aria-hidden="true" tabindex="-1"></a>dates <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;2022-07-11&quot;</span>, <span class="st">&quot;2012-04-19&quot;</span>, <span class="st">&quot;2017-03-08&quot;</span>)</span>
<span id="cb21-2"><a href="preliminaries.html#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="preliminaries.html#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="co"># extract years from dates</span></span>
<span id="cb21-4"><a href="preliminaries.html#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="fu">year</span>(dates)</span></code></pre></div>
<pre><code>## [1] 2022 2012 2017</code></pre>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="preliminaries.html#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># extract months from dates</span></span>
<span id="cb23-2"><a href="preliminaries.html#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="fu">month</span>(dates)</span></code></pre></div>
<pre><code>## [1] 7 4 3</code></pre>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="preliminaries.html#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># extract days from dates</span></span>
<span id="cb25-2"><a href="preliminaries.html#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mday</span>(dates)</span></code></pre></div>
<pre><code>## [1] 11 19  8</code></pre>
<p><strong>Panel</strong><br />
Panel data (also known as longitudinal data) is the richest data type as it includes information fro multiple entities observed across time. These entities could be people, households, countries, firms, etc. Note in the example below, we use the <code>plm</code> <a href="https://cran.r-project.org/web/packages/plm/vignettes/A_plmPackage.html">package</a> which has several data sets. The first one, <code>EmplUK</code>, is summarized below:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="preliminaries.html#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(foreign)</span>
<span id="cb27-2"><a href="preliminaries.html#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(plm)</span>
<span id="cb27-3"><a href="preliminaries.html#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;EmplUK&quot;</span>, <span class="at">package=</span><span class="st">&quot;plm&quot;</span>)</span>
<span id="cb27-4"><a href="preliminaries.html#cb27-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-5"><a href="preliminaries.html#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(EmplUK, <span class="dv">15</span>)</span></code></pre></div>
<pre><code>##    firm year sector    emp    wage capital   output
## 1     1 1977      7  5.041 13.1516  0.5894  95.7072
## 2     1 1978      7  5.600 12.3018  0.6318  97.3569
## 3     1 1979      7  5.015 12.8395  0.6771  99.6083
## 4     1 1980      7  4.715 13.8039  0.6171 100.5501
## 5     1 1981      7  4.093 14.2897  0.5076  99.5581
## 6     1 1982      7  3.166 14.8681  0.4229  98.6151
## 7     1 1983      7  2.936 13.7784  0.3920 100.0301
## 8     2 1977      7 71.319 14.7909 16.9363  95.7072
## 9     2 1978      7 70.643 14.1036 17.2422  97.3569
## 10    2 1979      7 70.918 14.9534 17.5413  99.6083
## 11    2 1980      7 72.031 15.4910 17.6574 100.5501
## 12    2 1981      7 73.689 16.1969 16.7133  99.5581
## 13    2 1982      7 72.419 16.1314 16.2469  98.6151
## 14    2 1983      7 68.518 16.3051 17.3696 100.0301
## 15    3 1977      7 19.156 22.6920  7.0975  95.7072</code></pre>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="preliminaries.html#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(<span class="fu">unique</span>(EmplUK<span class="sc">$</span>firm))</span></code></pre></div>
<pre><code>## [1] 140</code></pre>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="preliminaries.html#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(EmplUK<span class="sc">$</span>year)</span></code></pre></div>
<pre><code>## 
## 1976 1977 1978 1979 1980 1981 1982 1983 1984 
##   80  138  140  140  140  140  140   78   35</code></pre>
<p>As you can see, we have 140 unique subjects (firms) each of which is observed between 1977 and 1983. However, there are some firms with missing years. This type of panel data is called as “unbalanced panel”.</p>
</div>
<div id="plots" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Plots<a href="preliminaries.html#plots" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Often, a proper visualization can illuminate features of the data that can inform further analysis. We will look at four methods of visualizing data that we will use throughout the book:</p>
<ul>
<li><strong>Histograms</strong><br />
</li>
<li><strong>Barplots</strong><br />
</li>
<li><strong>Boxplots</strong><br />
</li>
<li><strong>Scatterplots</strong></li>
</ul>
<p>We can use the data <code>mpg</code> provided by the <code>ggplot2</code> package. To begin, we can get a sense of the data by looking at the first few data points and some summary statistics.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="preliminaries.html#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb33-2"><a href="preliminaries.html#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(mpg, <span class="dv">5</span>)</span></code></pre></div>
<pre><code>## # A tibble: 5 × 11
##   manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class 
##   &lt;chr&gt;        &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; 
## 1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa…
## 2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa…
## 3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa…
## 4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa…
## 5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p     compa…</code></pre>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="preliminaries.html#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tail</span>(mpg, <span class="dv">5</span>)</span></code></pre></div>
<pre><code>## # A tibble: 5 × 11
##   manufacturer model  displ  year   cyl trans      drv     cty   hwy fl    class
##   &lt;chr&gt;        &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;
## 1 volkswagen   passat   2    2008     4 auto(s6)   f        19    28 p     mids…
## 2 volkswagen   passat   2    2008     4 manual(m6) f        21    29 p     mids…
## 3 volkswagen   passat   2.8  1999     6 auto(l5)   f        16    26 p     mids…
## 4 volkswagen   passat   2.8  1999     6 manual(m5) f        18    26 p     mids…
## 5 volkswagen   passat   3.6  2008     6 auto(s6)   f        17    26 p     mids…</code></pre>
<p>When visualizing a single numerical variable, a histogram would be very handy:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="preliminaries.html#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(mpg<span class="sc">$</span>cty, <span class="at">xlab =</span> <span class="st">&quot;Miles Per Gallon (City)&quot;</span>,</span>
<span id="cb37-2"><a href="preliminaries.html#cb37-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Histogram of MPG (City)&quot;</span>, <span class="at">breaks =</span> <span class="dv">12</span>,</span>
<span id="cb37-3"><a href="preliminaries.html#cb37-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">&quot;dodgerblue&quot;</span>,<span class="at">cex.main=</span><span class="dv">1</span>, <span class="at">cex.lab=</span>.<span class="dv">75</span>, <span class="at">cex.axis=</span><span class="fl">0.75</span>,</span>
<span id="cb37-4"><a href="preliminaries.html#cb37-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">border =</span> <span class="st">&quot;darkorange&quot;</span>)</span></code></pre></div>
<p><img src="02-Preliminaries_files/figure-html/s6-1.png" width="672" /></p>
<p>Similar to a histogram, a barplot provides a visual summary of a categorical variable, or a numeric variable with a finite number of values, like a ranking from 1 to 10.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="preliminaries.html#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="fu">barplot</span>(<span class="fu">table</span>(mpg<span class="sc">$</span>drv), </span>
<span id="cb38-2"><a href="preliminaries.html#cb38-2" aria-hidden="true" tabindex="-1"></a>        <span class="at">xlab =</span> <span class="st">&quot;Drivetrain (f = FWD, r = RWD, 4 = 4WD)&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Frequency&quot;</span>,</span>
<span id="cb38-3"><a href="preliminaries.html#cb38-3" aria-hidden="true" tabindex="-1"></a>        <span class="at">main =</span> <span class="st">&quot;Drivetrains&quot;</span>,</span>
<span id="cb38-4"><a href="preliminaries.html#cb38-4" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="st">&quot;dodgerblue&quot;</span>,<span class="at">cex.main=</span><span class="dv">1</span>, <span class="at">cex.lab=</span>.<span class="dv">75</span>, <span class="at">cex.axis=</span><span class="fl">0.75</span>,</span>
<span id="cb38-5"><a href="preliminaries.html#cb38-5" aria-hidden="true" tabindex="-1"></a>        <span class="at">border =</span> <span class="st">&quot;darkorange&quot;</span>)</span></code></pre></div>
<p><img src="02-Preliminaries_files/figure-html/s77-1.png" width="672" /></p>
<p>To visualize the relationship between a numerical and categorical variable, we will use a boxplot. In the <code>mpg</code> dataset, the <code>drv</code> few categories: front-wheel drive, 4-wheel drive, or rear-wheel drive.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="preliminaries.html#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(hwy <span class="sc">~</span> drv, <span class="at">data =</span> mpg,</span>
<span id="cb39-2"><a href="preliminaries.html#cb39-2" aria-hidden="true" tabindex="-1"></a>        <span class="at">xlab =</span> <span class="st">&quot;Drivetrain (f = FWD, r = RWD, 4 = 4WD)&quot;</span>,</span>
<span id="cb39-3"><a href="preliminaries.html#cb39-3" aria-hidden="true" tabindex="-1"></a>        <span class="at">ylab =</span> <span class="st">&quot;Miles Per Gallon (Highway)&quot;</span>,</span>
<span id="cb39-4"><a href="preliminaries.html#cb39-4" aria-hidden="true" tabindex="-1"></a>        <span class="at">main =</span> <span class="st">&quot;MPG (Highway) vs Drivetrain&quot;</span>,</span>
<span id="cb39-5"><a href="preliminaries.html#cb39-5" aria-hidden="true" tabindex="-1"></a>        <span class="at">pch =</span> <span class="dv">20</span>, <span class="at">cex =</span><span class="dv">2</span>,<span class="at">cex.main=</span><span class="dv">1</span>, <span class="at">cex.lab=</span>.<span class="dv">75</span>, <span class="at">cex.axis=</span><span class="fl">0.75</span>,</span>
<span id="cb39-6"><a href="preliminaries.html#cb39-6" aria-hidden="true" tabindex="-1"></a>        <span class="at">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="at">border =</span> <span class="st">&quot;dodgerblue&quot;</span>)</span></code></pre></div>
<p><img src="02-Preliminaries_files/figure-html/s8-1.png" width="672" /></p>
<p>Finally, to visualize the relationship between two numeric variables we will use a scatterplot.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="preliminaries.html#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hwy <span class="sc">~</span> displ, <span class="at">data =</span> mpg,</span>
<span id="cb40-2"><a href="preliminaries.html#cb40-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Engine Displacement (in Liters)&quot;</span>,</span>
<span id="cb40-3"><a href="preliminaries.html#cb40-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Miles Per Gallon (Highway)&quot;</span>,</span>
<span id="cb40-4"><a href="preliminaries.html#cb40-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;MPG (Highway) vs Engine Displacement&quot;</span>,</span>
<span id="cb40-5"><a href="preliminaries.html#cb40-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">20</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">cex.main=</span><span class="dv">1</span>, <span class="at">cex.lab=</span>.<span class="dv">75</span>, <span class="at">cex.axis=</span><span class="fl">0.75</span>,</span>
<span id="cb40-6"><a href="preliminaries.html#cb40-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">&quot;dodgerblue&quot;</span>)</span></code></pre></div>
<p><img src="02-Preliminaries_files/figure-html/s9-1.png" width="672" /></p>
<p>While visualization is not enough to draw definitive conclusions, it can help us identify insights about data. R is well-know for its graphical capabilities. The package <code>ggplot</code> is the main tool for more advance graphical representations that we will see later.</p>
</div>
<div id="probability-distributions-with-r" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Probability Distributions with R<a href="preliminaries.html#probability-distributions-with-r" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We often want to make probabilistic statements based on a distribution. Typically, we will want to know one of four things:</p>
<ul>
<li>The probability density function (pdf) at a particular value.<br />
</li>
<li>The cumulative probability distribution (cdf) at a particular value.<br />
</li>
<li>The quantile value corresponding to a particular probability.<br />
</li>
<li>A random draw of values from a particular distribution.</li>
</ul>
<p>The general naming structure of the relevant R functions is:</p>
<ul>
<li><code>dname</code> calculates density (pdf) at input <span class="math inline">\(x\)</span>.<br />
</li>
<li><code>pname</code> calculates distribution (cdf) at input <span class="math inline">\(x\)</span>.<br />
</li>
<li><code>qname</code> calculates the quantile at an input probability.<br />
</li>
<li><code>rname</code> generates a random draw from a particular distribution,</li>
</ul>
<p>where <code>name</code> represents the name of the given distribution, like <code>rnorm</code> for a random draw from a normal distribution</p>
<p>For example, consider a random variable <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[
X \sim N\left(\mu=2, \sigma^{2}=25\right)
\]</span></p>
<p>To calculate the value of the pdf at <span class="math inline">\(x = 4\)</span>, we use <code>dnorm()</code>:</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="preliminaries.html#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dnorm</span>(<span class="at">x =</span> <span class="dv">4</span>, <span class="at">mean =</span> <span class="dv">2</span>, <span class="at">sd =</span> <span class="dv">5</span>)</span></code></pre></div>
<pre><code>## [1] 0.07365403</code></pre>
<p>Note that R uses the standard deviation.</p>
<p>To calculate the value of the cdf at <span class="math inline">\(x = 4\)</span>, that is, <span class="math inline">\(P(X \leq{4})\)</span>, the probability that <span class="math inline">\(X\)</span> is less than or equal to 4, we use <code>pnorm()</code>:</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="preliminaries.html#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pnorm</span>(<span class="at">q =</span> <span class="dv">4</span>, <span class="at">mean =</span> <span class="dv">2</span>, <span class="at">sd =</span> <span class="dv">5</span>)</span></code></pre></div>
<pre><code>## [1] 0.6554217</code></pre>
<p>Or, to calculate the quantile for probability 0.975, we use <code>qnorm()</code>:</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="preliminaries.html#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="at">p =</span> <span class="fl">0.975</span>, <span class="at">mean =</span> <span class="dv">2</span>, <span class="at">sd =</span> <span class="dv">5</span>)</span></code></pre></div>
<pre><code>## [1] 11.79982</code></pre>
<p>Lastly, to generate a random sample of size n = 10, we use <code>rnorm()</code></p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="preliminaries.html#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rnorm</span>(<span class="at">n =</span> <span class="dv">10</span>, <span class="at">mean =</span> <span class="dv">2</span>, <span class="at">sd =</span> <span class="dv">5</span>)</span></code></pre></div>
<pre><code>##  [1] -3.0133006  5.0115537 -0.7435808  3.6728432  7.3410279 -1.0957213
##  [7] -7.6244962 -0.8472389  1.9223362 -6.9185522</code></pre>
<p>These functions exist for many other distributions such as: <code>binom</code> (Binomial), <code>t</code> (Student’s t), <code>pois</code> (Poisson), <code>f</code> (F), <code>chisq</code> (Chi-Squared) and so on.</p>
</div>
<div id="regressions" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Regressions<a href="preliminaries.html#regressions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Regressions allow us to make estimations of the relationships between variables. Let’s consider a simple example of how the speed of a car affects its stopping distance. To examine this relationship, we will use the <code>cars</code> data set from the <code>datasets</code> package. The data give the speed of cars and the distances taken to stop. Note that the data were recorded in the 1920s.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="preliminaries.html#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(cars)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    50 obs. of  2 variables:
##  $ speed: num  4 4 7 7 8 9 10 10 10 11 ...
##  $ dist : num  2 10 4 22 16 10 18 26 34 17 ...</code></pre>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="preliminaries.html#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(cars)</span></code></pre></div>
<pre><code>##      speed           dist       
##  Min.   : 4.0   Min.   :  2.00  
##  1st Qu.:12.0   1st Qu.: 26.00  
##  Median :15.0   Median : 36.00  
##  Mean   :15.4   Mean   : 42.98  
##  3rd Qu.:19.0   3rd Qu.: 56.00  
##  Max.   :25.0   Max.   :120.00</code></pre>
<p>We can plot the speed and stopping distance in a scatterplot to get a sense of their relationship before proceeding with a formal regression.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="preliminaries.html#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(dist <span class="sc">~</span> speed, <span class="at">data =</span> cars,</span>
<span id="cb53-2"><a href="preliminaries.html#cb53-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Speed (in Miles Per Hour)&quot;</span>,</span>
<span id="cb53-3"><a href="preliminaries.html#cb53-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Stopping Distance (in Feet)&quot;</span>,</span>
<span id="cb53-4"><a href="preliminaries.html#cb53-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Stopping Distance vs Speed&quot;</span>,</span>
<span id="cb53-5"><a href="preliminaries.html#cb53-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">pch =</span> <span class="dv">20</span>, <span class="at">cex =</span> <span class="dv">2</span>,<span class="at">cex.main=</span><span class="dv">1</span>,</span>
<span id="cb53-6"><a href="preliminaries.html#cb53-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">cex.lab=</span>.<span class="dv">75</span>, <span class="at">cex.axis=</span><span class="fl">0.75</span>, <span class="at">col =</span> <span class="st">&quot;grey&quot;</span>)</span></code></pre></div>
<p><img src="02-Preliminaries_files/figure-html/s15-1.png" width="672" /></p>
<p>This visualization suggests there could be a relationship.</p>
<p>In this example, we are interested in using the predictor variable <code>speed</code> to predict and explain the response variable <code>dist</code>. We could express the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> using the following “Data Generating Process” (DGP):</p>
<p><span class="math display" id="eq:2-1">\[\begin{equation}
Y=f(X)+\epsilon
  \tag{2.1}
\end{equation}\]</span></p>
<p>Here <span class="math inline">\(Y\)</span> from 2.1 is the outcome determined by two parts: <span class="math inline">\(f(X)\)</span>, which is the deterministic part (a.k.a Data Generating Model - DGM) and <span class="math inline">\(\epsilon\)</span> the random part that makes the outcome different for the same <span class="math inline">\(X\)</span> for each observation. What’s <span class="math inline">\(f(X)\)</span>? We will see later that this question is very important. For now, however, we assume that <span class="math inline">\(f(X)\)</span> is linear as</p>
<p><span class="math display" id="eq:2-2">\[\begin{equation}
f(X)=\beta_{0}+\beta_{1} x_{i}.
  \tag{2.2}
\end{equation}\]</span></p>
<p>And,</p>
<p><span class="math display">\[
\begin{array}{l}{\qquad Y_{i}=\beta_{0}+\beta_{1} x_{i}+\epsilon_{i}} \ {\text { where }} \ {\qquad \epsilon_{i} \sim N\left(0, \sigma^{2}\right)}\end{array}
\]</span>
We could think that <span class="math inline">\(Y\)</span> has different distribution for each value of <span class="math inline">\(X\)</span>. Hence, <span class="math inline">\(f(X)\)</span> becomes the conditional mean of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>.</p>
<p><span class="math display">\[
f(X) = \mathrm{E}\left[Y | X=x_{i}\right]=\beta_{0}+\beta_{1} x_{i},
\]</span></p>
<p>which means that <span class="math inline">\(\mathrm{E}\left[\epsilon | X=x_{i}\right]=0\)</span>. This model, which is also called as the population regression function (PRF), has three parameters to be estimated: <span class="math inline">\(\beta_{0}\)</span>, <span class="math inline">\(\beta_{1}\)</span>, and <span class="math inline">\(\sigma^{2}\)</span>, which are fixed but unknown constants. The coefficient <span class="math inline">\(\beta_{1}\)</span> defines the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Inferential statistics deals with estimating these population parameters using a sample drawn from the population. The statistical inference requires an estimator of a population parameter to be BLUE (Best Linear Unbiased Estimator) which is usually challenging to satisfy. A BLU estimator also requires several assumptions on PRF. These include that:</p>
<ul>
<li>The errors are <strong>independent</strong> (no serial correlation).<br />
</li>
<li>The errors are <strong>identically</strong> distributed (constant variance of <span class="math inline">\(Y\)</span> for different values of <span class="math inline">\(X\)</span>) .</li>
</ul>
<p>How do we actually find a line that represents the best relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> best? One way to find a line is to find a set of parameters that minimize the sum of squared “errors”. This is called as the Ordinary Least Squares (OLS) method:</p>
<p><span class="math display" id="eq:2-3">\[\begin{equation}
\underset{\beta_{0}, \beta_{1}}{\operatorname{argmin}} \sum_{i=1}^{n}\left(y_{i}-\left(\beta_{0}+\beta_{1} x_{i}\right)\right)^{2}
  \tag{2.3}
\end{equation}\]</span></p>
<p>Using R, we can apply this method very simply with a bit of code.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="preliminaries.html#cb54-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(dist <span class="sc">~</span> speed, <span class="at">data =</span> cars)</span>
<span id="cb54-2"><a href="preliminaries.html#cb54-2" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fu">coef</span>(model)</span>
<span id="cb54-3"><a href="preliminaries.html#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cars, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">20</span>)</span>
<span id="cb54-4"><a href="preliminaries.html#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(b, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lty =</span> <span class="dv">5</span>)</span></code></pre></div>
<p><img src="02-Preliminaries_files/figure-html/s16-1.png" width="672" /></p>
<p>Although we can see the red line which seems to minimize the sum of squared errors, we can understand this problem better mathematically.</p>
<div id="ordinary-least-squares-ols" class="section level3 hasAnchor" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Ordinary Least Squares (OLS)<a href="preliminaries.html#ordinary-least-squares-ols" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The solution to this problem starts with defining the loss (cost) function, finding the first order conditions (F.O.C.) and solving them through <strong>normal equations</strong>.</p>
<p><span class="math display" id="eq:2-4">\[\begin{equation}
f\left(\beta_{0}, \beta_{1}\right)=\sum_{i=1}^{n}\left(y_{i}-\left(\beta_{0}+\beta_{1}
x_{i}\right)\right)^{2}=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1} x_{i}\right)^{2}
  \tag{2.4}
\end{equation}\]</span></p>
<p><span class="math display">\[
\begin{aligned} \frac{\partial f}{\partial \beta_{0}} &amp;=-2 \sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1} x_{i}\right) \\ \frac{\partial f}{\partial \beta_{1}} &amp;=-2 \sum_{i=1}^{n}\left(x_{i}\right)\left(y_{i}-\beta_{0}-\beta_{1} x_{i}\right) \end{aligned}
\]</span></p>
<p>Here, we have two equations and two unknowns:</p>
<p><span class="math display">\[
\begin{array}{c}{\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1} x_{i}\right)=0} \end{array}
\]</span></p>
<p><span class="math display">\[
\begin{array}{c}{\sum_{i=1}^{n}\left(x_{i}\right)\left(y_{i}-\beta_{0}-\beta_{1} x_{i}\right)=0}\end{array}
\]</span></p>
<p>Which can be expressed:</p>
<p><span class="math display" id="eq:2-5">\[\begin{equation}
\begin{array}{c}{n \beta_{0}+\beta_{1} \sum_{i=1}^{n} x_{i}=\sum_{i=1}^{n} y_{i}} \end{array}
  \tag{2.5}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:2-6">\[\begin{equation}
\begin{array}{c}{\beta_{0} \sum_{i=1}^{n} x_{i}+\beta_{1} \sum_{i=1}^{n} x_{i}^{2}=\sum_{i=1}^{n} x_{i} y_{i}}\end{array}
  \tag{2.6}
\end{equation}\]</span></p>
<p>These functions (2.5 and 2.6) are also called <strong>normal equations</strong>. Solving them gives us:</p>
<p><span class="math display" id="eq:2-7">\[\begin{equation}
\beta_{1}=\frac{\text{cov}(Y,X)}{\text{var}(X)}
  \tag{2.7}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:2-8">\[\begin{equation}
\beta_{0}=\overline{y}-\beta_{1} \overline{x}
  \tag{2.8}
\end{equation}\]</span></p>
<p>As this is a simple review, we will not cover the OLS method in more depth here. Let’s use these variance/covariance values to get the parameters.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="preliminaries.html#cb55-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> cars<span class="sc">$</span>speed</span>
<span id="cb55-2"><a href="preliminaries.html#cb55-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> cars<span class="sc">$</span>dist</span>
<span id="cb55-3"><a href="preliminaries.html#cb55-3" aria-hidden="true" tabindex="-1"></a>Sxy <span class="ot">&lt;-</span> <span class="fu">sum</span>((x <span class="sc">-</span> <span class="fu">mean</span>(x)) <span class="sc">*</span> (y <span class="sc">-</span> <span class="fu">mean</span>(y)))</span>
<span id="cb55-4"><a href="preliminaries.html#cb55-4" aria-hidden="true" tabindex="-1"></a>Sxx <span class="ot">=</span> <span class="fu">sum</span>((x <span class="sc">-</span> <span class="fu">mean</span>(x)) <span class="sc">^</span> <span class="dv">2</span>) <span class="co">#Here to show, &quot;=&quot; would work as well</span></span>
<span id="cb55-5"><a href="preliminaries.html#cb55-5" aria-hidden="true" tabindex="-1"></a>Syy <span class="ot">&lt;-</span> <span class="fu">sum</span>((y <span class="sc">-</span> <span class="fu">mean</span>(y)) <span class="sc">^</span> <span class="dv">2</span>)</span>
<span id="cb55-6"><a href="preliminaries.html#cb55-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-7"><a href="preliminaries.html#cb55-7" aria-hidden="true" tabindex="-1"></a>beta_1 <span class="ot">&lt;-</span> Sxy <span class="sc">/</span> Sxx</span>
<span id="cb55-8"><a href="preliminaries.html#cb55-8" aria-hidden="true" tabindex="-1"></a>beta_0 <span class="ot">&lt;-</span> <span class="fu">mean</span>(y) <span class="sc">-</span> beta_1 <span class="sc">*</span> <span class="fu">mean</span>(x)</span>
<span id="cb55-9"><a href="preliminaries.html#cb55-9" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(beta_0, beta_1)</span></code></pre></div>
<pre><code>## [1] -17.579095   3.932409</code></pre>
<p>Instead of coding each of the steps ourselves, we can also use the <code>lm()</code> function to achieve the same thing.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="preliminaries.html#cb57-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(dist <span class="sc">~</span> speed, <span class="at">data =</span> cars)</span>
<span id="cb57-2"><a href="preliminaries.html#cb57-2" aria-hidden="true" tabindex="-1"></a>model</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = dist ~ speed, data = cars)
## 
## Coefficients:
## (Intercept)        speed  
##     -17.579        3.932</code></pre>
<p>The slope parameter (<span class="math inline">\(\beta_1\)</span>) tells us that the stopping distance is predicted to increase by <span class="math inline">\(3.93\)</span> feet on average for an increase in speed of one mile per hour. The intercept parameter tells us that we have “modelling” issues: when the car’s speed is zero, it moves backwards. This indicates a modeling problem. One way to handle it to remove the intercept from the model that starts from the origin.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="preliminaries.html#cb59-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> cars<span class="sc">$</span>speed</span>
<span id="cb59-2"><a href="preliminaries.html#cb59-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> cars<span class="sc">$</span>dist</span>
<span id="cb59-3"><a href="preliminaries.html#cb59-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-4"><a href="preliminaries.html#cb59-4" aria-hidden="true" tabindex="-1"></a>beta_1 <span class="ot">&lt;-</span> <span class="fu">sum</span>(x<span class="sc">*</span>y) <span class="sc">/</span> <span class="fu">sum</span>(x<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb59-5"><a href="preliminaries.html#cb59-5" aria-hidden="true" tabindex="-1"></a>beta_1</span></code></pre></div>
<pre><code>## [1] 2.909132</code></pre>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="preliminaries.html#cb61-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(dist <span class="sc">~</span> speed <span class="sc">-</span> <span class="dv">1</span>, <span class="at">data =</span> cars)</span>
<span id="cb61-2"><a href="preliminaries.html#cb61-2" aria-hidden="true" tabindex="-1"></a>model</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = dist ~ speed - 1, data = cars)
## 
## Coefficients:
## speed  
## 2.909</code></pre>
<p>As we can see changing the model affects the prediction. Unfortunately the single-variable case is usually not a realistic model to capture the determination of the output. Let’s use a better dataset, <code>mtcars</code> from the same library, <code>datasets</code>:</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="preliminaries.html#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(mtcars)</span></code></pre></div>
<pre><code>##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1</code></pre>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="preliminaries.html#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(mtcars)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    32 obs. of  11 variables:
##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...
##  $ disp: num  160 160 108 258 360 ...
##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...
##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...
##  $ qsec: num  16.5 17 18.6 19.4 17 ...
##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...
##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...
##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...
##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...</code></pre>
<p>We may want to model the fuel efficiency (<code>mpg</code>) of a car as a function of its weight (<code>wt</code>) and horse power (<code>hp</code>). We can do this using our method of normal equations.</p>
<p><span class="math display">\[
Y_{i}=\beta_{0}+\beta_{1} x_{i 1}+\beta_{2} x_{i 2}+\epsilon_{i}, \quad i=1,2, \ldots, n
\]</span></p>
<p><span class="math display">\[
f\left(\beta_{0}, \beta_{1}, \beta_{2}\right)=\sum_{i=1}^{n}\left(y_{i}-\left(\beta_{0}+\beta_{1} x_{i 1}+\beta_{2} x_{i 2}\right)\right)^{2}
\]</span></p>
<p><span class="math display">\[
\begin{aligned} \frac{\partial f}{\partial \beta_{0}} &amp;=0 \\ \frac{\partial f}{\partial \beta_{1}} &amp;=0 \\ \frac{\partial f}{\partial \beta_{2}} &amp;=0 \end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{array}{c}{n \beta_{0}+\beta_{1} \sum_{i=1}^{n} x_{i 1}+\beta_{2} \sum_{i=1}^{n} x_{i 2}=\sum_{i=1}^{n} y_{i}} \end{array}
\]</span>
<span class="math display">\[
\begin{array}{c}{\beta_{0} \sum_{i=1}^{n} x_{i 1}+\beta_{1} \sum_{i=1}^{n} x_{i 1}^{2}+\beta_{2} \sum_{i=1}^{n} x_{i 1} x_{i 2}=\sum_{i=1}^{n} x_{i 1} y_{i}} \end{array}
\]</span></p>
<p><span class="math display">\[
\begin{array}{c}{\beta_{0} \sum_{i=1}^{n} x_{i 2}+\beta_{1} \sum_{i=1}^{n} x_{i 1} x_{i 2}+\beta_{2} \sum_{i=1}^{n} x_{i 2}^{2}=\sum_{i=1}^{n} x_{i 2} y_{i}}\end{array}
\]</span></p>
<p>We now have three equations and three variables. While we could solve them by scalar algebra, it becomes increasingly cumbersome. Although we can apply linear algebra to see the analytical solutions, we just let R solve it for us:</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="preliminaries.html#cb67-1" aria-hidden="true" tabindex="-1"></a>mpg_model <span class="ot">=</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> wt <span class="sc">+</span> hp, <span class="at">data =</span> mtcars)</span>
<span id="cb67-2"><a href="preliminaries.html#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(mpg_model)</span></code></pre></div>
<pre><code>## (Intercept)          wt          hp 
## 37.22727012 -3.87783074 -0.03177295</code></pre>
<p>Up to this point we used OLS that finds the parameters minimizing the residual sum of squares (RSS - or the sum of squared errors). Instead of OLS, there are other methods that we can use. One of them is called Maximum likelihood Estimator or MLE.</p>
</div>
<div id="maximum-likelihood-estimators" class="section level3 hasAnchor" number="2.4.2">
<h3><span class="header-section-number">2.4.2</span> Maximum Likelihood Estimators<a href="preliminaries.html#maximum-likelihood-estimators" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Understanding the MLE method starts with probability density functions (pdf), which characterize the distribution of a continuous random variable. Recall that pdf of a random variable <span class="math inline">\(X \sim N\left(\mu, \sigma^{2}\right)\)</span> is given by:</p>
<p><span class="math display">\[
f_{x}\left(x ; \mu, \sigma^{2}\right)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left[-\frac{1}{2}\left(\frac{x_i-\mu}{\sigma}\right)^{2}\right]
\]</span>
In R, you can use <code>dnorm(x, mean, sd)</code> to calculate the pdf of normal distribution.</p>
<ul>
<li>The argument <span class="math inline">\(x\)</span> represent the location(s) at which to compute the pdf.</li>
<li>The arguments <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> represent the mean and standard deviation of the normal distribution, respectively.</li>
</ul>
<p>For example, <code>dnorm (0, mean=1, sd=2)</code> computes the pdf at location 0 of <span class="math inline">\(N(1,4)\)</span>, normal distribution with mean 1 and variance 4. Let’s see examples of computing the pdf at 2 locations for.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="preliminaries.html#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dnorm</span>(<span class="dv">0</span>, <span class="at">mean =</span> <span class="dv">1</span>, <span class="at">sd =</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 0.1760327</code></pre>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="preliminaries.html#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> <span class="dv">1</span>, <span class="at">sd =</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 0.1994711</code></pre>
<p>In addition to computing the pdf at one location for a single normal distribution, <code>dnorm</code> also accepts vectors with more than one elements in all three arguments. For example, suppose that we have the following data, <span class="math inline">\(x\)</span>. We can now compute pdf values for each <span class="math inline">\(x\)</span>.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="preliminaries.html#cb73-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="sc">-</span><span class="dv">10</span>, <span class="at">to =</span> <span class="sc">+</span><span class="dv">22</span>, <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb73-2"><a href="preliminaries.html#cb73-2" aria-hidden="true" tabindex="-1"></a>pdfs <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(x, <span class="at">mean =</span> <span class="dv">1</span>, <span class="at">sd =</span> <span class="dv">2</span>)</span>
<span id="cb73-3"><a href="preliminaries.html#cb73-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, pdfs)</span></code></pre></div>
<p><img src="02-Preliminaries_files/figure-html/s23-1.png" width="672" /></p>
<p>There are two implicit assumptions made here: (1) <span class="math inline">\(x\)</span> is normally distributed, i.e. <span class="math inline">\(X \sim N\left(\mu = 1, \sigma^{2}=4\right)\)</span>. As you can see from the code it’s a wrong assumption, but ignore it for now; (2) the distribution is defined by mean = 1 and sd = 2.</p>
<p>The main goal in defining the likelihood function is to find the distribution parameters (mean and sd in a normal distribution) that fit the observed data best.</p>
<p>Let’s have an example. Pretend that we do not see and know the following data creation:</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="preliminaries.html#cb74-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1000</span>, <span class="dv">2</span>, <span class="dv">7</span>)</span></code></pre></div>
<p>Of course we can plot the data and calculate the parameters of its distribution (the mean and standard deviation of <span class="math inline">\(x\)</span>). However, how can we do it with likelihood function? Let’s plot three different pdf’s. Which one is the best distribution, representing the true distribution of the data? How can we find the parameters of the last plot? This is the idea behind Maximum likelihood method.</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="preliminaries.html#cb75-1" aria-hidden="true" tabindex="-1"></a>pdfs1 <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(x, <span class="at">mean =</span> <span class="dv">1</span>, <span class="at">sd=</span><span class="dv">2</span>)</span>
<span id="cb75-2"><a href="preliminaries.html#cb75-2" aria-hidden="true" tabindex="-1"></a>pdfs2 <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(x, <span class="at">mean =</span> <span class="dv">5</span>, <span class="at">sd=</span><span class="dv">7</span>)</span>
<span id="cb75-3"><a href="preliminaries.html#cb75-3" aria-hidden="true" tabindex="-1"></a>pdfs3 <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(x, <span class="at">mean =</span> <span class="dv">2</span>, <span class="at">sd=</span><span class="dv">7</span>)</span>
<span id="cb75-4"><a href="preliminaries.html#cb75-4" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</span>
<span id="cb75-5"><a href="preliminaries.html#cb75-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x,pdfs1)</span>
<span id="cb75-6"><a href="preliminaries.html#cb75-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x,pdfs2)</span>
<span id="cb75-7"><a href="preliminaries.html#cb75-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x,pdfs3)</span></code></pre></div>
<p><img src="02-Preliminaries_files/figure-html/s25-1.png" width="672" /></p>
<p>It seems reasonable that a good estimate of the unknown parameter <span class="math inline">\(\mu\)</span> would be the value that maximizes the the likelihood (not the probability) of getting the data we observed. The probability density (or mass) function of each <span class="math inline">\(x_{i}\)</span> is <span class="math inline">\(f\left(x_{i} ; \mu, \sigma^2\right)\)</span>. Then, the joint probability density function of <span class="math inline">\(x_{1}, x_{2}, \cdots, x_{n}\)</span>, which we’ll call <span class="math inline">\(L(\mu, \sigma^2)\)</span> is:</p>
<p><span class="math display">\[
L(\mu, \sigma^2)=P\left(X_{1}=x_{1}, X_{2}=x_{2}, \ldots, X_{n}=x_{n}\right)=\\
f\left(x_{1} ; \mu,\sigma^2\right) \cdot f\left(x_{2} ; \mu,\sigma^2\right) \cdots f\left(x_{n} ; \mu, \sigma^2\right)=\\
\prod_{i=1}^{n} f\left(x_{i} ; \mu, \sigma^2\right)
\]</span>
The first equality is just the definition of the joint probability density function. The second equality comes from that fact that we have a random sample, <span class="math inline">\(x_i\)</span>, that are independent and identically distributed. Hence, the likelihood function is:</p>
<p><span class="math display">\[
L(\mu, \sigma)=\sigma^{-n}(2 \pi)^{-n / 2} \exp \left[-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}\right]
\]</span></p>
<p>and therefore the log of the likelihood function:</p>
<p><span class="math display">\[
\log L\left(\mu, \sigma\right)=-n \log \sigma^{2}-\frac{n}{2} \log (2 \pi)-\frac{\sum\left(x_{i}-\mu\right)^{2}}{2 \sigma^{2}}
\]</span>
Now, upon taking the partial derivative of the log likelihood with respect to <span class="math inline">\(\mu\)</span> and setting to 0, we see that:</p>
<p><span class="math display">\[
\frac{\partial \log L\left(\mu, \sigma\right)}{\partial \mu}=\frac{-\not 2 \sum\left(x_{i}-\mu\right)(-\not1)}{\not2 \sigma^{2}} \stackrel{\mathrm{SET}}{=} 0
\]</span>
and we get</p>
<p><span class="math display">\[
\sum x_{i}-n \mu=0
\]</span>
<span class="math display">\[
\hat{\mu}=\frac{\sum x_{i}}{n}=\bar{x}
\]</span></p>
<p>We can solve for <span class="math inline">\(\sigma^2\)</span> by the same way.</p>
<p>As for the regression, since,</p>
<p><span class="math display">\[
Y_{i} | X_{i} \sim N\left(\beta_{0}+\beta_{1} x_{i}, \sigma^{2}\right)
\]</span></p>
<p><span class="math display">\[
f_{y_{i}}\left(y_{i} ; x_{i}, \beta_{0}, \beta_{1}, \sigma^{2}\right)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left[-\frac{1}{2}\left(\frac{y_{i}-\left(\beta_{0}+\beta_{1} x_{i}\right)}{\sigma}\right)^{2}\right]
\]</span></p>
<p>Given <span class="math inline">\(n\)</span> data points <span class="math inline">\((x_i,y_i)\)</span> we can write the likelihood as a function of the three parameters <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\sigma^2\)</span>.</p>
<p><span class="math display" id="eq:2-9">\[\begin{equation}
L\left(\beta_{0}, \beta_{1}, \sigma^{2}\right)=\prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left[-\frac{1}{2}\left(\frac{y_{i}-\beta_{0}-\beta_{1} x_{i}}{\sigma}\right)^{2}\right]
  \tag{2.9}
\end{equation}\]</span></p>
<p>Our goal is to find values of <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\sigma^2\)</span> which maximize function 2.9. It is a straightforward multivariate calculus problem. First, let’s re-write 2.9 as follows:</p>
<p><span class="math display" id="eq:2-10">\[\begin{equation}
L\left(\beta_{0}, \beta_{1}, \sigma^{2}\right)=\left(\frac{1}{\sqrt{2 \pi \sigma^{2}}}\right)^{n} \exp \left[-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1} x_{i}\right)^{2}\right]
  \tag{2.10}
\end{equation}\]</span></p>
<p>We can make 2.10 linear by taking the log of this function, which is called as <strong>the log-likelihood function</strong>.</p>
<p><span class="math display" id="eq:2-11">\[\begin{equation}
\log L\left(\beta_{0}, \beta_{1}, \sigma^{2}\right)=-\frac{n}{2} \log (2 \pi)-\frac{n}{2} \log \left(\sigma^{2}\right)-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1} x_{i}\right)^{2}
  \tag{2.11}
\end{equation}\]</span></p>
<p>The rest would be simple calculus:</p>
<p><span class="math display">\[
\frac{\partial \log L(\beta_{0}, \beta_{1}, \sigma^{2})}{\partial \beta_{0}}=\frac{1}{\sigma^{2}} \sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1} x_{i}\right),
\]</span></p>
<p><span class="math display">\[
\frac{\partial \log L\left(\beta_{0}, \beta_{1}, \sigma^{2}\right)}{\partial \beta_{1}}=\frac{1}{\sigma^{2}} \sum_{i=1}^{n}\left(x_{i}\right)\left(y_{i}-\beta_{0}-\beta_{1} x_{i}\right),
\]</span></p>
<p>and,</p>
<p><span class="math display">\[
\frac{\partial \log L\left(\beta_{0}, \beta_{1}, \sigma^{2}\right)}{\partial \sigma^{2}}=-\frac{n}{2 \sigma^{2}}+\frac{1}{2\left(\sigma^{2}\right)^{2}} \sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1} x_{i}\right)^{2}
\]</span></p>
<p>These first order conditions yield the following three equations with three unknown parameters:</p>
<p><span class="math display">\[
\begin{aligned} \sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1} x_{i}\right) &amp;=0 \\ \sum_{i=1}^{n}\left(x_{i}\right)\left(y_{i}-\beta_{0}-\beta_{1} x_{i}\right) &amp;=0 \\-\frac{n}{2 \sigma^{2}}+\frac{1}{2\left(\sigma^{2}\right)^{2}} \sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1} x_{i}\right)^{2} &amp;=0 \end{aligned}
\]</span></p>
<p>We call these estimates the maximum likelihood estimates. They are exactly the same as OLS parameters, except for the variance.</p>
<p>So, we now have two different estimates of <span class="math inline">\(\sigma^{2}\)</span>.</p>
<p><span class="math display">\[
\begin{aligned} s_{e}^{2} &amp;=\frac{1}{n-2} \sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}=\frac{1}{n-2} \sum_{i=1}^{n} e_{i}^{2} \quad \text { Least Squares } \end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}  \hat{\sigma}^{2} &amp;=\frac{1}{n} \sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}=\frac{1}{n} \sum_{i=1}^{n} e_{i}^{2} \quad \text {MLE}\end{aligned}
\]</span></p>
<p>That’s why MLS is only an efficient estimator for large samples.</p>
</div>
<div id="estimating-mle-with-r" class="section level3 hasAnchor" number="2.4.3">
<h3><span class="header-section-number">2.4.3</span> Estimating MLE with R<a href="preliminaries.html#estimating-mle-with-r" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>How can we make estimations with MLE? It is good to look at a simple example. Suppose the observations <span class="math inline">\(X_1,X_2,...,X_n\)</span> are from <span class="math inline">\(N(\mu,\sigma^{2})\)</span> distribution (two parameters: mean and variance). The likelihood function is:</p>
<p><span class="math display" id="eq:2-12">\[\begin{equation}
L(x)=\prod_{i=1}^{i=n} \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}
  \tag{2.12}
\end{equation}\]</span></p>
<p>The objective is to find out the mean and the variance of the sample. Of course this a silly example: instead of using MLE to calculate them, we can use our middle-school algebra and find them right away. But the point here is to show how MLE works. And more importantly, we now have a different way to estimate the mean and the variance.</p>
<p>The question here is, <strong>given the data, what parameters (mean and variance) would give us the maximum joint density</strong>. Hence, <strong>the likelihood function is a function of the parameter only, with the data held as a fixed constant</strong>, which gives us an idea of how well the data summarizes these parameters. Because we are interested in observing all the data points jointly, it can be calculated as a product of marginal densities of each observation assuming that observations are independent and identically distributed.</p>
<p>Here is an example:</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="preliminaries.html#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Let&#39;s create a sample of normal variables</span></span>
<span id="cb76-2"><a href="preliminaries.html#cb76-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2019</span>)</span>
<span id="cb76-3"><a href="preliminaries.html#cb76-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb76-4"><a href="preliminaries.html#cb76-4" aria-hidden="true" tabindex="-1"></a><span class="co"># And the likelihood of these x&#39;s is</span></span>
<span id="cb76-5"><a href="preliminaries.html#cb76-5" aria-hidden="true" tabindex="-1"></a><span class="fu">prod</span>(<span class="fu">dnorm</span>(x))</span></code></pre></div>
<pre><code>## [1] 2.23626e-58</code></pre>
<p>What’s happening here? One issue with the MLE method is that, as probability densities are often smaller than 1, the value of <span class="math inline">\(L(x)\)</span> would be very small. Or very high, if the variance is very high. This could be a worse problem for large samples and create a problem for computers in terms of storage and precision. The solution would be the log-likelihood:</p>
<p><span class="math display" id="eq:2-13">\[\begin{equation}
\log (\mathcal{L}(\mu, \sigma))=-\frac{n}{2} \log \left(2 \pi \sigma^{2}\right)-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}\left(x_{i}-\mu\right)^{2}
  \tag{2.13}
\end{equation}\]</span></p>
<p>In a more realistic case we can only observe a sample of some data points and <strong>assume</strong> how it is distributed. With this assumption, we can have a log-likelihood function. Hence, if it’s a wrong assumption, our estimations will be wrong as well. The fact that we have to make assumptions about pdf’s will be very important issue when we cover nonparamateric estimations.</p>
<p>Let’s assume that we have 100 <span class="math inline">\(x\)</span>’s with <span class="math inline">\(x\sim N\left(\mu, \sigma^{2}\right)\)</span>. We can now compute the derivatives of this log-likelihood and calculate the parameters. However, instead of this manual analytic optimization procedure, we can use R packages or algorithmic/numerical optimization methods. In fact, except for trivial models, the analytic methods cannot be applied to solve for the parameters. R has two packages <code>optim()</code> and <code>nlm()</code> that use <strong>algorithmic optimization</strong> methods, which we will see in the Appendix. For these optimization methods, it really does not matter how complex or simple the function is, as they will treat it as a black box.</p>
<p>Here we can re-write function 2.13 :</p>
<p><span class="math display" id="eq:2-14">\[\begin{equation}
-\sum\left(\frac{\left(x_{i}-\mu\right)^{2}}{2 \sigma^{2}}+1 / 2 \log 2 \pi+1 / 2 \log \sigma^{2}\right),
  \tag{2.14}
\end{equation}\]</span></p>
<p>Instead of finding the parameters that minimize this <strong>negative</strong> function, we can find the maximum of the negative of this function. We can omit the term <span class="math inline">\(-1/2\log2\pi\)</span> and define the function to R as follows:</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="preliminaries.html#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Here is our function f(x)</span></span>
<span id="cb78-2"><a href="preliminaries.html#cb78-2" aria-hidden="true" tabindex="-1"></a>fn <span class="ot">&lt;-</span> <span class="cf">function</span>(prmt){</span>
<span id="cb78-3"><a href="preliminaries.html#cb78-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sum</span>(<span class="fl">0.5</span><span class="sc">*</span>(x <span class="sc">-</span> prmt[<span class="dv">1</span>])<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>prmt[<span class="dv">2</span>] <span class="sc">+</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">log</span>(prmt[<span class="dv">2</span>]))</span>
<span id="cb78-4"><a href="preliminaries.html#cb78-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb78-5"><a href="preliminaries.html#cb78-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-6"><a href="preliminaries.html#cb78-6" aria-hidden="true" tabindex="-1"></a><span class="co">#We have two packages nlm() and optim() to solve it</span></span>
<span id="cb78-7"><a href="preliminaries.html#cb78-7" aria-hidden="true" tabindex="-1"></a><span class="co">#We arbitrarily pick starting points for the parameters</span></span>
<span id="cb78-8"><a href="preliminaries.html#cb78-8" aria-hidden="true" tabindex="-1"></a>sol1 <span class="ot">&lt;-</span> <span class="fu">nlm</span>(fn, prmt <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>), <span class="at">hessian=</span><span class="cn">TRUE</span>)</span>
<span id="cb78-9"><a href="preliminaries.html#cb78-9" aria-hidden="true" tabindex="-1"></a>sol2 <span class="ot">&lt;-</span> <span class="fu">optim</span>(prmt <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), fn, <span class="at">hessian=</span><span class="cn">TRUE</span>)</span>
<span id="cb78-10"><a href="preliminaries.html#cb78-10" aria-hidden="true" tabindex="-1"></a>sol1</span></code></pre></div>
<pre><code>## $minimum
## [1] 39.56555
## 
## $estimate
## [1] -0.07333445  0.81164723
## 
## $gradient
## [1] 5.478284e-06 4.384049e-06
## 
## $hessian
##               [,1]         [,2]
## [1,] 123.206236680 -0.007519674
## [2,]  -0.007519674 75.861573379
## 
## $code
## [1] 1
## 
## $iterations
## [1] 10</code></pre>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="preliminaries.html#cb80-1" aria-hidden="true" tabindex="-1"></a>sol2</span></code></pre></div>
<pre><code>## $par
## [1] -0.07328781  0.81161672
## 
## $value
## [1] 39.56555
## 
## $counts
## function gradient 
##       51       NA 
## 
## $convergence
## [1] 0
## 
## $message
## NULL
## 
## $hessian
##               [,1]         [,2]
## [1,] 123.210867601 -0.007012263
## [2,]  -0.007012263 75.911070194</code></pre>
<p>Let’s check if these estimates are correct:</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="preliminaries.html#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="co">#mean</span></span>
<span id="cb82-2"><a href="preliminaries.html#cb82-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(x)</span></code></pre></div>
<pre><code>## [1] -0.073334</code></pre>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="preliminaries.html#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="co">#sd</span></span>
<span id="cb84-2"><a href="preliminaries.html#cb84-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>((x<span class="sc">-</span><span class="fu">mean</span>(x))<span class="sc">^</span><span class="dv">2</span> )<span class="sc">/</span><span class="fu">length</span>(x)</span></code></pre></div>
<pre><code>## [1] 0.8116477</code></pre>
<p>This is nice. But we need to know little bit more about how <code>optim()</code> and <code>nlp()</code> works. More specifically, what’s an algorithmic optimization? We leave it to <strong>Algorithmic Optimization</strong> in Appendix.</p>
</div>
</div>
<div id="blue" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> BLUE<a href="preliminaries.html#blue" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There two universes in inferential statistics: the population and a sample. Statistical inference makes propositions about unknown population parameters using the sample data randomly drawn from the same population. For example, if we want to estimate the population mean of <span class="math inline">\(X\)</span>, <span class="math inline">\(\mu_X\)</span>, we use <span class="math inline">\(\bar{x} =n^{-1}\Sigma x_i\)</span> as an estimator on the sample. The choice of <span class="math inline">\(n^{-1}\Sigma x_i\)</span> as an estimator of <span class="math inline">\(\mu_X\)</span> seems commonsense, but why? What’s the criteria for a “good” estimator? The answer to this question is the key subject in econometrics and causal analysis. The chosen estimator must be the best (B) linear (L) unbiased (U) estimator (E) of the population parameter for a proper statistical inference.</p>
<p>The Gauss–Markov theorem states that <span class="math inline">\(\hat{\beta}_{0}\)</span> and <span class="math inline">\(\hat{\beta}_{1}\)</span> in the sample regression function <span class="math inline">\((Y_{i}=\hat{\beta}_{0}+\hat{\beta}_{1} x_{i}+\hat{\epsilon}_{i})\)</span> are BLU estimators of <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span> provided that certain assumptions on the population regression function <span class="math inline">\((Y_{i}=\beta_{0}+\beta_{1} x_{i}+\epsilon_{i})\)</span> are satisfied. The property of unbiasedness requires that the expected value of the estimator is equal to the true value of the parameter being estimated. In the case of a regression,</p>
<p><span class="math display">\[
\begin{array}{l}{\mathrm{E}\left[\hat{\beta}_{0}\right]=\beta_{0}} \\ {\mathrm{E}\left[\hat{\beta}_{1}\right]=\beta_{1}}\end{array}
\]</span></p>
<p>Recall the sampling distribution you’ve learned in statistics. The idea is simple. You have a population, but it is not accessible usually because it is too large. Thus, we use a random sample to estimate the parameters of interest for the population. This generalization from the sample to the population requires the concept of sampling distribution.</p>
<p>Suppose we want to estimate the average age of the local population. You calculate the average age as 23 from a sample of 200 people randomly selected from the population. But, if you keep sampling 1000 times (1000 samples, 200 people in each), each sample will give you a different estimate. Which one should be used for the generalization (population)? None of them. We know that the average of all average ages calculated from 1000 samples will be the most correct average of the population, although only if the estimator is unbiased. For a simple average, the proof is easy. Let’s create our own sampling distribution for <span class="math inline">\(\bar{x}\)</span>. We will draw 1000 samples from <span class="math inline">\(X \sim N\left(5, 1\right)\)</span>. Each sample will have 200 <span class="math inline">\(x\)</span>’s. Thus, we will calculate 1000 <span class="math inline">\(\bar{x}\)</span>’s. The objective is to see if</p>
<p><span class="math display">\[
\mathrm{E}\left(\bar{x}\right)=\mu_x
\]</span></p>
<p>There are multiple ways to do this simulation.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="preliminaries.html#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Population (1 million x&#39;s)</span></span>
<span id="cb86-2"><a href="preliminaries.html#cb86-2" aria-hidden="true" tabindex="-1"></a>pop_x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1000000</span>, <span class="at">mean =</span> <span class="dv">5</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb86-3"><a href="preliminaries.html#cb86-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-4"><a href="preliminaries.html#cb86-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Random Sampling</span></span>
<span id="cb86-5"><a href="preliminaries.html#cb86-5" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">200</span> <span class="co"># number of x&#39;s in each sample</span></span>
<span id="cb86-6"><a href="preliminaries.html#cb86-6" aria-hidden="true" tabindex="-1"></a>mcn <span class="ot">&lt;-</span> <span class="dv">1000</span> <span class="co"># number of samples in the simulation</span></span>
<span id="cb86-7"><a href="preliminaries.html#cb86-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-8"><a href="preliminaries.html#cb86-8" aria-hidden="true" tabindex="-1"></a>samp <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> n, <span class="at">ncol =</span> mcn) <span class="co"># a Container: matrix by 200 x 1000</span></span>
<span id="cb86-9"><a href="preliminaries.html#cb86-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span> mcn){</span>
<span id="cb86-10"><a href="preliminaries.html#cb86-10" aria-hidden="true" tabindex="-1"></a>  samp[,i] <span class="ot">&lt;-</span> <span class="fu">sample</span>(pop_x, n, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb86-11"><a href="preliminaries.html#cb86-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb86-12"><a href="preliminaries.html#cb86-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-13"><a href="preliminaries.html#cb86-13" aria-hidden="true" tabindex="-1"></a>xbar <span class="ot">&lt;-</span> <span class="fu">colMeans</span>(samp) <span class="co"># We calculate the column means</span></span>
<span id="cb86-14"><a href="preliminaries.html#cb86-14" aria-hidden="true" tabindex="-1"></a>mxbar <span class="ot">&lt;-</span> <span class="fu">mean</span>(xbar) <span class="co"># the mean of xbars</span></span>
<span id="cb86-15"><a href="preliminaries.html#cb86-15" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(mxbar, <span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 5</code></pre>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="preliminaries.html#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(xbar, <span class="at">breaks=</span><span class="dv">20</span>)</span></code></pre></div>
<p><img src="02-Preliminaries_files/figure-html/s29-1.png" width="672" /></p>
<p>This is our sampling distribution of <span class="math inline">\(\bar{x}\)</span> and <span class="math inline">\(\bar{x}\)</span> is an unbiased estimator of <span class="math inline">\(\mu_x\)</span>. But is that enough? We may have another estimator, like <span class="math inline">\(\tilde{x} = (x_1 + x_{200})/2\)</span>, which could be an unbiased estimator as well.</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="preliminaries.html#cb89-1" aria-hidden="true" tabindex="-1"></a>xtilde <span class="ot">&lt;-</span> <span class="fu">apply</span>(samp, <span class="dv">2</span>, <span class="cf">function</span>(x) (<span class="fu">head</span>(x,<span class="dv">1</span>)<span class="sc">+</span> <span class="fu">tail</span>(x,<span class="dv">1</span>))<span class="sc">/</span><span class="dv">2</span>) </span>
<span id="cb89-2"><a href="preliminaries.html#cb89-2" aria-hidden="true" tabindex="-1"></a>mtilde <span class="ot">&lt;-</span> <span class="fu">mean</span>(xtilde) <span class="co"># the mean of xbars</span></span>
<span id="cb89-3"><a href="preliminaries.html#cb89-3" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(mtilde, <span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 5.03</code></pre>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="preliminaries.html#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(xtilde, <span class="at">breaks=</span><span class="dv">20</span>)</span></code></pre></div>
<p><img src="02-Preliminaries_files/figure-html/s30-1.png" width="672" /></p>
<p>Now, if we are happy with our unbiased estimators, how do we choose one estimator among all other unbiased estimators? How do we define the best estimator? The answer is simple: we choose the one with the minimum sampling variance.</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="preliminaries.html#cb92-1" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(xbar)</span></code></pre></div>
<pre><code>## [1] 0.004704642</code></pre>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="preliminaries.html#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(xtilde)</span></code></pre></div>
<pre><code>## [1] 0.4781128</code></pre>
<p>Why do we need the minimum variance? Remember more variance means higher differences in <span class="math inline">\(\hat{\beta}_{0} \text { and } \hat{\beta}_{1}\)</span> from sample to sample. That means a very large confidence interval for the <span class="math inline">\(\mu_x\)</span>. Since we have only one sample in practice, the high sampling variance results in greater likelihood that we will get results further away from the <strong>mean</strong> of <span class="math inline">\(\bar{x}\)</span>, which is captured by the confidence interval.</p>
<p>First, note that it is very easy to create an estimator for <span class="math inline">\(\beta_{1}\)</span> that has very low variance, but is not unbiased. For example, define <span class="math inline">\(\hat{\theta}_{B A D}=5\)</span>. Since <span class="math inline">\(\hat{\theta}_{B A D}\)</span> is constant,</p>
<p><span class="math display">\[
\begin{array}{r}{\mathbf{Var}\left[\hat{\theta}_{B A D}\right]=0} \end{array}
\]</span></p>
<p>However since, <span class="math inline">\(\mathbf{E}\left[\hat{\theta}_{B A D}\right]=5\)</span>, we can say that <span class="math inline">\(\hat{\theta}_{B A D}\)</span> is not a good estimator even though it has the smallest possible variance. Hence two conditions, unbiasedness and minimum variance, have an order: we look for an estimator with the minimum variance among unbiased estimators.</p>
<p><strong>Omitted Variable Bias (OVB)</strong></p>
<p>A regression analysis requires a correctly specified regression model. When the estimated model is misspecified by “omitting” some necessary variables, the resulting sample regression function (SRF) will be a biased estimator of the true data generating model (DGM). While the solution seems simple - “correctly specify your SRF by including all necessary variables as in the true DGM”, we often have to assume what the true DGM is. In other words, we do our best to control all the confounding variables, variables that are correlated with the outcome (<span class="math inline">\(y_i\)</span>) and the explanatory variables. When we miss (omit) one of those “control” variables, perhaps due to our ignorance on the true DGM, the effect of <span class="math inline">\(x_i\)</span> on <span class="math inline">\(y_i\)</span> will be misleading and biased. One gross example is the significant and positive relationship between ice-cream sales and the crime rates. When we include the confounding factor, the hot weather, which is strongly and positively related to both ice-cream sales and the crime rates, the effect disappears.</p>
<p>What we are looking for is the isolated effect of <span class="math inline">\(x_i\)</span> on <span class="math inline">\(y_i\)</span>, after controlling for all the other possible reasons of the variation in <span class="math inline">\(y_i\)</span>. Let’s illustrate it with Venn diagrams.</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="preliminaries.html#cb96-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(broom)        </span>
<span id="cb96-2"><a href="preliminaries.html#cb96-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(faux)      </span></code></pre></div>
<pre><code>## 
## ************
## Welcome to faux. For support and examples visit:
## https://debruine.github.io/faux/
## - Get and set global package options with: faux_options()
## ************</code></pre>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="preliminaries.html#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(eulerr) <span class="co">#For Euler and Venn diagrams</span></span>
<span id="cb98-2"><a href="preliminaries.html#cb98-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code></pre></div>
<pre><code>## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2
## ──</code></pre>
<pre><code>## ✔ tibble  3.1.8      ✔ dplyr   1.0.10
## ✔ tidyr   1.2.1      ✔ stringr 1.5.0 
## ✔ readr   2.1.3      ✔ forcats 0.5.2 
## ✔ purrr   1.0.0      
## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ purrr::%||%()            masks faux::%||%()
## ✖ lubridate::as.difftime() masks base::as.difftime()
## ✖ dplyr::between()         masks plm::between()
## ✖ lubridate::date()        masks base::date()
## ✖ dplyr::filter()          masks stats::filter()
## ✖ lubridate::intersect()   masks base::intersect()
## ✖ dplyr::lag()             masks plm::lag(), stats::lag()
## ✖ dplyr::lead()            masks plm::lead()
## ✖ lubridate::setdiff()     masks base::setdiff()
## ✖ lubridate::union()       masks base::union()</code></pre>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="preliminaries.html#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Correlated variables and PRF</span></span>
<span id="cb101-2"><a href="preliminaries.html#cb101-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb101-3"><a href="preliminaries.html#cb101-3" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">rnorm_multi</span>(<span class="at">n =</span> <span class="dv">100</span>,</span>
<span id="cb101-4"><a href="preliminaries.html#cb101-4" aria-hidden="true" tabindex="-1"></a>                  <span class="at">mu =</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">9</span>, <span class="dv">9</span>),</span>
<span id="cb101-5"><a href="preliminaries.html#cb101-5" aria-hidden="true" tabindex="-1"></a>                  <span class="at">sd =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="fl">1.7</span>, <span class="fl">1.3</span>),</span>
<span id="cb101-6"><a href="preliminaries.html#cb101-6" aria-hidden="true" tabindex="-1"></a>                  <span class="at">r =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.4</span>, <span class="fl">0.8</span>),</span>
<span id="cb101-7"><a href="preliminaries.html#cb101-7" aria-hidden="true" tabindex="-1"></a>                  <span class="at">varnames =</span> <span class="fu">c</span>(<span class="st">&quot;Y&quot;</span>, <span class="st">&quot;X1&quot;</span>, <span class="st">&quot;X2&quot;</span>),</span>
<span id="cb101-8"><a href="preliminaries.html#cb101-8" aria-hidden="true" tabindex="-1"></a>                  <span class="at">empirical =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>Here is our simulated data and the SRF:</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="preliminaries.html#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(df, <span class="dv">6</span>)</span></code></pre></div>
<pre><code>##           Y        X1        X2
## 1 12.510529 10.266830 10.180426
## 2  9.058085  8.682856  9.391148
## 3  8.223374  7.284975  7.968759
## 4 13.509343 12.554053 12.030183
## 5  8.419466  8.878325  9.273916
## 6  9.337398  7.832841  8.773532</code></pre>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="preliminaries.html#cb104-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(df)</span></code></pre></div>
<pre><code>##            Y        X1        X2
## Y  1.0000000 0.4739343 0.3878172
## X1 0.4739343 1.0000000 0.8107924
## X2 0.3878172 0.8107924 1.0000000</code></pre>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="preliminaries.html#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="fu">lm</span>(Y <span class="sc">~</span>., <span class="at">data =</span> df) <span class="co"># true model</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Y ~ ., data = df)
## 
## Coefficients:
## (Intercept)           X1           X2  
##     5.15034      0.54801      0.01717</code></pre>
<p>Here is the Venn diagram reflecting the effect of <span class="math inline">\(X_1\)</span>, <span class="math inline">\(\mathbf{B}\)</span>, when we omit <span class="math inline">\(X_2\)</span> in the SRF. The sizes also reflect the decomposition of the variances. Thus, <span class="math inline">\(\mathbf{B}/(\mathbf{A+D})\)</span> is % of <span class="math inline">\(y\)</span> is explained by <span class="math inline">\(X_1\)</span>:</p>
<p><img src="02-Preliminaries_files/figure-html/s34-1.png" width="672" /></p>
<p>Now, after controlling for the effect of <span class="math inline">\(X_2\)</span>, <span class="math inline">\(\mathbf{B}\)</span>, the isolated effect of <span class="math inline">\(X_1\)</span> is reduced to <span class="math inline">\(\mathbf{D}\)</span>:</p>
<p><img src="02-Preliminaries_files/figure-html/s35-1.png" width="672" /></p>
<p>Hence, when we omit <span class="math inline">\(X_2\)</span>, the effect of <span class="math inline">\(X_1\)</span> is the area of <span class="math inline">\(\mathbf{D+G}\)</span>, while the true effect is <span class="math inline">\(\mathbf{D}\)</span>. Of course, if <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent, omitting <span class="math inline">\(X_2\)</span> would not be a problem. It is the same thing to say that, if we have independent regressors, running separate regressions each regressor would give us the same results as the one when we combine them. However, when omitted variables are correlated with the regressors (<span class="math inline">\(X\)</span>), the conditional expectation on the error term will be non-zero, <span class="math inline">\(\text{E}(\epsilon|x)\neq0\)</span>. An excellent demonstration of regressions with Venn diagrams can be found in the Andrew Heiss’ <a href="https://www.andrewheiss.com/blog/2021/08/21/r2-euler/">post</a>.</p>
<p>Here is a more realistic example from Stock and Watson (2015, p. 196): suppose we want to understand the relationship between test score and class size. If we run a regression without considering possible confounding factors, we may face the same problem we described above. This is because the percentage of English learners in the school district might be correlated with both test score and class size. Hence, the “true” model should look like equation 2.15:</p>
<p><span class="math display" id="eq:2-15">\[\begin{equation}
Test Score =\beta_{0}+\beta_{1}S T R+\beta_{2}PctEL+\epsilon_{i},
  \tag{2.15}
\end{equation}\]</span></p>
<p>where <em>STR</em> and <em>PctEL</em> are correlated, that is <span class="math inline">\(\rho_{str,~ pctel} \neq 0\)</span>. We can omit <em>PctEL</em> in 2.15 and estimate it as</p>
<p><span class="math display" id="eq:2-16">\[\begin{equation}
Test Score =\hat{\beta}_{0}+\hat{\beta}_{1}STR+v_{i}.
  \tag{2.16}
\end{equation}\]</span></p>
<p>Intuitively, the errors are the residuals in regressions as they collect all “other” factors that are not explicitly modeled in the regrassion but affect the outcome randomly. They are supposed to be independent from all the regressors in the model. As the omitted variable, <em>PctEL</em>, joins to the residual (<span class="math inline">\(v_i\)</span>), <span class="math inline">\(\hat{\beta}_{1}\)</span> will not reflect the true effect of changes in STR on the test score. We can formally see the result of this omission as follows:<br />
<span class="math display">\[
\hat{\beta}_{1}=\frac{\mathrm{Cov}\left(STR_{i},TestScore_{i}\right)}{\mathrm{Var}\left(STR_{i}\right)}=\frac{\sum_{i}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i}\left(x_{i}-\bar{x}\right)^{2}}=\frac{\sum_{i}\left(x_{i}-\bar{x}\right) y_{i}}{\sum_{i}\left(x_{i}-\bar{x}\right) x_{i}}
\]</span></p>
<p>We can substitute <span class="math inline">\(y_i\)</span> into the last term and simplify:</p>
<p><span class="math display">\[
\begin{aligned} \hat{\beta}_{1} &amp;=\frac{\sum_{i}\left(x_{i}-\bar{x}\right)\left(\beta_{0}+\beta_{1} x_{i}+\beta_{2} z_{i}+\epsilon_{i}\right)}{\sum_{i}\left(x_{i}-\bar{x}\right) x_{i}} =\beta_{1}+\beta_{2} \frac{\sum_{i}\left(x_{i}-\bar{x}\right) z_{i}}{\sum_{i}\left(x_{i}-\bar{x}\right) x_{i}}+\frac{\sum_{i}\left(x_{i}-\bar{x}\right) \epsilon_{i}}{\sum_{i}\left(x_{i}-\bar{x}\right) x_{i}}, \end{aligned}
\]</span></p>
<p>where <span class="math inline">\(z\)</span> is <span class="math inline">\(PctEL\)</span> (omitted variable), <span class="math inline">\(x\)</span> is <span class="math inline">\(STR\)</span>, <span class="math inline">\(y\)</span> is <span class="math inline">\(TestScore\)</span>. The second term is a result of our omission of variable <em>PctEL</em> (<span class="math inline">\(z\)</span>). If we take the expectation of the last line:</p>
<p><span class="math display">\[
\begin{aligned} \mathrm{E}\left[\hat{\beta}_{1}\right] &amp;=\mathrm{E}\left[\beta_{1}+\beta_{2} \frac{\sum_{i}\left(x_{i}-\bar{x}\right) z_{i}}{\sum_{i}\left(x_{i}-\bar{x}\right) x_{i}}+\frac{\sum_{i}\left(x_{i}-\bar{x}\right) \epsilon_{i}}{\sum_{i}\left(x_{i}-\bar{x}\right) x_{i}}\right] \\ &amp;=\beta_{1}+\beta_{2} \mathrm{E}\left[\frac{\sum_{i}\left(x_{i}-\bar{x}\right) z_{i}}{\sum_{i}\left(x_{i}-\bar{x}\right) x_{i}}\right]+\mathrm{E}\left[\frac{\sum_{i}\left(x_{i}-\bar{x}\right) \epsilon_{i}}{\sum_{i}\left(x_{i}-\bar{x}\right) x_{i}}\right] \\ &amp;=\beta_{1}+\beta_{2} [\mathrm{Cov}(x_i,z_i)/\mathrm{Var}(x_i)] \end{aligned}
\]</span></p>
<p>What this means is that on average, our regression estimate is going to miss the true population parameter by the second term. Here is the OVB in action:</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="preliminaries.html#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load the AER package</span></span>
<span id="cb108-2"><a href="preliminaries.html#cb108-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(AER)</span>
<span id="cb108-3"><a href="preliminaries.html#cb108-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb108-4"><a href="preliminaries.html#cb108-4" aria-hidden="true" tabindex="-1"></a><span class="co"># load the data set</span></span>
<span id="cb108-5"><a href="preliminaries.html#cb108-5" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(CASchools)</span>
<span id="cb108-6"><a href="preliminaries.html#cb108-6" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(CASchools)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    420 obs. of  14 variables:
##  $ district   : chr  &quot;75119&quot; &quot;61499&quot; &quot;61549&quot; &quot;61457&quot; ...
##  $ school     : chr  &quot;Sunol Glen Unified&quot; &quot;Manzanita Elementary&quot; &quot;Thermalito Union Elementary&quot; &quot;Golden Feather Union Elementary&quot; ...
##  $ county     : Factor w/ 45 levels &quot;Alameda&quot;,&quot;Butte&quot;,..: 1 2 2 2 2 6 29 11 6 25 ...
##  $ grades     : Factor w/ 2 levels &quot;KK-06&quot;,&quot;KK-08&quot;: 2 2 2 2 2 2 2 2 2 1 ...
##  $ students   : num  195 240 1550 243 1335 ...
##  $ teachers   : num  10.9 11.1 82.9 14 71.5 ...
##  $ calworks   : num  0.51 15.42 55.03 36.48 33.11 ...
##  $ lunch      : num  2.04 47.92 76.32 77.05 78.43 ...
##  $ computer   : num  67 101 169 85 171 25 28 66 35 0 ...
##  $ expenditure: num  6385 5099 5502 7102 5236 ...
##  $ income     : num  22.69 9.82 8.98 8.98 9.08 ...
##  $ english    : num  0 4.58 30 0 13.86 ...
##  $ read       : num  692 660 636 652 642 ...
##  $ math       : num  690 662 651 644 640 ...</code></pre>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="preliminaries.html#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define variables</span></span>
<span id="cb110-2"><a href="preliminaries.html#cb110-2" aria-hidden="true" tabindex="-1"></a>CASchools<span class="sc">$</span>STR <span class="ot">&lt;-</span> CASchools<span class="sc">$</span>students<span class="sc">/</span>CASchools<span class="sc">$</span>teachers       </span>
<span id="cb110-3"><a href="preliminaries.html#cb110-3" aria-hidden="true" tabindex="-1"></a>CASchools<span class="sc">$</span>score <span class="ot">&lt;-</span> (CASchools<span class="sc">$</span>read <span class="sc">+</span> CASchools<span class="sc">$</span>math)<span class="sc">/</span><span class="dv">2</span></span></code></pre></div>
<p>Let’s estimate both regression models and compare.</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="preliminaries.html#cb111-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate both regressions</span></span>
<span id="cb111-2"><a href="preliminaries.html#cb111-2" aria-hidden="true" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(score <span class="sc">~</span> STR, <span class="at">data =</span> CASchools) <span class="co"># Underfitted model</span></span>
<span id="cb111-3"><a href="preliminaries.html#cb111-3" aria-hidden="true" tabindex="-1"></a>model2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(score <span class="sc">~</span> STR <span class="sc">+</span> english, <span class="at">data =</span> CASchools) <span class="co"># True model</span></span>
<span id="cb111-4"><a href="preliminaries.html#cb111-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb111-5"><a href="preliminaries.html#cb111-5" aria-hidden="true" tabindex="-1"></a>model1</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ STR, data = CASchools)
## 
## Coefficients:
## (Intercept)          STR  
##      698.93        -2.28</code></pre>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="preliminaries.html#cb113-1" aria-hidden="true" tabindex="-1"></a>model2</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ STR + english, data = CASchools)
## 
## Coefficients:
## (Intercept)          STR      english  
##    686.0322      -1.1013      -0.6498</code></pre>
<p>Is the magnitude of the bias, <span class="math inline">\(-1.1787 = -2.28 - (-1.1013)\)</span>, consistent with the formula, <span class="math inline">\(\beta_{2} [\mathrm{Cov}(x_i,z_i)/\mathrm{Var}(x_i)]\)</span>?</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="preliminaries.html#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cov</span>(CASchools<span class="sc">$</span>STR, CASchools<span class="sc">$</span>english)<span class="sc">/</span><span class="fu">var</span>(CASchools<span class="sc">$</span>STR)<span class="sc">*</span>model2<span class="sc">$</span>coefficients[<span class="dv">3</span>]</span></code></pre></div>
<pre><code>##   english 
## -1.178512</code></pre>
</div>
<div id="modeling-the-data" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> Modeling the data<a href="preliminaries.html#modeling-the-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When modeling data, there are a number of choices that need to be made.</p>
<p>What <strong>family</strong> of models will be considered? In linear regression, we specified models with parameters (<span class="math inline">\(\beta{j}\)</span>) and fit the model by finding the best values of these parameters. This is a <em>parametric</em> approach. A <em>non-parametric</em> approach skips the step of specifying a model with parameters and are often described as more of an algorithm. Non-parametric models are often used in machine learning, which we will see in Part II. Here is an example</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="preliminaries.html#cb117-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> CASchools<span class="sc">$</span>STR</span>
<span id="cb117-2"><a href="preliminaries.html#cb117-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> CASchools<span class="sc">$</span>score</span>
<span id="cb117-3"><a href="preliminaries.html#cb117-3" aria-hidden="true" tabindex="-1"></a>xt <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(x), <span class="fu">max</span>(x), <span class="at">length.out =</span> <span class="fu">length</span>(x))</span>
<span id="cb117-4"><a href="preliminaries.html#cb117-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="st">&quot;grey&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Parametric (red) vs Nonparametric (blue)&quot;</span>)</span>
<span id="cb117-5"><a href="preliminaries.html#cb117-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">predict</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x)), <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb117-6"><a href="preliminaries.html#cb117-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(xt, <span class="fu">predict</span>(<span class="fu">loess</span>(y <span class="sc">~</span> x, <span class="at">degree =</span> <span class="dv">2</span>, <span class="at">span =</span><span class="fl">0.2</span>), xt), <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="02-Preliminaries_files/figure-html/s39-1.png" width="672" /></p>
<p>If we define a parametric model, what <strong>form</strong> of the model will be used for <span class="math inline">\(f(.)\)</span> shown below?</p>
<p><span class="math display">\[
y =f\left(x_{1}, x_{2}, x_{3}, \ldots, x_{p}\right)+\epsilon
\]</span></p>
<p>Would it be linear or polynomial?</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="preliminaries.html#cb118-1" aria-hidden="true" tabindex="-1"></a>pfit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x <span class="sc">+</span> <span class="fu">I</span>(x<span class="sc">^</span><span class="dv">2</span>)<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">3</span>)<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">4</span>)<span class="sc">+</span><span class="fu">I</span>(x<span class="sc">^</span><span class="dv">5</span>))</span>
<span id="cb118-2"><a href="preliminaries.html#cb118-2" aria-hidden="true" tabindex="-1"></a>dt <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">yhat =</span> pfit<span class="sc">$</span>fitted.values, <span class="at">x =</span> x)</span>
<span id="cb118-3"><a href="preliminaries.html#cb118-3" aria-hidden="true" tabindex="-1"></a>dt <span class="ot">&lt;-</span> dt[<span class="fu">order</span>(dt<span class="sc">$</span>x), ]</span>
<span id="cb118-4"><a href="preliminaries.html#cb118-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-5"><a href="preliminaries.html#cb118-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">col =</span> <span class="st">&quot;grey&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Linear (red) vs Polynomial (blue)&quot;</span>)</span>
<span id="cb118-6"><a href="preliminaries.html#cb118-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x, <span class="fu">predict</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x)), <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb118-7"><a href="preliminaries.html#cb118-7" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(dt<span class="sc">$</span>x, dt<span class="sc">$</span>yhat, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="02-Preliminaries_files/figure-html/s40-1.png" width="672" /></p>
<p>If we are going to add non-linear terms, which variables would be selected with what degree of polynomial terms? Moreover, if there are interaction between the predictors, the effect of a regressor on the response will change depending on the values of the other predictors. Hence, we need to assume what the “true” population model (DGM) would be when searching for a model.</p>
<p>How will the model be <strong>fit</strong>? Although we have seen two of the most common techniques, OLS and MLE, there are more techniques in the literature.</p>
<p>Addressing these three questions are the fundamental steps in defining the relationships between variables and could be different in causal and predictive analyses.</p>
</div>
<div id="causal-vs.-predictive-models" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> Causal vs. Predictive Models<a href="preliminaries.html#causal-vs.-predictive-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>What is the purpose of fitting a model to data? Usually it is to accomplish one of two goals. We can use a model to <strong>explain</strong> the causal relationship between the response and the explanatory variables. And, we can also use a model to <strong>predict</strong> the outocme variable.</p>
<div id="causal-models" class="section level3 hasAnchor" number="2.7.1">
<h3><span class="header-section-number">2.7.1</span> Causal Models<a href="preliminaries.html#causal-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If the goal of a model is to explain the causal relationship between the response and one or more of the explanatory variables, we are looking for a model that is <strong>small and interpretable</strong>, but still fits the data well.</p>
<p>Suppose we would like to identify the factors that explain fuel efficiency (mpg - miles per gallon) based on a car’s attributes, <code>weight</code>, <code>year</code>, <code>hp</code>, etc. If we are trying to understand how fuel efficiency is determined by a car’s attributes, we may want to have a <strong>less complex and interpretable</strong> model. Note that parametric models, particularly linear models of any size, are the most interpretable models to begin with. If our objective is to predict if a car would be classified as efficient or not given its attributes, we may give up interpretablity and use more complicated methods that may have better prediction accuracy. We will see later many examples of this trade-off.</p>
<p>To find small and interpretable models, we use <strong>inferential</strong> techniques with additional assumptions about the error terms in a model:</p>
<p><span class="math display">\[
\epsilon \sim N\left(0, \sigma^{2}\right)
\]</span></p>
<p>This assumption states that the the error is normally distributed with some common variance. Also, this assumption states that the expected value of the error term is zero. In order words, the model has to be correctly specified without any omitted variable.</p>
<p>One very important issue to understand a causal relationship is to distinguish two terms often used to describe a relationship between two variables: <strong>causation</strong> and <strong>correlation</strong>, both of which <strong>explain</strong> the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>.</p>
<p>Correlation is often also referred to as association. One good example is the empirical relationship between ice cream sales and the crime rate in a given region. <a href="https://en.wikipedia.org/wiki/The_Book_of_Why">It has been shown that</a> <span class="citation">(<a href="#ref-Pearl_Mackenzie_2018" role="doc-biblioref"><strong>Pearl_Mackenzie_2018?</strong></a>)</span> the correlation between these two measures are strong and positive. Just because these two variables are correlated does not necessarily mean that one causes the other (as people eat more ice cream, they commit more crimes?). Perhaps there is a third variable that explains both! And, it is known that very hot weather is that third missing factor that causes both ice cream sales and crime rates to go up. You can see many more absurd examples on the <a href="http://tylervigen.com/spurious-correlations">Spurious Correlations website</a>.</p>
<p>Causation is distinct from correlation, because it reflects a relationship in which one variable directly effects another. Rather than just an association between variables that may be caused by a third hidden variable, causation implies a direct link between the two. Continuing the example from earlier, the very hot weather has a causal connection with both ice cream sales and crime, even though those two outcomes only share a correlation with each other.</p>
</div>
<div id="predictive-models" class="section level3 hasAnchor" number="2.7.2">
<h3><span class="header-section-number">2.7.2</span> Predictive Models<a href="preliminaries.html#predictive-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If the goal of a model is to predict the response, then the only consideration is how well the model fits the data. Hence we do not need to have distributional assumptions as stated above. More specifically, correlation and causation are not an issue here. If a predictor is correlated with the response, it would be useful for prediction. For example, ice-cream sales are perfectly fine if they predict the crime rates.</p>
<p>However, as we see later, a word of caution is needed when using a model to predict an outcome. Mud on the ground would predict that we had a rain. Or if a person has been hospitalized for the last 3 months, we can predict that the person was sick. These types of predictions are useless and called usually <strong>model or data leaking</strong> in machine learning.</p>
<p><strong>What happens if we use a model built for causal analysis to predict the outcome?</strong> We will answer this question later. Keep this question in mind, because it will be a fundamental question to understand how statistical learning would be different than a model that seeks a causation between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>.</p>
<p>Since we are not performing inference, life is relatively easier with predictive models. Therefore, the extra assumptions about the model specifications and the distributional aspects of the estimators are not needed. We only care about prediction error or the prediction accuracy. Although we may have a secondary objective in predictive models, which is to identify the most important predictors, they would be useless for causal inference in most cases. The best predictive model minimizes the prediction error, which is the following root-mean-squared-prediction-error for numerical outcomes:</p>
<p><span class="math display">\[
\text { RMSPE }=\sqrt{\frac{1}{n} \sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}},
\]</span></p>
<p>where <span class="math inline">\(y_i\)</span> are the actual values of the response for the “given data” and <span class="math inline">\(\hat{y}\)</span> are the predicted values using the fitted model and the predictors from the data. Note that RMSPE has the same unit as the response variable. Later, we will see more performance metrics in predictive models.</p>
<p>An important issue in calculating RMSPE is which <span class="math inline">\(y\)</span>’s are supposed to be predicted. If we use the same” <span class="math inline">\(y\)</span>’s that we also use to calculate <span class="math inline">\(\hat{y}\)</span>’s, RMSPE will tend to be lower for a larger and more complex models. However, a model becomes too specific for a sample as it gets more complex, which is called <strong>overfitting</strong>. In other words, when a model overfits, it will be less “generalizable” for another sample. Consequently, these overly specific models would have very poor predictions for “out-of-sample” data. This topic will be covered in the next section. But before that, lets have an example that shows an overfitting model:</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="preliminaries.html#cb119-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple OLS</span></span>
<span id="cb119-2"><a href="preliminaries.html#cb119-2" aria-hidden="true" tabindex="-1"></a>model_ols <span class="ot">&lt;-</span> <span class="fu">lm</span>(dist <span class="sc">~</span> speed, <span class="at">data =</span> cars)</span>
<span id="cb119-3"><a href="preliminaries.html#cb119-3" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fu">coef</span>(model_ols)</span>
<span id="cb119-4"><a href="preliminaries.html#cb119-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-5"><a href="preliminaries.html#cb119-5" aria-hidden="true" tabindex="-1"></a><span class="co"># A complex model</span></span>
<span id="cb119-6"><a href="preliminaries.html#cb119-6" aria-hidden="true" tabindex="-1"></a>model_cmpx <span class="ot">&lt;-</span> <span class="fu">smooth.spline</span>(cars<span class="sc">$</span>speed, cars<span class="sc">$</span>dist, <span class="at">df=</span><span class="dv">19</span>)</span>
<span id="cb119-7"><a href="preliminaries.html#cb119-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-8"><a href="preliminaries.html#cb119-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cars, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">1</span>, <span class="at">main =</span> <span class="st">&quot;Complex Model vs. Simple OLS&quot;</span>)</span>
<span id="cb119-9"><a href="preliminaries.html#cb119-9" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(b, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb119-10"><a href="preliminaries.html#cb119-10" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(model_cmpx,<span class="at">col=</span><span class="st">&#39;green&#39;</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="02-Preliminaries_files/figure-html/s41-1.png" width="672" /></p>
<p>The figure above shows two models to predict the stopping distance a car by its speed. The “complex” model uses a non-parametric method (green line), which has the minimum RMSPE relative to the red dashed line representing a simple linear model. We have only one sample and the “complex” model is the winner with the smallest RMSPE.</p>
<p>But if we use these two models on “unseen” (out-of-sample) data, would the winner change? Would it be possible to have the following results (consider only the order of the numbers)?</p>
<table>
<thead>
<tr class="header">
<th align="center">Type of Model</th>
<th align="center">In-Sample RMSPE</th>
<th align="center">Out-Sample RMSPE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Simple model</td>
<td align="center">1.71</td>
<td align="center">1.45</td>
</tr>
<tr class="even">
<td align="center">Complex model</td>
<td align="center">1.41</td>
<td align="center">2.07</td>
</tr>
</tbody>
</table>
<p>We will answer it in coming chapters in details but, for now, let use our first simulation exercise.</p>
</div>
</div>
<div id="simulation" class="section level2 hasAnchor" number="2.8">
<h2><span class="header-section-number">2.8</span> Simulation<a href="preliminaries.html#simulation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Simulations are tools to see whether statistical arguments are true or not. In simulations, we know the data generating process (DGP) because we define them by a selected model and a set of parameters. On of the biggest strengths of R is its ability to carry out simulations with a simple design. We will see more examples on simulations in Chapter 37.</p>
<p>We are going to generate a sample of observations on <span class="math inline">\(Y\)</span> from a data generation model (DGM):</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="preliminaries.html#cb120-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb120-2"><a href="preliminaries.html#cb120-2" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">0</span>, <span class="at">to =</span> <span class="dv">20</span>, <span class="at">by =</span> <span class="fl">0.1</span>)</span>
<span id="cb120-3"><a href="preliminaries.html#cb120-3" aria-hidden="true" tabindex="-1"></a>dgm <span class="ot">&lt;-</span> <span class="dv">500</span> <span class="sc">+</span> <span class="dv">20</span><span class="sc">*</span>X <span class="sc">-</span> <span class="dv">90</span><span class="sc">*</span><span class="fu">sin</span>(X)                   <span class="co">#This is our DGM</span></span>
<span id="cb120-4"><a href="preliminaries.html#cb120-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-5"><a href="preliminaries.html#cb120-5" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> dgm <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="fu">length</span>(X), <span class="at">mean =</span> <span class="dv">10</span>, <span class="at">sd =</span> <span class="dv">100</span>) <span class="co">#This is our DGP</span></span>
<span id="cb120-6"><a href="preliminaries.html#cb120-6" aria-hidden="true" tabindex="-1"></a>data <span class="ot">=</span> <span class="fu">data.frame</span>(y, X)</span>
<span id="cb120-7"><a href="preliminaries.html#cb120-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-8"><a href="preliminaries.html#cb120-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X, y, <span class="at">col=</span><span class="st">&#39;deepskyblue4&#39;</span>,</span>
<span id="cb120-9"><a href="preliminaries.html#cb120-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">&#39;X&#39;</span>, <span class="at">main=</span><span class="st">&#39;Observed data &amp; DGM&#39;</span>)</span>
<span id="cb120-10"><a href="preliminaries.html#cb120-10" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(X, dgm, <span class="at">col=</span><span class="st">&#39;firebrick1&#39;</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="02-Preliminaries_files/figure-html/s42-1.png" width="672" /></p>
<p>This is the plot of our simulated data. The simulated data points are the blue dots while the red line is the DGM or the systematic part. Now we have the data (<span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>) and we also know the underlying data generating procedure (DGP) that produces these observations. Let’s pretend that we do not know DGP. Our job is to estimate DGM from the data we have. We will use three alternative models to estimate the true DGM.</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="preliminaries.html#cb121-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear model</span></span>
<span id="cb121-2"><a href="preliminaries.html#cb121-2" aria-hidden="true" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> X)</span>
<span id="cb121-3"><a href="preliminaries.html#cb121-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X, y, <span class="at">col=</span><span class="st">&#39;deepskyblue4&#39;</span>, <span class="at">xlab=</span><span class="st">&#39;X&#39;</span>, <span class="at">main=</span><span class="st">&#39;Linear&#39;</span>)</span>
<span id="cb121-4"><a href="preliminaries.html#cb121-4" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(X, model1<span class="sc">$</span>fitted.values, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="02-Preliminaries_files/figure-html/s43-1.png" width="672" /></p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="preliminaries.html#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Polynomial model (there is an easier way!)</span></span>
<span id="cb122-2"><a href="preliminaries.html#cb122-2" aria-hidden="true" tabindex="-1"></a>model2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> X <span class="sc">+</span> <span class="fu">I</span>(X<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> <span class="fu">I</span>(X<span class="sc">^</span><span class="dv">3</span>) <span class="sc">+</span> <span class="fu">I</span>(X<span class="sc">^</span><span class="dv">4</span>) <span class="sc">+</span>  <span class="fu">I</span>(X<span class="sc">^</span><span class="dv">5</span>) <span class="sc">+</span> <span class="fu">I</span>(X<span class="sc">^</span><span class="dv">6</span>) <span class="sc">+</span> <span class="fu">I</span>(X<span class="sc">^</span><span class="dv">7</span>) <span class="sc">+</span> <span class="fu">I</span>(X<span class="sc">^</span><span class="dv">8</span>)</span>
<span id="cb122-3"><a href="preliminaries.html#cb122-3" aria-hidden="true" tabindex="-1"></a>             <span class="sc">+</span> <span class="fu">I</span>(X<span class="sc">^</span><span class="dv">9</span>) <span class="sc">+</span> <span class="fu">I</span>(X<span class="sc">^</span><span class="dv">10</span>) <span class="sc">+</span> <span class="fu">I</span>(X<span class="sc">^</span><span class="dv">11</span>) <span class="sc">+</span> <span class="fu">I</span>(X<span class="sc">^</span><span class="dv">12</span>) <span class="sc">+</span> <span class="fu">I</span>(X<span class="sc">^</span><span class="dv">13</span>) <span class="sc">+</span> <span class="fu">I</span>(X<span class="sc">^</span><span class="dv">14</span>)</span>
<span id="cb122-4"><a href="preliminaries.html#cb122-4" aria-hidden="true" tabindex="-1"></a>             <span class="sc">+</span> <span class="fu">I</span>(X<span class="sc">^</span><span class="dv">15</span>) <span class="sc">+</span> <span class="fu">I</span>(X<span class="sc">^</span><span class="dv">16</span>) <span class="sc">+</span> <span class="fu">I</span>(X<span class="sc">^</span><span class="dv">17</span>) <span class="sc">+</span> <span class="fu">I</span>(X<span class="sc">^</span><span class="dv">18</span>), <span class="at">data=</span>data)</span>
<span id="cb122-5"><a href="preliminaries.html#cb122-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X, y, <span class="at">col=</span> <span class="st">&#39;deepskyblue4&#39;</span>, <span class="at">xlab=</span><span class="st">&#39;X&#39;</span>, <span class="at">main=</span><span class="st">&#39;Polynomial&#39;</span>)</span>
<span id="cb122-6"><a href="preliminaries.html#cb122-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(X, <span class="fu">fitted</span>(model2), <span class="at">col=</span><span class="st">&#39;darkgreen&#39;</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="02-Preliminaries_files/figure-html/s43-2.png" width="672" /></p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="preliminaries.html#cb123-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Nonparametric</span></span>
<span id="cb123-2"><a href="preliminaries.html#cb123-2" aria-hidden="true" tabindex="-1"></a>model3 <span class="ot">&lt;-</span> <span class="fu">smooth.spline</span>(X, y, <span class="at">df=</span><span class="dv">200</span>)</span>
<span id="cb123-3"><a href="preliminaries.html#cb123-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X, y, <span class="at">col=</span><span class="st">&#39;deepskyblue4&#39;</span>, <span class="at">xlab=</span><span class="st">&#39;X&#39;</span>, <span class="at">main=</span><span class="st">&#39;Spline&#39;</span>)</span>
<span id="cb123-4"><a href="preliminaries.html#cb123-4" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(model3, <span class="at">col=</span><span class="st">&#39;orange&#39;</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="02-Preliminaries_files/figure-html/s43-3.png" width="672" /></p>
<p>As obvious from the plots, the nonparametric spline model (we’ll see later what it is) should have the minimum RMSPE.</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="preliminaries.html#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Let create a function for RMSPE</span></span>
<span id="cb124-2"><a href="preliminaries.html#cb124-2" aria-hidden="true" tabindex="-1"></a>rmse <span class="ot">=</span> <span class="cf">function</span>(actual, predicted) {</span>
<span id="cb124-3"><a href="preliminaries.html#cb124-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sqrt</span>(<span class="fu">mean</span>((actual <span class="sc">-</span> predicted) <span class="sc">^</span> <span class="dv">2</span>))</span>
<span id="cb124-4"><a href="preliminaries.html#cb124-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb124-5"><a href="preliminaries.html#cb124-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-6"><a href="preliminaries.html#cb124-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Predicted values by the 3 models using the &quot;seen&quot; data</span></span>
<span id="cb124-7"><a href="preliminaries.html#cb124-7" aria-hidden="true" tabindex="-1"></a>predicted1 <span class="ot">&lt;-</span> <span class="fu">fitted</span>(model1)</span>
<span id="cb124-8"><a href="preliminaries.html#cb124-8" aria-hidden="true" tabindex="-1"></a>predicted2 <span class="ot">&lt;-</span> <span class="fu">fitted</span>(model2)</span>
<span id="cb124-9"><a href="preliminaries.html#cb124-9" aria-hidden="true" tabindex="-1"></a>predicted3 <span class="ot">&lt;-</span> <span class="fu">predict</span>(model3, X)</span>
<span id="cb124-10"><a href="preliminaries.html#cb124-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-11"><a href="preliminaries.html#cb124-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Note that the actual y is the same for all models</span></span>
<span id="cb124-12"><a href="preliminaries.html#cb124-12" aria-hidden="true" tabindex="-1"></a>rmse1_s <span class="ot">&lt;-</span> <span class="fu">rmse</span>(predicted1, y)</span>
<span id="cb124-13"><a href="preliminaries.html#cb124-13" aria-hidden="true" tabindex="-1"></a>rmse2_s <span class="ot">&lt;-</span> <span class="fu">rmse</span>(predicted2, y)</span>
<span id="cb124-14"><a href="preliminaries.html#cb124-14" aria-hidden="true" tabindex="-1"></a>rmse3_s <span class="ot">&lt;-</span> <span class="fu">rmse</span>(predicted3<span class="sc">$</span>y, y)</span>
<span id="cb124-15"><a href="preliminaries.html#cb124-15" aria-hidden="true" tabindex="-1"></a>seen <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;RMSPE for model1 (linear)&quot;</span> <span class="ot">=</span> rmse1_s, <span class="st">&quot;RMSPE for model2 (polynomial)&quot;</span> <span class="ot">=</span> rmse2_s,</span>
<span id="cb124-16"><a href="preliminaries.html#cb124-16" aria-hidden="true" tabindex="-1"></a>          <span class="st">&quot;RMSPE for model3 (nonparametric)&quot;</span> <span class="ot">=</span> rmse3_s )</span>
<span id="cb124-17"><a href="preliminaries.html#cb124-17" aria-hidden="true" tabindex="-1"></a>seen</span></code></pre></div>
<pre><code>##        RMSPE for model1 (linear)    RMSPE for model2 (polynomial) 
##                        119.46405                         88.87396 
## RMSPE for model3 (nonparametric) 
##                         67.72450</code></pre>
<p>Now we will test them on another sample from the same DGP that we haven’t seen before:</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="preliminaries.html#cb126-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Since DGM is the same the only difference is the random error</span></span>
<span id="cb126-2"><a href="preliminaries.html#cb126-2" aria-hidden="true" tabindex="-1"></a><span class="co"># in this sample</span></span>
<span id="cb126-3"><a href="preliminaries.html#cb126-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb126-4"><a href="preliminaries.html#cb126-4" aria-hidden="true" tabindex="-1"></a>y2 <span class="ot">=</span> dgm <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="fu">length</span>(X), <span class="at">mean =</span> <span class="dv">10</span>, <span class="at">sd =</span> <span class="dv">100</span>) </span>
<span id="cb126-5"><a href="preliminaries.html#cb126-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(X, y2, <span class="at">col=</span><span class="st">&#39;deepskyblue4&#39;</span>,</span>
<span id="cb126-6"><a href="preliminaries.html#cb126-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">&#39;X&#39;</span>,</span>
<span id="cb126-7"><a href="preliminaries.html#cb126-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&#39;The &quot;Unseen&quot; 2nd Sample&#39;</span>)</span></code></pre></div>
<p><img src="02-Preliminaries_files/figure-html/s45-1.png" width="672" /></p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="preliminaries.html#cb127-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Since DGM is the same X&#39;s are the same</span></span>
<span id="cb127-2"><a href="preliminaries.html#cb127-2" aria-hidden="true" tabindex="-1"></a>rmse1_us <span class="ot">&lt;-</span> <span class="fu">rmse</span>(predicted1, y2)</span>
<span id="cb127-3"><a href="preliminaries.html#cb127-3" aria-hidden="true" tabindex="-1"></a>rmse2_us<span class="ot">&lt;-</span> <span class="fu">rmse</span>(predicted2, y2)</span>
<span id="cb127-4"><a href="preliminaries.html#cb127-4" aria-hidden="true" tabindex="-1"></a>rmse3_us <span class="ot">&lt;-</span> <span class="fu">rmse</span>(predicted3<span class="sc">$</span>y, y2)</span>
<span id="cb127-5"><a href="preliminaries.html#cb127-5" aria-hidden="true" tabindex="-1"></a>unseen <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;RMSPE for model1 (linear)&quot;</span> <span class="ot">=</span> rmse1_us,</span>
<span id="cb127-6"><a href="preliminaries.html#cb127-6" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;RMSPE for model2 (polynomial)&quot;</span> <span class="ot">=</span> rmse2_us,</span>
<span id="cb127-7"><a href="preliminaries.html#cb127-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;RMSPE for model3 (nonparametric)&quot;</span> <span class="ot">=</span> rmse3_us)</span></code></pre></div>
<p>Let’s put them together:</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="preliminaries.html#cb128-1" aria-hidden="true" tabindex="-1"></a>table <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb128-2"><a href="preliminaries.html#cb128-2" aria-hidden="true" tabindex="-1"></a><span class="fu">row.names</span>(table) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Seen-data&quot;</span>, <span class="st">&quot;Unseen-data&quot;</span>)</span>
<span id="cb128-3"><a href="preliminaries.html#cb128-3" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(table) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Linear&quot;</span>, <span class="st">&quot;Polynomial&quot;</span>, <span class="st">&quot;Spline&quot;</span>)</span>
<span id="cb128-4"><a href="preliminaries.html#cb128-4" aria-hidden="true" tabindex="-1"></a>table[<span class="dv">1</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span> seen[<span class="dv">1</span>]</span>
<span id="cb128-5"><a href="preliminaries.html#cb128-5" aria-hidden="true" tabindex="-1"></a>table[<span class="dv">1</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span> seen[<span class="dv">2</span>]</span>
<span id="cb128-6"><a href="preliminaries.html#cb128-6" aria-hidden="true" tabindex="-1"></a>table[<span class="dv">1</span>,<span class="dv">3</span>] <span class="ot">&lt;-</span> seen[<span class="dv">3</span>]</span>
<span id="cb128-7"><a href="preliminaries.html#cb128-7" aria-hidden="true" tabindex="-1"></a>table[<span class="dv">2</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span> unseen[<span class="dv">1</span>]</span>
<span id="cb128-8"><a href="preliminaries.html#cb128-8" aria-hidden="true" tabindex="-1"></a>table[<span class="dv">2</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span> unseen[<span class="dv">2</span>]</span>
<span id="cb128-9"><a href="preliminaries.html#cb128-9" aria-hidden="true" tabindex="-1"></a>table[<span class="dv">2</span>,<span class="dv">3</span>] <span class="ot">&lt;-</span> unseen[<span class="dv">3</span>]</span>
<span id="cb128-10"><a href="preliminaries.html#cb128-10" aria-hidden="true" tabindex="-1"></a>table</span></code></pre></div>
<pre><code>##               Linear Polynomial   Spline
## Seen-data   119.4640   88.87396  67.7245
## Unseen-data 123.4378  109.99681 122.3018</code></pre>
<p>The last model estimated by Spline has the minimum RMSPE using the seen data. It fits very well when we use the <strong>seen</strong> data but it is not so good at predicting the outcomes in the <strong>unseen</strong> data. A better fitting model using only the <strong>seen</strong> data could be worse in prediction. This is called <strong>overfitting</strong>, which is what we will see in the next chapter.</p>

</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="intro.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="learning-systems.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/yaydede/toolbox/edit/master/02-Preliminaries.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["YA_TextBook.pdf", "YA_TextBook.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
