<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Nonparametric Classifier - kNN | Toolbox for Social Scientists and Policy Analysts</title>
  <meta name="description" content="Chapter 8 Nonparametric Classifier - kNN | Toolbox for Social Scientists and Policy Analysts" />
  <meta name="generator" content="bookdown 0.32.1 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Nonparametric Classifier - kNN | Toolbox for Social Scientists and Policy Analysts" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://yaydede.github.io/toolbox//png/cover2.png" />
  
  <meta name="github-repo" content="yaydede/ToolShed_draft" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Nonparametric Classifier - kNN | Toolbox for Social Scientists and Policy Analysts" />
  
  
  <meta name="twitter:image" content="https://yaydede.github.io/toolbox//png/cover2.png" />

<meta name="author" content="Yigit Aydede" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="smoothing.html"/>
<link rel="next" href="hyperparameter-tuning.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Toolbox</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who"><i class="fa fa-check"></i>Who</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> How we define Machine Learning</a></li>
<li class="chapter" data-level="2" data-path="preliminaries.html"><a href="preliminaries.html"><i class="fa fa-check"></i><b>2</b> Preliminaries</a>
<ul>
<li class="chapter" data-level="2.1" data-path="preliminaries.html"><a href="preliminaries.html#data-and-dataset-types"><i class="fa fa-check"></i><b>2.1</b> Data and dataset types</a></li>
<li class="chapter" data-level="2.2" data-path="preliminaries.html"><a href="preliminaries.html#plots"><i class="fa fa-check"></i><b>2.2</b> Plots</a></li>
<li class="chapter" data-level="2.3" data-path="preliminaries.html"><a href="preliminaries.html#probability-distributions-with-r"><i class="fa fa-check"></i><b>2.3</b> Probability Distributions with R</a></li>
<li class="chapter" data-level="2.4" data-path="preliminaries.html"><a href="preliminaries.html#regressions"><i class="fa fa-check"></i><b>2.4</b> Regressions</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="preliminaries.html"><a href="preliminaries.html#ordinary-least-squares-ols"><i class="fa fa-check"></i><b>2.4.1</b> Ordinary Least Squares (OLS)</a></li>
<li class="chapter" data-level="2.4.2" data-path="preliminaries.html"><a href="preliminaries.html#maximum-likelihood-estimators"><i class="fa fa-check"></i><b>2.4.2</b> Maximum Likelihood Estimators</a></li>
<li class="chapter" data-level="2.4.3" data-path="preliminaries.html"><a href="preliminaries.html#estimating-mle-with-r"><i class="fa fa-check"></i><b>2.4.3</b> Estimating MLE with R</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="preliminaries.html"><a href="preliminaries.html#blue"><i class="fa fa-check"></i><b>2.5</b> BLUE</a></li>
<li class="chapter" data-level="2.6" data-path="preliminaries.html"><a href="preliminaries.html#modeling-the-data"><i class="fa fa-check"></i><b>2.6</b> Modeling the data</a></li>
<li class="chapter" data-level="2.7" data-path="preliminaries.html"><a href="preliminaries.html#causal-vs.-predictive-models"><i class="fa fa-check"></i><b>2.7</b> Causal vs. Predictive Models</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="preliminaries.html"><a href="preliminaries.html#causal-models"><i class="fa fa-check"></i><b>2.7.1</b> Causal Models</a></li>
<li class="chapter" data-level="2.7.2" data-path="preliminaries.html"><a href="preliminaries.html#predictive-models"><i class="fa fa-check"></i><b>2.7.2</b> Predictive Models</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="preliminaries.html"><a href="preliminaries.html#simulation"><i class="fa fa-check"></i><b>2.8</b> Simulation</a></li>
</ul></li>
<li class="part"><span><b>I Formal Look at Prediction</b></span></li>
<li class="chapter" data-level="" data-path="learning-systems.html"><a href="learning-systems.html"><i class="fa fa-check"></i>Learning Systems</a></li>
<li class="chapter" data-level="3" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html"><i class="fa fa-check"></i><b>3</b> Bias-Variance Tradeoff</a>
<ul>
<li class="chapter" data-level="3.1" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#estimator-and-mse"><i class="fa fa-check"></i><b>3.1</b> Estimator and MSE</a></li>
<li class="chapter" data-level="3.2" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#prediction---mspe"><i class="fa fa-check"></i><b>3.2</b> Prediction - MSPE</a></li>
<li class="chapter" data-level="3.3" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#biased-estimator-as-a-predictor"><i class="fa fa-check"></i><b>3.3</b> Biased estimator as a predictor</a></li>
<li class="chapter" data-level="3.4" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#dropping-a-variable-in-a-regression"><i class="fa fa-check"></i><b>3.4</b> Dropping a variable in a regression</a></li>
<li class="chapter" data-level="3.5" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#uncertainty-in-estimations-and-predictions"><i class="fa fa-check"></i><b>3.5</b> Uncertainty in estimations and predictions</a></li>
<li class="chapter" data-level="3.6" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#prediction-interval-for-unbiased-ols-predictor"><i class="fa fa-check"></i><b>3.6</b> Prediction interval for unbiased OLS predictor</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="overfitting.html"><a href="overfitting.html"><i class="fa fa-check"></i><b>4</b> Overfitting</a></li>
<li class="part"><span><b>II Nonparametric Estimations</b></span></li>
<li class="chapter" data-level="" data-path="parametric-vs.-nonparametric-methods.html"><a href="parametric-vs.-nonparametric-methods.html"><i class="fa fa-check"></i>Parametric vs. Nonparametric methods</a></li>
<li class="chapter" data-level="5" data-path="parametric-estimations.html"><a href="parametric-estimations.html"><i class="fa fa-check"></i><b>5</b> Parametric Estimations</a>
<ul>
<li class="chapter" data-level="5.1" data-path="parametric-estimations.html"><a href="parametric-estimations.html#linear-probability-models-lpm"><i class="fa fa-check"></i><b>5.1</b> Linear Probability Models (LPM)</a></li>
<li class="chapter" data-level="5.2" data-path="parametric-estimations.html"><a href="parametric-estimations.html#logistic-regression"><i class="fa fa-check"></i><b>5.2</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="parametric-estimations.html"><a href="parametric-estimations.html#estimating-logistic-regression"><i class="fa fa-check"></i><b>5.2.1</b> Estimating Logistic Regression</a></li>
<li class="chapter" data-level="5.2.2" data-path="parametric-estimations.html"><a href="parametric-estimations.html#cost-functions"><i class="fa fa-check"></i><b>5.2.2</b> Cost functions</a></li>
<li class="chapter" data-level="5.2.3" data-path="parametric-estimations.html"><a href="parametric-estimations.html#deviance"><i class="fa fa-check"></i><b>5.2.3</b> Deviance</a></li>
<li class="chapter" data-level="5.2.4" data-path="parametric-estimations.html"><a href="parametric-estimations.html#predictive-accuracy"><i class="fa fa-check"></i><b>5.2.4</b> Predictive accuracy</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html"><i class="fa fa-check"></i><b>6</b> Nonparametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="6.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#density-estimations"><i class="fa fa-check"></i><b>6.1</b> Density Estimations</a></li>
<li class="chapter" data-level="6.2" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#kernel-regressions"><i class="fa fa-check"></i><b>6.2</b> Kernel regressions</a></li>
<li class="chapter" data-level="6.3" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#regression-splines"><i class="fa fa-check"></i><b>6.3</b> Regression Splines</a></li>
<li class="chapter" data-level="6.4" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#mars---multivariate-adaptive-regression-splines"><i class="fa fa-check"></i><b>6.4</b> MARS - Multivariate Adaptive Regression Splines</a></li>
<li class="chapter" data-level="6.5" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#gam---generalized-additive-model"><i class="fa fa-check"></i><b>6.5</b> GAM - Generalized Additive Model</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>7</b> Smoothing</a>
<ul>
<li class="chapter" data-level="7.1" data-path="smoothing.html"><a href="smoothing.html#using-bins"><i class="fa fa-check"></i><b>7.1</b> Using bins</a></li>
<li class="chapter" data-level="7.2" data-path="smoothing.html"><a href="smoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>7.2</b> Kernel smoothing</a></li>
<li class="chapter" data-level="7.3" data-path="smoothing.html"><a href="smoothing.html#locally-weighted-regression-loess"><i class="fa fa-check"></i><b>7.3</b> Locally weighted regression <code>loess()</code></a></li>
<li class="chapter" data-level="7.4" data-path="smoothing.html"><a href="smoothing.html#smooth-spline-regression"><i class="fa fa-check"></i><b>7.4</b> Smooth Spline Regression</a></li>
<li class="chapter" data-level="7.5" data-path="smoothing.html"><a href="smoothing.html#multivariate-loess"><i class="fa fa-check"></i><b>7.5</b> Multivariate Loess</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html"><i class="fa fa-check"></i><b>8</b> Nonparametric Classifier - kNN</a>
<ul>
<li class="chapter" data-level="8.1" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#mnist-dataset"><i class="fa fa-check"></i><b>8.1</b> <code>mnist</code> Dataset</a></li>
<li class="chapter" data-level="8.2" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#linear-classifiers-again"><i class="fa fa-check"></i><b>8.2</b> Linear classifiers (again)</a></li>
<li class="chapter" data-level="8.3" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>8.3</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="8.4" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#knn-with-caret"><i class="fa fa-check"></i><b>8.4</b> kNN with caret</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#mnist_27"><i class="fa fa-check"></i><b>8.4.1</b> <code>mnist_27</code></a></li>
<li class="chapter" data-level="8.4.2" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#adult-dataset"><i class="fa fa-check"></i><b>8.4.2</b> Adult dataset</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Self-Learning</b></span></li>
<li class="chapter" data-level="9" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html"><i class="fa fa-check"></i><b>9</b> Hyperparameter Tuning</a>
<ul>
<li class="chapter" data-level="9.1" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#training-validation-and-test-datasets"><i class="fa fa-check"></i><b>9.1</b> Training, validation, and test datasets</a></li>
<li class="chapter" data-level="9.2" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#splitting-the-data-randomly"><i class="fa fa-check"></i><b>9.2</b> Splitting the data randomly</a></li>
<li class="chapter" data-level="9.3" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>9.3</b> k-fold cross validation</a></li>
<li class="chapter" data-level="9.4" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#cross-validated-grid-search"><i class="fa fa-check"></i><b>9.4</b> Cross-validated grid search</a></li>
<li class="chapter" data-level="9.5" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#bootstrapped-grid-search"><i class="fa fa-check"></i><b>9.5</b> Bootstrapped grid search</a></li>
<li class="chapter" data-level="9.6" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#when-the-data-is-time-series"><i class="fa fa-check"></i><b>9.6</b> When the data is time-series</a></li>
<li class="chapter" data-level="9.7" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#speed"><i class="fa fa-check"></i><b>9.7</b> Speed</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html"><i class="fa fa-check"></i><b>10</b> Tuning in Classification</a>
<ul>
<li class="chapter" data-level="10.1" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html#confusion-matrix"><i class="fa fa-check"></i><b>10.1</b> Confusion matrix</a></li>
<li class="chapter" data-level="10.2" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html#performance-measures"><i class="fa fa-check"></i><b>10.2</b> Performance measures</a></li>
<li class="chapter" data-level="10.3" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html#roc-curve"><i class="fa fa-check"></i><b>10.3</b> ROC Curve</a></li>
<li class="chapter" data-level="10.4" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html#auc---area-under-the-curve"><i class="fa fa-check"></i><b>10.4</b> AUC - Area Under the Curve</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="classification-example.html"><a href="classification-example.html"><i class="fa fa-check"></i><b>11</b> Classification Example</a>
<ul>
<li class="chapter" data-level="11.1" data-path="classification-example.html"><a href="classification-example.html#lpm"><i class="fa fa-check"></i><b>11.1</b> LPM</a></li>
<li class="chapter" data-level="11.2" data-path="classification-example.html"><a href="classification-example.html#logistic-regression-1"><i class="fa fa-check"></i><b>11.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="11.3" data-path="classification-example.html"><a href="classification-example.html#knn"><i class="fa fa-check"></i><b>11.3</b> kNN</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="classification-example.html"><a href="classification-example.html#knn-10-fold-cv"><i class="fa fa-check"></i><b>11.3.1</b> kNN 10-fold CV</a></li>
<li class="chapter" data-level="11.3.2" data-path="classification-example.html"><a href="classification-example.html#knn-with-caret-1"><i class="fa fa-check"></i><b>11.3.2</b> kNN with <code>caret</code></a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Tree-based Models</b></span></li>
<li class="chapter" data-level="12" data-path="cart.html"><a href="cart.html"><i class="fa fa-check"></i><b>12</b> CART</a>
<ul>
<li class="chapter" data-level="12.1" data-path="cart.html"><a href="cart.html#cart---classification-tree"><i class="fa fa-check"></i><b>12.1</b> CART - Classification Tree</a></li>
<li class="chapter" data-level="12.2" data-path="cart.html"><a href="cart.html#rpart---recursive-partitioning"><i class="fa fa-check"></i><b>12.2</b> <code>rpart</code> - Recursive Partitioning</a></li>
<li class="chapter" data-level="12.3" data-path="cart.html"><a href="cart.html#pruning"><i class="fa fa-check"></i><b>12.3</b> Pruning</a></li>
<li class="chapter" data-level="12.4" data-path="cart.html"><a href="cart.html#classification-with-titanic"><i class="fa fa-check"></i><b>12.4</b> Classification with Titanic</a></li>
<li class="chapter" data-level="12.5" data-path="cart.html"><a href="cart.html#regression-tree"><i class="fa fa-check"></i><b>12.5</b> Regression Tree</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ensemble-learning.html"><a href="ensemble-learning.html"><i class="fa fa-check"></i><b>13</b> Ensemble Learning</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ensemble-learning.html"><a href="ensemble-learning.html#bagging"><i class="fa fa-check"></i><b>13.1</b> Bagging</a></li>
<li class="chapter" data-level="13.2" data-path="ensemble-learning.html"><a href="ensemble-learning.html#random-forest"><i class="fa fa-check"></i><b>13.2</b> Random Forest</a></li>
<li class="chapter" data-level="13.3" data-path="ensemble-learning.html"><a href="ensemble-learning.html#boosting"><i class="fa fa-check"></i><b>13.3</b> Boosting</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="ensemble-learning.html"><a href="ensemble-learning.html#sequential-ensemble-with-gbm"><i class="fa fa-check"></i><b>13.3.1</b> Sequential ensemble with <code>gbm</code></a></li>
<li class="chapter" data-level="13.3.2" data-path="ensemble-learning.html"><a href="ensemble-learning.html#adaboost"><i class="fa fa-check"></i><b>13.3.2</b> AdaBoost</a></li>
<li class="chapter" data-level="13.3.3" data-path="ensemble-learning.html"><a href="ensemble-learning.html#xgboost"><i class="fa fa-check"></i><b>13.3.3</b> XGBoost</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ensemble-applications.html"><a href="ensemble-applications.html"><i class="fa fa-check"></i><b>14</b> Ensemble Applications</a>
<ul>
<li class="chapter" data-level="14.1" data-path="ensemble-applications.html"><a href="ensemble-applications.html#classification"><i class="fa fa-check"></i><b>14.1</b> Classification</a></li>
<li class="chapter" data-level="14.2" data-path="ensemble-applications.html"><a href="ensemble-applications.html#regression"><i class="fa fa-check"></i><b>14.2</b> Regression</a></li>
<li class="chapter" data-level="14.3" data-path="ensemble-applications.html"><a href="ensemble-applications.html#exploration"><i class="fa fa-check"></i><b>14.3</b> Exploration</a></li>
<li class="chapter" data-level="14.4" data-path="ensemble-applications.html"><a href="ensemble-applications.html#boosting-applications"><i class="fa fa-check"></i><b>14.4</b> Boosting Applications</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="ensemble-applications.html"><a href="ensemble-applications.html#regression-1"><i class="fa fa-check"></i><b>14.4.1</b> Regression</a></li>
<li class="chapter" data-level="14.4.2" data-path="ensemble-applications.html"><a href="ensemble-applications.html#random-search-with-parallel-processing"><i class="fa fa-check"></i><b>14.4.2</b> Random search with parallel processing</a></li>
<li class="chapter" data-level="14.4.3" data-path="ensemble-applications.html"><a href="ensemble-applications.html#boosting-vs.-others"><i class="fa fa-check"></i><b>14.4.3</b> Boosting vs. Others</a></li>
<li class="chapter" data-level="14.4.4" data-path="ensemble-applications.html"><a href="ensemble-applications.html#classification-1"><i class="fa fa-check"></i><b>14.4.4</b> Classification</a></li>
<li class="chapter" data-level="14.4.5" data-path="ensemble-applications.html"><a href="ensemble-applications.html#adaboost.m1"><i class="fa fa-check"></i><b>14.4.5</b> AdaBoost.M1</a></li>
<li class="chapter" data-level="14.4.6" data-path="ensemble-applications.html"><a href="ensemble-applications.html#classification-with-xgboost"><i class="fa fa-check"></i><b>14.4.6</b> Classification with XGBoost</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>V SVM &amp; Neural Networks</b></span></li>
<li class="chapter" data-level="15" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>15</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="15.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#optimal-separating-classifier"><i class="fa fa-check"></i><b>15.1</b> Optimal Separating Classifier</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#the-margin"><i class="fa fa-check"></i><b>15.1.1</b> The Margin</a></li>
<li class="chapter" data-level="15.1.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#the-non-separable-case"><i class="fa fa-check"></i><b>15.1.2</b> The Non-Separable Case</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#nonlinear-boundary-with-kernels"><i class="fa fa-check"></i><b>15.2</b> Nonlinear Boundary with Kernels</a></li>
<li class="chapter" data-level="15.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#application-with-svm"><i class="fa fa-check"></i><b>15.3</b> Application with SVM</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html"><i class="fa fa-check"></i><b>16</b> Artificial Neural Networks</a>
<ul>
<li class="chapter" data-level="16.1" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#neural-network---the-idea"><i class="fa fa-check"></i><b>16.1</b> Neural Network - the idea</a></li>
<li class="chapter" data-level="16.2" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#backpropagation"><i class="fa fa-check"></i><b>16.2</b> Backpropagation</a></li>
<li class="chapter" data-level="16.3" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#neural-network---more-inputs"><i class="fa fa-check"></i><b>16.3</b> Neural Network - More inputs</a></li>
<li class="chapter" data-level="16.4" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#deep-learning"><i class="fa fa-check"></i><b>16.4</b> Deep Learning</a></li>
</ul></li>
<li class="part"><span><b>VI Penalized Regressions</b></span></li>
<li class="chapter" data-level="" data-path="parametric-models-in-prediction.html"><a href="parametric-models-in-prediction.html"><i class="fa fa-check"></i>Parametric models in prediction</a></li>
<li class="chapter" data-level="17" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>17</b> Ridge</a></li>
<li class="chapter" data-level="18" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>18</b> Lasso</a></li>
<li class="chapter" data-level="19" data-path="adaptive-lasso.html"><a href="adaptive-lasso.html"><i class="fa fa-check"></i><b>19</b> Adaptive Lasso</a></li>
<li class="chapter" data-level="20" data-path="sparsity.html"><a href="sparsity.html"><i class="fa fa-check"></i><b>20</b> Sparsity</a></li>
<li class="part"><span><b>VII Time Series</b></span></li>
<li class="chapter" data-level="" data-path="forecasting.html"><a href="forecasting.html"><i class="fa fa-check"></i>Forecasting</a></li>
<li class="chapter" data-level="21" data-path="arima-models.html"><a href="arima-models.html"><i class="fa fa-check"></i><b>21</b> ARIMA models</a>
<ul>
<li class="chapter" data-level="21.1" data-path="arima-models.html"><a href="arima-models.html#hyndman-khandakar-algorithm"><i class="fa fa-check"></i><b>21.1</b> Hyndman-Khandakar algorithm</a></li>
<li class="chapter" data-level="21.2" data-path="arima-models.html"><a href="arima-models.html#ts-plots"><i class="fa fa-check"></i><b>21.2</b> TS Plots</a></li>
<li class="chapter" data-level="21.3" data-path="arima-models.html"><a href="arima-models.html#box-cox-transformation"><i class="fa fa-check"></i><b>21.3</b> Box-Cox transformation</a></li>
<li class="chapter" data-level="21.4" data-path="arima-models.html"><a href="arima-models.html#stationarity"><i class="fa fa-check"></i><b>21.4</b> Stationarity</a></li>
<li class="chapter" data-level="21.5" data-path="arima-models.html"><a href="arima-models.html#modeling-arima"><i class="fa fa-check"></i><b>21.5</b> Modeling ARIMA</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="grid-search-for-arima.html"><a href="grid-search-for-arima.html"><i class="fa fa-check"></i><b>22</b> Grid search for ARIMA</a></li>
<li class="chapter" data-level="23" data-path="time-series-embedding.html"><a href="time-series-embedding.html"><i class="fa fa-check"></i><b>23</b> Time Series Embedding</a>
<ul>
<li class="chapter" data-level="23.1" data-path="time-series-embedding.html"><a href="time-series-embedding.html#var-for-recursive-forecasting"><i class="fa fa-check"></i><b>23.1</b> VAR for Recursive Forecasting</a></li>
<li class="chapter" data-level="23.2" data-path="time-series-embedding.html"><a href="time-series-embedding.html#embedding-for-direct-forecast"><i class="fa fa-check"></i><b>23.2</b> Embedding for Direct Forecast</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="random-forest-1.html"><a href="random-forest-1.html"><i class="fa fa-check"></i><b>24</b> Random Forest</a>
<ul>
<li class="chapter" data-level="24.1" data-path="random-forest-1.html"><a href="random-forest-1.html#univariate"><i class="fa fa-check"></i><b>24.1</b> Univariate</a></li>
<li class="chapter" data-level="24.2" data-path="random-forest-1.html"><a href="random-forest-1.html#multivariate"><i class="fa fa-check"></i><b>24.2</b> Multivariate</a></li>
<li class="chapter" data-level="24.3" data-path="random-forest-1.html"><a href="random-forest-1.html#rolling-and-expanding-windows"><i class="fa fa-check"></i><b>24.3</b> Rolling and expanding windows</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html"><i class="fa fa-check"></i><b>25</b> Recurrent Neural Networks</a>
<ul>
<li class="chapter" data-level="25.1" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#keras"><i class="fa fa-check"></i><b>25.1</b> Keras</a></li>
<li class="chapter" data-level="25.2" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#input-tensors"><i class="fa fa-check"></i><b>25.2</b> Input Tensors</a></li>
<li class="chapter" data-level="25.3" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#plain-rnn"><i class="fa fa-check"></i><b>25.3</b> Plain RNN</a></li>
<li class="chapter" data-level="25.4" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#lstm"><i class="fa fa-check"></i><b>25.4</b> LSTM</a></li>
</ul></li>
<li class="part"><span><b>VIII Dimension Reduction Methods</b></span></li>
<li class="chapter" data-level="" data-path="matrix-decompositions.html"><a href="matrix-decompositions.html"><i class="fa fa-check"></i>Matrix Decompositions</a></li>
<li class="chapter" data-level="26" data-path="eigenvectors-and-eigenvalues.html"><a href="eigenvectors-and-eigenvalues.html"><i class="fa fa-check"></i><b>26</b> Eigenvectors and eigenvalues</a></li>
<li class="chapter" data-level="27" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html"><i class="fa fa-check"></i><b>27</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="28" data-path="rankr-approximations.html"><a href="rankr-approximations.html"><i class="fa fa-check"></i><b>28</b> Rank(r) Approximations</a></li>
<li class="chapter" data-level="29" data-path="moore-penrose-inverse.html"><a href="moore-penrose-inverse.html"><i class="fa fa-check"></i><b>29</b> Moore-Penrose inverse</a></li>
<li class="chapter" data-level="30" data-path="principle-component-analysis.html"><a href="principle-component-analysis.html"><i class="fa fa-check"></i><b>30</b> Principle Component Analysis</a></li>
<li class="chapter" data-level="31" data-path="factor-analysis.html"><a href="factor-analysis.html"><i class="fa fa-check"></i><b>31</b> Factor Analysis</a></li>
<li class="part"><span><b>IX Network Analysis</b></span></li>
<li class="chapter" data-level="" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html"><i class="fa fa-check"></i>Graphical Network Analysis</a></li>
<li class="chapter" data-level="32" data-path="fundementals.html"><a href="fundementals.html"><i class="fa fa-check"></i><b>32</b> Fundementals</a>
<ul>
<li class="chapter" data-level="32.1" data-path="fundementals.html"><a href="fundementals.html#covariance"><i class="fa fa-check"></i><b>32.1</b> Covariance</a></li>
<li class="chapter" data-level="32.2" data-path="fundementals.html"><a href="fundementals.html#correlation"><i class="fa fa-check"></i><b>32.2</b> Correlation</a></li>
<li class="chapter" data-level="32.3" data-path="fundementals.html"><a href="fundementals.html#precision-matrix"><i class="fa fa-check"></i><b>32.3</b> Precision Matrix</a></li>
<li class="chapter" data-level="32.4" data-path="fundementals.html"><a href="fundementals.html#semi-partial-correlation"><i class="fa fa-check"></i><b>32.4</b> Semi-partial Correlation</a></li>
</ul></li>
<li class="chapter" data-level="33" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html"><i class="fa fa-check"></i><b>33</b> Regularized Covariance Matrix</a>
<ul>
<li class="chapter" data-level="33.1" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html#multivariate-gaussian-distribution"><i class="fa fa-check"></i><b>33.1</b> Multivariate Gaussian Distribution</a></li>
<li class="chapter" data-level="33.2" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html#high-dimensional-data"><i class="fa fa-check"></i><b>33.2</b> High-dimensional data</a></li>
<li class="chapter" data-level="33.3" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html#ridge-ell_2-and-glasso-ell_1"><i class="fa fa-check"></i><b>33.3</b> Ridge (<span class="math inline">\(\ell_{2}\)</span>) and glasso (<span class="math inline">\(\ell_{1}\)</span>)</a></li>
</ul></li>
<li class="part"><span><b>X Labs</b></span></li>
<li class="chapter" data-level="34" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html"><i class="fa fa-check"></i><b>34</b> R Lab 1 - Basics I</a>
<ul>
<li class="chapter" data-level="34.1" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#r-rstudio-and-r-packages"><i class="fa fa-check"></i><b>34.1</b> R, RStudio, and R Packages</a></li>
<li class="chapter" data-level="34.2" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#rstudio"><i class="fa fa-check"></i><b>34.2</b> RStudio</a></li>
<li class="chapter" data-level="34.3" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#working-directory"><i class="fa fa-check"></i><b>34.3</b> Working directory</a></li>
<li class="chapter" data-level="34.4" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#data-types-and-stuctures"><i class="fa fa-check"></i><b>34.4</b> Data Types and Stuctures</a></li>
<li class="chapter" data-level="34.5" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#vectors"><i class="fa fa-check"></i><b>34.5</b> Vectors</a></li>
<li class="chapter" data-level="34.6" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#subsetting-vectors"><i class="fa fa-check"></i><b>34.6</b> Subsetting Vectors</a></li>
<li class="chapter" data-level="34.7" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#vectorization-or-vector-operations"><i class="fa fa-check"></i><b>34.7</b> Vectorization or vector operations</a></li>
<li class="chapter" data-level="34.8" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#matrices"><i class="fa fa-check"></i><b>34.8</b> Matrices</a></li>
<li class="chapter" data-level="34.9" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#matrix-operations"><i class="fa fa-check"></i><b>34.9</b> Matrix Operations</a></li>
<li class="chapter" data-level="34.10" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#subsetting-matrix"><i class="fa fa-check"></i><b>34.10</b> Subsetting Matrix</a></li>
<li class="chapter" data-level="34.11" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#r-style-guide"><i class="fa fa-check"></i><b>34.11</b> R-Style Guide</a></li>
</ul></li>
<li class="chapter" data-level="35" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html"><i class="fa fa-check"></i><b>35</b> R Lab 2 - Basics II</a>
<ul>
<li class="chapter" data-level="35.1" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#data-frames-and-lists"><i class="fa fa-check"></i><b>35.1</b> Data frames and lists</a>
<ul>
<li class="chapter" data-level="35.1.1" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#lists"><i class="fa fa-check"></i><b>35.1.1</b> Lists</a></li>
<li class="chapter" data-level="35.1.2" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#data-frames"><i class="fa fa-check"></i><b>35.1.2</b> Data Frames</a></li>
<li class="chapter" data-level="35.1.3" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#reading-importing-and-writting-exporting-data-files"><i class="fa fa-check"></i><b>35.1.3</b> Reading (importing) and writting (exporting) data files</a></li>
<li class="chapter" data-level="35.1.4" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#subsetting-data-frames"><i class="fa fa-check"></i><b>35.1.4</b> Subsetting Data Frames</a></li>
<li class="chapter" data-level="35.1.5" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#plotting-from-data-frame"><i class="fa fa-check"></i><b>35.1.5</b> Plotting from data frame</a></li>
<li class="chapter" data-level="35.1.6" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#some-useful-functions"><i class="fa fa-check"></i><b>35.1.6</b> Some useful functions</a></li>
<li class="chapter" data-level="35.1.7" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#categorical-variables-in-data-frames"><i class="fa fa-check"></i><b>35.1.7</b> Categorical Variables in Data Frames</a></li>
</ul></li>
<li class="chapter" data-level="35.2" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#programming-basics"><i class="fa fa-check"></i><b>35.2</b> Programming Basics</a>
<ul>
<li class="chapter" data-level="35.2.1" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#ifelse"><i class="fa fa-check"></i><b>35.2.1</b> if/Else</a></li>
<li class="chapter" data-level="35.2.2" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#loops"><i class="fa fa-check"></i><b>35.2.2</b> Loops</a></li>
<li class="chapter" data-level="35.2.3" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#the-apply-family"><i class="fa fa-check"></i><b>35.2.3</b> The <code>apply()</code> family</a></li>
<li class="chapter" data-level="35.2.4" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#functions"><i class="fa fa-check"></i><b>35.2.4</b> Functions</a></li>
<li class="chapter" data-level="35.2.5" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#dplyr"><i class="fa fa-check"></i><b>35.2.5</b> <code>dplyr()</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="36" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html"><i class="fa fa-check"></i><b>36</b> R Lab 3 - Preparing the data</a>
<ul>
<li class="chapter" data-level="36.1" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#preparing-the-data-for-a-regression-analysis-with-lm"><i class="fa fa-check"></i><b>36.1</b> Preparing the data for a regression analysis with <code>lm()</code></a>
<ul>
<li class="chapter" data-level="36.1.1" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#factor-variables"><i class="fa fa-check"></i><b>36.1.1</b> Factor variables</a></li>
<li class="chapter" data-level="36.1.2" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#dummy-coding"><i class="fa fa-check"></i><b>36.1.2</b> Dummy Coding</a></li>
<li class="chapter" data-level="36.1.3" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#column-variable-names"><i class="fa fa-check"></i><b>36.1.3</b> Column (Variable) names</a></li>
<li class="chapter" data-level="36.1.4" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#data-subsetting-and-missing-values"><i class="fa fa-check"></i><b>36.1.4</b> Data subsetting and missing values</a></li>
</ul></li>
<li class="chapter" data-level="36.2" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#dummy-variable-models"><i class="fa fa-check"></i><b>36.2</b> “DUMMY” variable models</a>
<ul>
<li class="chapter" data-level="36.2.1" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#mtcars-example"><i class="fa fa-check"></i><b>36.2.1</b> <code>mtcars</code> example</a></li>
<li class="chapter" data-level="36.2.2" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#model.matrix"><i class="fa fa-check"></i><b>36.2.2</b> <code>model.matrix()</code></a></li>
<li class="chapter" data-level="36.2.3" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#example-with-a-bigger-data-set-autompg"><i class="fa fa-check"></i><b>36.2.3</b> Example with a bigger data set: <code>Autompg</code></a></li>
<li class="chapter" data-level="36.2.4" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#some-more-data-management-tools-for-subsetting-complete.cases-is.na-and-within"><i class="fa fa-check"></i><b>36.2.4</b> Some more data management tools for subsetting: <code>complete.cases()</code>, <code>is.na()</code>, and <code>within()</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="37" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html"><i class="fa fa-check"></i><b>37</b> R Lab 4 - Simulation in R</a>
<ul>
<li class="chapter" data-level="37.1" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#sampling-in-r-sample"><i class="fa fa-check"></i><b>37.1</b> Sampling in R: <code>sample()</code></a></li>
<li class="chapter" data-level="37.2" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#random-number-generating-with-probablity-distributions"><i class="fa fa-check"></i><b>37.2</b> Random number generating with probablity distributions</a></li>
<li class="chapter" data-level="37.3" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#simulation-for-statistical-inference"><i class="fa fa-check"></i><b>37.3</b> Simulation for statistical inference</a></li>
<li class="chapter" data-level="37.4" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#creataing-data-with-a-data-generating-model-dgm"><i class="fa fa-check"></i><b>37.4</b> Creataing data with a Data Generating Model (DGM)</a></li>
<li class="chapter" data-level="37.5" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#bootstrapping"><i class="fa fa-check"></i><b>37.5</b> Bootstrapping</a></li>
<li class="chapter" data-level="37.6" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#monty-hall---fun-example"><i class="fa fa-check"></i><b>37.6</b> Monty Hall - Fun example</a></li>
</ul></li>
<li class="part"><span><b>XI Appendix</b></span></li>
<li class="chapter" data-level="38" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html"><i class="fa fa-check"></i><b>38</b> Algorithmic Optimization</a>
<ul>
<li class="chapter" data-level="38.1" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#brute-force-optimization"><i class="fa fa-check"></i><b>38.1</b> Brute-force optimization</a></li>
<li class="chapter" data-level="38.2" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#derivative-based-methods"><i class="fa fa-check"></i><b>38.2</b> Derivative-based methods</a></li>
<li class="chapter" data-level="38.3" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#ml-estimation-with-logistic-regression"><i class="fa fa-check"></i><b>38.3</b> ML Estimation with logistic regression</a></li>
<li class="chapter" data-level="38.4" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#gradient-descent-algorithm"><i class="fa fa-check"></i><b>38.4</b> Gradient Descent Algorithm</a>
<ul>
<li class="chapter" data-level="38.4.1" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#one-variable"><i class="fa fa-check"></i><b>38.4.1</b> One-variable</a></li>
<li class="chapter" data-level="38.4.2" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#adjustable-lr-and-sgd"><i class="fa fa-check"></i><b>38.4.2</b> Adjustable <code>lr</code> and SGD</a></li>
<li class="chapter" data-level="38.4.3" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#multivariable"><i class="fa fa-check"></i><b>38.4.3</b> Multivariable</a></li>
</ul></li>
<li class="chapter" data-level="38.5" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#optimization-with-r"><i class="fa fa-check"></i><b>38.5</b> Optimization with R</a></li>
</ul></li>
<li class="chapter" data-level="39" data-path="imbalanced-data.html"><a href="imbalanced-data.html"><i class="fa fa-check"></i><b>39</b> Imbalanced Data</a>
<ul>
<li class="chapter" data-level="39.1" data-path="imbalanced-data.html"><a href="imbalanced-data.html#smote"><i class="fa fa-check"></i><b>39.1</b> <code>SMOTE</code></a></li>
<li class="chapter" data-level="39.2" data-path="imbalanced-data.html"><a href="imbalanced-data.html#fraud-detection"><i class="fa fa-check"></i><b>39.2</b> Fraud detection</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/yaydede/toolbox" target="blank"> 2023 Yigit Aydede - Bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Toolbox for Social Scientists and Policy Analysts</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="nonparametric-classifier---knn" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">Chapter 8</span> Nonparametric Classifier - kNN<a href="nonparametric-classifier---knn.html#nonparametric-classifier---knn" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>We complete this section, Nonparametric Estimations, with a nonparametric classifier and compare its performance with parametric classifiers, LPM and Logistic.</p>
<div id="mnist-dataset" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> <code>mnist</code> Dataset<a href="nonparametric-classifier---knn.html#mnist-dataset" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Reading hand-written letters and numbers is not a big deal nowadays. For example, In Canada Post, computers read postal codes and robots sorts them for each postal code groups. This application is mainly achieved by machine learning algorithms. In order to understand how, let’s use a real dataset, Mnist. Here is the description of the dataset by Wikipedia:</p>
<blockquote>
<p>The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The MNIST database contains 60,000 training images and 10,000 testing images. Half of the training set and half of the test set were taken from NIST’s training dataset, while the other half of the training set and the other half of the test set were taken from NIST’s testing dataset. There have been a number of scientific papers on attempts to achieve the lowest error rate; one paper, using a hierarchical system of convolutional neural networks, manages to get an error rate on the MNIST database of 0.23%.</p>
</blockquote>
<p><img src="png/digits.png" width="130%" height="130%" /></p>
<p>These images are converted into <span class="math inline">\(28 \times 28 = 784\)</span> pixels and, for each pixel, there is a measure that scales the darkness in that pixel between 0 (white) and 255 (black). Hence, for each digitized image, we have an indicator variable <span class="math inline">\(Y\)</span> between 0 and 9, and we have 784 variables that identifies each pixel in the digitized image. Let’s download the data. (<a href="http://yann.lecun.com/exdb/mnist/">More details about the data</a>).</p>
<div class="sourceCode" id="cb327"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb327-1"><a href="nonparametric-classifier---knn.html#cb327-1" aria-hidden="true" tabindex="-1"></a><span class="co">#loading the data</span></span>
<span id="cb327-2"><a href="nonparametric-classifier---knn.html#cb327-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb327-3"><a href="nonparametric-classifier---knn.html#cb327-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dslabs)</span>
<span id="cb327-4"><a href="nonparametric-classifier---knn.html#cb327-4" aria-hidden="true" tabindex="-1"></a><span class="co">#Download the data to your directory.  It&#39;s big!</span></span>
<span id="cb327-5"><a href="nonparametric-classifier---knn.html#cb327-5" aria-hidden="true" tabindex="-1"></a><span class="co">#mnist &lt;- read_mnist() </span></span>
<span id="cb327-6"><a href="nonparametric-classifier---knn.html#cb327-6" aria-hidden="true" tabindex="-1"></a><span class="co">#save(mnist, file = &quot;mnist.Rdata&quot;)</span></span>
<span id="cb327-7"><a href="nonparametric-classifier---knn.html#cb327-7" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;mnist.Rdata&quot;</span>)</span>
<span id="cb327-8"><a href="nonparametric-classifier---knn.html#cb327-8" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(mnist)</span></code></pre></div>
<pre><code>## List of 2
##  $ train:List of 2
##   ..$ images: int [1:60000, 1:784] 0 0 0 0 0 0 0 0 0 0 ...
##   ..$ labels: int [1:60000] 5 0 4 1 9 2 1 3 1 4 ...
##  $ test :List of 2
##   ..$ images: int [1:10000, 1:784] 0 0 0 0 0 0 0 0 0 0 ...
##   ..$ labels: int [1:10000] 7 2 1 0 4 1 4 9 5 9 ...</code></pre>
<p>The data is given as a list and already divided into train and test sets. We have 60,000 images in the train set and 10,000 images in the test set. For the train set, we have two nested sets: <code>images</code>, which contains all 784 features for 60,000 images. Hence, it’s a <span class="math inline">\(60000 \times 784\)</span> matrix. And, <code>labels</code> contains the labes (from 0 to 9) for each image.</p>
<p>The digitizing can be understood from this image better:</p>
<p><img src="png/mnist.png" width="130%" height="130%" /></p>
<p>Each image has <span class="math inline">\(28 \times 28\)</span> = 784 pixels. For each image, the pixels are features with a label that shows the true number between 0 and 9. This methods is called as “flattening”, which is a technique that is used to convert multi-dimensional image into a one-dimension array (vector).</p>
<p>For now, we will use a smaller version of this data set given in the <code>dslabs</code> package, which is a random sample of 1,000 images (only for 2 and 7 digits), 800 in the training set and 200 in the test set, with only two features: the proportion of dark pixels that are in the upper left quadrant, <code>x_1</code>, and the lower right quadrant, <code>x_2</code>.</p>
<div class="sourceCode" id="cb329"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb329-1"><a href="nonparametric-classifier---knn.html#cb329-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;mnist_27&quot;</span>)</span>
<span id="cb329-2"><a href="nonparametric-classifier---knn.html#cb329-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(mnist_27)</span></code></pre></div>
<pre><code>## List of 5
##  $ train      :&#39;data.frame&#39;: 800 obs. of  3 variables:
##   ..$ y  : Factor w/ 2 levels &quot;2&quot;,&quot;7&quot;: 1 2 1 1 2 1 2 2 2 1 ...
##   ..$ x_1: num [1:800] 0.0395 0.1607 0.0213 0.1358 0.3902 ...
##   ..$ x_2: num [1:800] 0.1842 0.0893 0.2766 0.2222 0.3659 ...
##  $ test       :&#39;data.frame&#39;: 200 obs. of  3 variables:
##   ..$ y  : Factor w/ 2 levels &quot;2&quot;,&quot;7&quot;: 1 2 2 2 2 1 1 1 1 2 ...
##   ..$ x_1: num [1:200] 0.148 0.283 0.29 0.195 0.218 ...
##   ..$ x_2: num [1:200] 0.261 0.348 0.435 0.115 0.397 ...
##  $ index_train: int [1:800] 40334 33996 3200 38360 36239 38816 8085 9098 15470 5096 ...
##  $ index_test : int [1:200] 46218 35939 23443 30466 2677 54248 5909 13402 11031 47308 ...
##  $ true_p     :&#39;data.frame&#39;: 22500 obs. of  3 variables:
##   ..$ x_1: num [1:22500] 0 0.00352 0.00703 0.01055 0.01406 ...
##   ..$ x_2: num [1:22500] 0 0 0 0 0 0 0 0 0 0 ...
##   ..$ p  : num [1:22500] 0.703 0.711 0.719 0.727 0.734 ...
##   ..- attr(*, &quot;out.attrs&quot;)=List of 2
##   .. ..$ dim     : Named int [1:2] 150 150
##   .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;x_1&quot; &quot;x_2&quot;
##   .. ..$ dimnames:List of 2
##   .. .. ..$ x_1: chr [1:150] &quot;x_1=0.0000000&quot; &quot;x_1=0.0035155&quot; &quot;x_1=0.0070310&quot; &quot;x_1=0.0105465&quot; ...
##   .. .. ..$ x_2: chr [1:150] &quot;x_2=0.000000000&quot; &quot;x_2=0.004101417&quot; &quot;x_2=0.008202834&quot; &quot;x_2=0.012304251&quot; ...</code></pre>
</div>
<div id="linear-classifiers-again" class="section level2 hasAnchor" number="8.2">
<h2><span class="header-section-number">8.2</span> Linear classifiers (again)<a href="nonparametric-classifier---knn.html#linear-classifiers-again" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A linear classifier (like LPM and Logistic) is one where a “hyperplane” is formed by taking a linear combination of the features. Hyperplane represents a decision boundary chosen by our classifier to separate the data points in different class labels. let’s start with LPM:</p>
<p><span class="math display" id="eq:8-1">\[\begin{equation}
\operatorname{Pr}\left(Y=1 | X_{1}=x_{1}, X_{2}=x_{2}\right)=\beta_{0}+\beta_{1} x_{1}+\beta_{2} x_{2}
  \tag{8.1}
\end{equation}\]</span></p>
<div class="sourceCode" id="cb331"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb331-1"><a href="nonparametric-classifier---knn.html#cb331-1" aria-hidden="true" tabindex="-1"></a><span class="co"># LPM requires numerical 1 and 0</span></span>
<span id="cb331-2"><a href="nonparametric-classifier---knn.html#cb331-2" aria-hidden="true" tabindex="-1"></a>y10 <span class="ot">=</span> <span class="fu">ifelse</span>(mnist_27<span class="sc">$</span>train<span class="sc">$</span>y <span class="sc">==</span> <span class="dv">7</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb331-3"><a href="nonparametric-classifier---knn.html#cb331-3" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(mnist_27<span class="sc">$</span>train, y10)</span>
<span id="cb331-4"><a href="nonparametric-classifier---knn.html#cb331-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(train<span class="sc">$</span>x_1, train<span class="sc">$</span>x_2, <span class="at">col =</span> train<span class="sc">$</span>y10 <span class="sc">+</span> <span class="dv">1</span>, <span class="at">cex =</span> <span class="fl">0.5</span>)</span></code></pre></div>
<p><img src="08-Nonparametric_kNN_files/figure-html/knn5-1.png" width="672" /></p>
<p>Here, the black dots are 2 and red dots are 7. Note that if we use 0.5 as a decision rule such that it separates pairs (<span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>) for which <span class="math inline">\(\operatorname{Pr}\left(Y=1 | X_{1}=x_{1}, X_{2}=x_{2}\right) &lt; 0.5\)</span> then we can have a hyperplane as</p>
<p><span class="math display">\[
\hat{\beta}_{0}+\hat{\beta}_{1} x_{1}+\hat{\beta}_{2} x_{2}=0.5 \Longrightarrow x_{2}=\left(0.5-\hat{\beta}_{0}\right) / \hat{\beta}_{2}-\hat{\beta}_{1} / \hat{\beta}_{2} x_{1}.
\]</span></p>
<p>If we incorporate this into our plot for the train data:</p>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb332-1"><a href="nonparametric-classifier---knn.html#cb332-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(y10 <span class="sc">~</span> x_1 <span class="sc">+</span> x_2, train)</span>
<span id="cb332-2"><a href="nonparametric-classifier---knn.html#cb332-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb332-3"><a href="nonparametric-classifier---knn.html#cb332-3" aria-hidden="true" tabindex="-1"></a>tr <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb332-4"><a href="nonparametric-classifier---knn.html#cb332-4" aria-hidden="true" tabindex="-1"></a>a <span class="ot">&lt;-</span> tr <span class="sc">-</span> model<span class="sc">$</span>coefficients[<span class="dv">1</span>]</span>
<span id="cb332-5"><a href="nonparametric-classifier---knn.html#cb332-5" aria-hidden="true" tabindex="-1"></a>a <span class="ot">&lt;-</span> a <span class="sc">/</span> model<span class="sc">$</span>coefficients[<span class="dv">3</span>]</span>
<span id="cb332-6"><a href="nonparametric-classifier---knn.html#cb332-6" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="sc">-</span>model<span class="sc">$</span>coefficients[<span class="dv">2</span>] <span class="sc">/</span> model<span class="sc">$</span>coefficients[<span class="dv">3</span>]</span>
<span id="cb332-7"><a href="nonparametric-classifier---knn.html#cb332-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(train<span class="sc">$</span>x_1, train<span class="sc">$</span>x_2, <span class="at">col =</span> train<span class="sc">$</span>y10 <span class="sc">+</span> <span class="dv">1</span>, <span class="at">cex =</span> <span class="fl">0.72</span>)</span>
<span id="cb332-8"><a href="nonparametric-classifier---knn.html#cb332-8" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(a, b, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="fl">2.8</span>)</span></code></pre></div>
<p><img src="08-Nonparametric_kNN_files/figure-html/knn6-1.png" width="672" /></p>
<p>Play with the (discriminating) threshold and see how the hyperplane moves. When we change it to different numbers between 0 and 1, the number of correct and wrong predictions, a separation of red and black dots located in different sides, changes as well. Moreover <strong>the decision boundary is linear</strong>. That’s why LPM is called a linear classifier.</p>
<p>Would including interactions and polynomials (nonlinear parts) would place the line such a way that separation of these dots (2s and 7s) would be better?</p>
<p>Let’s see if adding a polynomial to our LPM improves this.</p>
<div class="sourceCode" id="cb333"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb333-1"><a href="nonparametric-classifier---knn.html#cb333-1" aria-hidden="true" tabindex="-1"></a>model2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y10 <span class="sc">~</span> x_1 <span class="sc">+</span> <span class="fu">I</span>(x_1 <span class="sc">^</span> <span class="dv">2</span>) <span class="sc">+</span> x_2, train)</span>
<span id="cb333-2"><a href="nonparametric-classifier---knn.html#cb333-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(model2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y10 ~ x_1 + I(x_1^2) + x_2, data = train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.14744 -0.28816  0.03999  0.28431  1.06759 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.09328    0.06571   1.419   0.1562    
## x_1          4.81884    0.55310   8.712  &lt; 2e-16 ***
## I(x_1^2)    -2.75520    1.40760  -1.957   0.0507 .  
## x_2         -1.18864    0.17252  -6.890 1.14e-11 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3891 on 796 degrees of freedom
## Multiple R-squared:  0.3956, Adjusted R-squared:  0.3933 
## F-statistic: 173.7 on 3 and 796 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb335"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb335-1"><a href="nonparametric-classifier---knn.html#cb335-1" aria-hidden="true" tabindex="-1"></a>tr <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb335-2"><a href="nonparametric-classifier---knn.html#cb335-2" aria-hidden="true" tabindex="-1"></a>s <span class="ot">&lt;-</span> model2<span class="sc">$</span>coefficients</span>
<span id="cb335-3"><a href="nonparametric-classifier---knn.html#cb335-3" aria-hidden="true" tabindex="-1"></a>a <span class="ot">=</span> tr <span class="sc">/</span> s[<span class="dv">3</span>]</span>
<span id="cb335-4"><a href="nonparametric-classifier---knn.html#cb335-4" aria-hidden="true" tabindex="-1"></a>b <span class="ot">=</span> s[<span class="dv">1</span>] <span class="sc">/</span> s[<span class="dv">3</span>]</span>
<span id="cb335-5"><a href="nonparametric-classifier---knn.html#cb335-5" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> s[<span class="dv">2</span>] <span class="sc">/</span> s[<span class="dv">3</span>]</span>
<span id="cb335-6"><a href="nonparametric-classifier---knn.html#cb335-6" aria-hidden="true" tabindex="-1"></a>e <span class="ot">=</span> s[<span class="dv">4</span>] <span class="sc">/</span> s[<span class="dv">3</span>]</span>
<span id="cb335-7"><a href="nonparametric-classifier---knn.html#cb335-7" aria-hidden="true" tabindex="-1"></a>x22 <span class="ot">=</span> a <span class="sc">-</span> b <span class="sc">-</span> d <span class="sc">*</span> train<span class="sc">$</span>x_1 <span class="sc">-</span> e <span class="sc">*</span> (train<span class="sc">$</span>x_1 <span class="sc">^</span> <span class="dv">2</span>)</span>
<span id="cb335-8"><a href="nonparametric-classifier---knn.html#cb335-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(train<span class="sc">$</span>x_1, train<span class="sc">$</span>x_2, <span class="at">col =</span> train<span class="sc">$</span>y10 <span class="sc">+</span> <span class="dv">1</span>, <span class="at">cex =</span> <span class="fl">0.72</span>)</span>
<span id="cb335-9"><a href="nonparametric-classifier---knn.html#cb335-9" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(train<span class="sc">$</span>x_1[<span class="fu">order</span>(x22)], x22[<span class="fu">order</span>(x22)], <span class="at">lwd =</span> <span class="fl">2.8</span>)</span></code></pre></div>
<p><img src="08-Nonparametric_kNN_files/figure-html/knn7-1.png" width="672" /></p>
<p>The coefficient of the polynomial is barely significant and very negligible in magnitude. And in fact the classification seems worse than the previous one.</p>
<p>Would a logistic regression give us a better line? We don’t need to estimate it, but we can obtain the decision boundary for the logistic regression. Remember,</p>
<p><span class="math display">\[
P(Y=1 | x)=\frac{\exp \left(w_{0}+\sum_{i} w_{i} x_{i}\right)}{1+\exp \left(w_{0}+\sum_{i} w_{i} x_{i}\right)}
\]</span></p>
<p>And,</p>
<p><span class="math display">\[
P(Y=0 | x)=1-P(Y=1 | x)= \frac{1}{1+\exp \left(w_{0}+\sum_{i} w_{i} x_{i}\right)}
\]</span></p>
<p>if we take the ratio of success over failure, <span class="math inline">\(P/1-P\)</span>,</p>
<p><span class="math display">\[
\frac{P}{1-P}=\exp \left(w_{0}+\sum_{i} w_{i} x_{i}\right)
\]</span></p>
<p>If this ratio is higher than 1, we think that the probability for <span class="math inline">\(Y=1\)</span> is higher than the probability for <span class="math inline">\(Y=0\)</span>. And this only happens when <span class="math inline">\(P&gt;0.5\)</span>. Hence, the condition to classify the observation as <span class="math inline">\(Y=1\)</span> is:</p>
<p><span class="math display">\[
\frac{P}{1-P}=\exp \left(w_{0}+\sum_{i} w_{i} x_{i}\right) &gt; 1
\]</span></p>
<p>If we take the log of both sides,</p>
<p><span class="math display">\[
w_{0}+\sum_{i} w_{i} X_{i}&gt;0
\]</span></p>
<p>From here, the hyperplane function in our case becomes,</p>
<p><span class="math display">\[
\hat{\beta}_{0}+\hat{\beta}_{1} x_{1}+\hat{\beta}_{2} x_{2}=0 \Longrightarrow x_{2}=-\hat{\beta}_{0} / \hat{\beta}_{2}-\hat{\beta}_{1} / \hat{\beta}_{2} x_{1}.
\]</span></p>
<p>We see that the decision boundary is again linear. Therefore, LPM and logistic regressions are called as <strong>linear classifiers</strong>, which are good <strong>only if the problem on hand is linearly separable</strong>.</p>
<p>Would it be possible to have a nonlinear boundary condition so that we can get a better classification for our predicted probabilities?</p>
</div>
<div id="k-nearest-neighbors" class="section level2 hasAnchor" number="8.3">
<h2><span class="header-section-number">8.3</span> k-Nearest Neighbors<a href="nonparametric-classifier---knn.html#k-nearest-neighbors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>k-nearest neighbors (kNN) is a nonparametric method used for classification (or regression), which estimate <span class="math inline">\(p(x_1, x_2)\)</span> by using a method similar to <em>bin smoothing</em>. In <em>kNN classification</em>, the output is a class membership. An object is assigned to the class most common among its k-nearest neighbors. In <em>kNN regressions</em>, the output is the average of the values of k-nearest neighbors, which we’ve seen in bin smoothing applications.</p>
<p><img src="png/kNN1.png" width="402" /></p>
<p>Suppose we have to classify (identify) the red dot as 7 or 2. Since it’s a nonparametric approach, we have to define bins. If the number of observations in bins set to 1 (<span class="math inline">\(k = 1\)</span>), then we need to find one observation that is nearest to the red dot. How? Since we know to coordinates (<span class="math inline">\(x_1, x_2\)</span>) of that red dot, we can find its nearest neighbors by some distance functions among all points (observations) in the data. A popular choice is the Euclidean distance given by</p>
<p><span class="math display">\[
d\left(x, x^{\prime}\right)=\sqrt{\left(x_{1}-x_{1}^{\prime}\right)^{2}+\ldots+\left(x_{n}-x_{n}^{\prime}\right)^{2}}.
\]</span></p>
<p>Other measures are also available and can be more suitable in different settings including the Manhattan, Chebyshev and Hamming distance. The last one is used if the features are binary. In our case the features are continuous so we can use the Euclidean distance. We now have to calculate this measure for every point (observation) in our data. In our graph we have 10 points, and we have to have 10 distance measures from the red dot. Usually, in practice, we calculate all distance measures between each point, which becomes a symmetric matrix with <span class="math inline">\(n\)</span>x<span class="math inline">\(n\)</span> dimensions.</p>
<p>For example, for two dimensional space, we can calculate the distances as follows</p>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb336-1"><a href="nonparametric-classifier---knn.html#cb336-1" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="fl">2.1</span>, <span class="dv">4</span>, <span class="fl">4.3</span>)</span>
<span id="cb336-2"><a href="nonparametric-classifier---knn.html#cb336-2" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="fl">3.3</span>, <span class="dv">5</span>, <span class="fl">5.1</span>)</span>
<span id="cb336-3"><a href="nonparametric-classifier---knn.html#cb336-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb336-4"><a href="nonparametric-classifier---knn.html#cb336-4" aria-hidden="true" tabindex="-1"></a>EDistance <span class="ot">&lt;-</span> <span class="cf">function</span>(x, y){</span>
<span id="cb336-5"><a href="nonparametric-classifier---knn.html#cb336-5" aria-hidden="true" tabindex="-1"></a>  dx <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="fu">length</span>(x), <span class="fu">length</span>(x))</span>
<span id="cb336-6"><a href="nonparametric-classifier---knn.html#cb336-6" aria-hidden="true" tabindex="-1"></a>  dy <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="fu">length</span>(x), <span class="fu">length</span>(x))</span>
<span id="cb336-7"><a href="nonparametric-classifier---knn.html#cb336-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb336-8"><a href="nonparametric-classifier---knn.html#cb336-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(x)) {</span>
<span id="cb336-9"><a href="nonparametric-classifier---knn.html#cb336-9" aria-hidden="true" tabindex="-1"></a>    dx[i,] <span class="ot">&lt;-</span> (x[i] <span class="sc">-</span> x)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb336-10"><a href="nonparametric-classifier---knn.html#cb336-10" aria-hidden="true" tabindex="-1"></a>    dy[i,] <span class="ot">&lt;-</span> (y[i] <span class="sc">-</span> y)<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb336-11"><a href="nonparametric-classifier---knn.html#cb336-11" aria-hidden="true" tabindex="-1"></a>    dd <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(dx<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> dy<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb336-12"><a href="nonparametric-classifier---knn.html#cb336-12" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb336-13"><a href="nonparametric-classifier---knn.html#cb336-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(dd)</span>
<span id="cb336-14"><a href="nonparametric-classifier---knn.html#cb336-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb336-15"><a href="nonparametric-classifier---knn.html#cb336-15" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb336-16"><a href="nonparametric-classifier---knn.html#cb336-16" aria-hidden="true" tabindex="-1"></a><span class="fu">EDistance</span>(x1, x2)</span></code></pre></div>
<pre><code>##            [,1]       [,2]       [,3]       [,4]
## [1,] 0.00000000 0.09055385 5.65685425 6.88710389
## [2,] 0.09055385 0.00000000 4.62430535 5.82436263
## [3,] 5.65685425 4.62430535 0.00000000 0.09055385
## [4,] 6.88710389 5.82436263 0.09055385 0.00000000</code></pre>
<div class="sourceCode" id="cb338"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb338-1"><a href="nonparametric-classifier---knn.html#cb338-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x1, x2, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span>
<span id="cb338-2"><a href="nonparametric-classifier---knn.html#cb338-2" aria-hidden="true" tabindex="-1"></a><span class="co">#segments(x1[1], x2[1], x1[2:4], x2[2:4], col = &quot;blue&quot; )</span></span>
<span id="cb338-3"><a href="nonparametric-classifier---knn.html#cb338-3" aria-hidden="true" tabindex="-1"></a><span class="co">#segments(x1[2], x2[2], x1[c(1, 3:4)], x2[c(1, 3:4)], col = &quot;green&quot; )</span></span>
<span id="cb338-4"><a href="nonparametric-classifier---knn.html#cb338-4" aria-hidden="true" tabindex="-1"></a><span class="co">#segments(x1[3], x2[3], x1[c(1:2, 4)], x2[c(1:2, 4)], col = &quot;orange&quot; )</span></span>
<span id="cb338-5"><a href="nonparametric-classifier---knn.html#cb338-5" aria-hidden="true" tabindex="-1"></a><span class="fu">segments</span>(x1[<span class="dv">4</span>], x2[<span class="dv">4</span>], x1[<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>], x2[<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>], <span class="at">col =</span> <span class="st">&quot;darkgreen&quot;</span> )</span></code></pre></div>
<p><img src="08-Nonparametric_kNN_files/figure-html/knn9-1.png" width="672" /></p>
<p>The matrix shows all distances for four points and, as we expect, it is symmetric. The green lines show the distance from the last point (<span class="math inline">\(x = 4.3,~ y = 5.1\)</span>) to all other points. Using this matrix, we can easily find the k-nearest neighbors for any point.</p>
<p>When <span class="math inline">\(k=1\)</span>, the observation that has the shortest distance is going to be the one to predict what the red dot could be. This is shown in the figure below:</p>
<p><img src="png/kNN2.png" width="402" /></p>
<p>If we define the bin as <span class="math inline">\(k=3\)</span>, we look for the 3 nearest points to the red dot and then take an average of the 1s (7s) and 0s (2s) associated with these points. Here is an example:</p>
<p><img src="png/kNN3.png" width="402" /></p>
<p>Using <span class="math inline">\(k\)</span> neighbors to estimate the probability of <span class="math inline">\(Y=1\)</span> (the dot is 7), that is</p>
<p><span class="math display" id="eq:8-2">\[\begin{equation}
\hat{P}_{k}(Y=1 | X=x)=\frac{1}{k} \sum_{i \in \mathcal{N}_{k}(x, D)} I\left(y_{i}=1\right)
  \tag{8.2}
\end{equation}\]</span></p>
<p>With this predicted probability, we classify the red dot to the class with the most observations in the <span class="math inline">\(k\)</span> nearest neighbors (we assign a class at random to one of the classes tied for highest). Here is the rule in our case:</p>
<p><span class="math display">\[
\hat{C}_{k}(x)=\left\{\begin{array}{ll}{1} &amp; {\hat{p}_{k 0}(x)&gt;0.5} \\ {0} &amp; {\hat{p}_{k 1}(x)&lt;0.5}\end{array}\right.
\]</span></p>
<p>Suppose our red dot has <span class="math inline">\(x=(x_1,x_2)=(4,3)\)</span></p>
<p><span class="math display">\[
\begin{aligned} \hat{P}\left(Y=\text { Seven } | X_{1}=4, X_{2}=3\right)=\frac{2}{3} \\ \hat{P}\left(Y=\text { Two} | X_{1}=4, X_{2}=3\right)=\frac{1}{3} \end{aligned}
\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[
\hat{C}_{k=4}\left(x_{1}=4, x_{2}=3\right)=\text { Seven }
\]</span></p>
<p>As it’s clear from this application, <span class="math inline">\(k\)</span> is our hyperparameter and we need to tune it as to have the best predictive kNN algorithm. The following section will show its application. But before that, we need to understand how decision boundaries can be found in kNN</p>
<div class="sourceCode" id="cb339"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb339-1"><a href="nonparametric-classifier---knn.html#cb339-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb339-2"><a href="nonparametric-classifier---knn.html#cb339-2" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">50</span>)</span>
<span id="cb339-3"><a href="nonparametric-classifier---knn.html#cb339-3" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="dv">50</span>)</span>
<span id="cb339-4"><a href="nonparametric-classifier---knn.html#cb339-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb339-5"><a href="nonparametric-classifier---knn.html#cb339-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(deldir)</span>
<span id="cb339-6"><a href="nonparametric-classifier---knn.html#cb339-6" aria-hidden="true" tabindex="-1"></a>tesselation <span class="ot">&lt;-</span> <span class="fu">deldir</span>(x1, x2)</span>
<span id="cb339-7"><a href="nonparametric-classifier---knn.html#cb339-7" aria-hidden="true" tabindex="-1"></a>tiles <span class="ot">&lt;-</span> <span class="fu">tile.list</span>(tesselation)</span>
<span id="cb339-8"><a href="nonparametric-classifier---knn.html#cb339-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb339-9"><a href="nonparametric-classifier---knn.html#cb339-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(tiles, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">close =</span> <span class="cn">TRUE</span>,</span>
<span id="cb339-10"><a href="nonparametric-classifier---knn.html#cb339-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">fillcol =</span>  <span class="fu">hcl.colors</span>(<span class="dv">4</span>, <span class="st">&quot;Sunset&quot;</span>),</span>
<span id="cb339-11"><a href="nonparametric-classifier---knn.html#cb339-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="fl">0.2</span><span class="sc">:</span><span class="fl">1.1</span>))</span></code></pre></div>
<p><img src="08-Nonparametric_kNN_files/figure-html/knn12-1.png" width="672" /></p>
<p>These are called Voronoi cells associated with 1-NN, which is the set of polygons whose edges are the perpendicular bisectors of the lines joining the neighboring points. Thus, the decision boundary is the result of fusing adjacent Voronoi cells that are associated with same class. In the example above, it’s the boundary of unions of each colors. Finding the boundaries that trace each adjacent Vorono regions can be done with additional several steps.</p>
<p>To see all in an application, we will use <code>knn3()</code> from the <em>Caret</em> package. We will not train a model but only see how the separation between classes will be nonlinear and different for different <span class="math inline">\(k\)</span>.</p>
<div class="sourceCode" id="cb340"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb340-1"><a href="nonparametric-classifier---knn.html#cb340-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb340-2"><a href="nonparametric-classifier---knn.html#cb340-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb340-3"><a href="nonparametric-classifier---knn.html#cb340-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dslabs)</span>
<span id="cb340-4"><a href="nonparametric-classifier---knn.html#cb340-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb340-5"><a href="nonparametric-classifier---knn.html#cb340-5" aria-hidden="true" tabindex="-1"></a><span class="co">#With k = 50</span></span>
<span id="cb340-6"><a href="nonparametric-classifier---knn.html#cb340-6" aria-hidden="true" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">knn3</span>(y <span class="sc">~</span> ., <span class="at">data =</span> mnist_27<span class="sc">$</span>train, <span class="at">k =</span> <span class="dv">2</span>)</span>
<span id="cb340-7"><a href="nonparametric-classifier---knn.html#cb340-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb340-8"><a href="nonparametric-classifier---knn.html#cb340-8" aria-hidden="true" tabindex="-1"></a>x_1 <span class="ot">&lt;-</span> mnist_27<span class="sc">$</span>true_p<span class="sc">$</span>x_1</span>
<span id="cb340-9"><a href="nonparametric-classifier---knn.html#cb340-9" aria-hidden="true" tabindex="-1"></a>x_2 <span class="ot">&lt;-</span> mnist_27<span class="sc">$</span>true_p<span class="sc">$</span>x_2</span>
<span id="cb340-10"><a href="nonparametric-classifier---knn.html#cb340-10" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x_1, x_2) <span class="co">#This is whole data 22500 obs.</span></span>
<span id="cb340-11"><a href="nonparametric-classifier---knn.html#cb340-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb340-12"><a href="nonparametric-classifier---knn.html#cb340-12" aria-hidden="true" tabindex="-1"></a>p_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(model1, df, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>) <span class="co"># Predicting probabilities in each bin</span></span>
<span id="cb340-13"><a href="nonparametric-classifier---knn.html#cb340-13" aria-hidden="true" tabindex="-1"></a>p_7 <span class="ot">&lt;-</span> p_hat[,<span class="dv">2</span>] <span class="co">#Selecting the p_hat for 7</span></span>
<span id="cb340-14"><a href="nonparametric-classifier---knn.html#cb340-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb340-15"><a href="nonparametric-classifier---knn.html#cb340-15" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x_1, x_2, p_7)</span>
<span id="cb340-16"><a href="nonparametric-classifier---knn.html#cb340-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb340-17"><a href="nonparametric-classifier---knn.html#cb340-17" aria-hidden="true" tabindex="-1"></a>my_colors <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;red&quot;</span>)</span>
<span id="cb340-18"><a href="nonparametric-classifier---knn.html#cb340-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb340-19"><a href="nonparametric-classifier---knn.html#cb340-19" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb340-20"><a href="nonparametric-classifier---knn.html#cb340-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> mnist_27<span class="sc">$</span>train, <span class="fu">aes</span>(<span class="at">x =</span> x_1, <span class="at">y =</span> x_2, <span class="at">colour =</span> <span class="fu">factor</span>(y)),</span>
<span id="cb340-21"><a href="nonparametric-classifier---knn.html#cb340-21" aria-hidden="true" tabindex="-1"></a>             <span class="at">shape =</span> <span class="dv">21</span>, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">stroke =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb340-22"><a href="nonparametric-classifier---knn.html#cb340-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_contour</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> x_1, <span class="at">y =</span> x_2, <span class="at">z =</span> p_7), <span class="at">breaks=</span><span class="fu">c</span>(<span class="fl">0.5</span>), <span class="at">color=</span><span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb340-23"><a href="nonparametric-classifier---knn.html#cb340-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> my_colors)</span>
<span id="cb340-24"><a href="nonparametric-classifier---knn.html#cb340-24" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(p1)</span></code></pre></div>
<p><img src="08-Nonparametric_kNN_files/figure-html/knn13-1.png" width="672" /></p>
<div class="sourceCode" id="cb341"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb341-1"><a href="nonparametric-classifier---knn.html#cb341-1" aria-hidden="true" tabindex="-1"></a><span class="co">#With k = 400</span></span>
<span id="cb341-2"><a href="nonparametric-classifier---knn.html#cb341-2" aria-hidden="true" tabindex="-1"></a>model2 <span class="ot">&lt;-</span> <span class="fu">knn3</span>(y <span class="sc">~</span> ., <span class="at">data =</span> mnist_27<span class="sc">$</span>train, <span class="at">k =</span> <span class="dv">400</span>)</span>
<span id="cb341-3"><a href="nonparametric-classifier---knn.html#cb341-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb341-4"><a href="nonparametric-classifier---knn.html#cb341-4" aria-hidden="true" tabindex="-1"></a>p_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(model2, df, <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>) <span class="co"># Prediciting probabilities in each bin</span></span>
<span id="cb341-5"><a href="nonparametric-classifier---knn.html#cb341-5" aria-hidden="true" tabindex="-1"></a>p_7 <span class="ot">&lt;-</span> p_hat[,<span class="dv">2</span>] <span class="co">#Selecting the p_hat for 7</span></span>
<span id="cb341-6"><a href="nonparametric-classifier---knn.html#cb341-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb341-7"><a href="nonparametric-classifier---knn.html#cb341-7" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x_1, x_2, p_7)</span>
<span id="cb341-8"><a href="nonparametric-classifier---knn.html#cb341-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb341-9"><a href="nonparametric-classifier---knn.html#cb341-9" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>() <span class="sc">+</span></span>
<span id="cb341-10"><a href="nonparametric-classifier---knn.html#cb341-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> mnist_27<span class="sc">$</span>train, <span class="fu">aes</span>(<span class="at">x =</span> x_1, <span class="at">y =</span> x_2, <span class="at">colour =</span> <span class="fu">factor</span>(y)),</span>
<span id="cb341-11"><a href="nonparametric-classifier---knn.html#cb341-11" aria-hidden="true" tabindex="-1"></a>             <span class="at">shape =</span> <span class="dv">21</span>, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">stroke =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb341-12"><a href="nonparametric-classifier---knn.html#cb341-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_contour</span>(<span class="at">data =</span> df, <span class="fu">aes</span>(<span class="at">x =</span> x_1, <span class="at">y =</span> x_2, <span class="at">z =</span> p_7), <span class="at">breaks=</span><span class="fu">c</span>(<span class="fl">0.5</span>), <span class="at">color=</span><span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb341-13"><a href="nonparametric-classifier---knn.html#cb341-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> my_colors)</span>
<span id="cb341-14"><a href="nonparametric-classifier---knn.html#cb341-14" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(p1)</span></code></pre></div>
<p><img src="08-Nonparametric_kNN_files/figure-html/knn13-2.png" width="672" /></p>
<p>One with <span class="math inline">\(k=2\)</span> shows signs for overfitting, the other one with <span class="math inline">\(k=400\)</span> indicates oversmoothing or underfitting. We need to tune <span class="math inline">\(k\)</span> such a way that it will be best in terms of prediction accuracy.</p>
</div>
<div id="knn-with-caret" class="section level2 hasAnchor" number="8.4">
<h2><span class="header-section-number">8.4</span> kNN with caret<a href="nonparametric-classifier---knn.html#knn-with-caret" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are many different learning algorithms developed by different authors and often with different parametric structures. The <code>caret</code>, <strong>Classification And Regression Training</strong> package tries to consolidate these differences and provide consistency. It currently includes 237 (and growing) different methods which are summarized in the caret <a href="https://topepo.github.io/caret/available-models.html">package manual</a> <span class="citation">(<a href="#ref-Kuhn_2019" role="doc-biblioref">Kuhn 2019</a>)</span>. Here, we will use <code>mnset_27</code> to illustrate how we can use <code>caret</code> for kNN. For now, we will use the caret’s <code>train()</code> function to find the optimal <code>k</code> in kNN, which is basically an automated version of cross-validation that we will see in the next chapter.</p>
<div id="mnist_27" class="section level3 hasAnchor" number="8.4.1">
<h3><span class="header-section-number">8.4.1</span> <code>mnist_27</code><a href="nonparametric-classifier---knn.html#mnist_27" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Since, our dataset, <code>mnist_27</code>, is already split into train and test sets, we do not need to do it again. Here is the starting point:</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb342-1"><a href="nonparametric-classifier---knn.html#cb342-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb342-2"><a href="nonparametric-classifier---knn.html#cb342-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb342-3"><a href="nonparametric-classifier---knn.html#cb342-3" aria-hidden="true" tabindex="-1"></a><span class="co">#Training/Model building</span></span>
<span id="cb342-4"><a href="nonparametric-classifier---knn.html#cb342-4" aria-hidden="true" tabindex="-1"></a>model_knn <span class="ot">&lt;-</span> <span class="fu">train</span>(y <span class="sc">~</span> ., <span class="at">method =</span> <span class="st">&quot;knn&quot;</span>, <span class="at">data =</span> mnist_27<span class="sc">$</span>train)</span>
<span id="cb342-5"><a href="nonparametric-classifier---knn.html#cb342-5" aria-hidden="true" tabindex="-1"></a>model_knn</span></code></pre></div>
<pre><code>## k-Nearest Neighbors 
## 
## 800 samples
##   2 predictor
##   2 classes: &#39;2&#39;, &#39;7&#39; 
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 800, 800, 800, 800, 800, 800, ... 
## Resampling results across tuning parameters:
## 
##   k  Accuracy   Kappa    
##   5  0.8075980  0.6135168
##   7  0.8157975  0.6300494
##   9  0.8205824  0.6396302
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was k = 9.</code></pre>
<p>By default, the cross validation is performed by taking 25 bootstrap samples comprised of 25% of the observations. Moreover, the default is to try <span class="math inline">\(k=5,7,9\)</span>. We can to expand it:</p>
<div class="sourceCode" id="cb344"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb344-1"><a href="nonparametric-classifier---knn.html#cb344-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Training/Model building with our own grid</span></span>
<span id="cb344-2"><a href="nonparametric-classifier---knn.html#cb344-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2008</span>)</span>
<span id="cb344-3"><a href="nonparametric-classifier---knn.html#cb344-3" aria-hidden="true" tabindex="-1"></a>model_knn1 <span class="ot">&lt;-</span> <span class="fu">train</span>(</span>
<span id="cb344-4"><a href="nonparametric-classifier---knn.html#cb344-4" aria-hidden="true" tabindex="-1"></a>  y <span class="sc">~</span> .,</span>
<span id="cb344-5"><a href="nonparametric-classifier---knn.html#cb344-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;knn&quot;</span>,</span>
<span id="cb344-6"><a href="nonparametric-classifier---knn.html#cb344-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> mnist_27<span class="sc">$</span>train,</span>
<span id="cb344-7"><a href="nonparametric-classifier---knn.html#cb344-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">tuneGrid =</span> <span class="fu">data.frame</span>(<span class="at">k =</span> <span class="fu">seq</span>(<span class="dv">9</span>, <span class="dv">71</span>, <span class="dv">2</span>))</span>
<span id="cb344-8"><a href="nonparametric-classifier---knn.html#cb344-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb344-9"><a href="nonparametric-classifier---knn.html#cb344-9" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(model_knn1, <span class="at">highlight =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="08-Nonparametric_kNN_files/figure-html/knn15-1.png" width="672" /></p>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb345-1"><a href="nonparametric-classifier---knn.html#cb345-1" aria-hidden="true" tabindex="-1"></a>model_knn1<span class="sc">$</span>bestTune</span></code></pre></div>
<pre><code>##     k
## 10 27</code></pre>
<div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb347-1"><a href="nonparametric-classifier---knn.html#cb347-1" aria-hidden="true" tabindex="-1"></a>model_knn1<span class="sc">$</span>finalModel</span></code></pre></div>
<pre><code>## 27-nearest neighbor model
## Training set outcome distribution:
## 
##   2   7 
## 379 421</code></pre>
<p>We can change its tuning to cross-validation:</p>
<div class="sourceCode" id="cb349"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb349-1"><a href="nonparametric-classifier---knn.html#cb349-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Training/Model building with 10-k cross validation</span></span>
<span id="cb349-2"><a href="nonparametric-classifier---knn.html#cb349-2" aria-hidden="true" tabindex="-1"></a>cv <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span>, <span class="at">p =</span> <span class="fl">0.9</span>)</span>
<span id="cb349-3"><a href="nonparametric-classifier---knn.html#cb349-3" aria-hidden="true" tabindex="-1"></a>model_knn2 <span class="ot">&lt;-</span> <span class="fu">train</span>(y <span class="sc">~</span> ., <span class="at">method =</span> <span class="st">&quot;knn&quot;</span>, <span class="at">data =</span> mnist_27<span class="sc">$</span>train,</span>
<span id="cb349-4"><a href="nonparametric-classifier---knn.html#cb349-4" aria-hidden="true" tabindex="-1"></a>                   <span class="at">tuneGrid =</span> <span class="fu">data.frame</span>(<span class="at">k=</span><span class="fu">seq</span>(<span class="dv">9</span>,<span class="dv">71</span>,<span class="dv">2</span>)),</span>
<span id="cb349-5"><a href="nonparametric-classifier---knn.html#cb349-5" aria-hidden="true" tabindex="-1"></a>                   <span class="at">trControl =</span> cv)</span>
<span id="cb349-6"><a href="nonparametric-classifier---knn.html#cb349-6" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(model_knn2, <span class="at">highlight =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="08-Nonparametric_kNN_files/figure-html/knn16-1.png" width="672" /></p>
<div class="sourceCode" id="cb350"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb350-1"><a href="nonparametric-classifier---knn.html#cb350-1" aria-hidden="true" tabindex="-1"></a>model_knn2<span class="sc">$</span>bestTune</span></code></pre></div>
<pre><code>##     k
## 11 29</code></pre>
<p>It seems like <span class="math inline">\(k=27\)</span> (<span class="math inline">\(k=29\)</span> with CV) gives us the best performing prediction model. We can see their prediction performance on the test set:</p>
<div class="sourceCode" id="cb352"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb352-1"><a href="nonparametric-classifier---knn.html#cb352-1" aria-hidden="true" tabindex="-1"></a>caret<span class="sc">::</span><span class="fu">confusionMatrix</span>(<span class="fu">predict</span>(model_knn1, mnist_27<span class="sc">$</span>test, <span class="at">type =</span> <span class="st">&quot;raw&quot;</span>),</span>
<span id="cb352-2"><a href="nonparametric-classifier---knn.html#cb352-2" aria-hidden="true" tabindex="-1"></a>                mnist_27<span class="sc">$</span>test<span class="sc">$</span>y)</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  2  7
##          2 92 19
##          7 14 75
##                                           
##                Accuracy : 0.835           
##                  95% CI : (0.7762, 0.8836)
##     No Information Rate : 0.53            
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.6678          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.4862          
##                                           
##             Sensitivity : 0.8679          
##             Specificity : 0.7979          
##          Pos Pred Value : 0.8288          
##          Neg Pred Value : 0.8427          
##              Prevalence : 0.5300          
##          Detection Rate : 0.4600          
##    Detection Prevalence : 0.5550          
##       Balanced Accuracy : 0.8329          
##                                           
##        &#39;Positive&#39; Class : 2               
## </code></pre>
<div class="sourceCode" id="cb354"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb354-1"><a href="nonparametric-classifier---knn.html#cb354-1" aria-hidden="true" tabindex="-1"></a>caret<span class="sc">::</span><span class="fu">confusionMatrix</span>(<span class="fu">predict</span>(model_knn2, mnist_27<span class="sc">$</span>test, <span class="at">type =</span> <span class="st">&quot;raw&quot;</span>),</span>
<span id="cb354-2"><a href="nonparametric-classifier---knn.html#cb354-2" aria-hidden="true" tabindex="-1"></a>                mnist_27<span class="sc">$</span>test<span class="sc">$</span>y)</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  2  7
##          2 91 18
##          7 15 76
##                                           
##                Accuracy : 0.835           
##                  95% CI : (0.7762, 0.8836)
##     No Information Rate : 0.53            
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.6682          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.7277          
##                                           
##             Sensitivity : 0.8585          
##             Specificity : 0.8085          
##          Pos Pred Value : 0.8349          
##          Neg Pred Value : 0.8352          
##              Prevalence : 0.5300          
##          Detection Rate : 0.4550          
##    Detection Prevalence : 0.5450          
##       Balanced Accuracy : 0.8335          
##                                           
##        &#39;Positive&#39; Class : 2               
## </code></pre>
<p>What are these measures? What is a “Confusion Matrix”? We will see them in the next section. But for now, let’s use another example.</p>
</div>
<div id="adult-dataset" class="section level3 hasAnchor" number="8.4.2">
<h3><span class="header-section-number">8.4.2</span> Adult dataset<a href="nonparametric-classifier---knn.html#adult-dataset" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This dataset provides information on income earning and attributes that may effect it. Information on the dataset is given at its <a href="https://archive.ics.uci.edu/ml/datasets/Adult">website</a> <span class="citation">(<a href="#ref-Kohavi_1996" role="doc-biblioref">Kohavi and Becker 1996</a>)</span>:</p>
<blockquote>
<p>Extraction from 1994 US. Census database. A set of reasonably clean records was extracted using the following conditions: ((<code>AAGE</code>&gt;16) &amp;&amp; (<code>AGI</code>&gt;100) &amp;&amp; (<code>AFNLWGT</code>&gt;1)&amp;&amp; (<code>HRSWK</code>&gt;0)).</p>
</blockquote>
<p>The prediction task is to determine whether a person makes over 50K a year.</p>
<div class="sourceCode" id="cb356"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb356-1"><a href="nonparametric-classifier---knn.html#cb356-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Download adult income data</span></span>
<span id="cb356-2"><a href="nonparametric-classifier---knn.html#cb356-2" aria-hidden="true" tabindex="-1"></a><span class="co"># SET YOUR WORKING DIRECTORY FIRST</span></span>
<span id="cb356-3"><a href="nonparametric-classifier---knn.html#cb356-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb356-4"><a href="nonparametric-classifier---knn.html#cb356-4" aria-hidden="true" tabindex="-1"></a><span class="co"># url.train &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data&quot;</span></span>
<span id="cb356-5"><a href="nonparametric-classifier---knn.html#cb356-5" aria-hidden="true" tabindex="-1"></a><span class="co"># url.test &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test&quot;</span></span>
<span id="cb356-6"><a href="nonparametric-classifier---knn.html#cb356-6" aria-hidden="true" tabindex="-1"></a><span class="co"># url.names &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names&quot;</span></span>
<span id="cb356-7"><a href="nonparametric-classifier---knn.html#cb356-7" aria-hidden="true" tabindex="-1"></a><span class="co"># download.file(url.train, destfile = &quot;adult_train.csv&quot;)</span></span>
<span id="cb356-8"><a href="nonparametric-classifier---knn.html#cb356-8" aria-hidden="true" tabindex="-1"></a><span class="co"># download.file(url.test, destfile = &quot;adult_test.csv&quot;)</span></span>
<span id="cb356-9"><a href="nonparametric-classifier---knn.html#cb356-9" aria-hidden="true" tabindex="-1"></a><span class="co"># download.file(url.names, destfile = &quot;adult_names.txt&quot;)</span></span>
<span id="cb356-10"><a href="nonparametric-classifier---knn.html#cb356-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb356-11"><a href="nonparametric-classifier---knn.html#cb356-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Read the training set into memory</span></span>
<span id="cb356-12"><a href="nonparametric-classifier---knn.html#cb356-12" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;adult_train.csv&quot;</span>, <span class="at">header =</span> <span class="cn">FALSE</span>)</span>
<span id="cb356-13"><a href="nonparametric-classifier---knn.html#cb356-13" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(train)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    32561 obs. of  15 variables:
##  $ V1 : int  39 50 38 53 28 37 49 52 31 42 ...
##  $ V2 : chr  &quot; State-gov&quot; &quot; Self-emp-not-inc&quot; &quot; Private&quot; &quot; Private&quot; ...
##  $ V3 : int  77516 83311 215646 234721 338409 284582 160187 209642 45781 159449 ...
##  $ V4 : chr  &quot; Bachelors&quot; &quot; Bachelors&quot; &quot; HS-grad&quot; &quot; 11th&quot; ...
##  $ V5 : int  13 13 9 7 13 14 5 9 14 13 ...
##  $ V6 : chr  &quot; Never-married&quot; &quot; Married-civ-spouse&quot; &quot; Divorced&quot; &quot; Married-civ-spouse&quot; ...
##  $ V7 : chr  &quot; Adm-clerical&quot; &quot; Exec-managerial&quot; &quot; Handlers-cleaners&quot; &quot; Handlers-cleaners&quot; ...
##  $ V8 : chr  &quot; Not-in-family&quot; &quot; Husband&quot; &quot; Not-in-family&quot; &quot; Husband&quot; ...
##  $ V9 : chr  &quot; White&quot; &quot; White&quot; &quot; White&quot; &quot; Black&quot; ...
##  $ V10: chr  &quot; Male&quot; &quot; Male&quot; &quot; Male&quot; &quot; Male&quot; ...
##  $ V11: int  2174 0 0 0 0 0 0 0 14084 5178 ...
##  $ V12: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ V13: int  40 13 40 40 40 40 16 45 50 40 ...
##  $ V14: chr  &quot; United-States&quot; &quot; United-States&quot; &quot; United-States&quot; &quot; United-States&quot; ...
##  $ V15: chr  &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; ...</code></pre>
<div class="sourceCode" id="cb358"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb358-1"><a href="nonparametric-classifier---knn.html#cb358-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Read the test set into memory</span></span>
<span id="cb358-2"><a href="nonparametric-classifier---knn.html#cb358-2" aria-hidden="true" tabindex="-1"></a>test <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;adult_test.csv&quot;</span>, <span class="at">header =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>The data doesn’t have the variable names. That’s bad because we don’t know which one is which. Check the <strong>adult_names.txt</strong> file. The list of variables is given in that file. Thanks to <a href="https://rpubs.com/mbaumer/knn">Matthew Baumer</a> <span class="citation">(<a href="#ref-Baumer_2015" role="doc-biblioref">Baumer 2015</a>)</span>, we can write them manually:</p>
<div class="sourceCode" id="cb359"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb359-1"><a href="nonparametric-classifier---knn.html#cb359-1" aria-hidden="true" tabindex="-1"></a>varNames <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Age&quot;</span>, </span>
<span id="cb359-2"><a href="nonparametric-classifier---knn.html#cb359-2" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;WorkClass&quot;</span>,</span>
<span id="cb359-3"><a href="nonparametric-classifier---knn.html#cb359-3" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;fnlwgt&quot;</span>,</span>
<span id="cb359-4"><a href="nonparametric-classifier---knn.html#cb359-4" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;Education&quot;</span>,</span>
<span id="cb359-5"><a href="nonparametric-classifier---knn.html#cb359-5" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;EducationNum&quot;</span>,</span>
<span id="cb359-6"><a href="nonparametric-classifier---knn.html#cb359-6" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;MaritalStatus&quot;</span>,</span>
<span id="cb359-7"><a href="nonparametric-classifier---knn.html#cb359-7" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;Occupation&quot;</span>,</span>
<span id="cb359-8"><a href="nonparametric-classifier---knn.html#cb359-8" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;Relationship&quot;</span>,</span>
<span id="cb359-9"><a href="nonparametric-classifier---knn.html#cb359-9" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;Race&quot;</span>,</span>
<span id="cb359-10"><a href="nonparametric-classifier---knn.html#cb359-10" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;Sex&quot;</span>,</span>
<span id="cb359-11"><a href="nonparametric-classifier---knn.html#cb359-11" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;CapitalGain&quot;</span>,</span>
<span id="cb359-12"><a href="nonparametric-classifier---knn.html#cb359-12" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;CapitalLoss&quot;</span>,</span>
<span id="cb359-13"><a href="nonparametric-classifier---knn.html#cb359-13" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;HoursPerWeek&quot;</span>,</span>
<span id="cb359-14"><a href="nonparametric-classifier---knn.html#cb359-14" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;NativeCountry&quot;</span>,</span>
<span id="cb359-15"><a href="nonparametric-classifier---knn.html#cb359-15" aria-hidden="true" tabindex="-1"></a>              <span class="st">&quot;IncomeLevel&quot;</span>)</span>
<span id="cb359-16"><a href="nonparametric-classifier---knn.html#cb359-16" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(train) <span class="ot">&lt;-</span> varNames</span>
<span id="cb359-17"><a href="nonparametric-classifier---knn.html#cb359-17" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(test) <span class="ot">&lt;-</span> varNames</span>
<span id="cb359-18"><a href="nonparametric-classifier---knn.html#cb359-18" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(train)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    32561 obs. of  15 variables:
##  $ Age          : int  39 50 38 53 28 37 49 52 31 42 ...
##  $ WorkClass    : chr  &quot; State-gov&quot; &quot; Self-emp-not-inc&quot; &quot; Private&quot; &quot; Private&quot; ...
##  $ fnlwgt       : int  77516 83311 215646 234721 338409 284582 160187 209642 45781 159449 ...
##  $ Education    : chr  &quot; Bachelors&quot; &quot; Bachelors&quot; &quot; HS-grad&quot; &quot; 11th&quot; ...
##  $ EducationNum : int  13 13 9 7 13 14 5 9 14 13 ...
##  $ MaritalStatus: chr  &quot; Never-married&quot; &quot; Married-civ-spouse&quot; &quot; Divorced&quot; &quot; Married-civ-spouse&quot; ...
##  $ Occupation   : chr  &quot; Adm-clerical&quot; &quot; Exec-managerial&quot; &quot; Handlers-cleaners&quot; &quot; Handlers-cleaners&quot; ...
##  $ Relationship : chr  &quot; Not-in-family&quot; &quot; Husband&quot; &quot; Not-in-family&quot; &quot; Husband&quot; ...
##  $ Race         : chr  &quot; White&quot; &quot; White&quot; &quot; White&quot; &quot; Black&quot; ...
##  $ Sex          : chr  &quot; Male&quot; &quot; Male&quot; &quot; Male&quot; &quot; Male&quot; ...
##  $ CapitalGain  : int  2174 0 0 0 0 0 0 0 14084 5178 ...
##  $ CapitalLoss  : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ HoursPerWeek : int  40 13 40 40 40 40 16 45 50 40 ...
##  $ NativeCountry: chr  &quot; United-States&quot; &quot; United-States&quot; &quot; United-States&quot; &quot; United-States&quot; ...
##  $ IncomeLevel  : chr  &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; &quot; &lt;=50K&quot; ...</code></pre>
<p>Since the dataset is large we are not going to use the test set but split the train set into our own test and train sets. Note that, however, if we had used the original test set, we would have had to make some adjustments/cleaning before using it. For example, if you look at <code>Age</code> variable, it seems as a factor variable. It is an integer in the training set. We have to change it first. Moreover, our <span class="math inline">\(Y\)</span> has two levels in the train set, it has 3 levels in the test set. We have to go over each variable and make sure that the test and train sets have the same features and class types. This task is left to you if you want to use the original train and test sets. A final tip: remove the first row in the original test set!</p>
<div class="sourceCode" id="cb361"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb361-1"><a href="nonparametric-classifier---knn.html#cb361-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Caret needs some preparations!</span></span>
<span id="cb361-2"><a href="nonparametric-classifier---knn.html#cb361-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(train<span class="sc">$</span>IncomeLevel)</span></code></pre></div>
<pre><code>## 
##  &lt;=50K   &gt;50K 
##  24720   7841</code></pre>
<div class="sourceCode" id="cb363"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb363-1"><a href="nonparametric-classifier---knn.html#cb363-1" aria-hidden="true" tabindex="-1"></a><span class="co"># This is b/c we will use the same data for LPM</span></span>
<span id="cb363-2"><a href="nonparametric-classifier---knn.html#cb363-2" aria-hidden="true" tabindex="-1"></a>train<span class="sc">$</span>Y <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(train<span class="sc">$</span>IncomeLevel <span class="sc">==</span> <span class="st">&quot; &lt;=50K&quot;</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb363-3"><a href="nonparametric-classifier---knn.html#cb363-3" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> train[,<span class="sc">-</span><span class="dv">15</span>]</span>
<span id="cb363-4"><a href="nonparametric-classifier---knn.html#cb363-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb363-5"><a href="nonparametric-classifier---knn.html#cb363-5" aria-hidden="true" tabindex="-1"></a><span class="co"># kNN needs Y to be a factor variable</span></span>
<span id="cb363-6"><a href="nonparametric-classifier---knn.html#cb363-6" aria-hidden="true" tabindex="-1"></a>train<span class="sc">$</span>Y <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(train<span class="sc">$</span>Y)</span>
<span id="cb363-7"><a href="nonparametric-classifier---knn.html#cb363-7" aria-hidden="true" tabindex="-1"></a><span class="fu">levels</span>(train<span class="sc">$</span>Y)[<span class="fu">levels</span>(train<span class="sc">$</span>Y) <span class="sc">==</span> <span class="st">&quot;0&quot;</span>] <span class="ot">&lt;-</span> <span class="st">&quot;Less&quot;</span></span>
<span id="cb363-8"><a href="nonparametric-classifier---knn.html#cb363-8" aria-hidden="true" tabindex="-1"></a><span class="fu">levels</span>(train<span class="sc">$</span>Y)[<span class="fu">levels</span>(train<span class="sc">$</span>Y) <span class="sc">==</span> <span class="st">&quot;1&quot;</span>] <span class="ot">&lt;-</span> <span class="st">&quot;More&quot;</span></span>
<span id="cb363-9"><a href="nonparametric-classifier---knn.html#cb363-9" aria-hidden="true" tabindex="-1"></a><span class="fu">levels</span>(train<span class="sc">$</span>Y)</span></code></pre></div>
<pre><code>## [1] &quot;Less&quot; &quot;More&quot;</code></pre>
<div class="sourceCode" id="cb365"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb365-1"><a href="nonparametric-classifier---knn.html#cb365-1" aria-hidden="true" tabindex="-1"></a><span class="co">#kNN</span></span>
<span id="cb365-2"><a href="nonparametric-classifier---knn.html#cb365-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">3033</span>)</span>
<span id="cb365-3"><a href="nonparametric-classifier---knn.html#cb365-3" aria-hidden="true" tabindex="-1"></a>train_df <span class="ot">&lt;-</span></span>
<span id="cb365-4"><a href="nonparametric-classifier---knn.html#cb365-4" aria-hidden="true" tabindex="-1"></a>  caret<span class="sc">::</span><span class="fu">createDataPartition</span>(<span class="at">y =</span> train<span class="sc">$</span>Y, <span class="at">p =</span> <span class="fl">0.7</span>, <span class="at">list =</span> <span class="cn">FALSE</span>)</span>
<span id="cb365-5"><a href="nonparametric-classifier---knn.html#cb365-5" aria-hidden="true" tabindex="-1"></a>training <span class="ot">&lt;-</span> train[train_df, ]</span>
<span id="cb365-6"><a href="nonparametric-classifier---knn.html#cb365-6" aria-hidden="true" tabindex="-1"></a>testing <span class="ot">&lt;-</span> train[<span class="sc">-</span>train_df, ]</span>
<span id="cb365-7"><a href="nonparametric-classifier---knn.html#cb365-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb365-8"><a href="nonparametric-classifier---knn.html#cb365-8" aria-hidden="true" tabindex="-1"></a><span class="co">#Training/Model building with 10-k cross validation</span></span>
<span id="cb365-9"><a href="nonparametric-classifier---knn.html#cb365-9" aria-hidden="true" tabindex="-1"></a>cv <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span>, <span class="at">p =</span> <span class="fl">0.9</span>)</span>
<span id="cb365-10"><a href="nonparametric-classifier---knn.html#cb365-10" aria-hidden="true" tabindex="-1"></a>model_knn3 <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">train</span>(</span>
<span id="cb365-11"><a href="nonparametric-classifier---knn.html#cb365-11" aria-hidden="true" tabindex="-1"></a>  Y <span class="sc">~</span> .,</span>
<span id="cb365-12"><a href="nonparametric-classifier---knn.html#cb365-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;knn&quot;</span>,</span>
<span id="cb365-13"><a href="nonparametric-classifier---knn.html#cb365-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> training,</span>
<span id="cb365-14"><a href="nonparametric-classifier---knn.html#cb365-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">tuneGrid =</span> <span class="fu">data.frame</span>(<span class="at">k =</span> <span class="fu">seq</span>(<span class="dv">9</span>, <span class="dv">41</span> , <span class="dv">2</span>)),</span>
<span id="cb365-15"><a href="nonparametric-classifier---knn.html#cb365-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">trControl =</span> cv</span>
<span id="cb365-16"><a href="nonparametric-classifier---knn.html#cb365-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb365-17"><a href="nonparametric-classifier---knn.html#cb365-17" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(model_knn3, <span class="at">highlight =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="08-Nonparametric_kNN_files/figure-html/knn20-1.png" width="672" /></p>
<p>Now we are going to use the test set to see the model’s performance.</p>
<div class="sourceCode" id="cb366"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb366-1"><a href="nonparametric-classifier---knn.html#cb366-1" aria-hidden="true" tabindex="-1"></a>caret<span class="sc">::</span><span class="fu">confusionMatrix</span>(<span class="fu">predict</span>(model_knn3, testing, <span class="at">type =</span> <span class="st">&quot;raw&quot;</span>),</span>
<span id="cb366-2"><a href="nonparametric-classifier---knn.html#cb366-2" aria-hidden="true" tabindex="-1"></a>                testing<span class="sc">$</span>Y)</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction Less More
##       Less 7311 1871
##       More  105  481
##                                           
##                Accuracy : 0.7977          
##                  95% CI : (0.7896, 0.8056)
##     No Information Rate : 0.7592          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.256           
##                                           
##  Mcnemar&#39;s Test P-Value : &lt; 2.2e-16       
##                                           
##             Sensitivity : 0.9858          
##             Specificity : 0.2045          
##          Pos Pred Value : 0.7962          
##          Neg Pred Value : 0.8208          
##              Prevalence : 0.7592          
##          Detection Rate : 0.7485          
##    Detection Prevalence : 0.9400          
##       Balanced Accuracy : 0.5952          
##                                           
##        &#39;Positive&#39; Class : Less            
## </code></pre>
<p>Next, as you can guess, we will delve into these performance measures.</p>
<p>Learning algorithm may not be evaluated only by its predictive capacity. We may want to interpret the results by identifying the important predictors and their importance. <strong>There is always a trade-off between interpretability and predictive accuracy</strong>. Here is a an illustration. We will talk about this later in the book.</p>
<p><img src="png/tradeoff.png" width="130%" height="130%" /></p>

</div>
</div>
</div>



<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Baumer_2015" class="csl-entry">
Baumer, Matthew. 2015. <span>“K Nearest Neighbors.”</span> <a href="https://rpubs.com/mbaumer/knn">https://rpubs.com/mbaumer/knn</a>.
</div>
<div id="ref-Kohavi_1996" class="csl-entry">
Kohavi, Ronny, and Barry Becker. 1996. <span>“Adult Data Set.”</span> University of California, Irvine, School of Information &amp; Computer Sciences. <a href="https://archive.ics.uci.edu/ml/datasets/Adult">https://archive.ics.uci.edu/ml/datasets/Adult</a>.
</div>
<div id="ref-Kuhn_2019" class="csl-entry">
Kuhn, Max. 2019. <em>The Caret Package</em>. Bookdown. <a href="https://topepo.github.io/caret/index.html">https://topepo.github.io/caret/index.html</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="smoothing.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="hyperparameter-tuning.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/yaydede/toolbox/edit/master/08-Nonparametric_kNN.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["YA_TextBook.pdf", "YA_TextBook.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
