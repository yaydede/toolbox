<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 20 Sparsity | Toolbox for Social Scientists and Policy Analysts</title>
  <meta name="description" content="Chapter 20 Sparsity | Toolbox for Social Scientists and Policy Analysts" />
  <meta name="generator" content="bookdown 0.32.1 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 20 Sparsity | Toolbox for Social Scientists and Policy Analysts" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://yaydede.github.io/toolbox//png/cover2.png" />
  
  <meta name="github-repo" content="yaydede/ToolShed_draft" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 20 Sparsity | Toolbox for Social Scientists and Policy Analysts" />
  
  
  <meta name="twitter:image" content="https://yaydede.github.io/toolbox//png/cover2.png" />

<meta name="author" content="Yigit Aydede" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="adaptive-lasso.html"/>
<link rel="next" href="forecasting.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Toolbox</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who"><i class="fa fa-check"></i>Who</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> How we define Machine Learning</a></li>
<li class="chapter" data-level="2" data-path="preliminaries.html"><a href="preliminaries.html"><i class="fa fa-check"></i><b>2</b> Preliminaries</a>
<ul>
<li class="chapter" data-level="2.1" data-path="preliminaries.html"><a href="preliminaries.html#data-and-dataset-types"><i class="fa fa-check"></i><b>2.1</b> Data and dataset types</a></li>
<li class="chapter" data-level="2.2" data-path="preliminaries.html"><a href="preliminaries.html#plots"><i class="fa fa-check"></i><b>2.2</b> Plots</a></li>
<li class="chapter" data-level="2.3" data-path="preliminaries.html"><a href="preliminaries.html#probability-distributions-with-r"><i class="fa fa-check"></i><b>2.3</b> Probability Distributions with R</a></li>
<li class="chapter" data-level="2.4" data-path="preliminaries.html"><a href="preliminaries.html#regressions"><i class="fa fa-check"></i><b>2.4</b> Regressions</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="preliminaries.html"><a href="preliminaries.html#ordinary-least-squares-ols"><i class="fa fa-check"></i><b>2.4.1</b> Ordinary Least Squares (OLS)</a></li>
<li class="chapter" data-level="2.4.2" data-path="preliminaries.html"><a href="preliminaries.html#maximum-likelihood-estimators"><i class="fa fa-check"></i><b>2.4.2</b> Maximum Likelihood Estimators</a></li>
<li class="chapter" data-level="2.4.3" data-path="preliminaries.html"><a href="preliminaries.html#estimating-mle-with-r"><i class="fa fa-check"></i><b>2.4.3</b> Estimating MLE with R</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="preliminaries.html"><a href="preliminaries.html#blue"><i class="fa fa-check"></i><b>2.5</b> BLUE</a></li>
<li class="chapter" data-level="2.6" data-path="preliminaries.html"><a href="preliminaries.html#modeling-the-data"><i class="fa fa-check"></i><b>2.6</b> Modeling the data</a></li>
<li class="chapter" data-level="2.7" data-path="preliminaries.html"><a href="preliminaries.html#causal-vs.-predictive-models"><i class="fa fa-check"></i><b>2.7</b> Causal vs. Predictive Models</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="preliminaries.html"><a href="preliminaries.html#causal-models"><i class="fa fa-check"></i><b>2.7.1</b> Causal Models</a></li>
<li class="chapter" data-level="2.7.2" data-path="preliminaries.html"><a href="preliminaries.html#predictive-models"><i class="fa fa-check"></i><b>2.7.2</b> Predictive Models</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="preliminaries.html"><a href="preliminaries.html#simulation"><i class="fa fa-check"></i><b>2.8</b> Simulation</a></li>
</ul></li>
<li class="part"><span><b>I Formal Look at Prediction</b></span></li>
<li class="chapter" data-level="" data-path="learning-systems.html"><a href="learning-systems.html"><i class="fa fa-check"></i>Learning Systems</a></li>
<li class="chapter" data-level="3" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html"><i class="fa fa-check"></i><b>3</b> Bias-Variance Tradeoff</a>
<ul>
<li class="chapter" data-level="3.1" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#estimator-and-mse"><i class="fa fa-check"></i><b>3.1</b> Estimator and MSE</a></li>
<li class="chapter" data-level="3.2" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#prediction---mspe"><i class="fa fa-check"></i><b>3.2</b> Prediction - MSPE</a></li>
<li class="chapter" data-level="3.3" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#biased-estimator-as-a-predictor"><i class="fa fa-check"></i><b>3.3</b> Biased estimator as a predictor</a></li>
<li class="chapter" data-level="3.4" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#dropping-a-variable-in-a-regression"><i class="fa fa-check"></i><b>3.4</b> Dropping a variable in a regression</a></li>
<li class="chapter" data-level="3.5" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#uncertainty-in-estimations-and-predictions"><i class="fa fa-check"></i><b>3.5</b> Uncertainty in estimations and predictions</a></li>
<li class="chapter" data-level="3.6" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#prediction-interval-for-unbiased-ols-predictor"><i class="fa fa-check"></i><b>3.6</b> Prediction interval for unbiased OLS predictor</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="overfitting.html"><a href="overfitting.html"><i class="fa fa-check"></i><b>4</b> Overfitting</a></li>
<li class="part"><span><b>II Nonparametric Estimations</b></span></li>
<li class="chapter" data-level="" data-path="parametric-vs.-nonparametric-methods.html"><a href="parametric-vs.-nonparametric-methods.html"><i class="fa fa-check"></i>Parametric vs. Nonparametric methods</a></li>
<li class="chapter" data-level="5" data-path="parametric-estimations.html"><a href="parametric-estimations.html"><i class="fa fa-check"></i><b>5</b> Parametric Estimations</a>
<ul>
<li class="chapter" data-level="5.1" data-path="parametric-estimations.html"><a href="parametric-estimations.html#linear-probability-models-lpm"><i class="fa fa-check"></i><b>5.1</b> Linear Probability Models (LPM)</a></li>
<li class="chapter" data-level="5.2" data-path="parametric-estimations.html"><a href="parametric-estimations.html#logistic-regression"><i class="fa fa-check"></i><b>5.2</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="parametric-estimations.html"><a href="parametric-estimations.html#estimating-logistic-regression"><i class="fa fa-check"></i><b>5.2.1</b> Estimating Logistic Regression</a></li>
<li class="chapter" data-level="5.2.2" data-path="parametric-estimations.html"><a href="parametric-estimations.html#cost-functions"><i class="fa fa-check"></i><b>5.2.2</b> Cost functions</a></li>
<li class="chapter" data-level="5.2.3" data-path="parametric-estimations.html"><a href="parametric-estimations.html#deviance"><i class="fa fa-check"></i><b>5.2.3</b> Deviance</a></li>
<li class="chapter" data-level="5.2.4" data-path="parametric-estimations.html"><a href="parametric-estimations.html#predictive-accuracy"><i class="fa fa-check"></i><b>5.2.4</b> Predictive accuracy</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html"><i class="fa fa-check"></i><b>6</b> Nonparametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="6.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#density-estimations"><i class="fa fa-check"></i><b>6.1</b> Density Estimations</a></li>
<li class="chapter" data-level="6.2" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#kernel-regressions"><i class="fa fa-check"></i><b>6.2</b> Kernel regressions</a></li>
<li class="chapter" data-level="6.3" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#regression-splines"><i class="fa fa-check"></i><b>6.3</b> Regression Splines</a></li>
<li class="chapter" data-level="6.4" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#mars---multivariate-adaptive-regression-splines"><i class="fa fa-check"></i><b>6.4</b> MARS - Multivariate Adaptive Regression Splines</a></li>
<li class="chapter" data-level="6.5" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#gam---generalized-additive-model"><i class="fa fa-check"></i><b>6.5</b> GAM - Generalized Additive Model</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>7</b> Smoothing</a>
<ul>
<li class="chapter" data-level="7.1" data-path="smoothing.html"><a href="smoothing.html#using-bins"><i class="fa fa-check"></i><b>7.1</b> Using bins</a></li>
<li class="chapter" data-level="7.2" data-path="smoothing.html"><a href="smoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>7.2</b> Kernel smoothing</a></li>
<li class="chapter" data-level="7.3" data-path="smoothing.html"><a href="smoothing.html#locally-weighted-regression-loess"><i class="fa fa-check"></i><b>7.3</b> Locally weighted regression <code>loess()</code></a></li>
<li class="chapter" data-level="7.4" data-path="smoothing.html"><a href="smoothing.html#smooth-spline-regression"><i class="fa fa-check"></i><b>7.4</b> Smooth Spline Regression</a></li>
<li class="chapter" data-level="7.5" data-path="smoothing.html"><a href="smoothing.html#multivariate-loess"><i class="fa fa-check"></i><b>7.5</b> Multivariate Loess</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html"><i class="fa fa-check"></i><b>8</b> Nonparametric Classifier - kNN</a>
<ul>
<li class="chapter" data-level="8.1" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#mnist-dataset"><i class="fa fa-check"></i><b>8.1</b> <code>mnist</code> Dataset</a></li>
<li class="chapter" data-level="8.2" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#linear-classifiers-again"><i class="fa fa-check"></i><b>8.2</b> Linear classifiers (again)</a></li>
<li class="chapter" data-level="8.3" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>8.3</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="8.4" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#knn-with-caret"><i class="fa fa-check"></i><b>8.4</b> kNN with caret</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#mnist_27"><i class="fa fa-check"></i><b>8.4.1</b> <code>mnist_27</code></a></li>
<li class="chapter" data-level="8.4.2" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#adult-dataset"><i class="fa fa-check"></i><b>8.4.2</b> Adult dataset</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Self-Learning</b></span></li>
<li class="chapter" data-level="9" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html"><i class="fa fa-check"></i><b>9</b> Hyperparameter Tuning</a>
<ul>
<li class="chapter" data-level="9.1" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#training-validation-and-test-datasets"><i class="fa fa-check"></i><b>9.1</b> Training, validation, and test datasets</a></li>
<li class="chapter" data-level="9.2" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#splitting-the-data-randomly"><i class="fa fa-check"></i><b>9.2</b> Splitting the data randomly</a></li>
<li class="chapter" data-level="9.3" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>9.3</b> k-fold cross validation</a></li>
<li class="chapter" data-level="9.4" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#cross-validated-grid-search"><i class="fa fa-check"></i><b>9.4</b> Cross-validated grid search</a></li>
<li class="chapter" data-level="9.5" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#bootstrapped-grid-search"><i class="fa fa-check"></i><b>9.5</b> Bootstrapped grid search</a></li>
<li class="chapter" data-level="9.6" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#when-the-data-is-time-series"><i class="fa fa-check"></i><b>9.6</b> When the data is time-series</a></li>
<li class="chapter" data-level="9.7" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#speed"><i class="fa fa-check"></i><b>9.7</b> Speed</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html"><i class="fa fa-check"></i><b>10</b> Tuning in Classification</a>
<ul>
<li class="chapter" data-level="10.1" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html#confusion-matrix"><i class="fa fa-check"></i><b>10.1</b> Confusion matrix</a></li>
<li class="chapter" data-level="10.2" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html#performance-measures"><i class="fa fa-check"></i><b>10.2</b> Performance measures</a></li>
<li class="chapter" data-level="10.3" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html#roc-curve"><i class="fa fa-check"></i><b>10.3</b> ROC Curve</a></li>
<li class="chapter" data-level="10.4" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html#auc---area-under-the-curve"><i class="fa fa-check"></i><b>10.4</b> AUC - Area Under the Curve</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="classification-example.html"><a href="classification-example.html"><i class="fa fa-check"></i><b>11</b> Classification Example</a>
<ul>
<li class="chapter" data-level="11.1" data-path="classification-example.html"><a href="classification-example.html#lpm"><i class="fa fa-check"></i><b>11.1</b> LPM</a></li>
<li class="chapter" data-level="11.2" data-path="classification-example.html"><a href="classification-example.html#logistic-regression-1"><i class="fa fa-check"></i><b>11.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="11.3" data-path="classification-example.html"><a href="classification-example.html#knn"><i class="fa fa-check"></i><b>11.3</b> kNN</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="classification-example.html"><a href="classification-example.html#knn-10-fold-cv"><i class="fa fa-check"></i><b>11.3.1</b> kNN 10-fold CV</a></li>
<li class="chapter" data-level="11.3.2" data-path="classification-example.html"><a href="classification-example.html#knn-with-caret-1"><i class="fa fa-check"></i><b>11.3.2</b> kNN with <code>caret</code></a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Tree-based Models</b></span></li>
<li class="chapter" data-level="12" data-path="cart.html"><a href="cart.html"><i class="fa fa-check"></i><b>12</b> CART</a>
<ul>
<li class="chapter" data-level="12.1" data-path="cart.html"><a href="cart.html#cart---classification-tree"><i class="fa fa-check"></i><b>12.1</b> CART - Classification Tree</a></li>
<li class="chapter" data-level="12.2" data-path="cart.html"><a href="cart.html#rpart---recursive-partitioning"><i class="fa fa-check"></i><b>12.2</b> <code>rpart</code> - Recursive Partitioning</a></li>
<li class="chapter" data-level="12.3" data-path="cart.html"><a href="cart.html#pruning"><i class="fa fa-check"></i><b>12.3</b> Pruning</a></li>
<li class="chapter" data-level="12.4" data-path="cart.html"><a href="cart.html#classification-with-titanic"><i class="fa fa-check"></i><b>12.4</b> Classification with Titanic</a></li>
<li class="chapter" data-level="12.5" data-path="cart.html"><a href="cart.html#regression-tree"><i class="fa fa-check"></i><b>12.5</b> Regression Tree</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ensemble-learning.html"><a href="ensemble-learning.html"><i class="fa fa-check"></i><b>13</b> Ensemble Learning</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ensemble-learning.html"><a href="ensemble-learning.html#bagging"><i class="fa fa-check"></i><b>13.1</b> Bagging</a></li>
<li class="chapter" data-level="13.2" data-path="ensemble-learning.html"><a href="ensemble-learning.html#random-forest"><i class="fa fa-check"></i><b>13.2</b> Random Forest</a></li>
<li class="chapter" data-level="13.3" data-path="ensemble-learning.html"><a href="ensemble-learning.html#boosting"><i class="fa fa-check"></i><b>13.3</b> Boosting</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="ensemble-learning.html"><a href="ensemble-learning.html#sequential-ensemble-with-gbm"><i class="fa fa-check"></i><b>13.3.1</b> Sequential ensemble with <code>gbm</code></a></li>
<li class="chapter" data-level="13.3.2" data-path="ensemble-learning.html"><a href="ensemble-learning.html#adaboost"><i class="fa fa-check"></i><b>13.3.2</b> AdaBoost</a></li>
<li class="chapter" data-level="13.3.3" data-path="ensemble-learning.html"><a href="ensemble-learning.html#xgboost"><i class="fa fa-check"></i><b>13.3.3</b> XGBoost</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ensemble-applications.html"><a href="ensemble-applications.html"><i class="fa fa-check"></i><b>14</b> Ensemble Applications</a>
<ul>
<li class="chapter" data-level="14.1" data-path="ensemble-applications.html"><a href="ensemble-applications.html#classification"><i class="fa fa-check"></i><b>14.1</b> Classification</a></li>
<li class="chapter" data-level="14.2" data-path="ensemble-applications.html"><a href="ensemble-applications.html#regression"><i class="fa fa-check"></i><b>14.2</b> Regression</a></li>
<li class="chapter" data-level="14.3" data-path="ensemble-applications.html"><a href="ensemble-applications.html#exploration"><i class="fa fa-check"></i><b>14.3</b> Exploration</a></li>
<li class="chapter" data-level="14.4" data-path="ensemble-applications.html"><a href="ensemble-applications.html#boosting-applications"><i class="fa fa-check"></i><b>14.4</b> Boosting Applications</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="ensemble-applications.html"><a href="ensemble-applications.html#regression-1"><i class="fa fa-check"></i><b>14.4.1</b> Regression</a></li>
<li class="chapter" data-level="14.4.2" data-path="ensemble-applications.html"><a href="ensemble-applications.html#random-search-with-parallel-processing"><i class="fa fa-check"></i><b>14.4.2</b> Random search with parallel processing</a></li>
<li class="chapter" data-level="14.4.3" data-path="ensemble-applications.html"><a href="ensemble-applications.html#boosting-vs.-others"><i class="fa fa-check"></i><b>14.4.3</b> Boosting vs. Others</a></li>
<li class="chapter" data-level="14.4.4" data-path="ensemble-applications.html"><a href="ensemble-applications.html#classification-1"><i class="fa fa-check"></i><b>14.4.4</b> Classification</a></li>
<li class="chapter" data-level="14.4.5" data-path="ensemble-applications.html"><a href="ensemble-applications.html#adaboost.m1"><i class="fa fa-check"></i><b>14.4.5</b> AdaBoost.M1</a></li>
<li class="chapter" data-level="14.4.6" data-path="ensemble-applications.html"><a href="ensemble-applications.html#classification-with-xgboost"><i class="fa fa-check"></i><b>14.4.6</b> Classification with XGBoost</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>V SVM &amp; Neural Networks</b></span></li>
<li class="chapter" data-level="15" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>15</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="15.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#optimal-separating-classifier"><i class="fa fa-check"></i><b>15.1</b> Optimal Separating Classifier</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#the-margin"><i class="fa fa-check"></i><b>15.1.1</b> The Margin</a></li>
<li class="chapter" data-level="15.1.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#the-non-separable-case"><i class="fa fa-check"></i><b>15.1.2</b> The Non-Separable Case</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#nonlinear-boundary-with-kernels"><i class="fa fa-check"></i><b>15.2</b> Nonlinear Boundary with Kernels</a></li>
<li class="chapter" data-level="15.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#application-with-svm"><i class="fa fa-check"></i><b>15.3</b> Application with SVM</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html"><i class="fa fa-check"></i><b>16</b> Artificial Neural Networks</a>
<ul>
<li class="chapter" data-level="16.1" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#neural-network---the-idea"><i class="fa fa-check"></i><b>16.1</b> Neural Network - the idea</a></li>
<li class="chapter" data-level="16.2" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#backpropagation"><i class="fa fa-check"></i><b>16.2</b> Backpropagation</a></li>
<li class="chapter" data-level="16.3" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#neural-network---more-inputs"><i class="fa fa-check"></i><b>16.3</b> Neural Network - More inputs</a></li>
<li class="chapter" data-level="16.4" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#deep-learning"><i class="fa fa-check"></i><b>16.4</b> Deep Learning</a></li>
</ul></li>
<li class="part"><span><b>VI Penalized Regressions</b></span></li>
<li class="chapter" data-level="" data-path="parametric-models-in-prediction.html"><a href="parametric-models-in-prediction.html"><i class="fa fa-check"></i>Parametric models in prediction</a></li>
<li class="chapter" data-level="17" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>17</b> Ridge</a></li>
<li class="chapter" data-level="18" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>18</b> Lasso</a></li>
<li class="chapter" data-level="19" data-path="adaptive-lasso.html"><a href="adaptive-lasso.html"><i class="fa fa-check"></i><b>19</b> Adaptive Lasso</a></li>
<li class="chapter" data-level="20" data-path="sparsity.html"><a href="sparsity.html"><i class="fa fa-check"></i><b>20</b> Sparsity</a></li>
<li class="part"><span><b>VII Time Series</b></span></li>
<li class="chapter" data-level="" data-path="forecasting.html"><a href="forecasting.html"><i class="fa fa-check"></i>Forecasting</a></li>
<li class="chapter" data-level="21" data-path="arima-models.html"><a href="arima-models.html"><i class="fa fa-check"></i><b>21</b> ARIMA models</a>
<ul>
<li class="chapter" data-level="21.1" data-path="arima-models.html"><a href="arima-models.html#hyndman-khandakar-algorithm"><i class="fa fa-check"></i><b>21.1</b> Hyndman-Khandakar algorithm</a></li>
<li class="chapter" data-level="21.2" data-path="arima-models.html"><a href="arima-models.html#ts-plots"><i class="fa fa-check"></i><b>21.2</b> TS Plots</a></li>
<li class="chapter" data-level="21.3" data-path="arima-models.html"><a href="arima-models.html#box-cox-transformation"><i class="fa fa-check"></i><b>21.3</b> Box-Cox transformation</a></li>
<li class="chapter" data-level="21.4" data-path="arima-models.html"><a href="arima-models.html#stationarity"><i class="fa fa-check"></i><b>21.4</b> Stationarity</a></li>
<li class="chapter" data-level="21.5" data-path="arima-models.html"><a href="arima-models.html#modeling-arima"><i class="fa fa-check"></i><b>21.5</b> Modeling ARIMA</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="grid-search-for-arima.html"><a href="grid-search-for-arima.html"><i class="fa fa-check"></i><b>22</b> Grid search for ARIMA</a></li>
<li class="chapter" data-level="23" data-path="time-series-embedding.html"><a href="time-series-embedding.html"><i class="fa fa-check"></i><b>23</b> Time Series Embedding</a>
<ul>
<li class="chapter" data-level="23.1" data-path="time-series-embedding.html"><a href="time-series-embedding.html#var-for-recursive-forecasting"><i class="fa fa-check"></i><b>23.1</b> VAR for Recursive Forecasting</a></li>
<li class="chapter" data-level="23.2" data-path="time-series-embedding.html"><a href="time-series-embedding.html#embedding-for-direct-forecast"><i class="fa fa-check"></i><b>23.2</b> Embedding for Direct Forecast</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="random-forest-1.html"><a href="random-forest-1.html"><i class="fa fa-check"></i><b>24</b> Random Forest</a>
<ul>
<li class="chapter" data-level="24.1" data-path="random-forest-1.html"><a href="random-forest-1.html#univariate"><i class="fa fa-check"></i><b>24.1</b> Univariate</a></li>
<li class="chapter" data-level="24.2" data-path="random-forest-1.html"><a href="random-forest-1.html#multivariate"><i class="fa fa-check"></i><b>24.2</b> Multivariate</a></li>
<li class="chapter" data-level="24.3" data-path="random-forest-1.html"><a href="random-forest-1.html#rolling-and-expanding-windows"><i class="fa fa-check"></i><b>24.3</b> Rolling and expanding windows</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html"><i class="fa fa-check"></i><b>25</b> Recurrent Neural Networks</a>
<ul>
<li class="chapter" data-level="25.1" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#keras"><i class="fa fa-check"></i><b>25.1</b> Keras</a></li>
<li class="chapter" data-level="25.2" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#input-tensors"><i class="fa fa-check"></i><b>25.2</b> Input Tensors</a></li>
<li class="chapter" data-level="25.3" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#plain-rnn"><i class="fa fa-check"></i><b>25.3</b> Plain RNN</a></li>
<li class="chapter" data-level="25.4" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#lstm"><i class="fa fa-check"></i><b>25.4</b> LSTM</a></li>
</ul></li>
<li class="part"><span><b>VIII Dimension Reduction Methods</b></span></li>
<li class="chapter" data-level="" data-path="matrix-decompositions.html"><a href="matrix-decompositions.html"><i class="fa fa-check"></i>Matrix Decompositions</a></li>
<li class="chapter" data-level="26" data-path="eigenvectors-and-eigenvalues.html"><a href="eigenvectors-and-eigenvalues.html"><i class="fa fa-check"></i><b>26</b> Eigenvectors and eigenvalues</a></li>
<li class="chapter" data-level="27" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html"><i class="fa fa-check"></i><b>27</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="28" data-path="rankr-approximations.html"><a href="rankr-approximations.html"><i class="fa fa-check"></i><b>28</b> Rank(r) Approximations</a></li>
<li class="chapter" data-level="29" data-path="moore-penrose-inverse.html"><a href="moore-penrose-inverse.html"><i class="fa fa-check"></i><b>29</b> Moore-Penrose inverse</a></li>
<li class="chapter" data-level="30" data-path="principle-component-analysis.html"><a href="principle-component-analysis.html"><i class="fa fa-check"></i><b>30</b> Principle Component Analysis</a></li>
<li class="chapter" data-level="31" data-path="factor-analysis.html"><a href="factor-analysis.html"><i class="fa fa-check"></i><b>31</b> Factor Analysis</a></li>
<li class="part"><span><b>IX Network Analysis</b></span></li>
<li class="chapter" data-level="" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html"><i class="fa fa-check"></i>Graphical Network Analysis</a></li>
<li class="chapter" data-level="32" data-path="fundementals.html"><a href="fundementals.html"><i class="fa fa-check"></i><b>32</b> Fundementals</a>
<ul>
<li class="chapter" data-level="32.1" data-path="fundementals.html"><a href="fundementals.html#covariance"><i class="fa fa-check"></i><b>32.1</b> Covariance</a></li>
<li class="chapter" data-level="32.2" data-path="fundementals.html"><a href="fundementals.html#correlation"><i class="fa fa-check"></i><b>32.2</b> Correlation</a></li>
<li class="chapter" data-level="32.3" data-path="fundementals.html"><a href="fundementals.html#precision-matrix"><i class="fa fa-check"></i><b>32.3</b> Precision Matrix</a></li>
<li class="chapter" data-level="32.4" data-path="fundementals.html"><a href="fundementals.html#semi-partial-correlation"><i class="fa fa-check"></i><b>32.4</b> Semi-partial Correlation</a></li>
</ul></li>
<li class="chapter" data-level="33" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html"><i class="fa fa-check"></i><b>33</b> Regularized Covariance Matrix</a>
<ul>
<li class="chapter" data-level="33.1" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html#multivariate-gaussian-distribution"><i class="fa fa-check"></i><b>33.1</b> Multivariate Gaussian Distribution</a></li>
<li class="chapter" data-level="33.2" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html#high-dimensional-data"><i class="fa fa-check"></i><b>33.2</b> High-dimensional data</a></li>
<li class="chapter" data-level="33.3" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html#ridge-ell_2-and-glasso-ell_1"><i class="fa fa-check"></i><b>33.3</b> Ridge (<span class="math inline">\(\ell_{2}\)</span>) and glasso (<span class="math inline">\(\ell_{1}\)</span>)</a></li>
</ul></li>
<li class="part"><span><b>X Labs</b></span></li>
<li class="chapter" data-level="34" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html"><i class="fa fa-check"></i><b>34</b> R Lab 1 - Basics I</a>
<ul>
<li class="chapter" data-level="34.1" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#r-rstudio-and-r-packages"><i class="fa fa-check"></i><b>34.1</b> R, RStudio, and R Packages</a></li>
<li class="chapter" data-level="34.2" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#rstudio"><i class="fa fa-check"></i><b>34.2</b> RStudio</a></li>
<li class="chapter" data-level="34.3" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#working-directory"><i class="fa fa-check"></i><b>34.3</b> Working directory</a></li>
<li class="chapter" data-level="34.4" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#data-types-and-stuctures"><i class="fa fa-check"></i><b>34.4</b> Data Types and Stuctures</a></li>
<li class="chapter" data-level="34.5" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#vectors"><i class="fa fa-check"></i><b>34.5</b> Vectors</a></li>
<li class="chapter" data-level="34.6" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#subsetting-vectors"><i class="fa fa-check"></i><b>34.6</b> Subsetting Vectors</a></li>
<li class="chapter" data-level="34.7" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#vectorization-or-vector-operations"><i class="fa fa-check"></i><b>34.7</b> Vectorization or vector operations</a></li>
<li class="chapter" data-level="34.8" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#matrices"><i class="fa fa-check"></i><b>34.8</b> Matrices</a></li>
<li class="chapter" data-level="34.9" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#matrix-operations"><i class="fa fa-check"></i><b>34.9</b> Matrix Operations</a></li>
<li class="chapter" data-level="34.10" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#subsetting-matrix"><i class="fa fa-check"></i><b>34.10</b> Subsetting Matrix</a></li>
<li class="chapter" data-level="34.11" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#r-style-guide"><i class="fa fa-check"></i><b>34.11</b> R-Style Guide</a></li>
</ul></li>
<li class="chapter" data-level="35" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html"><i class="fa fa-check"></i><b>35</b> R Lab 2 - Basics II</a>
<ul>
<li class="chapter" data-level="35.1" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#data-frames-and-lists"><i class="fa fa-check"></i><b>35.1</b> Data frames and lists</a>
<ul>
<li class="chapter" data-level="35.1.1" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#lists"><i class="fa fa-check"></i><b>35.1.1</b> Lists</a></li>
<li class="chapter" data-level="35.1.2" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#data-frames"><i class="fa fa-check"></i><b>35.1.2</b> Data Frames</a></li>
<li class="chapter" data-level="35.1.3" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#reading-importing-and-writting-exporting-data-files"><i class="fa fa-check"></i><b>35.1.3</b> Reading (importing) and writting (exporting) data files</a></li>
<li class="chapter" data-level="35.1.4" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#subsetting-data-frames"><i class="fa fa-check"></i><b>35.1.4</b> Subsetting Data Frames</a></li>
<li class="chapter" data-level="35.1.5" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#plotting-from-data-frame"><i class="fa fa-check"></i><b>35.1.5</b> Plotting from data frame</a></li>
<li class="chapter" data-level="35.1.6" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#some-useful-functions"><i class="fa fa-check"></i><b>35.1.6</b> Some useful functions</a></li>
<li class="chapter" data-level="35.1.7" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#categorical-variables-in-data-frames"><i class="fa fa-check"></i><b>35.1.7</b> Categorical Variables in Data Frames</a></li>
</ul></li>
<li class="chapter" data-level="35.2" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#programming-basics"><i class="fa fa-check"></i><b>35.2</b> Programming Basics</a>
<ul>
<li class="chapter" data-level="35.2.1" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#ifelse"><i class="fa fa-check"></i><b>35.2.1</b> if/Else</a></li>
<li class="chapter" data-level="35.2.2" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#loops"><i class="fa fa-check"></i><b>35.2.2</b> Loops</a></li>
<li class="chapter" data-level="35.2.3" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#the-apply-family"><i class="fa fa-check"></i><b>35.2.3</b> The <code>apply()</code> family</a></li>
<li class="chapter" data-level="35.2.4" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#functions"><i class="fa fa-check"></i><b>35.2.4</b> Functions</a></li>
<li class="chapter" data-level="35.2.5" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#dplyr"><i class="fa fa-check"></i><b>35.2.5</b> <code>dplyr()</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="36" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html"><i class="fa fa-check"></i><b>36</b> R Lab 3 - Preparing the data</a>
<ul>
<li class="chapter" data-level="36.1" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#preparing-the-data-for-a-regression-analysis-with-lm"><i class="fa fa-check"></i><b>36.1</b> Preparing the data for a regression analysis with <code>lm()</code></a>
<ul>
<li class="chapter" data-level="36.1.1" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#factor-variables"><i class="fa fa-check"></i><b>36.1.1</b> Factor variables</a></li>
<li class="chapter" data-level="36.1.2" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#dummy-coding"><i class="fa fa-check"></i><b>36.1.2</b> Dummy Coding</a></li>
<li class="chapter" data-level="36.1.3" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#column-variable-names"><i class="fa fa-check"></i><b>36.1.3</b> Column (Variable) names</a></li>
<li class="chapter" data-level="36.1.4" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#data-subsetting-and-missing-values"><i class="fa fa-check"></i><b>36.1.4</b> Data subsetting and missing values</a></li>
</ul></li>
<li class="chapter" data-level="36.2" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#dummy-variable-models"><i class="fa fa-check"></i><b>36.2</b> “DUMMY” variable models</a>
<ul>
<li class="chapter" data-level="36.2.1" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#mtcars-example"><i class="fa fa-check"></i><b>36.2.1</b> <code>mtcars</code> example</a></li>
<li class="chapter" data-level="36.2.2" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#model.matrix"><i class="fa fa-check"></i><b>36.2.2</b> <code>model.matrix()</code></a></li>
<li class="chapter" data-level="36.2.3" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#example-with-a-bigger-data-set-autompg"><i class="fa fa-check"></i><b>36.2.3</b> Example with a bigger data set: <code>Autompg</code></a></li>
<li class="chapter" data-level="36.2.4" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#some-more-data-management-tools-for-subsetting-complete.cases-is.na-and-within"><i class="fa fa-check"></i><b>36.2.4</b> Some more data management tools for subsetting: <code>complete.cases()</code>, <code>is.na()</code>, and <code>within()</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="37" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html"><i class="fa fa-check"></i><b>37</b> R Lab 4 - Simulation in R</a>
<ul>
<li class="chapter" data-level="37.1" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#sampling-in-r-sample"><i class="fa fa-check"></i><b>37.1</b> Sampling in R: <code>sample()</code></a></li>
<li class="chapter" data-level="37.2" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#random-number-generating-with-probablity-distributions"><i class="fa fa-check"></i><b>37.2</b> Random number generating with probablity distributions</a></li>
<li class="chapter" data-level="37.3" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#simulation-for-statistical-inference"><i class="fa fa-check"></i><b>37.3</b> Simulation for statistical inference</a></li>
<li class="chapter" data-level="37.4" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#creataing-data-with-a-data-generating-model-dgm"><i class="fa fa-check"></i><b>37.4</b> Creataing data with a Data Generating Model (DGM)</a></li>
<li class="chapter" data-level="37.5" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#bootstrapping"><i class="fa fa-check"></i><b>37.5</b> Bootstrapping</a></li>
<li class="chapter" data-level="37.6" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#monty-hall---fun-example"><i class="fa fa-check"></i><b>37.6</b> Monty Hall - Fun example</a></li>
</ul></li>
<li class="part"><span><b>XI Appendix</b></span></li>
<li class="chapter" data-level="38" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html"><i class="fa fa-check"></i><b>38</b> Algorithmic Optimization</a>
<ul>
<li class="chapter" data-level="38.1" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#brute-force-optimization"><i class="fa fa-check"></i><b>38.1</b> Brute-force optimization</a></li>
<li class="chapter" data-level="38.2" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#derivative-based-methods"><i class="fa fa-check"></i><b>38.2</b> Derivative-based methods</a></li>
<li class="chapter" data-level="38.3" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#ml-estimation-with-logistic-regression"><i class="fa fa-check"></i><b>38.3</b> ML Estimation with logistic regression</a></li>
<li class="chapter" data-level="38.4" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#gradient-descent-algorithm"><i class="fa fa-check"></i><b>38.4</b> Gradient Descent Algorithm</a>
<ul>
<li class="chapter" data-level="38.4.1" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#one-variable"><i class="fa fa-check"></i><b>38.4.1</b> One-variable</a></li>
<li class="chapter" data-level="38.4.2" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#multivariable"><i class="fa fa-check"></i><b>38.4.2</b> Multivariable</a></li>
</ul></li>
<li class="chapter" data-level="38.5" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#optimization-with-r"><i class="fa fa-check"></i><b>38.5</b> Optimization with R</a></li>
</ul></li>
<li class="chapter" data-level="39" data-path="imbalanced-data.html"><a href="imbalanced-data.html"><i class="fa fa-check"></i><b>39</b> Imbalanced Data</a>
<ul>
<li class="chapter" data-level="39.1" data-path="imbalanced-data.html"><a href="imbalanced-data.html#smote"><i class="fa fa-check"></i><b>39.1</b> <code>SMOTE</code></a></li>
<li class="chapter" data-level="39.2" data-path="imbalanced-data.html"><a href="imbalanced-data.html#fraud-detection"><i class="fa fa-check"></i><b>39.2</b> Fraud detection</a></li>
</ul></li>
<li class="chapter" data-level="40" data-path="footnotes-and-citations.html"><a href="footnotes-and-citations.html"><i class="fa fa-check"></i><b>40</b> Footnotes and citations</a>
<ul>
<li class="chapter" data-level="40.1" data-path="footnotes-and-citations.html"><a href="footnotes-and-citations.html#footnotes"><i class="fa fa-check"></i><b>40.1</b> Footnotes</a></li>
<li class="chapter" data-level="40.2" data-path="footnotes-and-citations.html"><a href="footnotes-and-citations.html#citations"><i class="fa fa-check"></i><b>40.2</b> Citations</a></li>
</ul></li>
<li class="chapter" data-level="41" data-path="blocks.html"><a href="blocks.html"><i class="fa fa-check"></i><b>41</b> Blocks</a>
<ul>
<li class="chapter" data-level="41.1" data-path="blocks.html"><a href="blocks.html#equations"><i class="fa fa-check"></i><b>41.1</b> Equations</a></li>
<li class="chapter" data-level="41.2" data-path="blocks.html"><a href="blocks.html#theorems-and-proofs"><i class="fa fa-check"></i><b>41.2</b> Theorems and proofs</a></li>
<li class="chapter" data-level="41.3" data-path="blocks.html"><a href="blocks.html#callout-blocks"><i class="fa fa-check"></i><b>41.3</b> Callout blocks</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/yaydede/toolbox" target="blank"> 2023 Yigit Aydede - Bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Toolbox for Social Scientists and Policy Analysts</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sparsity" class="section level1 hasAnchor" number="20">
<h1><span class="header-section-number">Chapter 20</span> Sparsity<a href="sparsity.html#sparsity" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This is a simulation to illustrate some of the properties of Lasso-type estimations. There are two objectives in using these penalized regressions: model selection (identifying “correct” sparsity) and prediction accuracy. These two objectives require different optimization approaches and usually are not compatible. In model selection, the objective is to shrink the dimension of the model to the “true” sparsity. This is usually evaluated by checking whether the Oracle properties are satisfied. These asymptotic properties look at (1) if the model identified by the penalized regression converges to the “true” sparsity, (2) if the coefficients are consistent.</p>
<p>The literature suggests that Lasso is not an oracle estimator. Adaptive Lasso was developed (Zou 2006) to fill this gap.</p>
<p>Let’s specify a data generating process with a linear regression model:</p>
<p><span class="math display">\[
y_i=x_i^{\prime} \beta+u_i, ~~~~~i=1, \ldots, n
\]</span></p>
<p>where <span class="math inline">\(\beta\)</span> is <span class="math inline">\(p \times 1\)</span>. First, we consider the case where <span class="math inline">\(p&lt;n\)</span> then move to the case where <span class="math inline">\(p \geq n\)</span>. We define <span class="math inline">\(\beta=(1,1,0,0)^{\prime}\)</span> and <span class="math inline">\(n=100\)</span>.</p>
<div class="sourceCode" id="cb833"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb833-1"><a href="sparsity.html#cb833-1" aria-hidden="true" tabindex="-1"></a><span class="co">#This function generates the data</span></span>
<span id="cb833-2"><a href="sparsity.html#cb833-2" aria-hidden="true" tabindex="-1"></a>dgp <span class="ot">&lt;-</span> <span class="cf">function</span>(N, Beta) {</span>
<span id="cb833-3"><a href="sparsity.html#cb833-3" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">=</span> <span class="fu">length</span>(Beta)</span>
<span id="cb833-4"><a href="sparsity.html#cb833-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb833-5"><a href="sparsity.html#cb833-5" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(N <span class="sc">*</span> p), <span class="at">ncol =</span> p)</span>
<span id="cb833-6"><a href="sparsity.html#cb833-6" aria-hidden="true" tabindex="-1"></a>  u <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(N), <span class="at">ncol =</span> <span class="dv">1</span>)</span>
<span id="cb833-7"><a href="sparsity.html#cb833-7" aria-hidden="true" tabindex="-1"></a>  dgm <span class="ot">&lt;-</span> X <span class="sc">%*%</span> Beta</span>
<span id="cb833-8"><a href="sparsity.html#cb833-8" aria-hidden="true" tabindex="-1"></a>  y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> Beta <span class="sc">+</span> u</span>
<span id="cb833-9"><a href="sparsity.html#cb833-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb833-10"><a href="sparsity.html#cb833-10" aria-hidden="true" tabindex="-1"></a>  return <span class="ot">&lt;-</span> <span class="fu">list</span>(y, X)</span>
<span id="cb833-11"><a href="sparsity.html#cb833-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb833-12"><a href="sparsity.html#cb833-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb833-13"><a href="sparsity.html#cb833-13" aria-hidden="true" tabindex="-1"></a>N <span class="ot">=</span> <span class="dv">100</span></span>
<span id="cb833-14"><a href="sparsity.html#cb833-14" aria-hidden="true" tabindex="-1"></a>Beta <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb833-15"><a href="sparsity.html#cb833-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb833-16"><a href="sparsity.html#cb833-16" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">148</span>)</span>
<span id="cb833-17"><a href="sparsity.html#cb833-17" aria-hidden="true" tabindex="-1"></a>Output <span class="ot">&lt;-</span> <span class="fu">dgp</span>(N, Beta)</span>
<span id="cb833-18"><a href="sparsity.html#cb833-18" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> Output[[<span class="dv">1</span>]]</span>
<span id="cb833-19"><a href="sparsity.html#cb833-19" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> Output[[<span class="dv">2</span>]]</span></code></pre></div>
<p>First, we apply lasso</p>
<div class="sourceCode" id="cb834"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb834-1"><a href="sparsity.html#cb834-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb834-2"><a href="sparsity.html#cb834-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb834-3"><a href="sparsity.html#cb834-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">432</span>)</span>
<span id="cb834-4"><a href="sparsity.html#cb834-4" aria-hidden="true" tabindex="-1"></a>lasso <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(<span class="at">x =</span> X, <span class="at">y =</span> y, <span class="at">family =</span> <span class="st">&quot;gaussian&quot;</span>)</span>
<span id="cb834-5"><a href="sparsity.html#cb834-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb834-6"><a href="sparsity.html#cb834-6" aria-hidden="true" tabindex="-1"></a>beta_hat <span class="ot">&lt;-</span> lasso<span class="sc">$</span>beta</span>
<span id="cb834-7"><a href="sparsity.html#cb834-7" aria-hidden="true" tabindex="-1"></a>S_matrix <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">t</span>(beta_hat), <span class="st">&quot;lambda&quot;</span> <span class="ot">=</span> lasso<span class="sc">$</span>lambda)</span>
<span id="cb834-8"><a href="sparsity.html#cb834-8" aria-hidden="true" tabindex="-1"></a>S_matrix[<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">8</span>, <span class="dv">25</span><span class="sc">:</span><span class="dv">30</span>, <span class="dv">55</span><span class="sc">:</span><span class="dv">60</span>), ] <span class="co"># selected rows</span></span></code></pre></div>
<pre><code>## 20 x 5 sparse Matrix of class &quot;dgCMatrix&quot;
##             V1        V2          V3         V4      lambda
## s0  .          .          .          .          1.083220708
## s1  0.09439841 0.0283513  .          .          0.986990366
## s2  0.17344129 0.1097255  .          .          0.899308862
## s3  0.24546220 0.1838706  .          .          0.819416741
## s4  0.31108496 0.2514289  .          .          0.746622016
## s5  0.37087798 0.3129855  .          .          0.680294174
## s6  0.42535915 0.3690736  .          .          0.619858715
## s7  0.47500037 0.4201789  .          .          0.564792175
## s24 0.87944075 0.8365481  .          .          0.116150206
## s25 0.88874261 0.8461243  .          .          0.105831742
## s26 0.89685610 0.8542117 -0.00686322 .          0.096429941
## s27 0.90418482 0.8614679 -0.01432988 .          0.087863371
## s28 0.91086250 0.8680794 -0.02113323 .          0.080057832
## s29 0.91694695 0.8741036 -0.02733218 .          0.072945714
## s54 0.98352129 0.9289175 -0.09282009 0.05192379 0.007126869
## s55 0.98423271 0.9294382 -0.09350608 0.05278151 0.006493738
## s56 0.98488092 0.9299126 -0.09413113 0.05356303 0.005916852
## s57 0.98547155 0.9303449 -0.09470066 0.05427512 0.005391215
## s58 0.98600972 0.9307388 -0.09521958 0.05492395 0.004912274
## s59 0.98650007 0.9310977 -0.09569241 0.05551515 0.004475881</code></pre>
<p>Which set of beta_hat should we select? To answer this question we need to find the lambda. We need <span class="math inline">\(\lambda_n \rightarrow \infty\)</span> in order to shrink the truly zero coefficients to zero. This requires <span class="math inline">\(\lambda_n\)</span> to be sufficiently large. This would introduce asymptotic bias to the non-zero coefficients.</p>
<p>In practice, choosing <span class="math inline">\(\lambda_n\)</span> by <span class="math inline">\(\mathrm{BIC}\)</span> (Bayesian Information Criterion) results in a consistent model selection in the fixed <span class="math inline">\(p\)</span> setting. That is, let <span class="math inline">\(\mathcal{A}=\left\{j: \beta_{0, j} \neq 0\right\}\)</span>, active set or relevant variables,</p>
<p><span class="math display">\[
P\left(\hat{\mathcal{A}}_{\lambda_{BIC}}=\mathcal{A}\right) \rightarrow 1
\]</span></p>
<p>Thus, let <span class="math inline">\(S S E_\lambda\)</span> be the sum of squared error terms for a given value of <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(n z_\lambda\)</span> be the number of non-zero coefficients. Then, it can be shown that</p>
<p><span class="math display">\[
B I C_\lambda=\log \left(S S E_\lambda\right)+\frac{\log (n)}{n} n z_\lambda
\]</span></p>
<div class="sourceCode" id="cb836"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb836-1"><a href="sparsity.html#cb836-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict yhat for each of 61 lambda (s)</span></span>
<span id="cb836-2"><a href="sparsity.html#cb836-2" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">=</span> <span class="fu">predict</span>(lasso, <span class="at">newx =</span> X)</span>
<span id="cb836-3"><a href="sparsity.html#cb836-3" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(y_hat)</span></code></pre></div>
<pre><code>## [1] 100  60</code></pre>
<div class="sourceCode" id="cb838"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb838-1"><a href="sparsity.html#cb838-1" aria-hidden="true" tabindex="-1"></a><span class="co"># SSE for each lambda (s)</span></span>
<span id="cb838-2"><a href="sparsity.html#cb838-2" aria-hidden="true" tabindex="-1"></a>SSE <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb838-3"><a href="sparsity.html#cb838-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(y_hat)) {</span>
<span id="cb838-4"><a href="sparsity.html#cb838-4" aria-hidden="true" tabindex="-1"></a>  SSE_each <span class="ot">&lt;-</span> <span class="fu">sum</span>((y_hat[, i] <span class="sc">-</span> y[, <span class="dv">1</span>]) <span class="sc">^</span> (<span class="dv">2</span>))</span>
<span id="cb838-5"><a href="sparsity.html#cb838-5" aria-hidden="true" tabindex="-1"></a>  SSE <span class="ot">&lt;-</span> <span class="fu">c</span>(SSE, SSE_each)</span>
<span id="cb838-6"><a href="sparsity.html#cb838-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb838-7"><a href="sparsity.html#cb838-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb838-8"><a href="sparsity.html#cb838-8" aria-hidden="true" tabindex="-1"></a><span class="co"># BIC</span></span>
<span id="cb838-9"><a href="sparsity.html#cb838-9" aria-hidden="true" tabindex="-1"></a>nz <span class="ot">&lt;-</span> <span class="fu">colSums</span>(beta_hat <span class="sc">!=</span> <span class="dv">0</span>) <span class="co"># Number of non-zero coefficients for each lambda</span></span>
<span id="cb838-10"><a href="sparsity.html#cb838-10" aria-hidden="true" tabindex="-1"></a>BIC <span class="ot">&lt;-</span> <span class="fu">log</span>(SSE) <span class="sc">+</span> (<span class="fu">log</span>(N) <span class="sc">/</span> N) <span class="sc">*</span> nz <span class="co"># BIC</span></span>
<span id="cb838-11"><a href="sparsity.html#cb838-11" aria-hidden="true" tabindex="-1"></a>BIC</span></code></pre></div>
<pre><code>##       s0       s1       s2       s3       s4       s5       s6       s7 
## 5.598919 5.595359 5.468287 5.348947 5.237755 5.135013 5.040883 4.955387 
##       s8       s9      s10      s11      s12      s13      s14      s15 
## 4.878394 4.809638 4.748729 4.695181 4.648437 4.607898 4.572946 4.542971 
##      s16      s17      s18      s19      s20      s21      s22      s23 
## 4.517383 4.495631 4.477205 4.461646 4.448541 4.437530 4.428295 4.420563 
##      s24      s25      s26      s27      s28      s29      s30      s31 
## 4.414098 4.408698 4.448661 4.443309 4.438844 4.435121 4.432021 4.429439 
##      s32      s33      s34      s35      s36      s37      s38      s39 
## 4.427290 4.425503 4.424017 4.468004 4.466218 4.464732 4.463498 4.462471 
##      s40      s41      s42      s43      s44      s45      s46      s47 
## 4.461618 4.460910 4.460321 4.459832 4.459426 4.459088 4.458808 4.458575 
##      s48      s49      s50      s51      s52      s53      s54      s55 
## 4.458382 4.458222 4.458088 4.457978 4.457886 4.457810 4.457746 4.457694 
##      s56      s57      s58      s59 
## 4.457650 4.457614 4.457584 4.457559</code></pre>
<p>And, the selected model that has the minimum BIC</p>
<div class="sourceCode" id="cb840"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb840-1"><a href="sparsity.html#cb840-1" aria-hidden="true" tabindex="-1"></a>beta_lasso <span class="ot">&lt;-</span> beta_hat[, <span class="fu">which</span>(BIC <span class="sc">==</span> <span class="fu">min</span>(BIC))]</span>
<span id="cb840-2"><a href="sparsity.html#cb840-2" aria-hidden="true" tabindex="-1"></a>beta_lasso</span></code></pre></div>
<pre><code>##        V1        V2        V3        V4 
## 0.8887426 0.8461243 0.0000000 0.0000000</code></pre>
<p>This is the <code>beta_hat</code> that identifies the true sparsity. And, the second Oracle property, the <span class="math inline">\(\ell_2\)</span> error:</p>
<div class="sourceCode" id="cb842"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb842-1"><a href="sparsity.html#cb842-1" aria-hidden="true" tabindex="-1"></a>l_2 <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>((beta_lasso <span class="sc">-</span> Beta) <span class="sc">^</span> <span class="dv">2</span>))</span>
<span id="cb842-2"><a href="sparsity.html#cb842-2" aria-hidden="true" tabindex="-1"></a>l_2</span></code></pre></div>
<pre><code>## [1] 0.189884</code></pre>
<p>Here we will create a simulation that will report two Oracle Properties for Lasso and Adaptive Lasso:</p>
<ul>
<li>True sparsity,<br />
</li>
<li><span class="math inline">\(\ell_2\)</span> error.</li>
</ul>
<p><strong>Lasso</strong></p>
<p>We first have a function, <code>msc()</code>, that executes a simulation with all the steps shown before:</p>
<div class="sourceCode" id="cb844"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb844-1"><a href="sparsity.html#cb844-1" aria-hidden="true" tabindex="-1"></a>mcs <span class="ot">&lt;-</span> <span class="cf">function</span>(mc, N, Beta) {</span>
<span id="cb844-2"><a href="sparsity.html#cb844-2" aria-hidden="true" tabindex="-1"></a>  mcmat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> mc, <span class="at">ncol =</span> <span class="dv">3</span>)</span>
<span id="cb844-3"><a href="sparsity.html#cb844-3" aria-hidden="true" tabindex="-1"></a>  beta_lasso_mat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nr =</span> mc, <span class="at">nc =</span> <span class="fu">length</span>(Beta))</span>
<span id="cb844-4"><a href="sparsity.html#cb844-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb844-5"><a href="sparsity.html#cb844-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>mc) {</span>
<span id="cb844-6"><a href="sparsity.html#cb844-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set.seed</span>(i)</span>
<span id="cb844-7"><a href="sparsity.html#cb844-7" aria-hidden="true" tabindex="-1"></a>    data <span class="ot">&lt;-</span> <span class="fu">dgp</span>(N, Beta)</span>
<span id="cb844-8"><a href="sparsity.html#cb844-8" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">&lt;-</span> data[[<span class="dv">1</span>]]</span>
<span id="cb844-9"><a href="sparsity.html#cb844-9" aria-hidden="true" tabindex="-1"></a>    X <span class="ot">&lt;-</span> data[[<span class="dv">2</span>]]</span>
<span id="cb844-10"><a href="sparsity.html#cb844-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb844-11"><a href="sparsity.html#cb844-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">set.seed</span>(i)</span>
<span id="cb844-12"><a href="sparsity.html#cb844-12" aria-hidden="true" tabindex="-1"></a>    lasso <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(<span class="at">x =</span> X, <span class="at">y =</span> y, <span class="at">family =</span> <span class="st">&quot;gaussian&quot;</span>)</span>
<span id="cb844-13"><a href="sparsity.html#cb844-13" aria-hidden="true" tabindex="-1"></a>    beta_hat <span class="ot">&lt;-</span> lasso<span class="sc">$</span>beta    <span class="co"># beta_hat is a matrix</span></span>
<span id="cb844-14"><a href="sparsity.html#cb844-14" aria-hidden="true" tabindex="-1"></a>    y_hat <span class="ot">=</span> <span class="fu">predict</span>(lasso, <span class="at">newx =</span> X)</span>
<span id="cb844-15"><a href="sparsity.html#cb844-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb844-16"><a href="sparsity.html#cb844-16" aria-hidden="true" tabindex="-1"></a>    SSE <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb844-17"><a href="sparsity.html#cb844-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(y_hat)) {</span>
<span id="cb844-18"><a href="sparsity.html#cb844-18" aria-hidden="true" tabindex="-1"></a>      SSE_each <span class="ot">&lt;-</span> <span class="fu">sum</span>((y_hat[, j] <span class="sc">-</span> y[, <span class="dv">1</span>]) <span class="sc">^</span> (<span class="dv">2</span>))</span>
<span id="cb844-19"><a href="sparsity.html#cb844-19" aria-hidden="true" tabindex="-1"></a>      SSE <span class="ot">&lt;-</span> <span class="fu">c</span>(SSE, SSE_each)</span>
<span id="cb844-20"><a href="sparsity.html#cb844-20" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb844-21"><a href="sparsity.html#cb844-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb844-22"><a href="sparsity.html#cb844-22" aria-hidden="true" tabindex="-1"></a>    nz <span class="ot">&lt;-</span> <span class="fu">colSums</span>(beta_hat <span class="sc">!=</span> <span class="dv">0</span>)</span>
<span id="cb844-23"><a href="sparsity.html#cb844-23" aria-hidden="true" tabindex="-1"></a>    BIC <span class="ot">&lt;-</span> <span class="fu">log</span>(SSE) <span class="sc">+</span> (<span class="fu">log</span>(N) <span class="sc">/</span> N) <span class="sc">*</span> nz</span>
<span id="cb844-24"><a href="sparsity.html#cb844-24" aria-hidden="true" tabindex="-1"></a>    beta_lasso <span class="ot">&lt;-</span> beta_hat[, <span class="fu">which</span>(BIC <span class="sc">==</span> <span class="fu">min</span>(BIC))]</span>
<span id="cb844-25"><a href="sparsity.html#cb844-25" aria-hidden="true" tabindex="-1"></a>    nonz_beta <span class="ot">=</span> <span class="fu">length</span>(Beta[Beta <span class="sc">==</span> <span class="dv">0</span>])</span>
<span id="cb844-26"><a href="sparsity.html#cb844-26" aria-hidden="true" tabindex="-1"></a>    nonz_beta_hat <span class="ot">=</span> <span class="fu">length</span>(beta_lasso[beta_lasso <span class="sc">==</span> <span class="dv">0</span>])</span>
<span id="cb844-27"><a href="sparsity.html#cb844-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb844-28"><a href="sparsity.html#cb844-28" aria-hidden="true" tabindex="-1"></a>    mcmat[i, <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>((beta_lasso <span class="sc">-</span> Beta) <span class="sc">^</span> <span class="dv">2</span>))</span>
<span id="cb844-29"><a href="sparsity.html#cb844-29" aria-hidden="true" tabindex="-1"></a>    mcmat[i, <span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(nonz_beta <span class="sc">!=</span> nonz_beta_hat, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb844-30"><a href="sparsity.html#cb844-30" aria-hidden="true" tabindex="-1"></a>    mcmat[i, <span class="dv">3</span>] <span class="ot">&lt;-</span> <span class="fu">sum</span>(beta_lasso <span class="sc">!=</span> <span class="dv">0</span>)</span>
<span id="cb844-31"><a href="sparsity.html#cb844-31" aria-hidden="true" tabindex="-1"></a>    beta_lasso_mat[i, ] <span class="ot">&lt;-</span> beta_lasso</span>
<span id="cb844-32"><a href="sparsity.html#cb844-32" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb844-33"><a href="sparsity.html#cb844-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(mcmat, beta_lasso_mat))</span>
<span id="cb844-34"><a href="sparsity.html#cb844-34" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>We are ready for simulation:</p>
<div class="sourceCode" id="cb845"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb845-1"><a href="sparsity.html#cb845-1" aria-hidden="true" tabindex="-1"></a>mc <span class="ot">&lt;-</span> <span class="dv">500</span></span>
<span id="cb845-2"><a href="sparsity.html#cb845-2" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb845-3"><a href="sparsity.html#cb845-3" aria-hidden="true" tabindex="-1"></a>Beta <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>), <span class="at">nc =</span> <span class="dv">1</span>)</span>
<span id="cb845-4"><a href="sparsity.html#cb845-4" aria-hidden="true" tabindex="-1"></a>output <span class="ot">&lt;-</span> <span class="fu">mcs</span>(mc, N, Beta) <span class="co">#see the function</span></span>
<span id="cb845-5"><a href="sparsity.html#cb845-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb845-6"><a href="sparsity.html#cb845-6" aria-hidden="true" tabindex="-1"></a>MC_betas <span class="ot">=</span> output[[<span class="dv">2</span>]]</span>
<span id="cb845-7"><a href="sparsity.html#cb845-7" aria-hidden="true" tabindex="-1"></a>MC_performance <span class="ot">=</span> output[[<span class="dv">1</span>]]</span>
<span id="cb845-8"><a href="sparsity.html#cb845-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb845-9"><a href="sparsity.html#cb845-9" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(MC_performance[, <span class="dv">2</span>]) <span class="co">#how many times lasso finds true sparsity</span></span></code></pre></div>
<pre><code>## [1] 400</code></pre>
<p>This is the first property: lasso identifies the true sparsity <span class="math inline">\(400/500 = 80\%\)</span> of cases. And the second property, <span class="math inline">\(\ell_2\)</span> error, in the simulation is (in total):</p>
<div class="sourceCode" id="cb847"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb847-1"><a href="sparsity.html#cb847-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(MC_performance[, <span class="dv">1</span>])</span></code></pre></div>
<pre><code>## [1] 29.41841</code></pre>
<p><strong>Adaptive Lasso</strong></p>
<p>This time we let our adaptive lasso use lasso coefficients as penalty weights in <code>glmnet()</code>. Let’s have the same function with Adaptive Lasso for the simulation:</p>
<div class="sourceCode" id="cb849"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb849-1"><a href="sparsity.html#cb849-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Adaptive LASSO</span></span>
<span id="cb849-2"><a href="sparsity.html#cb849-2" aria-hidden="true" tabindex="-1"></a>mcsA <span class="ot">&lt;-</span> <span class="cf">function</span>(mc, N, Beta) {</span>
<span id="cb849-3"><a href="sparsity.html#cb849-3" aria-hidden="true" tabindex="-1"></a>  mcmat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nr =</span> mc, <span class="at">nc =</span> <span class="dv">3</span>)</span>
<span id="cb849-4"><a href="sparsity.html#cb849-4" aria-hidden="true" tabindex="-1"></a>  beta_lasso_mat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nr =</span> mc, <span class="at">nc =</span> <span class="fu">length</span>(Beta))</span>
<span id="cb849-5"><a href="sparsity.html#cb849-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb849-6"><a href="sparsity.html#cb849-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>mc) {</span>
<span id="cb849-7"><a href="sparsity.html#cb849-7" aria-hidden="true" tabindex="-1"></a>    data <span class="ot">&lt;-</span> <span class="fu">dgp</span>(N, Beta)</span>
<span id="cb849-8"><a href="sparsity.html#cb849-8" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">&lt;-</span> data[[<span class="dv">1</span>]]</span>
<span id="cb849-9"><a href="sparsity.html#cb849-9" aria-hidden="true" tabindex="-1"></a>    X <span class="ot">&lt;-</span> data[[<span class="dv">2</span>]]</span>
<span id="cb849-10"><a href="sparsity.html#cb849-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb849-11"><a href="sparsity.html#cb849-11" aria-hidden="true" tabindex="-1"></a>    lasso <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(<span class="at">x =</span> X, <span class="at">y =</span> y, <span class="at">family =</span> <span class="st">&quot;gaussian&quot;</span>)</span>
<span id="cb849-12"><a href="sparsity.html#cb849-12" aria-hidden="true" tabindex="-1"></a>    beta_hat <span class="ot">&lt;-</span> lasso<span class="sc">$</span>beta</span>
<span id="cb849-13"><a href="sparsity.html#cb849-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb849-14"><a href="sparsity.html#cb849-14" aria-hidden="true" tabindex="-1"></a>    y_hat <span class="ot">=</span> <span class="fu">predict</span>(lasso, <span class="at">newx =</span> X)</span>
<span id="cb849-15"><a href="sparsity.html#cb849-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb849-16"><a href="sparsity.html#cb849-16" aria-hidden="true" tabindex="-1"></a>    SSE <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb849-17"><a href="sparsity.html#cb849-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(y_hat)) {</span>
<span id="cb849-18"><a href="sparsity.html#cb849-18" aria-hidden="true" tabindex="-1"></a>      SSE_each <span class="ot">&lt;-</span> <span class="fu">sum</span>((y_hat[, j] <span class="sc">-</span> y[, <span class="dv">1</span>]) <span class="sc">^</span> (<span class="dv">2</span>))</span>
<span id="cb849-19"><a href="sparsity.html#cb849-19" aria-hidden="true" tabindex="-1"></a>      SSE <span class="ot">&lt;-</span> <span class="fu">c</span>(SSE, SSE_each)</span>
<span id="cb849-20"><a href="sparsity.html#cb849-20" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb849-21"><a href="sparsity.html#cb849-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb849-22"><a href="sparsity.html#cb849-22" aria-hidden="true" tabindex="-1"></a>    nz <span class="ot">&lt;-</span> <span class="fu">colSums</span>(beta_hat <span class="sc">!=</span> <span class="dv">0</span>)</span>
<span id="cb849-23"><a href="sparsity.html#cb849-23" aria-hidden="true" tabindex="-1"></a>    BIC <span class="ot">&lt;-</span> <span class="fu">log</span>(SSE) <span class="sc">+</span> (<span class="fu">log</span>(N) <span class="sc">/</span> N) <span class="sc">*</span> nz</span>
<span id="cb849-24"><a href="sparsity.html#cb849-24" aria-hidden="true" tabindex="-1"></a>    beta_lasso <span class="ot">&lt;-</span> beta_hat[, <span class="fu">which</span>(BIC <span class="sc">==</span> <span class="fu">min</span>(BIC))]</span>
<span id="cb849-25"><a href="sparsity.html#cb849-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb849-26"><a href="sparsity.html#cb849-26" aria-hidden="true" tabindex="-1"></a>    weights <span class="ot">=</span> <span class="fu">abs</span>(beta_lasso) <span class="sc">^</span> (<span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb849-27"><a href="sparsity.html#cb849-27" aria-hidden="true" tabindex="-1"></a>    weights[beta_lasso <span class="sc">==</span> <span class="dv">0</span>] <span class="ot">=</span> <span class="dv">10</span> <span class="sc">^</span> <span class="dv">10</span> <span class="co"># to handle inf&#39;s</span></span>
<span id="cb849-28"><a href="sparsity.html#cb849-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb849-29"><a href="sparsity.html#cb849-29" aria-hidden="true" tabindex="-1"></a>    <span class="co">#Now Adaptive Lasso</span></span>
<span id="cb849-30"><a href="sparsity.html#cb849-30" aria-hidden="true" tabindex="-1"></a>    lasso <span class="ot">&lt;-</span></span>
<span id="cb849-31"><a href="sparsity.html#cb849-31" aria-hidden="true" tabindex="-1"></a>      <span class="fu">glmnet</span>(</span>
<span id="cb849-32"><a href="sparsity.html#cb849-32" aria-hidden="true" tabindex="-1"></a>        <span class="at">x =</span> X,</span>
<span id="cb849-33"><a href="sparsity.html#cb849-33" aria-hidden="true" tabindex="-1"></a>        <span class="at">y =</span> y,</span>
<span id="cb849-34"><a href="sparsity.html#cb849-34" aria-hidden="true" tabindex="-1"></a>        <span class="at">family =</span> <span class="st">&quot;gaussian&quot;</span>,</span>
<span id="cb849-35"><a href="sparsity.html#cb849-35" aria-hidden="true" tabindex="-1"></a>        <span class="at">penalty.factor =</span> weights</span>
<span id="cb849-36"><a href="sparsity.html#cb849-36" aria-hidden="true" tabindex="-1"></a>      )</span>
<span id="cb849-37"><a href="sparsity.html#cb849-37" aria-hidden="true" tabindex="-1"></a>    beta_hat <span class="ot">&lt;-</span> lasso<span class="sc">$</span>beta</span>
<span id="cb849-38"><a href="sparsity.html#cb849-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb849-39"><a href="sparsity.html#cb849-39" aria-hidden="true" tabindex="-1"></a>    y_hat <span class="ot">=</span> <span class="fu">predict</span>(lasso, <span class="at">newx =</span> X)</span>
<span id="cb849-40"><a href="sparsity.html#cb849-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb849-41"><a href="sparsity.html#cb849-41" aria-hidden="true" tabindex="-1"></a>    SSE <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb849-42"><a href="sparsity.html#cb849-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(y_hat)) {</span>
<span id="cb849-43"><a href="sparsity.html#cb849-43" aria-hidden="true" tabindex="-1"></a>      SSE_each <span class="ot">&lt;-</span> <span class="fu">sum</span>((y_hat[, j] <span class="sc">-</span> y[, <span class="dv">1</span>]) <span class="sc">^</span> (<span class="dv">2</span>))</span>
<span id="cb849-44"><a href="sparsity.html#cb849-44" aria-hidden="true" tabindex="-1"></a>      SSE <span class="ot">&lt;-</span> <span class="fu">c</span>(SSE, SSE_each)</span>
<span id="cb849-45"><a href="sparsity.html#cb849-45" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb849-46"><a href="sparsity.html#cb849-46" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb849-47"><a href="sparsity.html#cb849-47" aria-hidden="true" tabindex="-1"></a>    nz <span class="ot">&lt;-</span> <span class="fu">colSums</span>(beta_hat <span class="sc">!=</span> <span class="dv">0</span>)</span>
<span id="cb849-48"><a href="sparsity.html#cb849-48" aria-hidden="true" tabindex="-1"></a>    BIC <span class="ot">&lt;-</span> <span class="fu">log</span>(SSE) <span class="sc">+</span> (<span class="fu">log</span>(N) <span class="sc">/</span> N) <span class="sc">*</span> nz</span>
<span id="cb849-49"><a href="sparsity.html#cb849-49" aria-hidden="true" tabindex="-1"></a>    beta_lasso <span class="ot">&lt;-</span> beta_hat[, <span class="fu">which</span>(BIC <span class="sc">==</span> <span class="fu">min</span>(BIC))]</span>
<span id="cb849-50"><a href="sparsity.html#cb849-50" aria-hidden="true" tabindex="-1"></a>    nonz_beta <span class="ot">=</span> <span class="fu">length</span>(Beta[Beta <span class="sc">==</span> <span class="dv">0</span>])</span>
<span id="cb849-51"><a href="sparsity.html#cb849-51" aria-hidden="true" tabindex="-1"></a>    nonz_beta_hat <span class="ot">=</span> <span class="fu">length</span>(beta_lasso[beta_lasso <span class="sc">==</span> <span class="dv">0</span>])</span>
<span id="cb849-52"><a href="sparsity.html#cb849-52" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb849-53"><a href="sparsity.html#cb849-53" aria-hidden="true" tabindex="-1"></a>    mcmat[i, <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>((beta_lasso <span class="sc">-</span> Beta) <span class="sc">^</span> <span class="dv">2</span>))</span>
<span id="cb849-54"><a href="sparsity.html#cb849-54" aria-hidden="true" tabindex="-1"></a>    mcmat[i, <span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(nonz_beta <span class="sc">!=</span> nonz_beta_hat, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb849-55"><a href="sparsity.html#cb849-55" aria-hidden="true" tabindex="-1"></a>    mcmat[i, <span class="dv">3</span>] <span class="ot">&lt;-</span> <span class="fu">sum</span>(beta_lasso <span class="sc">!=</span> <span class="dv">0</span>)</span>
<span id="cb849-56"><a href="sparsity.html#cb849-56" aria-hidden="true" tabindex="-1"></a>    beta_lasso_mat[i, ] <span class="ot">&lt;-</span> beta_lasso</span>
<span id="cb849-57"><a href="sparsity.html#cb849-57" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb849-58"><a href="sparsity.html#cb849-58" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(mcmat, beta_lasso_mat))</span>
<span id="cb849-59"><a href="sparsity.html#cb849-59" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Here are the results for adaptive lasso:</p>
<div class="sourceCode" id="cb850"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb850-1"><a href="sparsity.html#cb850-1" aria-hidden="true" tabindex="-1"></a>mc <span class="ot">&lt;-</span> <span class="dv">500</span></span>
<span id="cb850-2"><a href="sparsity.html#cb850-2" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb850-3"><a href="sparsity.html#cb850-3" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>), <span class="at">nc =</span> <span class="dv">1</span>)</span>
<span id="cb850-4"><a href="sparsity.html#cb850-4" aria-hidden="true" tabindex="-1"></a>output <span class="ot">&lt;-</span> <span class="fu">mcsA</span>(mc, N, beta) <span class="co">#see the function</span></span>
<span id="cb850-5"><a href="sparsity.html#cb850-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb850-6"><a href="sparsity.html#cb850-6" aria-hidden="true" tabindex="-1"></a>MC_betas <span class="ot">=</span> output[[<span class="dv">2</span>]]</span>
<span id="cb850-7"><a href="sparsity.html#cb850-7" aria-hidden="true" tabindex="-1"></a>MC_performance <span class="ot">=</span> output[[<span class="dv">1</span>]]</span>
<span id="cb850-8"><a href="sparsity.html#cb850-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb850-9"><a href="sparsity.html#cb850-9" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(MC_performance[, <span class="dv">2</span>])</span></code></pre></div>
<pre><code>## [1] 492</code></pre>
<p>And,</p>
<div class="sourceCode" id="cb852"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb852-1"><a href="sparsity.html#cb852-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(MC_performance[,<span class="dv">1</span>])</span></code></pre></div>
<pre><code>## [1] 20.21311</code></pre>
<p>The simulation results clearly show that Adaptive Lasso is an Oracle estimator and a better choice for sparsity applications.</p>
<p>We saw here a basic application of adaptive lasso, which has several different variations in practice, such as Thresholded Lasso and Rigorous Lasso. Model selections with lasso has been an active research area. One of the well-known applications is the double-selection lasso linear regression method that can be used for variable selections. Moreover, lasso type applications are also used in time-series forecasting and graphical network analysis for dimension reductions.</p>

</div>



            </section>

          </div>
        </div>
      </div>
<a href="adaptive-lasso.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="forecasting.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/yaydede/toolbox/edit/master/17-Lasso.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["YA_TextBook.pdf", "YA_TextBook.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
