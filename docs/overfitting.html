<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Overfitting | Toolbox for Social Scientists and Policy Analysts</title>
  <meta name="description" content="Chapter 4 Overfitting | Toolbox for Social Scientists and Policy Analysts" />
  <meta name="generator" content="bookdown 0.32.1 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Overfitting | Toolbox for Social Scientists and Policy Analysts" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://yaydede.github.io/toolbox//png/cover2.png" />
  
  <meta name="github-repo" content="yaydede/ToolShed_draft" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Overfitting | Toolbox for Social Scientists and Policy Analysts" />
  
  
  <meta name="twitter:image" content="https://yaydede.github.io/toolbox//png/cover2.png" />

<meta name="author" content="Yigit Aydede" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bias-variance-tradeoff.html"/>
<link rel="next" href="parametric-vs.-nonparametric-methods.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Toolbox</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who"><i class="fa fa-check"></i>Who</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> How we define Machine Learning</a></li>
<li class="chapter" data-level="2" data-path="preliminaries.html"><a href="preliminaries.html"><i class="fa fa-check"></i><b>2</b> Preliminaries</a>
<ul>
<li class="chapter" data-level="2.1" data-path="preliminaries.html"><a href="preliminaries.html#data-and-dataset-types"><i class="fa fa-check"></i><b>2.1</b> Data and dataset types</a></li>
<li class="chapter" data-level="2.2" data-path="preliminaries.html"><a href="preliminaries.html#plots"><i class="fa fa-check"></i><b>2.2</b> Plots</a></li>
<li class="chapter" data-level="2.3" data-path="preliminaries.html"><a href="preliminaries.html#probability-distributions-with-r"><i class="fa fa-check"></i><b>2.3</b> Probability Distributions with R</a></li>
<li class="chapter" data-level="2.4" data-path="preliminaries.html"><a href="preliminaries.html#regressions"><i class="fa fa-check"></i><b>2.4</b> Regressions</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="preliminaries.html"><a href="preliminaries.html#ordinary-least-squares-ols"><i class="fa fa-check"></i><b>2.4.1</b> Ordinary Least Squares (OLS)</a></li>
<li class="chapter" data-level="2.4.2" data-path="preliminaries.html"><a href="preliminaries.html#maximum-likelihood-estimators"><i class="fa fa-check"></i><b>2.4.2</b> Maximum Likelihood Estimators</a></li>
<li class="chapter" data-level="2.4.3" data-path="preliminaries.html"><a href="preliminaries.html#estimating-mle-with-r"><i class="fa fa-check"></i><b>2.4.3</b> Estimating MLE with R</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="preliminaries.html"><a href="preliminaries.html#blue"><i class="fa fa-check"></i><b>2.5</b> BLUE</a></li>
<li class="chapter" data-level="2.6" data-path="preliminaries.html"><a href="preliminaries.html#modeling-the-data"><i class="fa fa-check"></i><b>2.6</b> Modeling the data</a></li>
<li class="chapter" data-level="2.7" data-path="preliminaries.html"><a href="preliminaries.html#causal-vs.-predictive-models"><i class="fa fa-check"></i><b>2.7</b> Causal vs.Â Predictive Models</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="preliminaries.html"><a href="preliminaries.html#causal-models"><i class="fa fa-check"></i><b>2.7.1</b> Causal Models</a></li>
<li class="chapter" data-level="2.7.2" data-path="preliminaries.html"><a href="preliminaries.html#predictive-models"><i class="fa fa-check"></i><b>2.7.2</b> Predictive Models</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="preliminaries.html"><a href="preliminaries.html#simulation"><i class="fa fa-check"></i><b>2.8</b> Simulation</a></li>
</ul></li>
<li class="part"><span><b>I Formal Look at Prediction</b></span></li>
<li class="chapter" data-level="" data-path="learning-systems.html"><a href="learning-systems.html"><i class="fa fa-check"></i>Learning Systems</a></li>
<li class="chapter" data-level="3" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html"><i class="fa fa-check"></i><b>3</b> Bias-Variance Tradeoff</a>
<ul>
<li class="chapter" data-level="3.1" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#estimator-and-mse"><i class="fa fa-check"></i><b>3.1</b> Estimator and MSE</a></li>
<li class="chapter" data-level="3.2" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#prediction---mspe"><i class="fa fa-check"></i><b>3.2</b> Prediction - MSPE</a></li>
<li class="chapter" data-level="3.3" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#biased-estimator-as-a-predictor"><i class="fa fa-check"></i><b>3.3</b> Biased estimator as a predictor</a></li>
<li class="chapter" data-level="3.4" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#dropping-a-variable-in-a-regression"><i class="fa fa-check"></i><b>3.4</b> Dropping a variable in a regression</a></li>
<li class="chapter" data-level="3.5" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#uncertainty-in-estimations-and-predictions"><i class="fa fa-check"></i><b>3.5</b> Uncertainty in estimations and predictions</a></li>
<li class="chapter" data-level="3.6" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#prediction-interval-for-unbiased-ols-predictor"><i class="fa fa-check"></i><b>3.6</b> Prediction interval for unbiased OLS predictor</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="overfitting.html"><a href="overfitting.html"><i class="fa fa-check"></i><b>4</b> Overfitting</a></li>
<li class="part"><span><b>II Nonparametric Estimations</b></span></li>
<li class="chapter" data-level="" data-path="parametric-vs.-nonparametric-methods.html"><a href="parametric-vs.-nonparametric-methods.html"><i class="fa fa-check"></i>Parametric vs.Â Nonparametric methods</a></li>
<li class="chapter" data-level="5" data-path="parametric-estimations.html"><a href="parametric-estimations.html"><i class="fa fa-check"></i><b>5</b> Parametric Estimations</a>
<ul>
<li class="chapter" data-level="5.1" data-path="parametric-estimations.html"><a href="parametric-estimations.html#linear-probability-models-lpm"><i class="fa fa-check"></i><b>5.1</b> Linear Probability Models (LPM)</a></li>
<li class="chapter" data-level="5.2" data-path="parametric-estimations.html"><a href="parametric-estimations.html#logistic-regression"><i class="fa fa-check"></i><b>5.2</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="parametric-estimations.html"><a href="parametric-estimations.html#estimating-logistic-regression"><i class="fa fa-check"></i><b>5.2.1</b> Estimating Logistic Regression</a></li>
<li class="chapter" data-level="5.2.2" data-path="parametric-estimations.html"><a href="parametric-estimations.html#cost-functions"><i class="fa fa-check"></i><b>5.2.2</b> Cost functions</a></li>
<li class="chapter" data-level="5.2.3" data-path="parametric-estimations.html"><a href="parametric-estimations.html#deviance"><i class="fa fa-check"></i><b>5.2.3</b> Deviance</a></li>
<li class="chapter" data-level="5.2.4" data-path="parametric-estimations.html"><a href="parametric-estimations.html#predictive-accuracy"><i class="fa fa-check"></i><b>5.2.4</b> Predictive accuracy</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html"><i class="fa fa-check"></i><b>6</b> Nonparametric Estimations - Basics</a>
<ul>
<li class="chapter" data-level="6.1" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#density-estimations"><i class="fa fa-check"></i><b>6.1</b> Density Estimations</a></li>
<li class="chapter" data-level="6.2" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#kernel-regressions"><i class="fa fa-check"></i><b>6.2</b> Kernel regressions</a></li>
<li class="chapter" data-level="6.3" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#regression-splines"><i class="fa fa-check"></i><b>6.3</b> Regression Splines</a></li>
<li class="chapter" data-level="6.4" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#mars---multivariate-adaptive-regression-splines"><i class="fa fa-check"></i><b>6.4</b> MARS - Multivariate Adaptive Regression Splines</a></li>
<li class="chapter" data-level="6.5" data-path="nonparametric-estimations---basics.html"><a href="nonparametric-estimations---basics.html#gam---generalized-additive-model"><i class="fa fa-check"></i><b>6.5</b> GAM - Generalized Additive Model</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="smoothing.html"><a href="smoothing.html"><i class="fa fa-check"></i><b>7</b> Smoothing</a>
<ul>
<li class="chapter" data-level="7.1" data-path="smoothing.html"><a href="smoothing.html#using-bins"><i class="fa fa-check"></i><b>7.1</b> Using bins</a></li>
<li class="chapter" data-level="7.2" data-path="smoothing.html"><a href="smoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>7.2</b> Kernel smoothing</a></li>
<li class="chapter" data-level="7.3" data-path="smoothing.html"><a href="smoothing.html#locally-weighted-regression-loess"><i class="fa fa-check"></i><b>7.3</b> Locally weighted regression <code>loess()</code></a></li>
<li class="chapter" data-level="7.4" data-path="smoothing.html"><a href="smoothing.html#smooth-spline-regression"><i class="fa fa-check"></i><b>7.4</b> Smooth Spline Regression</a></li>
<li class="chapter" data-level="7.5" data-path="smoothing.html"><a href="smoothing.html#multivariate-loess"><i class="fa fa-check"></i><b>7.5</b> Multivariate Loess</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html"><i class="fa fa-check"></i><b>8</b> Nonparametric Classifier - kNN</a>
<ul>
<li class="chapter" data-level="8.1" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#mnist-dataset"><i class="fa fa-check"></i><b>8.1</b> <code>mnist</code> Dataset</a></li>
<li class="chapter" data-level="8.2" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#linear-classifiers-again"><i class="fa fa-check"></i><b>8.2</b> Linear classifiers (again)</a></li>
<li class="chapter" data-level="8.3" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>8.3</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="8.4" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#knn-with-caret"><i class="fa fa-check"></i><b>8.4</b> kNN with caret</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#mnist_27"><i class="fa fa-check"></i><b>8.4.1</b> <code>mnist_27</code></a></li>
<li class="chapter" data-level="8.4.2" data-path="nonparametric-classifier---knn.html"><a href="nonparametric-classifier---knn.html#adult-dataset"><i class="fa fa-check"></i><b>8.4.2</b> Adult dataset</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III Self-Learning</b></span></li>
<li class="chapter" data-level="9" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html"><i class="fa fa-check"></i><b>9</b> Hyperparameter Tuning</a>
<ul>
<li class="chapter" data-level="9.1" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#training-validation-and-test-datasets"><i class="fa fa-check"></i><b>9.1</b> Training, validation, and test datasets</a></li>
<li class="chapter" data-level="9.2" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#splitting-the-data-randomly"><i class="fa fa-check"></i><b>9.2</b> Splitting the data randomly</a></li>
<li class="chapter" data-level="9.3" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>9.3</b> k-fold cross validation</a></li>
<li class="chapter" data-level="9.4" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#cross-validated-grid-search"><i class="fa fa-check"></i><b>9.4</b> Cross-validated grid search</a></li>
<li class="chapter" data-level="9.5" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#bootstrapped-grid-search"><i class="fa fa-check"></i><b>9.5</b> Bootstrapped grid search</a></li>
<li class="chapter" data-level="9.6" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#when-the-data-is-time-series"><i class="fa fa-check"></i><b>9.6</b> When the data is time-series</a></li>
<li class="chapter" data-level="9.7" data-path="hyperparameter-tuning.html"><a href="hyperparameter-tuning.html#speed"><i class="fa fa-check"></i><b>9.7</b> Speed</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html"><i class="fa fa-check"></i><b>10</b> Tuning in Classification</a>
<ul>
<li class="chapter" data-level="10.1" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html#confusion-matrix"><i class="fa fa-check"></i><b>10.1</b> Confusion matrix</a></li>
<li class="chapter" data-level="10.2" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html#performance-measures"><i class="fa fa-check"></i><b>10.2</b> Performance measures</a></li>
<li class="chapter" data-level="10.3" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html#roc-curve"><i class="fa fa-check"></i><b>10.3</b> ROC Curve</a></li>
<li class="chapter" data-level="10.4" data-path="tuning-in-classification.html"><a href="tuning-in-classification.html#auc---area-under-the-curve"><i class="fa fa-check"></i><b>10.4</b> AUC - Area Under the Curve</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="classification-example.html"><a href="classification-example.html"><i class="fa fa-check"></i><b>11</b> Classification Example</a>
<ul>
<li class="chapter" data-level="11.1" data-path="classification-example.html"><a href="classification-example.html#lpm"><i class="fa fa-check"></i><b>11.1</b> LPM</a></li>
<li class="chapter" data-level="11.2" data-path="classification-example.html"><a href="classification-example.html#logistic-regression-1"><i class="fa fa-check"></i><b>11.2</b> Logistic Regression</a></li>
<li class="chapter" data-level="11.3" data-path="classification-example.html"><a href="classification-example.html#knn"><i class="fa fa-check"></i><b>11.3</b> kNN</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="classification-example.html"><a href="classification-example.html#knn-10-fold-cv"><i class="fa fa-check"></i><b>11.3.1</b> kNN 10-fold CV</a></li>
<li class="chapter" data-level="11.3.2" data-path="classification-example.html"><a href="classification-example.html#knn-with-caret-1"><i class="fa fa-check"></i><b>11.3.2</b> kNN with <code>caret</code></a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV Tree-based Models</b></span></li>
<li class="chapter" data-level="12" data-path="cart.html"><a href="cart.html"><i class="fa fa-check"></i><b>12</b> CART</a>
<ul>
<li class="chapter" data-level="12.1" data-path="cart.html"><a href="cart.html#cart---classification-tree"><i class="fa fa-check"></i><b>12.1</b> CART - Classification Tree</a></li>
<li class="chapter" data-level="12.2" data-path="cart.html"><a href="cart.html#rpart---recursive-partitioning"><i class="fa fa-check"></i><b>12.2</b> <code>rpart</code> - Recursive Partitioning</a></li>
<li class="chapter" data-level="12.3" data-path="cart.html"><a href="cart.html#pruning"><i class="fa fa-check"></i><b>12.3</b> Pruning</a></li>
<li class="chapter" data-level="12.4" data-path="cart.html"><a href="cart.html#classification-with-titanic"><i class="fa fa-check"></i><b>12.4</b> Classification with Titanic</a></li>
<li class="chapter" data-level="12.5" data-path="cart.html"><a href="cart.html#regression-tree"><i class="fa fa-check"></i><b>12.5</b> Regression Tree</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ensemble-learning.html"><a href="ensemble-learning.html"><i class="fa fa-check"></i><b>13</b> Ensemble Learning</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ensemble-learning.html"><a href="ensemble-learning.html#bagging"><i class="fa fa-check"></i><b>13.1</b> Bagging</a></li>
<li class="chapter" data-level="13.2" data-path="ensemble-learning.html"><a href="ensemble-learning.html#random-forest"><i class="fa fa-check"></i><b>13.2</b> Random Forest</a></li>
<li class="chapter" data-level="13.3" data-path="ensemble-learning.html"><a href="ensemble-learning.html#boosting"><i class="fa fa-check"></i><b>13.3</b> Boosting</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="ensemble-learning.html"><a href="ensemble-learning.html#sequential-ensemble-with-gbm"><i class="fa fa-check"></i><b>13.3.1</b> Sequential ensemble with <code>gbm</code></a></li>
<li class="chapter" data-level="13.3.2" data-path="ensemble-learning.html"><a href="ensemble-learning.html#adaboost"><i class="fa fa-check"></i><b>13.3.2</b> AdaBoost</a></li>
<li class="chapter" data-level="13.3.3" data-path="ensemble-learning.html"><a href="ensemble-learning.html#xgboost"><i class="fa fa-check"></i><b>13.3.3</b> XGBoost</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="ensemble-applications.html"><a href="ensemble-applications.html"><i class="fa fa-check"></i><b>14</b> Ensemble Applications</a>
<ul>
<li class="chapter" data-level="14.1" data-path="ensemble-applications.html"><a href="ensemble-applications.html#classification"><i class="fa fa-check"></i><b>14.1</b> Classification</a></li>
<li class="chapter" data-level="14.2" data-path="ensemble-applications.html"><a href="ensemble-applications.html#regression"><i class="fa fa-check"></i><b>14.2</b> Regression</a></li>
<li class="chapter" data-level="14.3" data-path="ensemble-applications.html"><a href="ensemble-applications.html#exploration"><i class="fa fa-check"></i><b>14.3</b> Exploration</a></li>
<li class="chapter" data-level="14.4" data-path="ensemble-applications.html"><a href="ensemble-applications.html#boosting-applications"><i class="fa fa-check"></i><b>14.4</b> Boosting Applications</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="ensemble-applications.html"><a href="ensemble-applications.html#regression-1"><i class="fa fa-check"></i><b>14.4.1</b> Regression</a></li>
<li class="chapter" data-level="14.4.2" data-path="ensemble-applications.html"><a href="ensemble-applications.html#random-search-with-parallel-processing"><i class="fa fa-check"></i><b>14.4.2</b> Random search with parallel processing</a></li>
<li class="chapter" data-level="14.4.3" data-path="ensemble-applications.html"><a href="ensemble-applications.html#boosting-vs.-others"><i class="fa fa-check"></i><b>14.4.3</b> Boosting vs.Â Others</a></li>
<li class="chapter" data-level="14.4.4" data-path="ensemble-applications.html"><a href="ensemble-applications.html#classification-1"><i class="fa fa-check"></i><b>14.4.4</b> Classification</a></li>
<li class="chapter" data-level="14.4.5" data-path="ensemble-applications.html"><a href="ensemble-applications.html#adaboost.m1"><i class="fa fa-check"></i><b>14.4.5</b> AdaBoost.M1</a></li>
<li class="chapter" data-level="14.4.6" data-path="ensemble-applications.html"><a href="ensemble-applications.html#classification-with-xgboost"><i class="fa fa-check"></i><b>14.4.6</b> Classification with XGBoost</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>V SVM &amp; Neural Networks</b></span></li>
<li class="chapter" data-level="15" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>15</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="15.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#optimal-separating-classifier"><i class="fa fa-check"></i><b>15.1</b> Optimal Separating Classifier</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#the-margin"><i class="fa fa-check"></i><b>15.1.1</b> The Margin</a></li>
<li class="chapter" data-level="15.1.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#the-non-separable-case"><i class="fa fa-check"></i><b>15.1.2</b> The Non-Separable Case</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#nonlinear-boundary-with-kernels"><i class="fa fa-check"></i><b>15.2</b> Nonlinear Boundary with Kernels</a></li>
<li class="chapter" data-level="15.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#application-with-svm"><i class="fa fa-check"></i><b>15.3</b> Application with SVM</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html"><i class="fa fa-check"></i><b>16</b> Artificial Neural Networks</a>
<ul>
<li class="chapter" data-level="16.1" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#neural-network---the-idea"><i class="fa fa-check"></i><b>16.1</b> Neural Network - the idea</a></li>
<li class="chapter" data-level="16.2" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#backpropagation"><i class="fa fa-check"></i><b>16.2</b> Backpropagation</a></li>
<li class="chapter" data-level="16.3" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#neural-network---more-inputs"><i class="fa fa-check"></i><b>16.3</b> Neural Network - More inputs</a></li>
<li class="chapter" data-level="16.4" data-path="artificial-neural-networks.html"><a href="artificial-neural-networks.html#deep-learning"><i class="fa fa-check"></i><b>16.4</b> Deep Learning</a></li>
</ul></li>
<li class="part"><span><b>VI Penalized Regressions</b></span></li>
<li class="chapter" data-level="" data-path="parametric-models-in-prediction.html"><a href="parametric-models-in-prediction.html"><i class="fa fa-check"></i>Parametric models in prediction</a></li>
<li class="chapter" data-level="17" data-path="ridge.html"><a href="ridge.html"><i class="fa fa-check"></i><b>17</b> Ridge</a></li>
<li class="chapter" data-level="18" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>18</b> Lasso</a></li>
<li class="chapter" data-level="19" data-path="adaptive-lasso.html"><a href="adaptive-lasso.html"><i class="fa fa-check"></i><b>19</b> Adaptive Lasso</a></li>
<li class="chapter" data-level="20" data-path="sparsity.html"><a href="sparsity.html"><i class="fa fa-check"></i><b>20</b> Sparsity</a></li>
<li class="part"><span><b>VII Time Series</b></span></li>
<li class="chapter" data-level="" data-path="forecasting.html"><a href="forecasting.html"><i class="fa fa-check"></i>Forecasting</a></li>
<li class="chapter" data-level="21" data-path="arima-models.html"><a href="arima-models.html"><i class="fa fa-check"></i><b>21</b> ARIMA models</a>
<ul>
<li class="chapter" data-level="21.1" data-path="arima-models.html"><a href="arima-models.html#hyndman-khandakar-algorithm"><i class="fa fa-check"></i><b>21.1</b> Hyndman-Khandakar algorithm</a></li>
<li class="chapter" data-level="21.2" data-path="arima-models.html"><a href="arima-models.html#ts-plots"><i class="fa fa-check"></i><b>21.2</b> TS Plots</a></li>
<li class="chapter" data-level="21.3" data-path="arima-models.html"><a href="arima-models.html#box-cox-transformation"><i class="fa fa-check"></i><b>21.3</b> Box-Cox transformation</a></li>
<li class="chapter" data-level="21.4" data-path="arima-models.html"><a href="arima-models.html#stationarity"><i class="fa fa-check"></i><b>21.4</b> Stationarity</a></li>
<li class="chapter" data-level="21.5" data-path="arima-models.html"><a href="arima-models.html#modeling-arima"><i class="fa fa-check"></i><b>21.5</b> Modeling ARIMA</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="grid-search-for-arima.html"><a href="grid-search-for-arima.html"><i class="fa fa-check"></i><b>22</b> Grid search for ARIMA</a></li>
<li class="chapter" data-level="23" data-path="time-series-embedding.html"><a href="time-series-embedding.html"><i class="fa fa-check"></i><b>23</b> Time Series Embedding</a>
<ul>
<li class="chapter" data-level="23.1" data-path="time-series-embedding.html"><a href="time-series-embedding.html#var-for-recursive-forecasting"><i class="fa fa-check"></i><b>23.1</b> VAR for Recursive Forecasting</a></li>
<li class="chapter" data-level="23.2" data-path="time-series-embedding.html"><a href="time-series-embedding.html#embedding-for-direct-forecast"><i class="fa fa-check"></i><b>23.2</b> Embedding for Direct Forecast</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="random-forest-1.html"><a href="random-forest-1.html"><i class="fa fa-check"></i><b>24</b> Random Forest</a>
<ul>
<li class="chapter" data-level="24.1" data-path="random-forest-1.html"><a href="random-forest-1.html#univariate"><i class="fa fa-check"></i><b>24.1</b> Univariate</a></li>
<li class="chapter" data-level="24.2" data-path="random-forest-1.html"><a href="random-forest-1.html#multivariate"><i class="fa fa-check"></i><b>24.2</b> Multivariate</a></li>
<li class="chapter" data-level="24.3" data-path="random-forest-1.html"><a href="random-forest-1.html#rolling-and-expanding-windows"><i class="fa fa-check"></i><b>24.3</b> Rolling and expanding windows</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html"><i class="fa fa-check"></i><b>25</b> Recurrent Neural Networks</a>
<ul>
<li class="chapter" data-level="25.1" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#keras"><i class="fa fa-check"></i><b>25.1</b> Keras</a></li>
<li class="chapter" data-level="25.2" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#input-tensors"><i class="fa fa-check"></i><b>25.2</b> Input Tensors</a></li>
<li class="chapter" data-level="25.3" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#plain-rnn"><i class="fa fa-check"></i><b>25.3</b> Plain RNN</a></li>
<li class="chapter" data-level="25.4" data-path="recurrent-neural-networks.html"><a href="recurrent-neural-networks.html#lstm"><i class="fa fa-check"></i><b>25.4</b> LSTM</a></li>
</ul></li>
<li class="part"><span><b>VIII Dimension Reduction Methods</b></span></li>
<li class="chapter" data-level="" data-path="matrix-decompositions.html"><a href="matrix-decompositions.html"><i class="fa fa-check"></i>Matrix Decompositions</a></li>
<li class="chapter" data-level="26" data-path="eigenvectors-and-eigenvalues.html"><a href="eigenvectors-and-eigenvalues.html"><i class="fa fa-check"></i><b>26</b> Eigenvectors and eigenvalues</a></li>
<li class="chapter" data-level="27" data-path="singular-value-decomposition.html"><a href="singular-value-decomposition.html"><i class="fa fa-check"></i><b>27</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="28" data-path="rankr-approximations.html"><a href="rankr-approximations.html"><i class="fa fa-check"></i><b>28</b> Rank(r) Approximations</a></li>
<li class="chapter" data-level="29" data-path="moore-penrose-inverse.html"><a href="moore-penrose-inverse.html"><i class="fa fa-check"></i><b>29</b> Moore-Penrose inverse</a></li>
<li class="chapter" data-level="30" data-path="principle-component-analysis.html"><a href="principle-component-analysis.html"><i class="fa fa-check"></i><b>30</b> Principle Component Analysis</a></li>
<li class="chapter" data-level="31" data-path="factor-analysis.html"><a href="factor-analysis.html"><i class="fa fa-check"></i><b>31</b> Factor Analysis</a></li>
<li class="part"><span><b>IX Network Analysis</b></span></li>
<li class="chapter" data-level="" data-path="graphical-network-analysis.html"><a href="graphical-network-analysis.html"><i class="fa fa-check"></i>Graphical Network Analysis</a></li>
<li class="chapter" data-level="32" data-path="fundementals.html"><a href="fundementals.html"><i class="fa fa-check"></i><b>32</b> Fundementals</a>
<ul>
<li class="chapter" data-level="32.1" data-path="fundementals.html"><a href="fundementals.html#covariance"><i class="fa fa-check"></i><b>32.1</b> Covariance</a></li>
<li class="chapter" data-level="32.2" data-path="fundementals.html"><a href="fundementals.html#correlation"><i class="fa fa-check"></i><b>32.2</b> Correlation</a></li>
<li class="chapter" data-level="32.3" data-path="fundementals.html"><a href="fundementals.html#precision-matrix"><i class="fa fa-check"></i><b>32.3</b> Precision Matrix</a></li>
<li class="chapter" data-level="32.4" data-path="fundementals.html"><a href="fundementals.html#semi-partial-correlation"><i class="fa fa-check"></i><b>32.4</b> Semi-partial Correlation</a></li>
</ul></li>
<li class="chapter" data-level="33" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html"><i class="fa fa-check"></i><b>33</b> Regularized Covariance Matrix</a>
<ul>
<li class="chapter" data-level="33.1" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html#multivariate-gaussian-distribution"><i class="fa fa-check"></i><b>33.1</b> Multivariate Gaussian Distribution</a></li>
<li class="chapter" data-level="33.2" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html#high-dimensional-data"><i class="fa fa-check"></i><b>33.2</b> High-dimensional data</a></li>
<li class="chapter" data-level="33.3" data-path="regularized-covariance-matrix.html"><a href="regularized-covariance-matrix.html#ridge-ell_2-and-glasso-ell_1"><i class="fa fa-check"></i><b>33.3</b> Ridge (<span class="math inline">\(\ell_{2}\)</span>) and glasso (<span class="math inline">\(\ell_{1}\)</span>)</a></li>
</ul></li>
<li class="part"><span><b>X Labs</b></span></li>
<li class="chapter" data-level="34" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html"><i class="fa fa-check"></i><b>34</b> R Lab 1 - Basics I</a>
<ul>
<li class="chapter" data-level="34.1" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#r-rstudio-and-r-packages"><i class="fa fa-check"></i><b>34.1</b> R, RStudio, and R Packages</a></li>
<li class="chapter" data-level="34.2" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#rstudio"><i class="fa fa-check"></i><b>34.2</b> RStudio</a></li>
<li class="chapter" data-level="34.3" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#working-directory"><i class="fa fa-check"></i><b>34.3</b> Working directory</a></li>
<li class="chapter" data-level="34.4" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#data-types-and-stuctures"><i class="fa fa-check"></i><b>34.4</b> Data Types and Stuctures</a></li>
<li class="chapter" data-level="34.5" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#vectors"><i class="fa fa-check"></i><b>34.5</b> Vectors</a></li>
<li class="chapter" data-level="34.6" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#subsetting-vectors"><i class="fa fa-check"></i><b>34.6</b> Subsetting Vectors</a></li>
<li class="chapter" data-level="34.7" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#vectorization-or-vector-operations"><i class="fa fa-check"></i><b>34.7</b> Vectorization or vector operations</a></li>
<li class="chapter" data-level="34.8" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#matrices"><i class="fa fa-check"></i><b>34.8</b> Matrices</a></li>
<li class="chapter" data-level="34.9" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#matrix-operations"><i class="fa fa-check"></i><b>34.9</b> Matrix Operations</a></li>
<li class="chapter" data-level="34.10" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#subsetting-matrix"><i class="fa fa-check"></i><b>34.10</b> Subsetting Matrix</a></li>
<li class="chapter" data-level="34.11" data-path="r-lab-1---basics-i.html"><a href="r-lab-1---basics-i.html#r-style-guide"><i class="fa fa-check"></i><b>34.11</b> R-Style Guide</a></li>
</ul></li>
<li class="chapter" data-level="35" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html"><i class="fa fa-check"></i><b>35</b> R Lab 2 - Basics II</a>
<ul>
<li class="chapter" data-level="35.1" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#data-frames-and-lists"><i class="fa fa-check"></i><b>35.1</b> Data frames and lists</a>
<ul>
<li class="chapter" data-level="35.1.1" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#lists"><i class="fa fa-check"></i><b>35.1.1</b> Lists</a></li>
<li class="chapter" data-level="35.1.2" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#data-frames"><i class="fa fa-check"></i><b>35.1.2</b> Data Frames</a></li>
<li class="chapter" data-level="35.1.3" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#reading-importing-and-writting-exporting-data-files"><i class="fa fa-check"></i><b>35.1.3</b> Reading (importing) and writting (exporting) data files</a></li>
<li class="chapter" data-level="35.1.4" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#subsetting-data-frames"><i class="fa fa-check"></i><b>35.1.4</b> Subsetting Data Frames</a></li>
<li class="chapter" data-level="35.1.5" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#plotting-from-data-frame"><i class="fa fa-check"></i><b>35.1.5</b> Plotting from data frame</a></li>
<li class="chapter" data-level="35.1.6" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#some-useful-functions"><i class="fa fa-check"></i><b>35.1.6</b> Some useful functions</a></li>
<li class="chapter" data-level="35.1.7" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#categorical-variables-in-data-frames"><i class="fa fa-check"></i><b>35.1.7</b> Categorical Variables in Data Frames</a></li>
</ul></li>
<li class="chapter" data-level="35.2" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#programming-basics"><i class="fa fa-check"></i><b>35.2</b> Programming Basics</a>
<ul>
<li class="chapter" data-level="35.2.1" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#ifelse"><i class="fa fa-check"></i><b>35.2.1</b> if/Else</a></li>
<li class="chapter" data-level="35.2.2" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#loops"><i class="fa fa-check"></i><b>35.2.2</b> Loops</a></li>
<li class="chapter" data-level="35.2.3" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#the-apply-family"><i class="fa fa-check"></i><b>35.2.3</b> The <code>apply()</code> family</a></li>
<li class="chapter" data-level="35.2.4" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#functions"><i class="fa fa-check"></i><b>35.2.4</b> Functions</a></li>
<li class="chapter" data-level="35.2.5" data-path="r-lab-2---basics-ii.html"><a href="r-lab-2---basics-ii.html#dplyr"><i class="fa fa-check"></i><b>35.2.5</b> <code>dplyr()</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="36" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html"><i class="fa fa-check"></i><b>36</b> R Lab 3 - Preparing the data</a>
<ul>
<li class="chapter" data-level="36.1" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#preparing-the-data-for-a-regression-analysis-with-lm"><i class="fa fa-check"></i><b>36.1</b> Preparing the data for a regression analysis with <code>lm()</code></a>
<ul>
<li class="chapter" data-level="36.1.1" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#factor-variables"><i class="fa fa-check"></i><b>36.1.1</b> Factor variables</a></li>
<li class="chapter" data-level="36.1.2" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#dummy-coding"><i class="fa fa-check"></i><b>36.1.2</b> Dummy Coding</a></li>
<li class="chapter" data-level="36.1.3" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#column-variable-names"><i class="fa fa-check"></i><b>36.1.3</b> Column (Variable) names</a></li>
<li class="chapter" data-level="36.1.4" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#data-subsetting-and-missing-values"><i class="fa fa-check"></i><b>36.1.4</b> Data subsetting and missing values</a></li>
</ul></li>
<li class="chapter" data-level="36.2" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#dummy-variable-models"><i class="fa fa-check"></i><b>36.2</b> âDUMMYâ variable models</a>
<ul>
<li class="chapter" data-level="36.2.1" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#mtcars-example"><i class="fa fa-check"></i><b>36.2.1</b> <code>mtcars</code> example</a></li>
<li class="chapter" data-level="36.2.2" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#model.matrix"><i class="fa fa-check"></i><b>36.2.2</b> <code>model.matrix()</code></a></li>
<li class="chapter" data-level="36.2.3" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#example-with-a-bigger-data-set-autompg"><i class="fa fa-check"></i><b>36.2.3</b> Example with a bigger data set: <code>Autompg</code></a></li>
<li class="chapter" data-level="36.2.4" data-path="r-lab-3---preparing-the-data.html"><a href="r-lab-3---preparing-the-data.html#some-more-data-management-tools-for-subsetting-complete.cases-is.na-and-within"><i class="fa fa-check"></i><b>36.2.4</b> Some more data management tools for subsetting: <code>complete.cases()</code>, <code>is.na()</code>, and <code>within()</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="37" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html"><i class="fa fa-check"></i><b>37</b> R Lab 4 - Simulation in R</a>
<ul>
<li class="chapter" data-level="37.1" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#sampling-in-r-sample"><i class="fa fa-check"></i><b>37.1</b> Sampling in R: <code>sample()</code></a></li>
<li class="chapter" data-level="37.2" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#random-number-generating-with-probablity-distributions"><i class="fa fa-check"></i><b>37.2</b> Random number generating with probablity distributions</a></li>
<li class="chapter" data-level="37.3" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#simulation-for-statistical-inference"><i class="fa fa-check"></i><b>37.3</b> Simulation for statistical inference</a></li>
<li class="chapter" data-level="37.4" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#creataing-data-with-a-data-generating-model-dgm"><i class="fa fa-check"></i><b>37.4</b> Creataing data with a Data Generating Model (DGM)</a></li>
<li class="chapter" data-level="37.5" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#bootstrapping"><i class="fa fa-check"></i><b>37.5</b> Bootstrapping</a></li>
<li class="chapter" data-level="37.6" data-path="r-lab-4---simulation-in-r.html"><a href="r-lab-4---simulation-in-r.html#monty-hall---fun-example"><i class="fa fa-check"></i><b>37.6</b> Monty Hall - Fun example</a></li>
</ul></li>
<li class="part"><span><b>XI Appendix</b></span></li>
<li class="chapter" data-level="38" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html"><i class="fa fa-check"></i><b>38</b> Algorithmic Optimization</a>
<ul>
<li class="chapter" data-level="38.1" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#brute-force-optimization"><i class="fa fa-check"></i><b>38.1</b> Brute-force optimization</a></li>
<li class="chapter" data-level="38.2" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#derivative-based-methods"><i class="fa fa-check"></i><b>38.2</b> Derivative-based methods</a></li>
<li class="chapter" data-level="38.3" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#ml-estimation-with-logistic-regression"><i class="fa fa-check"></i><b>38.3</b> ML Estimation with logistic regression</a></li>
<li class="chapter" data-level="38.4" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#gradient-descent-algorithm"><i class="fa fa-check"></i><b>38.4</b> Gradient Descent Algorithm</a>
<ul>
<li class="chapter" data-level="38.4.1" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#one-variable"><i class="fa fa-check"></i><b>38.4.1</b> One-variable</a></li>
<li class="chapter" data-level="38.4.2" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#adjustable-lr-and-sgd"><i class="fa fa-check"></i><b>38.4.2</b> Adjustable <code>lr</code> and SGD</a></li>
<li class="chapter" data-level="38.4.3" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#multivariable"><i class="fa fa-check"></i><b>38.4.3</b> Multivariable</a></li>
</ul></li>
<li class="chapter" data-level="38.5" data-path="algorithmic-optimization.html"><a href="algorithmic-optimization.html#optimization-with-r"><i class="fa fa-check"></i><b>38.5</b> Optimization with R</a></li>
</ul></li>
<li class="chapter" data-level="39" data-path="imbalanced-data.html"><a href="imbalanced-data.html"><i class="fa fa-check"></i><b>39</b> Imbalanced Data</a>
<ul>
<li class="chapter" data-level="39.1" data-path="imbalanced-data.html"><a href="imbalanced-data.html#smote"><i class="fa fa-check"></i><b>39.1</b> <code>SMOTE</code></a></li>
<li class="chapter" data-level="39.2" data-path="imbalanced-data.html"><a href="imbalanced-data.html#fraud-detection"><i class="fa fa-check"></i><b>39.2</b> Fraud detection</a></li>
</ul></li>
<li class="chapter" data-level="40" data-path="footnotes-and-citations.html"><a href="footnotes-and-citations.html"><i class="fa fa-check"></i><b>40</b> Footnotes and citations</a>
<ul>
<li class="chapter" data-level="40.1" data-path="footnotes-and-citations.html"><a href="footnotes-and-citations.html#footnotes"><i class="fa fa-check"></i><b>40.1</b> Footnotes</a></li>
<li class="chapter" data-level="40.2" data-path="footnotes-and-citations.html"><a href="footnotes-and-citations.html#citations"><i class="fa fa-check"></i><b>40.2</b> Citations</a></li>
</ul></li>
<li class="chapter" data-level="41" data-path="blocks.html"><a href="blocks.html"><i class="fa fa-check"></i><b>41</b> Blocks</a>
<ul>
<li class="chapter" data-level="41.1" data-path="blocks.html"><a href="blocks.html#equations"><i class="fa fa-check"></i><b>41.1</b> Equations</a></li>
<li class="chapter" data-level="41.2" data-path="blocks.html"><a href="blocks.html#theorems-and-proofs"><i class="fa fa-check"></i><b>41.2</b> Theorems and proofs</a></li>
<li class="chapter" data-level="41.3" data-path="blocks.html"><a href="blocks.html#callout-blocks"><i class="fa fa-check"></i><b>41.3</b> Callout blocks</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/yaydede/toolbox" target="blank"> 2023 Yigit Aydede - Bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Toolbox for Social Scientists and Policy Analysts</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="overfitting" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">Chapter 4</span> Overfitting<a href="overfitting.html#overfitting" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>While overfitting seems as a prediction problem, it is also a serious problem in estimations, where the unbiasedness is the main objective. Before going further, we need to see the connection between MSPE and MSE in a regression setting:</p>
<p><span class="math display" id="eq:4-1">\[\begin{equation}
\mathbf{MSPE}=\mathrm{E}\left[(y_0-\hat{f})^{2}\right]=(f-\mathrm{E}[\hat{f}])^{2}+\mathrm{E}\left[(\mathrm{E}[\hat{f}]-\hat{f})^{2}\right]+\mathrm{E}\left[\varepsilon^{2}\right]
  \tag{4.1}
\end{equation}\]</span></p>
<p>Equation 4.1 is simply an expected prediction error of predicting <span class="math inline">\(y_0\)</span> using <span class="math inline">\(\hat{f}(x_0)\)</span>. The estimate <span class="math inline">\(\hat{f}\)</span> is random depending on the sample we use to estimate it. Hence, it varies from sample to sample. We call the sum of the first two terms as âreducible errorâ, as we have seen before.</p>
<p>The MSE of the estimator <span class="math inline">\(\hat{f}\)</span> is, on the other hand, shows the expected squared error loss of estimating <span class="math inline">\(f(x)\)</span> by using <span class="math inline">\(\hat{f}\)</span> at a fixed point <span class="math inline">\(x\)</span>.</p>
<p><span class="math display">\[
\mathbf{MSE}(\hat{f})=\mathrm{E}\left[(\hat{f}-f)^{2}\right]=\mathrm{E}\left\{\left(\hat{f}-\mathrm{E}(\hat{f})+\mathrm{E}(\hat{f})-f\right)^{2}\right\}
\]</span>
<span class="math display">\[
=\mathrm{E}\left\{\left(\left[\hat{f}-\mathrm{E}\left(\hat{f}\right)\right]+\left[\mathrm{E}\left(\hat{f}\right)-f\right]\right)^{2}\right\}
\]</span></p>
<p><span class="math display" id="eq:4-2">\[\begin{equation}
  =\mathrm{E}\left\{\left[\hat{f}-\mathrm{E}(\hat{f})\right]^{2}\right   \}+\mathrm{E}\left\{\left[\mathrm{E}(\hat{f})-f\right]^{2}\right\}+2 \mathrm{E}\left\{\left[\hat{f}-\mathrm{E}(\hat{f})\right]\left[\mathrm{E}(\hat{f})-f\right]\right\}
  \tag{4.2}
\end{equation}\]</span></p>
<p>The first term is the variance. The second term is outside of expectation, as <span class="math inline">\([\mathbf{E}(\hat{f})-f]\)</span> is not random, which represents the bias. The last term is zero. Hence,</p>
<p><span class="math display" id="eq:4-3">\[\begin{equation}
\mathbf{MSE}(\hat{f})=\mathrm{E}\left\{\left[\hat{f}-\mathrm{E}(\hat{f})\right]^{2}\right\}+\mathrm{E}\left\{\left[\mathrm{E}(\hat{f})-f\right]^{2}\right\}=\mathrm{Var}(\hat{f})+\left[\mathbf{bias}(\hat{f})\right]^{2}
\tag{4.3}
\end{equation}\]</span></p>
<p>We can now see how MSPE is related to MSE. Since the estimator <span class="math inline">\(\hat{f}\)</span> is used in predicting <span class="math inline">\(y_0\)</span>, MSPE should include MSE:</p>
<p><span class="math display">\[
\mathbf{MSPE}=(f-\mathrm{E}[\hat{f}])^{2}+\mathrm{E}\left[(\mathrm{E}[\hat{f}]-\hat{f})^{2}\right]+\mathrm{E}\left[\varepsilon^{2}\right]=\mathrm{MSE}(\hat{f})+\mathrm{E}\left[\varepsilon^{2}\right]
\]</span></p>
<p>The important difference between estimation and prediction processes is the data points that we use to calculate the mean squared error loss functions. In estimations, our objective is to find the estimator that minimizes the MSE, <span class="math inline">\(\mathrm{E}\left[(\hat{f}-f)^{2}\right]\)</span>. However, since <span class="math inline">\(f\)</span> is not known to us, we use <span class="math inline">\(y_i\)</span> as a proxy for <span class="math inline">\(f\)</span> and calculate MSPE using in-sample data points. Therefore, using an estimator for predictions means that we use in-sample data points to calculate MSPE in predictions, which may result in overfitting and a poor out-of-sample prediction accuracy.</p>
<p>Letâs start with an example:</p>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb199-1"><a href="overfitting.html#cb199-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Getting one-sample.</span></span>
<span id="cb199-2"><a href="overfitting.html#cb199-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb199-3"><a href="overfitting.html#cb199-3" aria-hidden="true" tabindex="-1"></a>x_1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>) </span>
<span id="cb199-4"><a href="overfitting.html#cb199-4" aria-hidden="true" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>x_1 <span class="sc">-</span> <span class="dv">2</span><span class="sc">*</span>(x_1<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> <span class="dv">3</span><span class="sc">*</span>(x_1<span class="sc">^</span><span class="dv">3</span>) <span class="co"># DGM</span></span>
<span id="cb199-5"><a href="overfitting.html#cb199-5" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> f <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">8</span>)</span>
<span id="cb199-6"><a href="overfitting.html#cb199-6" aria-hidden="true" tabindex="-1"></a>inn <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y, x_1)</span>
<span id="cb199-7"><a href="overfitting.html#cb199-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb199-8"><a href="overfitting.html#cb199-8" aria-hidden="true" tabindex="-1"></a><span class="co"># OLS</span></span>
<span id="cb199-9"><a href="overfitting.html#cb199-9" aria-hidden="true" tabindex="-1"></a>ols1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span> <span class="fu">poly</span>(x_1, <span class="at">degree =</span> <span class="dv">1</span>), inn)</span>
<span id="cb199-10"><a href="overfitting.html#cb199-10" aria-hidden="true" tabindex="-1"></a>ols2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span> <span class="fu">poly</span>(x_1, <span class="at">degree =</span> <span class="dv">2</span>), inn)</span>
<span id="cb199-11"><a href="overfitting.html#cb199-11" aria-hidden="true" tabindex="-1"></a>ols3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span> <span class="fu">poly</span>(x_1, <span class="at">degree =</span> <span class="dv">3</span>), inn)</span>
<span id="cb199-12"><a href="overfitting.html#cb199-12" aria-hidden="true" tabindex="-1"></a>ols4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y<span class="sc">~</span> <span class="fu">poly</span>(x_1, <span class="at">degree =</span> <span class="dv">20</span>), inn)</span>
<span id="cb199-13"><a href="overfitting.html#cb199-13" aria-hidden="true" tabindex="-1"></a>ror <span class="ot">&lt;-</span> <span class="fu">order</span>(x_1)</span>
<span id="cb199-14"><a href="overfitting.html#cb199-14" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x_1, y, <span class="at">col=</span><span class="st">&quot;darkgrey&quot;</span>)</span>
<span id="cb199-15"><a href="overfitting.html#cb199-15" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_1[ror], <span class="fu">predict</span>(ols1)[ror], <span class="at">col=</span><span class="st">&quot;pink&quot;</span>, <span class="at">lwd =</span> <span class="fl">1.5</span>)</span>
<span id="cb199-16"><a href="overfitting.html#cb199-16" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_1[ror], <span class="fu">predict</span>(ols2)[ror], <span class="at">col=</span><span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="fl">1.5</span>)</span>
<span id="cb199-17"><a href="overfitting.html#cb199-17" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_1[ror], <span class="fu">predict</span>(ols3)[ror], <span class="at">col=</span><span class="st">&quot;green&quot;</span>, <span class="at">lwd =</span> <span class="fl">1.5</span>)</span>
<span id="cb199-18"><a href="overfitting.html#cb199-18" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(x_1[ror], <span class="fu">predict</span>(ols4)[ror], <span class="at">col=</span><span class="st">&quot;red&quot;</span> , <span class="at">lwd =</span> <span class="fl">1.5</span>)</span>
<span id="cb199-19"><a href="overfitting.html#cb199-19" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;bottomright&quot;</span>, <span class="fu">c</span>(<span class="st">&quot;ols1&quot;</span>, <span class="st">&quot;ols2&quot;</span>, <span class="st">&quot;ols3&quot;</span>, <span class="st">&quot;ols4&quot;</span>),</span>
<span id="cb199-20"><a href="overfitting.html#cb199-20" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;pink&quot;</span>, <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="04-Overfitting_files/figure-html/o1-1.png" width="672" /></p>
<p>The âtrueâ estimator, <span class="math inline">\(f(x)\)</span>, which is the âgreenâ line, is:</p>
<p><span class="math display">\[
f(x_i)=\beta_0+\beta_1 x_{1i}+\beta_2 x_{1i}^2+\beta_2 x_{1i}^3 = 1+2x_{1i}-2x_{1i}^2+3 x_{1i}^3.
\]</span></p>
<p>Now we can calculate in-sample <strong>empirical</strong> MSPE:<br />
<span class="math display">\[~\]</span></p>
<div class="sourceCode" id="cb200"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb200-1"><a href="overfitting.html#cb200-1" aria-hidden="true" tabindex="-1"></a><span class="co"># MSE</span></span>
<span id="cb200-2"><a href="overfitting.html#cb200-2" aria-hidden="true" tabindex="-1"></a>MSPE1 <span class="ot">&lt;-</span> <span class="fu">mean</span>((<span class="fu">predict</span>(ols1)<span class="sc">-</span>y)<span class="sc">^</span><span class="dv">2</span>) <span class="co"># which is also mean(ols1$residuals^2)</span></span>
<span id="cb200-3"><a href="overfitting.html#cb200-3" aria-hidden="true" tabindex="-1"></a>MSPE2 <span class="ot">&lt;-</span> <span class="fu">mean</span>((<span class="fu">predict</span>(ols2)<span class="sc">-</span>y)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb200-4"><a href="overfitting.html#cb200-4" aria-hidden="true" tabindex="-1"></a>MSPE3 <span class="ot">&lt;-</span> <span class="fu">mean</span>((<span class="fu">predict</span>(ols3)<span class="sc">-</span>y)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb200-5"><a href="overfitting.html#cb200-5" aria-hidden="true" tabindex="-1"></a>MSPE4 <span class="ot">&lt;-</span> <span class="fu">mean</span>((<span class="fu">predict</span>(ols4)<span class="sc">-</span>y)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb200-6"><a href="overfitting.html#cb200-6" aria-hidden="true" tabindex="-1"></a>all <span class="ot">&lt;-</span> <span class="fu">c</span>(MSPE1, MSPE2, MSPE3, MSPE4)</span>
<span id="cb200-7"><a href="overfitting.html#cb200-7" aria-hidden="true" tabindex="-1"></a>MSPE <span class="ot">&lt;-</span> <span class="fu">matrix</span>(all, <span class="dv">4</span>, <span class="dv">1</span>)</span>
<span id="cb200-8"><a href="overfitting.html#cb200-8" aria-hidden="true" tabindex="-1"></a><span class="fu">row.names</span>(MSPE) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;ols1&quot;</span>, <span class="st">&quot;ols2&quot;</span>, <span class="st">&quot;ols3&quot;</span>, <span class="st">&quot;ols4&quot;</span>)</span>
<span id="cb200-9"><a href="overfitting.html#cb200-9" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(MSPE) <span class="ot">&lt;-</span> <span class="st">&quot;In-sample MSPE&#39;s&quot;</span></span>
<span id="cb200-10"><a href="overfitting.html#cb200-10" aria-hidden="true" tabindex="-1"></a>MSPE</span></code></pre></div>
<pre><code>##      In-sample MSPE&#39;s
## ols1         86.57600
## ols2         79.56354
## ols3         58.40780
## ols4         48.88589</code></pre>
<p>As we see, the <strong>overfitted</strong> <span class="math inline">\(\hat{f}(x)\)</span>, the <span class="math inline">\(4^{th}\)</span> model, has a lower <strong>empirical in-sample</strong> MSPE. If we use nonparametric models, we can even find a better fitting model with a lower empirical in-sample MSPE. We call these MSPEâs <strong>empirical</strong> because they are not calculated based on repeated samples, which would give an <strong>expected value</strong> of squared errors over all samples. In practice, however, we have only one sample. Therefore, even if our objective is to find an <strong>unbiased</strong> estimator of <span class="math inline">\({f}(x)\)</span>, not a prediction of <span class="math inline">\(y\)</span>, since we choose our estimator, <span class="math inline">\(\hat{f}(x)\)</span>, by the <strong>empirical</strong> in-sample MSPE, we may end up with an <strong>overfitted</strong> <span class="math inline">\(\hat{f}(x)\)</span>, such as the <span class="math inline">\(4^{th}\)</span> estimator.</p>
<p>Would an overfitted model create a biased estimator? We will see the answer in a simulation later. However, in estimations, our objective is not only to find an unbiased estimator but also to find the one that has the minimum variance. We know that our <span class="math inline">\(3^{rd}\)</span> model is unbiased estimator of <span class="math inline">\(f(x)\)</span> as is the overfitted <span class="math inline">\(4^{th}\)</span> estimator. Which one should we choose? We have answered this question at the beginning of this chapter: the one with the minimum variance. Since overfitting would create a greater variance, our choice must be the <span class="math inline">\(3^{rd}\)</span> model.</p>
<p>That is why we do not use the <strong>empirical</strong> in-sample MSPE as a âcostâ or âriskâ function in finding the best estimator. This process is called a âdata miningâ exercise based on one sample without any theoretical justification on what the âtrueâ model would be. This is a general problem in <strong>empirical risk minimization</strong> specially in finding unbiased estimators of population parameters.</p>
<p>To see all these issues in actions, letâs have a simulation for the decomposition of in-sample unconditional MSPEâs.</p>
<div class="sourceCode" id="cb202"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb202-1"><a href="overfitting.html#cb202-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Function for X - fixed at repeated samples</span></span>
<span id="cb202-2"><a href="overfitting.html#cb202-2" aria-hidden="true" tabindex="-1"></a>xfunc <span class="ot">&lt;-</span> <span class="cf">function</span>(n){</span>
<span id="cb202-3"><a href="overfitting.html#cb202-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb202-4"><a href="overfitting.html#cb202-4" aria-hidden="true" tabindex="-1"></a>  x_1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>) </span>
<span id="cb202-5"><a href="overfitting.html#cb202-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(x_1)</span>
<span id="cb202-6"><a href="overfitting.html#cb202-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb202-7"><a href="overfitting.html#cb202-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb202-8"><a href="overfitting.html#cb202-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Function for simulation (M - number of samples)</span></span>
<span id="cb202-9"><a href="overfitting.html#cb202-9" aria-hidden="true" tabindex="-1"></a>simmse <span class="ot">&lt;-</span> <span class="cf">function</span>(M, n, sigma, poldeg){</span>
<span id="cb202-10"><a href="overfitting.html#cb202-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb202-11"><a href="overfitting.html#cb202-11" aria-hidden="true" tabindex="-1"></a>  x_1 <span class="ot">&lt;-</span> <span class="fu">xfunc</span>(n) <span class="co"># Repeated X&#39;s in each sample</span></span>
<span id="cb202-12"><a href="overfitting.html#cb202-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb202-13"><a href="overfitting.html#cb202-13" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Containers</span></span>
<span id="cb202-14"><a href="overfitting.html#cb202-14" aria-hidden="true" tabindex="-1"></a>  MSPE <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, M)</span>
<span id="cb202-15"><a href="overfitting.html#cb202-15" aria-hidden="true" tabindex="-1"></a>  yhat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, M, n)</span>
<span id="cb202-16"><a href="overfitting.html#cb202-16" aria-hidden="true" tabindex="-1"></a>  olscoef <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, M, poldeg<span class="sc">+</span><span class="dv">1</span>)</span>
<span id="cb202-17"><a href="overfitting.html#cb202-17" aria-hidden="true" tabindex="-1"></a>  ymat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, M, n)</span>
<span id="cb202-18"><a href="overfitting.html#cb202-18" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb202-19"><a href="overfitting.html#cb202-19" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Loop for samples</span></span>
<span id="cb202-20"><a href="overfitting.html#cb202-20" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>M) {</span>
<span id="cb202-21"><a href="overfitting.html#cb202-21" aria-hidden="true" tabindex="-1"></a>    f <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> <span class="dv">2</span><span class="sc">*</span>x_1 <span class="sc">-</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">I</span>(x_1<span class="sc">^</span><span class="dv">2</span>) <span class="co"># DGM</span></span>
<span id="cb202-22"><a href="overfitting.html#cb202-22" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">&lt;-</span> f <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, sigma)</span>
<span id="cb202-23"><a href="overfitting.html#cb202-23" aria-hidden="true" tabindex="-1"></a>    samp <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="st">&quot;y&quot;</span> <span class="ot">=</span> y, x_1)</span>
<span id="cb202-24"><a href="overfitting.html#cb202-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Estimator</span></span>
<span id="cb202-25"><a href="overfitting.html#cb202-25" aria-hidden="true" tabindex="-1"></a>    ols <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">poly</span>(x_1, <span class="at">degree =</span> poldeg, <span class="at">raw=</span><span class="cn">TRUE</span>), samp)</span>
<span id="cb202-26"><a href="overfitting.html#cb202-26" aria-hidden="true" tabindex="-1"></a>    olscoef[i, ] <span class="ot">&lt;-</span> ols<span class="sc">$</span>coefficients</span>
<span id="cb202-27"><a href="overfitting.html#cb202-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Yhat&#39;s</span></span>
<span id="cb202-28"><a href="overfitting.html#cb202-28" aria-hidden="true" tabindex="-1"></a>    yhat[i,] <span class="ot">&lt;-</span> <span class="fu">predict</span>(ols, samp)</span>
<span id="cb202-29"><a href="overfitting.html#cb202-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># MSPE - That is, residual sum of squares</span></span>
<span id="cb202-30"><a href="overfitting.html#cb202-30" aria-hidden="true" tabindex="-1"></a>    MSPE[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>((ols<span class="sc">$</span>residuals)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb202-31"><a href="overfitting.html#cb202-31" aria-hidden="true" tabindex="-1"></a>    ymat[i,] <span class="ot">&lt;-</span> y</span>
<span id="cb202-32"><a href="overfitting.html#cb202-32" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb202-33"><a href="overfitting.html#cb202-33" aria-hidden="true" tabindex="-1"></a>  output <span class="ot">&lt;-</span> <span class="fu">list</span>(MSPE, yhat, sigma, olscoef, f, ymat)</span>
<span id="cb202-34"><a href="overfitting.html#cb202-34" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(output)</span>
<span id="cb202-35"><a href="overfitting.html#cb202-35" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb202-36"><a href="overfitting.html#cb202-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb202-37"><a href="overfitting.html#cb202-37" aria-hidden="true" tabindex="-1"></a><span class="co"># running different fhat with different polynomial degrees</span></span>
<span id="cb202-38"><a href="overfitting.html#cb202-38" aria-hidden="true" tabindex="-1"></a>output1 <span class="ot">&lt;-</span> <span class="fu">simmse</span>(<span class="dv">2000</span>, <span class="dv">100</span>, <span class="dv">7</span>, <span class="dv">1</span>)</span>
<span id="cb202-39"><a href="overfitting.html#cb202-39" aria-hidden="true" tabindex="-1"></a>output2 <span class="ot">&lt;-</span> <span class="fu">simmse</span>(<span class="dv">2000</span>, <span class="dv">100</span>, <span class="dv">7</span>, <span class="dv">2</span>) <span class="co">#True model (i.e fhat = f)</span></span>
<span id="cb202-40"><a href="overfitting.html#cb202-40" aria-hidden="true" tabindex="-1"></a>output3 <span class="ot">&lt;-</span> <span class="fu">simmse</span>(<span class="dv">2000</span>, <span class="dv">100</span>, <span class="dv">7</span>, <span class="dv">5</span>) </span>
<span id="cb202-41"><a href="overfitting.html#cb202-41" aria-hidden="true" tabindex="-1"></a>output4 <span class="ot">&lt;-</span> <span class="fu">simmse</span>(<span class="dv">2000</span>, <span class="dv">100</span>, <span class="dv">7</span>, <span class="dv">20</span>)</span>
<span id="cb202-42"><a href="overfitting.html#cb202-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb202-43"><a href="overfitting.html#cb202-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Table</span></span>
<span id="cb202-44"><a href="overfitting.html#cb202-44" aria-hidden="true" tabindex="-1"></a>tab <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">5</span>)</span>
<span id="cb202-45"><a href="overfitting.html#cb202-45" aria-hidden="true" tabindex="-1"></a><span class="fu">row.names</span>(tab) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;ols1&quot;</span>, <span class="st">&quot;ols2&quot;</span>, <span class="st">&quot;ols3&quot;</span>, <span class="st">&quot;ols4&quot;</span>)</span>
<span id="cb202-46"><a href="overfitting.html#cb202-46" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(tab) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;bias^2&quot;</span>, <span class="st">&quot;var(yhat)&quot;</span>, <span class="st">&quot;MSE&quot;</span>, <span class="st">&quot;var(eps)&quot;</span>, <span class="st">&quot;In-sample MSPE&quot;</span>)</span>
<span id="cb202-47"><a href="overfitting.html#cb202-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb202-48"><a href="overfitting.html#cb202-48" aria-hidden="true" tabindex="-1"></a>f <span class="ot">&lt;-</span> output1[[<span class="dv">5</span>]]</span>
<span id="cb202-49"><a href="overfitting.html#cb202-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb202-50"><a href="overfitting.html#cb202-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Var(yhat) -  We use our own function instead of &quot;var()&quot;</span></span>
<span id="cb202-51"><a href="overfitting.html#cb202-51" aria-hidden="true" tabindex="-1"></a>tab[<span class="dv">1</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">apply</span>(output1[[<span class="dv">2</span>]], <span class="dv">2</span>, <span class="cf">function</span>(x) <span class="fu">mean</span>((x<span class="sc">-</span><span class="fu">mean</span>(x))<span class="sc">^</span><span class="dv">2</span>)))</span>
<span id="cb202-52"><a href="overfitting.html#cb202-52" aria-hidden="true" tabindex="-1"></a>tab[<span class="dv">2</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">apply</span>(output2[[<span class="dv">2</span>]], <span class="dv">2</span>, <span class="cf">function</span>(x) <span class="fu">mean</span>((x<span class="sc">-</span><span class="fu">mean</span>(x))<span class="sc">^</span><span class="dv">2</span>)))</span>
<span id="cb202-53"><a href="overfitting.html#cb202-53" aria-hidden="true" tabindex="-1"></a>tab[<span class="dv">3</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">apply</span>(output3[[<span class="dv">2</span>]], <span class="dv">2</span>, <span class="cf">function</span>(x) <span class="fu">mean</span>((x<span class="sc">-</span><span class="fu">mean</span>(x))<span class="sc">^</span><span class="dv">2</span>)))</span>
<span id="cb202-54"><a href="overfitting.html#cb202-54" aria-hidden="true" tabindex="-1"></a>tab[<span class="dv">4</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">apply</span>(output4[[<span class="dv">2</span>]], <span class="dv">2</span>, <span class="cf">function</span>(x) <span class="fu">mean</span>((x<span class="sc">-</span><span class="fu">mean</span>(x))<span class="sc">^</span><span class="dv">2</span>)))</span>
<span id="cb202-55"><a href="overfitting.html#cb202-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb202-56"><a href="overfitting.html#cb202-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Bias^2 = (mean(yhat))-f)^2</span></span>
<span id="cb202-57"><a href="overfitting.html#cb202-57" aria-hidden="true" tabindex="-1"></a>tab[<span class="dv">1</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>((<span class="fu">apply</span>(output1[[<span class="dv">2</span>]], <span class="dv">2</span>, mean) <span class="sc">-</span> f)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb202-58"><a href="overfitting.html#cb202-58" aria-hidden="true" tabindex="-1"></a>tab[<span class="dv">2</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>((<span class="fu">apply</span>(output2[[<span class="dv">2</span>]], <span class="dv">2</span>, mean) <span class="sc">-</span> f)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb202-59"><a href="overfitting.html#cb202-59" aria-hidden="true" tabindex="-1"></a>tab[<span class="dv">3</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>((<span class="fu">apply</span>(output3[[<span class="dv">2</span>]], <span class="dv">2</span>, mean) <span class="sc">-</span> f)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb202-60"><a href="overfitting.html#cb202-60" aria-hidden="true" tabindex="-1"></a>tab[<span class="dv">4</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>((<span class="fu">apply</span>(output4[[<span class="dv">2</span>]], <span class="dv">2</span>, mean) <span class="sc">-</span> f)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb202-61"><a href="overfitting.html#cb202-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb202-62"><a href="overfitting.html#cb202-62" aria-hidden="true" tabindex="-1"></a><span class="co"># MSE</span></span>
<span id="cb202-63"><a href="overfitting.html#cb202-63" aria-hidden="true" tabindex="-1"></a>fmat <span class="ot">&lt;-</span> <span class="fu">matrix</span>(f, <span class="fu">nrow</span>(output1[[<span class="dv">6</span>]]), <span class="fu">length</span>(f), <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb202-64"><a href="overfitting.html#cb202-64" aria-hidden="true" tabindex="-1"></a>tab[<span class="dv">1</span>,<span class="dv">3</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">colMeans</span>((fmat <span class="sc">-</span> output1[[<span class="dv">2</span>]])<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb202-65"><a href="overfitting.html#cb202-65" aria-hidden="true" tabindex="-1"></a>tab[<span class="dv">2</span>,<span class="dv">3</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">colMeans</span>((fmat <span class="sc">-</span> output2[[<span class="dv">2</span>]])<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb202-66"><a href="overfitting.html#cb202-66" aria-hidden="true" tabindex="-1"></a>tab[<span class="dv">3</span>,<span class="dv">3</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">colMeans</span>((fmat <span class="sc">-</span> output3[[<span class="dv">2</span>]])<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb202-67"><a href="overfitting.html#cb202-67" aria-hidden="true" tabindex="-1"></a>tab[<span class="dv">4</span>,<span class="dv">3</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">colMeans</span>((fmat <span class="sc">-</span> output4[[<span class="dv">2</span>]])<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb202-68"><a href="overfitting.html#cb202-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb202-69"><a href="overfitting.html#cb202-69" aria-hidden="true" tabindex="-1"></a><span class="co"># # MSPE - This can be used as well, which is RSS</span></span>
<span id="cb202-70"><a href="overfitting.html#cb202-70" aria-hidden="true" tabindex="-1"></a><span class="co"># tab[1,5] &lt;- mean(output1[[1]])</span></span>
<span id="cb202-71"><a href="overfitting.html#cb202-71" aria-hidden="true" tabindex="-1"></a><span class="co"># tab[2,5] &lt;- mean(output2[[1]])</span></span>
<span id="cb202-72"><a href="overfitting.html#cb202-72" aria-hidden="true" tabindex="-1"></a><span class="co"># tab[3,5] &lt;- mean(output3[[1]])</span></span>
<span id="cb202-73"><a href="overfitting.html#cb202-73" aria-hidden="true" tabindex="-1"></a><span class="co"># tab[4,5] &lt;- mean(output4[[1]])</span></span>
<span id="cb202-74"><a href="overfitting.html#cb202-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb202-75"><a href="overfitting.html#cb202-75" aria-hidden="true" tabindex="-1"></a><span class="co"># MSPE</span></span>
<span id="cb202-76"><a href="overfitting.html#cb202-76" aria-hidden="true" tabindex="-1"></a>tab[<span class="dv">1</span>,<span class="dv">5</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">colMeans</span>((output1[[<span class="dv">6</span>]] <span class="sc">-</span> output1[[<span class="dv">2</span>]])<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb202-77"><a href="overfitting.html#cb202-77" aria-hidden="true" tabindex="-1"></a>tab[<span class="dv">2</span>,<span class="dv">5</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">colMeans</span>((output2[[<span class="dv">6</span>]] <span class="sc">-</span> output2[[<span class="dv">2</span>]])<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb202-78"><a href="overfitting.html#cb202-78" aria-hidden="true" tabindex="-1"></a>tab[<span class="dv">3</span>,<span class="dv">5</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">colMeans</span>((output3[[<span class="dv">6</span>]] <span class="sc">-</span> output3[[<span class="dv">2</span>]])<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb202-79"><a href="overfitting.html#cb202-79" aria-hidden="true" tabindex="-1"></a>tab[<span class="dv">4</span>,<span class="dv">5</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">colMeans</span>((output4[[<span class="dv">6</span>]] <span class="sc">-</span> output4[[<span class="dv">2</span>]])<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb202-80"><a href="overfitting.html#cb202-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb202-81"><a href="overfitting.html#cb202-81" aria-hidden="true" tabindex="-1"></a><span class="co"># Irreducable error - var(eps) = var(y)</span></span>
<span id="cb202-82"><a href="overfitting.html#cb202-82" aria-hidden="true" tabindex="-1"></a>tab[<span class="dv">1</span>,<span class="dv">4</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">apply</span>(output1[[<span class="dv">6</span>]], <span class="dv">2</span>,  <span class="cf">function</span>(x) <span class="fu">mean</span>((x<span class="sc">-</span><span class="fu">mean</span>(x))<span class="sc">^</span><span class="dv">2</span>)))</span>
<span id="cb202-83"><a href="overfitting.html#cb202-83" aria-hidden="true" tabindex="-1"></a>tab[<span class="dv">2</span>,<span class="dv">4</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">apply</span>(output2[[<span class="dv">6</span>]], <span class="dv">2</span>,  <span class="cf">function</span>(x) <span class="fu">mean</span>((x<span class="sc">-</span><span class="fu">mean</span>(x))<span class="sc">^</span><span class="dv">2</span>)))</span>
<span id="cb202-84"><a href="overfitting.html#cb202-84" aria-hidden="true" tabindex="-1"></a>tab[<span class="dv">3</span>,<span class="dv">4</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">apply</span>(output3[[<span class="dv">6</span>]], <span class="dv">2</span>,  <span class="cf">function</span>(x) <span class="fu">mean</span>((x<span class="sc">-</span><span class="fu">mean</span>(x))<span class="sc">^</span><span class="dv">2</span>)))</span>
<span id="cb202-85"><a href="overfitting.html#cb202-85" aria-hidden="true" tabindex="-1"></a>tab[<span class="dv">4</span>,<span class="dv">4</span>] <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">apply</span>(output4[[<span class="dv">6</span>]], <span class="dv">2</span>,  <span class="cf">function</span>(x) <span class="fu">mean</span>((x<span class="sc">-</span><span class="fu">mean</span>(x))<span class="sc">^</span><span class="dv">2</span>)))</span>
<span id="cb202-86"><a href="overfitting.html#cb202-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb202-87"><a href="overfitting.html#cb202-87" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(tab, <span class="dv">4</span>)</span></code></pre></div>
<pre><code>##      bias^2 var(yhat)     MSE var(eps) In-sample MSPE
## ols1 4.9959    0.9467  5.9427  49.1493        53.2219
## ols2 0.0006    1.4224  1.4230  49.1493        47.7574
## ols3 0.0010    2.9011  2.9021  49.1493        46.2783
## ols4 0.0098   10.2528 10.2626  49.1493        38.9179</code></pre>
<p>The table verifies that <span class="math inline">\(\mathbf{MSE}(\hat{f})=\mathbf{Var}(\hat{f})+\left[\mathbf{bias}(\hat{f})\right]^{2}.\)</span> However, it seems that the MSPE (in-sample) of each model is âwrongâ, which is not the sum of MSE and <span class="math inline">\(\mathbf{Var}(\varepsilon)\)</span>. We will come back to this point, but before going further, to make the simulation calculations more understandable, I put here simple illustrations for each calculation.</p>
<p>Think of a simulation as a big matrix: each row contains one sample and each column contains one observation of <span class="math inline">\(x_i\)</span>. For example, if we have 500 samples and each sample we have 100 observations, the âmatrixâ will be 500 by 100. Below, each section illustrates how the simulations are designed and each term is calculated</p>
<p><strong>Bias - <span class="math inline">\((E(\hat{f}(x))-f(x))^2\)</span></strong></p>
<p><span class="math display">\[
\begin{array}{ccccc}
&amp; x_1 &amp; x_2 &amp; \ldots &amp; x_{100} \\
s_1 &amp; \hat{f}\left(x_1\right) &amp; \hat{f}\left(x_2\right) &amp; \ldots &amp; \hat{f}\left(x_{100}\right) \\
s_2 &amp; \hat{f}\left(x_1\right) &amp; \hat{f}\left(x_2\right) &amp; \ldots &amp; \vdots \\
\vdots &amp; \vdots &amp; \vdots &amp; \ldots &amp; \vdots \\
s_{500} &amp; \hat{f}\left(x_1\right) &amp; \hat{f}\left(x_2\right) &amp; \ldots &amp; \hat{f}\left(x_{100}\right)
\end{array}
\]</span>
<span class="math display">\[
\left[\frac{\Sigma \hat{f}\left(x_1\right)}{500}-f(x)\right]^2+\cdots+\left[\frac{\Sigma \hat{f}\left(x_{100}\right)}{500}-f(x)\right]^2=\sum\left[\frac{\sum \hat{f}\left(x_i\right)}{500}-f(x)\right]^2
\]</span></p>
<p><strong>Variance - <span class="math inline">\(\operatorname{var}[\hat{f}(x)]\)</span></strong></p>
<p><span class="math display">\[
\begin{array}{ccccc}
&amp; x_1 &amp; x_2 &amp; \cdots &amp; x_{100} \\
s_1 &amp; \hat{f}\left(x_1\right) &amp; \hat{f}\left(x_2\right) &amp; \ldots &amp; \hat{f}\left(x_{100}\right) \\
s_2 &amp; \hat{f}\left(x_1\right) &amp; \hat{f}\left(x_2\right) &amp; \ldots &amp; \hat{f}\left(x_{100}\right) \\
\vdots &amp; \vdots &amp; \vdots &amp; \ldots &amp; \vdots \\
s_{500} &amp; \hat{f}\left(x_1\right) &amp; \hat{f}\left(x_2\right) &amp; \ldots &amp; \hat{f}\left(x_{100}\right)
\end{array}
\]</span></p>
<p><span class="math display">\[
\operatorname{var}\left[\hat{f}\left(x_1\right)\right] +\cdots+ \operatorname{var}\left[\hat{f}\left(x_{100}\right)\right]=\frac{\sum\left(\operatorname{var}\left[\hat{f}\left(x_i\right)\right]\right)}{100}
\]</span></p>
<p><strong>MSE - <span class="math inline">\(E\left[f\left(x_i\right)-\hat{f}\left(x_i\right)\right]^2\)</span></strong></p>
<p><span class="math display">\[
\begin{array}{ccccc}
&amp; x_1 &amp; x_2 &amp; \ldots &amp; x_{100} \\
s_1 &amp; {\left[f\left(x_1\right)-\hat{f}\left(x_1\right)\right]^2} &amp; {\left[f\left(x_2\right)-\hat{f}\left(x_2\right)\right]^2} &amp; \ldots &amp; {\left[f\left(x_{100}\right)-\hat{f}\left(x_{100}\right)\right]^2} \\
s_2 &amp; {\left[f\left(x_1\right)-\hat{f}\left(x_1\right)\right]^2} &amp; {\left[f\left(x_2\right)-\hat{f}\left(x_2\right)\right]^2} &amp; \ldots &amp; {\left[f\left(x_{100}\right)-\hat{f}\left(x_{100}\right)\right]^2} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ldots &amp; \vdots \\
s_{500} &amp; {\left[f\left(x_1\right)-\hat{f}\left(x_1\right)\right]^2} &amp; {\left[f\left(x_2\right)-\hat{f}\left(x_2\right)\right]^2} &amp; \cdots &amp; {\left[f\left(x_{100}\right)-\hat{f}\left(x_{100}\right)\right]^2}
\end{array}
\]</span></p>
<p><span class="math display">\[
\frac{\Sigma\left[f\left(x_1\right)-\hat{f}\left(x_1\right)\right]^2}{500}+\cdots+\frac{\Sigma\left[f\left(x_{100}\right)-\hat{f}\left(x_{100}\right)\right]^2}{500}=\sum\left(\frac{\sum\left(f\left(x_1\right)-\hat{f}\left(x_1\right)\right)^2}{500}\right)
\]</span>
<strong>MSPE</strong></p>
<p><span class="math display">\[
\begin{array}{ccccc}
&amp; x_1 &amp; x_2 &amp; \cdots &amp; x_{100} \\
s_1 &amp; \left(y_1-\hat{f}\left(x_1\right)\right)^2 &amp; \left(y_2-\hat{f}\left(x_2\right)\right)^2 &amp; \cdots &amp; \left(y_{100}-\hat{f}\left(x_{100}\right)\right)^2 \\
s_2 &amp; \left(y_1-\hat{f}\left(x_1\right)\right)^2 &amp; \left(y_2-\hat{f}\left(x_2\right)\right)^2 &amp; \cdots &amp; \left(y_{100}-\hat{f}\left(x_{100}\right)\right)^2 \\
\vdots &amp; \vdots &amp; \vdots &amp; \cdots &amp; \vdots \\
s_{500} &amp; \left(y_1-\hat{f}\left(x_1\right)\right)^2 &amp; \left(y_2-\hat{f}\left(x_2\right)\right)^2 &amp; \cdots &amp; \left(y_{100}-\hat{f}\left(x_{100}\right)\right)^2 \end{array}
\]</span></p>
<p><span class="math display">\[
\frac{\sum\left(y_1-\hat{f}\left(x_1\right)\right)^2}{500}+\cdots+\frac{\sum\left(y_{100}-\hat{f}\left(x_{100}\right)\right)^2}{500}=\sum\left(\frac{\sum\left(y_i-\hat{f}\left(x_1\right)\right)^2}{500}\right)
\]</span>
Now, back to our question: Why is the in-sample MSPE different than the sum of MSE and <span class="math inline">\(\sigma^2\)</span>? Letâs look at MSPE again but this time with different angle. We define MSPE over some data points, as we did in our simulation above, and re-write it as follows:</p>
<p><span class="math display">\[
\mathbf{MSPE}_{out}=\mathrm{E}\left[\frac{1}{n} \sum_{i=1}^{n}\left(y&#39;_{i}-\hat{f}(x_i)\right)^{2}\right],~~~~~~\text{where}~~y&#39;_i=f(x_i)+\varepsilon&#39;_i
\]</span></p>
<p>This type of MSPE is also called as <strong>unconditional</strong> MSPE. Inside of the brackets is the âprediction errorâ for a range of out-of-sample data points. The only difference here is that we distinguish <span class="math inline">\(y&#39;_i\)</span> as out-of-sample data points. Likewise, we define MSPE for in-sample data points <span class="math inline">\(y_i\)</span> as</p>
<p><span class="math display">\[
\mathbf{MSPE}_{in}=\mathrm{E}\left[\frac{1}{n} \sum_{i=1}^{n}\left(y_{i}-\hat{f}(x_i)\right)^{2}\right],~~~~~~\text{where}~~y_i=f(x_i)+\varepsilon_i.
\]</span></p>
<p>Note that <span class="math inline">\(\varepsilon&#39;_i\)</span> and <span class="math inline">\(\varepsilon_i\)</span> are independent but identically distributed. Moreover <span class="math inline">\(y&#39;_i\)</span> and <span class="math inline">\(y_i\)</span> has the same distribution. Letâs look at <span class="math inline">\(\mathrm{E}\left[(y&#39;_i-\hat{f}(x_i))^{2}\right]\)</span> closer. By using the definition of variance,</p>
<p><span class="math display">\[
\begin{aligned}
\mathrm{E}\left[(y&#39;_i-\hat{f}(x_i))^{2}\right]
&amp;=\mathrm{Var}\left[y&#39;_{i}-\hat{f}(x_i)\right]+\left(\mathrm{E}\left[y&#39;_{i}-\hat{f}(x_i)\right]\right)^{2}\\
&amp;=\mathrm{Var}\left[y&#39;_{i}\right]+\mathrm{Var}\left[\hat{f}(x_i)\right]-2 \mathrm{Cov}\left[y&#39;_{i}, \hat{f}(x_i)\right]+\left(\mathrm{E}\left[y&#39;_{i}\right]-\mathrm{E}\left[\hat{f}(x_i)\right]\right)^{2}
\end{aligned}
\]</span></p>
<p>Similarly,</p>
<p><span class="math display">\[
\begin{aligned}
\mathrm{E}\left[(y_i-\hat{f}(x_i))^{2}\right]
&amp;=\mathrm{Var}\left[y_{i}-\hat{f}(x_i)\right]+\left(\mathrm{E}\left[y_{i}-\hat{f}(x_i)\right]\right)^{2}\\
&amp;=\mathrm{Var}\left[y_{i}\right]+\mathrm{Var}\left[\hat{f}(x_i)\right]-2 \mathrm{Cov}\left[y_{i}, \hat{f}(x_i)\right]+\left(\mathrm{E}\left[y_{i}\right]-\mathrm{E}\left[\hat{f}(x_i)\right]\right)^{2}
\end{aligned}
\]</span></p>
<p>Remember our earlier derivation of variance-bias decomposition: When we predict out-of-sample data points, we know that <span class="math inline">\(y_0\)</span> and <span class="math inline">\(\hat{f}(x_0)\)</span> are independent. We had stated it differently: <span class="math inline">\(\varepsilon_0\)</span> is independent from <span class="math inline">\(\hat{f}(x_0)\)</span>. In other words, how we estimate our estimator is an independent process from <span class="math inline">\(y&#39;_i\)</span>. Hence, <span class="math inline">\(\mathrm{Cov}\left[y&#39;_{i}, \hat{f}(x_i)\right]=0\)</span>. The critical point here is that <span class="math inline">\(\mathrm{Cov}\left[y_{i} \hat{f}(x_i)\right]\)</span> is <strong>not zero</strong>. This is because the estimator <span class="math inline">\(\hat{f}(x_i)\)</span> is chosen in a way that its difference from <span class="math inline">\(y_i\)</span> should be minimum. Hence, our estimator is not an independent than in-sample <span class="math inline">\(y_i\)</span> data points, on the contrary, we use them to estimate <span class="math inline">\(\hat{f}(x_i)\)</span>. In fact, we can even choose <span class="math inline">\(\hat{f}(x_i) = y_i\)</span> where the MSPE would be zero. In that case correlation between <span class="math inline">\(\hat{f}(x_i)\)</span> and <span class="math inline">\(y_i\)</span> would be 1.</p>
<p>Using the fact that <span class="math inline">\(\mathrm{E}(y&#39;_i) = \mathrm{E}(y_i)\)</span> and <span class="math inline">\(\mathrm{Var}(y&#39;_i) = \mathrm{Var}(y_i)\)</span>, we can now re-write <span class="math inline">\(\mathrm{E}\left[(y&#39;_i-\hat{f}(x_i))^{2}\right]\)</span> as follows:</p>
<p><span class="math display">\[
\mathrm{E}\left[(y&#39;_i-\hat{f}(x_i))^{2}\right]=\mathrm{Var}\left[y_{i}\right]+\mathrm{Var}\left[\hat{f}(x_i)\right]+\left(\mathrm{E}\left[y_{i}\right]-\mathrm{E}\left[\hat{f}(x_i)\right]\right)^{2}\\ =\mathrm{E}\left[(y_i-\hat{f}(x_i))^{2}\right]+2 \mathrm{Cov}\left[y_{i}, \hat{f}(x_i)\right].
\]</span></p>
<p>Averaging over data points,</p>
<p><span class="math display">\[
\mathrm{E}\left[\frac{1}{n} \sum_{i=1}^{n}\left(y_{i}^{\prime}-\hat{f}(x_i)\right)^{2}\right]=\mathrm{E}\left[\frac{1}{n} \sum_{i=1}^{n}\left(y_{i}-\hat{f}(x_i)\right)^{2}\right]+\frac{2}{n} \sum_{i=1}^{n} \mathrm{Cov}\left[y_{i}, \hat{f}(x_i)\right].
\]</span></p>
<p>For a linear model, it can be shown that</p>
<p><span class="math display">\[
\frac{2}{n} \sum_{i=1}^{n} \mathbf{Cov}\left[y_{i}, \hat{f}(x_i)\right]=\frac{2}{n} \sigma^{2}(p+1).
\]</span>
Hence,</p>
<p><span class="math display">\[
\mathbf{MSPE}_{out} =\mathbf{MSPE}_{in}+\frac{2}{n} \sigma^{2}(p+1).
\]</span></p>
<p>The last term quantifies the <strong>overfitting</strong>, the the amount by which the in-sample MSPE systematically underestimates its true MSPE, i.e.Â out-of-sample MSPE. Note also that the overfitting</p>
<ol style="list-style-type: decimal">
<li><strong>grows</strong> with the ânoiseâ (<span class="math inline">\(\sigma^2\)</span>) in the data,</li>
<li><strong>shrinks</strong> with the sample size (<span class="math inline">\(n\)</span>),</li>
<li><strong>grows</strong> with the number of variables (<span class="math inline">\(p\)</span>).</li>
</ol>
<p>Hence, as we had stated earlier, the overfitting problem gets worse as <span class="math inline">\(p/n\)</span> gets bigger. Minimizing the in-sample MSPE completely ignores the overfitting by picking models which are too large and with a very poor out-of-sample prediction accuracy.</p>
<p>Now we can calculate the size of overfitting in our simulation.</p>
<div class="sourceCode" id="cb204"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb204-1"><a href="overfitting.html#cb204-1" aria-hidden="true" tabindex="-1"></a><span class="co"># New Table</span></span>
<span id="cb204-2"><a href="overfitting.html#cb204-2" aria-hidden="true" tabindex="-1"></a>tabb <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">3</span>)</span>
<span id="cb204-3"><a href="overfitting.html#cb204-3" aria-hidden="true" tabindex="-1"></a><span class="fu">row.names</span>(tabb) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;ols1&quot;</span>, <span class="st">&quot;ols2&quot;</span>, <span class="st">&quot;ols3&quot;</span>, <span class="st">&quot;ols4&quot;</span>)</span>
<span id="cb204-4"><a href="overfitting.html#cb204-4" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(tabb) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Cov(yi, yhat)&quot;</span>,<span class="st">&quot;True MSPE&quot;</span>, <span class="st">&quot;TrueMSPE-Cov&quot;</span>)</span>
<span id="cb204-5"><a href="overfitting.html#cb204-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb204-6"><a href="overfitting.html#cb204-6" aria-hidden="true" tabindex="-1"></a><span class="co">#COV</span></span>
<span id="cb204-7"><a href="overfitting.html#cb204-7" aria-hidden="true" tabindex="-1"></a>tabb[<span class="dv">1</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">mean</span>(<span class="fu">diag</span>(<span class="fu">cov</span>(output1[[<span class="dv">2</span>]], output1[[<span class="dv">6</span>]])))</span>
<span id="cb204-8"><a href="overfitting.html#cb204-8" aria-hidden="true" tabindex="-1"></a>tabb[<span class="dv">2</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">mean</span>(<span class="fu">diag</span>(<span class="fu">cov</span>(output2[[<span class="dv">2</span>]], output2[[<span class="dv">6</span>]])))</span>
<span id="cb204-9"><a href="overfitting.html#cb204-9" aria-hidden="true" tabindex="-1"></a>tabb[<span class="dv">3</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">mean</span>(<span class="fu">diag</span>(<span class="fu">cov</span>(output3[[<span class="dv">2</span>]], output3[[<span class="dv">6</span>]])))</span>
<span id="cb204-10"><a href="overfitting.html#cb204-10" aria-hidden="true" tabindex="-1"></a>tabb[<span class="dv">4</span>,<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">mean</span>(<span class="fu">diag</span>(<span class="fu">cov</span>(output4[[<span class="dv">2</span>]], output4[[<span class="dv">6</span>]])))</span>
<span id="cb204-11"><a href="overfitting.html#cb204-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb204-12"><a href="overfitting.html#cb204-12" aria-hidden="true" tabindex="-1"></a><span class="co">#True MSPE</span></span>
<span id="cb204-13"><a href="overfitting.html#cb204-13" aria-hidden="true" tabindex="-1"></a>tabb[<span class="dv">1</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span> tab[<span class="dv">1</span>,<span class="dv">3</span>] <span class="sc">+</span> tab[<span class="dv">1</span>,<span class="dv">4</span>]</span>
<span id="cb204-14"><a href="overfitting.html#cb204-14" aria-hidden="true" tabindex="-1"></a>tabb[<span class="dv">2</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span> tab[<span class="dv">2</span>,<span class="dv">3</span>] <span class="sc">+</span> tab[<span class="dv">2</span>,<span class="dv">4</span>]</span>
<span id="cb204-15"><a href="overfitting.html#cb204-15" aria-hidden="true" tabindex="-1"></a>tabb[<span class="dv">3</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span> tab[<span class="dv">3</span>,<span class="dv">3</span>] <span class="sc">+</span> tab[<span class="dv">3</span>,<span class="dv">4</span>]</span>
<span id="cb204-16"><a href="overfitting.html#cb204-16" aria-hidden="true" tabindex="-1"></a>tabb[<span class="dv">4</span>,<span class="dv">2</span>] <span class="ot">&lt;-</span> tab[<span class="dv">4</span>,<span class="dv">3</span>] <span class="sc">+</span> tab[<span class="dv">4</span>,<span class="dv">4</span>]</span>
<span id="cb204-17"><a href="overfitting.html#cb204-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb204-18"><a href="overfitting.html#cb204-18" aria-hidden="true" tabindex="-1"></a><span class="co">#True MSPE - Cov (to compare with the measures in the earlier table)</span></span>
<span id="cb204-19"><a href="overfitting.html#cb204-19" aria-hidden="true" tabindex="-1"></a>tabb[<span class="dv">1</span>,<span class="dv">3</span>] <span class="ot">&lt;-</span> tabb[<span class="dv">1</span>,<span class="dv">2</span>] <span class="sc">-</span> tabb[<span class="dv">1</span>,<span class="dv">1</span>]</span>
<span id="cb204-20"><a href="overfitting.html#cb204-20" aria-hidden="true" tabindex="-1"></a>tabb[<span class="dv">2</span>,<span class="dv">3</span>] <span class="ot">&lt;-</span> tabb[<span class="dv">2</span>,<span class="dv">2</span>] <span class="sc">-</span> tabb[<span class="dv">2</span>,<span class="dv">1</span>]</span>
<span id="cb204-21"><a href="overfitting.html#cb204-21" aria-hidden="true" tabindex="-1"></a>tabb[<span class="dv">3</span>,<span class="dv">3</span>] <span class="ot">&lt;-</span> tabb[<span class="dv">3</span>,<span class="dv">2</span>] <span class="sc">-</span> tabb[<span class="dv">3</span>,<span class="dv">1</span>]</span>
<span id="cb204-22"><a href="overfitting.html#cb204-22" aria-hidden="true" tabindex="-1"></a>tabb[<span class="dv">4</span>,<span class="dv">3</span>] <span class="ot">&lt;-</span> tabb[<span class="dv">4</span>,<span class="dv">2</span>] <span class="sc">-</span> tabb[<span class="dv">4</span>,<span class="dv">1</span>]</span>
<span id="cb204-23"><a href="overfitting.html#cb204-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb204-24"><a href="overfitting.html#cb204-24" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">cbind</span>(tab, tabb)</span>
<span id="cb204-25"><a href="overfitting.html#cb204-25" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(t, <span class="dv">4</span>)</span></code></pre></div>
<pre><code>##      bias^2 var(yhat)     MSE var(eps) In-sample MSPE Cov(yi, yhat) True MSPE
## ols1 4.9959    0.9467  5.9427  49.1493        53.2219        1.8944   55.0920
## ols2 0.0006    1.4224  1.4230  49.1493        47.7574        2.8463   50.5723
## ols3 0.0010    2.9011  2.9021  49.1493        46.2783        5.8052   52.0514
## ols4 0.0098   10.2528 10.2626  49.1493        38.9179       20.5158   59.4119
##      TrueMSPE-Cov
## ols1      53.1976
## ols2      47.7260
## ols3      46.2462
## ols4      38.8961</code></pre>
<p>Letâs have a pause and look at this table:</p>
<ol style="list-style-type: decimal">
<li>We know that the âtrueâ model is <code>ols2</code> in this simulation. However, we cannot know the true model and we have only one sample in practice.</li>
<li>If we use the in-sample MSPE to choose a model, we pick <code>ols4</code> as it has the minimum MSPE.</li>
<li>Not only <code>ols4</code> is the worst <strong>predictor</strong> among all models, it is also the worst <strong>estimator</strong> among the <strong>unbiased</strong> estimators <code>ols1</code>, <code>ols2</code>, and <code>ols3</code>, as it has the highest MSE.</li>
<li>If our task is to find the best predictor, we cannot use in-sample MSPE, as it give us <code>ols4</code>, as the best predictor.</li>
</ol>
<p>As a side note: when we compare the models in terms their out-sample prediction accuracy, we usually use the root MSPE (RMSPE), which gives us the prediction error in original units.</p>
<p>When we calculate empirical in-sample MSPE with one sample, we can asses its out-of-sample prediction performance by the Mallows <span class="math inline">\(C_P\)</span> statistics, which just substitutes the feasible estimator of <span class="math inline">\(\sigma^2\)</span> into the overfitting penalty. That is, for a linear model with <span class="math inline">\(p + 1\)</span> coefficients fit by OLS,</p>
<p><span class="math display">\[
C_{p}=\frac{1}{n} \sum_{i=1}^{n}\left(y_{i}-\hat{f}(x_i)\right)^{2}+\frac{2 \widehat{\sigma}^{2}}{n}(p+1),
\]</span></p>
<p>which becomes a good proxy for the our-of-sample error. That is, a small value of <span class="math inline">\(C_p\)</span> means that the model is relatively precise. For comparing models, we really care about differences in empirical out-sample MSPEâs:</p>
<p><span class="math display">\[
\Delta C_{p}=\mathbf{MSPE}_{1}-\mathbf{MSPE}_{2}+\frac{2}{n} \widehat{\sigma}^{2}\left(p_{1}-p_{2}\right),
\]</span></p>
<p>where we use <span class="math inline">\(\hat{\sigma}^2\)</span> from the largest model.</p>
<p>How are we going to find the best predictor? In addition to <span class="math inline">\(C_p\)</span>, we can also use <strong>Akaike Information Criterion (AIC)</strong>, which also has the form of âin-sample performance plus penaltyâ. AIC can be applied whenever we have a likelihood function, whereas <span class="math inline">\(C_p\)</span> can be used when we use squared errors. We will see later AIC and BIC (Bayesian Information Criteria) in this book. With these measures, we can indirectly estimate the test (out-of-sample) error by making an adjustment to the training (in-sample) error to account for the bias due to overfitting. Therefore, these methods are <strong>ex-post</strong> tools to <strong>penalize</strong> the overfitting.</p>
<p>On the other hand, we can directly estimate the test error (out-sample) and choose the model that minimizes it. We can do it by directly validating the model using a cross-validation approach. Therefore, cross-validation methods provide <strong>ex-ante</strong> penalization for overfitting and are the main tools in selecting predictive models in machine learning applications as they have almost no assumptions.</p>

</div>



            </section>

          </div>
        </div>
      </div>
<a href="bias-variance-tradeoff.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="parametric-vs.-nonparametric-methods.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/yaydede/toolbox/edit/master/04-Overfitting.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["YA_TextBook.pdf", "YA_TextBook.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
